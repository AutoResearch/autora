{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line when running on Google Colab\n",
    "# !pip install \"autora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The q-learning experiment has to be initialized with a specific formula and effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autora.experiment_runner.synthetic.psychology.q_learning import  q_learning\n",
    "\n",
    "s = q_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the docstring to get information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function q_learning in module autora.experiment_runner.synthetic.psychology.q_learning:\n",
      "\n",
      "q_learning(name='Q-Learning', learning_rate: float = 0.2, decision_noise: float = 3.0, n_actions: int = 2, forget_rate: float = 0.0, perseverance_bias: float = 0.0, correlated_reward: bool = False)\n",
      "    An agent that runs simple Q-learning for an n-armed bandits tasks.\n",
      "    \n",
      "    Args:\n",
      "        name: name of the experiment\n",
      "        trials: number of trials\n",
      "        learning_rate: learning rate for Q-learning\n",
      "        decision_noise: softmax parameter for decision noise\n",
      "        n_actions: number of actions\n",
      "        forget_rate: rate of forgetting\n",
      "        perseverance_bias: bias towards choosing the previously chosen action\n",
      "        correlated_reward: whether rewards are correlated\n",
      "    \n",
      "    Examples:\n",
      "        >>> experiment = q_learning()\n",
      "    \n",
      "        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n",
      "        # always be a list of numpy arrays. Each array corresponds to the choices made by the agent\n",
      "        # for each trial in the input. Thus, arrays have shape (n_trials, n_actions).\n",
      "        >>> experiment.run(np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]),\n",
      "        ...                random_state=42)\n",
      "        [array([[1., 0.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [1., 0.],\n",
      "               [1., 0.]])]\n",
      "    \n",
      "        # The runner can accept pandas DataFrames. Each cell of the DataFrame should contain a\n",
      "        # numpy array with shape (n_trials, n_actions). The return value will be a list of numpy\n",
      "        # arrays, each corresponding to the choices made by the agent for each trial in the input.\n",
      "        >>> experiment.run(\n",
      "        ...     pd.DataFrame(\n",
      "        ...         {'reward array': [np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]])]}),\n",
      "        ...     random_state = 42)\n",
      "        [array([[1., 0.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [1., 0.],\n",
      "               [1., 0.]])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or use the describe function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    An agent that runs simple Q-learning for an n-armed bandits tasks.\n",
      "\n",
      "    Args:\n",
      "        name: name of the experiment\n",
      "        trials: number of trials\n",
      "        learning_rate: learning rate for Q-learning\n",
      "        decision_noise: softmax parameter for decision noise\n",
      "        n_actions: number of actions\n",
      "        forget_rate: rate of forgetting\n",
      "        perseverance_bias: bias towards choosing the previously chosen action\n",
      "        correlated_reward: whether rewards are correlated\n",
      "\n",
      "    Examples:\n",
      "        >>> experiment = q_learning()\n",
      "\n",
      "        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n",
      "        # always be a list of numpy arrays. Each array corresponds to the choices made by the agent\n",
      "        # for each trial in the input. Thus, arrays have shape (n_trials, n_actions).\n",
      "        >>> experiment.run(np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]),\n",
      "        ...                random_state=42)\n",
      "        [array([[1., 0.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [1., 0.],\n",
      "               [1., 0.]])]\n",
      "\n",
      "        # The runner can accept pandas DataFrames. Each cell of the DataFrame should contain a\n",
      "        # numpy array with shape (n_trials, n_actions). The return value will be a list of numpy\n",
      "        # arrays, each corresponding to the choices made by the agent for each trial in the input.\n",
      "        >>> experiment.run(\n",
      "        ...     pd.DataFrame(\n",
      "        ...         {'reward array': [np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]])]}),\n",
      "        ...     random_state = 42)\n",
      "        [array([[1., 0.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [0., 1.],\n",
      "               [1., 0.],\n",
      "               [1., 0.]])]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from autora.experiment_runner.synthetic.utilities import describe\n",
    "\n",
    "print(describe(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic experiement `s` has properties like the name of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Q-Learning'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a valid variables description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "VariableCollection(independent_variables=[IV(name='reward array', value_range=None, allowed_values=None, units='reward', type=<ValueType.BOOLEAN: 'boolean'>, variable_label='Reward Sequence', rescale=1, is_covariate=False)], dependent_variables=[DV(name='choice array', value_range=None, allowed_values=None, units='actions', type=<ValueType.REAL: 'real'>, variable_label='Action Sequence', rescale=1, is_covariate=False)], covariates=[])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the conditions for this experiment are reward sequences. This is a variable type not yet fully integrated in AutoRA. Therefore ther is no domain yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = s.domain()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "... the plotter is not implemented yet:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplotter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/src/autora/experiment_runner/synthetic/psychology/q_learning.py:257\u001B[0m, in \u001B[0;36mq_learning.<locals>.plotter\u001B[0;34m()\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplotter\u001B[39m():\n\u001B[0;32m--> 257\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "s.plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap this functions to use with the state logic of AutoRA:\n",
    "First, we create the state with the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.state import StandardState, on_state, Delta, experiment_runner_on_state, estimator_on_state\n",
    "# We can get the variables from the runner\n",
    "variables = s.variables\n",
    "\n",
    "# With the variables, we initialize a StandardState\n",
    "state = StandardState(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a special experimentalist that can generate random trial sequences and wrap it with the `on_state` function to use them on state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install autora-experimentalist-bandit-random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        reward array\n",
      "0  [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...\n",
      "1  [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...\n"
     ]
    }
   ],
   "source": [
    "from autora.experimentalist.bandit_random import bandit_random_pool\n",
    "# Wrap the functions to use on state\n",
    "# Experimentalists:\n",
    "\n",
    "@on_state()\n",
    "def pool_on_state(num_samples):\n",
    "      return Delta(conditions=bandit_random_pool(num_rewards=2, sequence_length=20, num_samples=num_samples))\n",
    "\n",
    "\n",
    "state = pool_on_state(state, num_samples=2)\n",
    "print(state.conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the runner with the `experiment_runner_on_state` wrapper to use it on state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                        reward array  \\\n0  [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...   \n1  [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...   \n\n                                        choice array  \n0  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0...  \n1  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reward array</th>\n      <th>choice array</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...</td>\n      <td>[[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...</td>\n      <td>[[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runner:\n",
    "run_on_state = experiment_runner_on_state(s.run)\n",
    "state = run_on_state(state)\n",
    "\n",
    "state.experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the regressor with the `estimator_on_state` wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorist = LinearRegression()\n",
    "theorist_on_state = estimator_on_state(theorist)\n",
    "\n",
    "state = theorist_on_state(state)\n",
    "# Access the last model:\n",
    "model = state.models[-1]\n",
    "\n",
    "\n",
    "print(f\"choose_A1 = \"\n",
    "      f\"{model.coef_[0][0]:.2f}*similarity_category_A1 \"\n",
    "      f\"{model.coef_[0][1]:.2f}*similarity_category_A2 \"\n",
    "      f\"{model.coef_[0][2]:.2f}*similarity_category_B1 \"\n",
    "      f\"{model.coef_[0][3]:.2f}*similarity_category_B2 \"\n",
    "      f\"{model.intercept_[0]:+.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 --- Loss: 0.5586882; Time: 0.0762s; Convergence value: 1.78e-01\n",
      "Epoch 87/100 --- Loss: 0.7901477; Time: 0.0767s; Convergence value: 1.82e-01\n",
      "Epoch 88/100 --- Loss: 0.5265486; Time: 0.0751s; Convergence value: 1.92e-01\n",
      "Epoch 89/100 --- Loss: 0.4401408; Time: 0.0743s; Convergence value: 1.86e-01\n",
      "Epoch 90/100 --- Loss: 0.3039415; Time: 0.0756s; Convergence value: 1.82e-01\n",
      "Epoch 91/100 --- Loss: 0.3906522; Time: 0.0771s; Convergence value: 1.73e-01\n",
      "Epoch 92/100 --- Loss: 0.5437022; Time: 0.0769s; Convergence value: 1.65e-01\n",
      "Epoch 93/100 --- Loss: 0.4635772; Time: 0.0737s; Convergence value: 1.54e-01\n",
      "Epoch 94/100 --- Loss: 0.4845441; Time: 0.0743s; Convergence value: 1.48e-01\n",
      "Epoch 95/100 --- Loss: 0.2648371; Time: 0.0770s; Convergence value: 1.56e-01\n",
      "Epoch 96/100 --- Loss: 0.3382604; Time: 0.0748s; Convergence value: 1.37e-01\n",
      "Epoch 97/100 --- Loss: 0.2581106; Time: 0.0742s; Convergence value: 1.25e-01\n",
      "Epoch 98/100 --- Loss: 0.6365235; Time: 0.0737s; Convergence value: 1.47e-01\n",
      "Epoch 99/100 --- Loss: 0.2228255; Time: 0.0741s; Convergence value: 1.67e-01\n",
      "Epoch 100/100 --- Loss: 0.3986339; Time: 0.0745s; Convergence value: 1.73e-01\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "Test the trained RNN on a test dataset...\n",
      "Epoch 1/1 --- Loss: 0.4823526; Time: 0.0079s; Convergence value: nan\n",
      "Maximum number of training epochs reached.\n",
      "Model did not converge yet.\n",
      "RNN training took 7.47 seconds.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory trained_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m     y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(experiment_data[dv]\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Delta(models\u001B[38;5;241m=\u001B[39m[theorist_a\u001B[38;5;241m.\u001B[39mfit(x, y)])\n\u001B[0;32m---> 12\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[43mtheorist_on_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Access the last model:\u001B[39;00m\n\u001B[1;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m state\u001B[38;5;241m.\u001B[39mmodels[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/state.py:939\u001B[0m, in \u001B[0;36mdelta_to_state.<locals>._f\u001B[0;34m(state_, **kwargs)\u001B[0m\n\u001B[1;32m    937\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    938\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_f\u001B[39m(state_: S, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m S:\n\u001B[0;32m--> 939\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    940\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(delta, Mapping), (\n\u001B[1;32m    941\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m must be a `Delta`, `UserDict`, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `dict`.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m f\n\u001B[1;32m    942\u001B[0m     )\n\u001B[1;32m    943\u001B[0m     new_state \u001B[38;5;241m=\u001B[39m state_ \u001B[38;5;241m+\u001B[39m delta\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/state.py:675\u001B[0m, in \u001B[0;36minputs_from_state.<locals>._f\u001B[0;34m(state_, **kwargs)\u001B[0m\n\u001B[1;32m    673\u001B[0m     arguments_from_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m state_\n\u001B[1;32m    674\u001B[0m arguments \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(arguments_from_state, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 675\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marguments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "Cell \u001B[0;32mIn[18], line 10\u001B[0m, in \u001B[0;36mtheorist_on_state\u001B[0;34m(experiment_data)\u001B[0m\n\u001B[1;32m      8\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(experiment_data[iv]\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[1;32m      9\u001B[0m y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(experiment_data[dv]\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Delta(models\u001B[38;5;241m=\u001B[39m[\u001B[43mtheorist_a\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m])\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/theorist/rnn_sindy_rl/__init__.py:159\u001B[0m, in \u001B[0;36mRNNSindy.fit\u001B[0;34m(self, conditions, observations, epochs, **kwargs)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epochs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    157\u001B[0m     epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs\n\u001B[0;32m--> 159\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrnn \u001B[38;5;241m=\u001B[39m \u001B[43mrnn_main\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconditions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobservations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msindy \u001B[38;5;241m=\u001B[39m sindy_main(\n\u001B[1;32m    168\u001B[0m     conditions,\n\u001B[1;32m    169\u001B[0m     observations,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    181\u001B[0m )\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/theorist/rnn_sindy_rl/rnn_main.py:103\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(xs, ys, model, epochs, n_steps_per_call, batch_size, learning_rate, convergence_threshold, analysis, save_name, checkpoint, **kwargs)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m# save trained parameters  \u001B[39;00m\n\u001B[1;32m     99\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    100\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model\u001B[38;5;241m.\u001B[39mstate_dict() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule) \u001B[38;5;28;01melse\u001B[39;00m [model_i\u001B[38;5;241m.\u001B[39mstate_dict() \u001B[38;5;28;01mfor\u001B[39;00m model_i \u001B[38;5;129;01min\u001B[39;00m model],\n\u001B[1;32m    101\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m: optimizer_rnn\u001B[38;5;241m.\u001B[39mstate_dict() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(optimizer_rnn, torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam) \u001B[38;5;28;01melse\u001B[39;00m [optim_i\u001B[38;5;241m.\u001B[39mstate_dict() \u001B[38;5;28;01mfor\u001B[39;00m optim_i \u001B[38;5;129;01min\u001B[39;00m optimizer_rnn],\n\u001B[1;32m    102\u001B[0m }\n\u001B[0;32m--> 103\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSaved RNN parameters to file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# Analysis\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/torch/serialization.py:651\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    648\u001B[0m _check_save_filelike(f)\n\u001B[1;32m    650\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m--> 651\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_zipfile_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m    652\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001B[1;32m    653\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/torch/serialization.py:525\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[0;34m(name_or_buffer)\u001B[0m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    524\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[0;32m--> 525\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcontainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/torch/serialization.py:496\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_stream))\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 496\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPyTorchFileWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Parent directory trained_models does not exist."
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

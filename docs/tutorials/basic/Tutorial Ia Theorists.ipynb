{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tutorial Ib: Theorists \n",
    "\n",
    "[Theorists](https://autoresearch.github.io/autora/theorist/) are classes designed to automate the construction of interpretable models from data. AutoRA theorists are implemented as sklearn regressors and can be used with the `fit` and `predict` methods.\n",
    "\n",
    "<img src=\"https://autoresearch.github.io/autora/img/theorist.png\" width=\"75%\" alt=\"Theorist Overview\">\n",
    "\n",
    "In order to use a theorist, you must first install the corresponding theorist package. Some theorists are installed by default when you install ``autora``. Once a theorist is installed, you can instantiate it and use it as you would any other sklearn regressor. That is, you can call the ``fit`` function of the theorist by passing in experimental conditions and corresponding observations, and then call the ``predict`` function to generate predicted observations for novel experimental conditions using the discovered model.\n",
    "\n",
    "The following tutorial demonstrates how to use the `BMSRegressor` (Guimerà et al., 2020, in Sci. Adv.)–a theorist that can discover an interpretable equation relating the independent variables of an experiment (experiment conditions) to predicted dependent variables (observations). \n",
    "\n",
    "We will compare the performance of the `BMSRegressor` with two other methods: a polynomial regressor and a neural network regressor. The polynomial regressor is a simple model that can only fit polynomial functions, while the neural network regressor is a more complex model that can fit a wider range of functions. The `BMSRegressor` is a hybrid model that can fit a wide range of functions while also providing an interpretable, potentially non-linear equation.\n",
    "\n",
    "Note: this tutorial requires Python 3.10 to run successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Relevant Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install relevant subpackages from AutoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix default prior and release new package version\n",
    "!pip install -q \"autora[theorist-bms]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant modules from AutoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.theorist.bms import BMSRegressor\n",
    "from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Plotting Results\n",
    "\n",
    "Before we begin, we also define some functions to plot the results of our models. Simply execute the following code block to define the plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_results(model, x_test, y_test, arg='default', model_name=None, variable_names=None, select_indices=None, figsize=None, *args):\n",
    "  compare_results(models=[model], x_test=x_test, y_test=y_test, arg=arg, model_names=[model_name], variable_names=variable_names, select_indices=select_indices, figsize=figsize, *args)\n",
    "  \n",
    "def compare_results(models, x_test, y_test, arg='default', model_names=None, variable_names=None, observation_name=None, select_indices=None, figsize=None, *args):\n",
    "  if model_names is None or model_names == [None]:\n",
    "    names = ['Model '+str(i+1) for i in range(len(models))]\n",
    "  else:\n",
    "    names = model_names\n",
    "  if len(x_test.shape) == 1:\n",
    "    x_test = x_test.reshape(1, -1)\n",
    "  num_var = x_test.shape[1]\n",
    "  if variable_names is None:\n",
    "    var_names = ['Variable '+str(i+1) for i in range(num_var)]\n",
    "  else:\n",
    "    var_names = variable_names\n",
    "  if observation_name is None:\n",
    "    obs_label = 'Observations'\n",
    "  else:\n",
    "    obs_label = observation_name\n",
    "  match arg:\n",
    "    case 'default':\n",
    "      for i, model in enumerate(models):\n",
    "        print(model)\n",
    "        synthetic_runner.plotter(model)\n",
    "    case '2d':\n",
    "      if figsize is None:\n",
    "        size = (8,3)\n",
    "      else:\n",
    "        assert len(figsize) == 2 and isinstance(figsize, tuple), 'incorrect format for figure shape\\nshould be tuple of form (i,j)'\n",
    "        size = figsize\n",
    "      for i, model in enumerate(models):\n",
    "        fig = plt.figure(figsize=size)\n",
    "        axes = []\n",
    "        y_predict = model.predict(x_test)\n",
    "        for j in range(num_var):\n",
    "          axes.append(fig.add_subplot(1, num_var, j+1))\n",
    "          axes[j].set_xlabel(var_names[j])\n",
    "          axes[j].set_ylabel(obs_label)\n",
    "          axes[j].set_title(names[i]+' fit on '+var_names[j])\n",
    "          axes[j].scatter(x_test[:,j], y_test, label='Ground Truth', alpha=0.5)\n",
    "          axes[j].scatter(x_test[:,j], y_predict, label='Predicted', alpha=0.5)\n",
    "          axes[j].legend()\n",
    "          for arg in args:\n",
    "            assert isinstance(arg, str), 'arguments must be in the form of a string'\n",
    "            try:\n",
    "              exec('axes[j].'+arg)\n",
    "            except:\n",
    "              raise RuntimeError(f'argument \"{arg}\" could not be executed')\n",
    "\n",
    "      fig.tight_layout()\n",
    "      plt.show()\n",
    "    case '3d':\n",
    "      if figsize is None:\n",
    "        size = (15,5)\n",
    "      else:\n",
    "        assert len(figsize) == 2 and isinstance(figsize, tuple), 'incorrect format for figure shape\\nshould be tuple of form (i,j)'\n",
    "        size = figsize\n",
    "      axes = []\n",
    "      fig = plt.figure(figsize=size)\n",
    "      if select_indices is None:\n",
    "        idx = (0,1)\n",
    "      else:\n",
    "        len(select_indices) == 2 and isinstance(select_indices, tuple), 'incorrect format for select_indices\\nshould be tuple of form (i,j)'\n",
    "        idx = select_indices\n",
    "      for i, model in enumerate(models):\n",
    "        y_predict = model.predict(x_test)\n",
    "        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n",
    "        ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "        ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "        ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "        axes.append(ax)\n",
    "        axes[i].set_xlabel(var_names[idx[0]])\n",
    "        axes[i].set_ylabel(var_names[idx[1]])\n",
    "        axes[i].set_zlabel(obs_label)\n",
    "        axes[i].scatter(x_test[:, idx[0]], x_test[:, idx[1]], y_test, s=1, label='Ground Truth')\n",
    "        axes[i].scatter(x_test[:, idx[0]], x_test[:, idx[1]], y_predict, s=1, label='Predicted')\n",
    "        axes[i].set_title(names[i])\n",
    "        axes[i].legend()\n",
    "        axes[i].set_facecolor('white')\n",
    "        for arg in args:\n",
    "            assert isinstance(arg, str), 'arguments must be in the form of a string'\n",
    "            try:\n",
    "              exec('axes[j].'+arg)\n",
    "            except:\n",
    "              raise RuntimeError(f'argument \"{arg}\" could not be executed')\n",
    "      fig.tight_layout()\n",
    "      plt.show()\n",
    "    case 'choice':\n",
    "      for model in models:\n",
    "        y_pred = np.where(model.predict(x_test) > 0.5, 1, 0)\n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "        cmd = ConfusionMatrixDisplay(cm)\n",
    "        cmd.plot()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Study Object: The Weber-Fechner Law\n",
    "\n",
    "We will evaluate our models to recover Weber-Fechner law. The Weber-Fechner law quantifies the minimum change in a stimulus required to be noticeable. Similar to Steven's power law, the greater the intensity of a stimulus, the larger the change needed to be perceivable. This relationship is hypothesized to be proportional to the logarithm of the ratio between the two stimuli:\n",
    "\n",
    "$y = c \\log\\left(\\dfrac{x_1}{x_2}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data From a Synthetic Psychophysics Experiment\n",
    "\n",
    "Here, we leverage a synthetic experiment to generate data from this equation. It is parameterized by the constant $c$. The independent variables are $x_1$ and $x_2$, corresponding to the intensity of a stimulus and the baseline stimulus intensity, respectively. The dependent variable is $y$ perceived stimulus intensity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = 3.0\n",
    "\n",
    "# synthetic experiment from autora inventory\n",
    "synthetic_runner = weber_fechner_law(constant=constant)\n",
    "\n",
    "# experiment meta data:\n",
    "synthetic_runner.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the variables in the synthetic experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent variables\n",
    "ivs = [iv.name for iv in synthetic_runner.variables.independent_variables]\n",
    "\n",
    "# dependent variable\n",
    "dvs = [dv.name for dv in synthetic_runner.variables.dependent_variables]\n",
    "\n",
    "ivs, dvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the experimental data by running the synthetic experiment. The ``conditions`` contain values for the independent variables. Once we have the conditions, we can run the experiment to obtain the ``experiment_data`` containing both the conditions and the observations from the synthetic experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental data\n",
    "conditions = synthetic_runner.domain()\n",
    "experiment_data = synthetic_runner.run(conditions, added_noise=0.01)\n",
    "\n",
    "# observations\n",
    "observations = experiment_data[dvs]\n",
    "\n",
    "experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test datasets\n",
    "conditions_train, conditions_test, observations_train, observations_test = train_test_split(conditions, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Models to the Data\n",
    "\n",
    "In this section, we will fit the data with three techniques:\n",
    "- Polynomial Regressor\n",
    "- Neural Network Regressor\n",
    "- Bayesian Machine Scientist\n",
    "\n",
    "The last technique is an equation discovery algorithm implemented in the AutoRA framework.  \n",
    " \n",
    "We will repeat the following steps for each method: \n",
    "1. Initialize Model\n",
    "2. Fit Model to the Data\n",
    "3. Plot the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regressor\n",
    "\n",
    "Expressivity: **Low**\n",
    "\n",
    "Interpretability: **High**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial regressor has a high interpretability, but it is limited in expressivity. It can only fit polynomial functions.\n",
    "\n",
    "We first initialize the polynomial regressor. The polynomial regressor fits a polynomial function to the data. Below, we set the degree of the polynomial to 3.\n",
    "\n",
    "Note that the PolynomialRegressor class is a simple implementation of a polynomial regressor using sklearn's PolynomialFeatures and LinearRegression classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "class PolynomialRegressor:\n",
    "    \"\"\"\n",
    "    This theorist fits a polynomial function to the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree: int = 3):\n",
    "      self.poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "      self.model = LinearRegression()\n",
    "\n",
    "    def fit(self, x, y):\n",
    "      features = self.poly.fit_transform(x, y)\n",
    "      self.model.fit(features, y)\n",
    "      return self\n",
    "\n",
    "    def predict(self, x):\n",
    "      features = self.poly.fit_transform(x)\n",
    "      return self.model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "poly_model = PolynomialRegressor(degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit it to the training data, consisting of the experimental conditions and corresponding observations of our synthetic experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model.fit(conditions_train, observations_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the results of the polynomial regressor to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_results(model=poly_model, x_test=conditions_test, y_test=observations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_results(model=poly_model, x_test=conditions_test, y_test=observations_test, arg='2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_results(model=poly_model, x_test=conditions_test, y_test=observations_test, arg='3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we can see that the polynomial regressor is a simple model. It has high interpretability which allows us to quantify the law underlying the data in terms of a polynomial. However, it has some trouble fitting the data generated from the logarithmic psychophysics law due to its limited expressivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Expressivity: **High**\n",
    "\n",
    "Interpretability: **Low**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are known for their high expressivity, allowing to fit any function. However, they are often considered black-box models due to their complex structure, limiting interpretability for the user. \n",
    " \n",
    "For this section, we are using torch: an open-source machine learning library. It provides a flexible and dynamic computational graph, allowing for complex neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize a simple multi-layer perceptron (MLP) regressor using the `MLPRegressor` class from the `sklearn.neural_network` module. We will train it for a maximum of 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import torch\n",
    "nn_model = MLPRegressor(random_state=1, max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the polynomial regressor above, we can fit the neural network regressor to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "nn_model.fit(conditions_train, observations_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the results of the neural network regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_runner.plotter(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may observe that the neural network regressor does a better job of fitting the data but it is less interpretable than the polynomial regressor. The neural network is a more complex model that can fit a wider range of functions, but it is also a black-box model that does not provide an interpretable equation, limiting its utility for scientific discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Machine Scientist\n",
    "\n",
    "Expressivity: **Medium**\n",
    "\n",
    "Interpretability: **High**\n",
    "\n",
    "The Bayesian Machine Scientist (BMS) is one of the theorists that comes with the autora package. It is an equation discovery method that can fit a wide range of functions while providing an interpretable equation. It uses MCMC-Sampling to explore the space of possible equations and find the best-fitting equation for the data while minimizing the number of parameters in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the BMS regressor to run 1500 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bms_model = BMSRegressor(epochs=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fit the model to the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bms_model.fit(conditions_train, observations_train, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_runner.plotter(bms_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can print the discovered equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bms_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that BMS is not only producing good fits but is also capable of recovering the underlying equation of the synthetic data, the Weber-Fechner law.\n",
    "\n",
    "In summary, the BMS regressor is a powerful tool for fitting a wide range of functions while providing an interpretable equation. It strikes a balance between expressivity and interpretability, making it a valuable tool for scientific discovery. In our case, it is capable of re-discovering the Weber-Fechner law from the synthetic data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Models\n",
    "\n",
    "Finally, we can compare all three fitted models in terms of their fit to the hold-out data using the `compare_results` function. This function plots the ground truth and predicted values for each model, allowing us to visually compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [poly_model, nn_model, bms_model]\n",
    "names =['poly_model', 'nn_model', 'bms_model']\n",
    "compare_results(models=models, x_test=conditions_test, y_test=observations_test, model_names=names, arg='2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the models in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results(models=models, x_test=conditions_test, y_test=observations_test, model_names=names, arg='3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we compared the performance of three different theorists: a polynomial regressor, a neural network regressor, and the Bayesian Machine Scientist (BMS) regressor. The polynomial regressor is a simple model that can only fit polynomial functions, while the neural network regressor is a more complex model that can fit a wider range of functions. The BMS regressor is a hybrid model that can fit a wide range of functions while also providing an interpretable, potentially non-linear equation.\n",
    "\n",
    "AutoRA provides interfaces for using theorists as sklearn regressors. This allows you to easily fit and evaluate theorists using the `fit` and `predict` methods. Note that such theorists may not be limited to fitting functions but may also discover complex computational models or algorithms describing the data. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

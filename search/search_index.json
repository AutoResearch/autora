{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Automated Research Assistant","text":"<p>AutoRA (Automated Research Assistant) is an open-source framework for  automating multiple stages of the empirical research process, including model discovery, experimental design, data collection, and documentation for open science.</p> <p></p> <p>AutoRA implements the Autonomous Empirical Research Paradigm, which involves a dynamic interplay between two artificial agents. The first agent, a theorist, is primarily responsible for constructing  computational models by relying on existing data to link experimental conditions to dependent measures.  The second agent, an experimentalist, is tasked with designing follow-up experiments that can refine and validate the models generated by the theorist. Together, these agents implement an automated scientific discovery process. To enable closed-loop empirical research, AutoRA  interfaces with platforms for automated data collection, such as Prolific or Amazon Mechanical Turk,  which enable the efficient acquisition of behavioral data from human participants. Finally, AutoRA  is designed to support the automated documentation and dissemination of steps in the empirical research process.</p> <p>AutoRA was initially intended for accelerating research in the behavioral and brain sciences.  However, AutoRA is designed as a general framework that enables automation of research processes in other empirical sciences, such as materials science or physics.</p>"},{"location":"#features","title":"Features","text":"<p>AutoRA consists of different modules that can be used independently or in combination, such as:</p> <ul> <li>Theorists that support the discovery of formal scientific models from data</li> <li>Experimentalists that support the design of follow-up experiments</li> <li>Experiment runners that support data collection from experimentation platforms (e.g., Prolific or Amazon Mechanical Turk)</li> <li>Workflow logic for defining interactions between different components of the research process</li> <li>Interfaces for automated documentation of the research process</li> </ul>"},{"location":"#uses","title":"Uses","text":"<p>AutoRA can be used for a variety of research purposes in empirical sciences, such as psychology,  neuroscience, economics, physics, or materials science. Usages, as illustrated in the following tutorials, include:</p> <ul> <li>Equation discovery from empirical data<ul> <li>Example: Theorist Tutorial.</li> </ul> </li> <li>Experimental design for follow-up experiments<ul> <li>Example: Experimentalist Tutorial.</li> </ul> </li> <li>Closed-loop empirical research<ul> <li>Example: Closed-Loop Psychophysics Study Tutorial.</li> </ul> </li> <li>Research documentation and dissemination</li> <li>Computational analyses of the scientific process (metascience, computational philosophy of science)</li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>The pace of empirical research is constrained by the rate at which scientists can alternate between the development of formal theories and the execution of experiments. However, attempts to increase this rate often compromise scientific rigor, leading to deficiencies such as the absence of formal modeling, non-replicable findings, and insufficient documentation. In order to surmount these limitations, we aim to develop AutoRA\u2013\u2013an open-source framework that automates the generation, estimation, and empirical testing of scientific models. By automating steps of the empirical research process, we hope AutoRA can accelerate scientific discovery while promoting greater transparency and rigor in empirical research.</p>"},{"location":"#how-to-get-started","title":"How to Get Started?","text":"<p>We recommend that you have a look at the Basic Tutorials to get started with AutoRA. These tutorials provide a step-by-step guide on how to set up a basic AutoRA workflow, including the creation of a theorist, experimentalist, and experiment runner.</p> <p>Once you are familiar with the basics, you can explore the Use Case Tutorials to see how AutoRA can be used for different real-world research purposes. We recommend the Closed-Loop Psychophysics Study as a starting point for understanding how AutoRA can be used for closed-loop behavioral research.</p>"},{"location":"#pointers","title":"Pointers","text":"<ul> <li>AutoRA Pip Package</li> <li>GitHub Repository</li> <li>Autonomous Empirical Research Group</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you would like to reference AutoRA, you can use the following BibTeX entry for the associated publication in JOSS:</p> <pre><code>@article{Musslick_AutoRA_Automated_Research_2024,\nauthor = {Musslick, Sebastian and Andrew, Benjamin and Williams, Chad C. and Hewson, Joshua T. S. and Li, Sida and Marinescu, Ioana and Dubova, Marina and Dang, George T. and Strittmatter, Younes and Holland, John G.},\ndoi = {10.21105/joss.06839},\njournal = {Journal of Open Source Software},\nmonth = dec,\nnumber = {104},\npages = {6839},\ntitle = {{AutoRA: Automated Research Assistant for Closed-Loop Empirical Research}},\nurl = {https://joss.theoj.org/papers/10.21105/joss.06839},\nvolume = {9},\nyear = {2024}\n}\n</code></pre>"},{"location":"#about","title":"About","text":"<p>This project is in active development by the Autonomous Empirical Research Group.</p> <p>The development of this package was supported by Schmidt Science Fellows, in partnership with the Rhodes Trust, as well as the Carney BRAINSTORM program at Brown University. The development of auxiliary packages for AutoRA, such as <code>autodoc</code>, is supported by Schmidt Sciences, LLC. and the Virtual Institute for Scientific Software (VISS). The AutoRA package was developed using computational resources and services at the Center for Computation and Visualization at Brown University.</p>"},{"location":"cheat-sheet/","title":"Cheat Sheet","text":"<p>This cheat sheet provides code snippets for key concepts and functionalities of AutoRA. You may use this as a resource for developing AutoRA workflows.</p>"},{"location":"cheat-sheet/#installation","title":"Installation","text":"<p>Installation Guide</p>"},{"location":"cheat-sheet/#installing-the-main-package","title":"Installing the main package:","text":"<pre><code>pip install \"autora\"\n</code></pre>"},{"location":"cheat-sheet/#installing-optional-packages","title":"Installing Optional Packages","text":"<p>e.g., for <code>autora-theorist-bms</code> package: <pre><code>pip install -U \"autora[theorist-bms]\"\n</code></pre></p>"},{"location":"cheat-sheet/#autora-variables","title":"AutoRA Variables","text":"<p>Variables Guide</p>"},{"location":"cheat-sheet/#defining-variables","title":"Defining Variables","text":"<pre><code>from autora.variable import VariableCollection, Variable\nimport numpy as np\n\nvariables = VariableCollection(\n    independent_variables=[\n        Variable(name=\"intensity\", allowed_values[1, 2, 3, 4, 5]),\n        Variable(name=\"duration\", allowed_values=np.linspace(1, 100, 100))\n    ],\n    dependent_variables=[Variable(name=\"accuracy\", value_range=(0, 1))]\n)\n</code></pre>"},{"location":"cheat-sheet/#extracting-variable-names","title":"Extracting Variable Names","text":"<pre><code>ivs = [iv.name for iv in variables.independent_variables]\ndvs = [dv.name for dv in variables.dependent_variables]\n</code></pre>"},{"location":"cheat-sheet/#autora-components","title":"AutoRA Components","text":"<p>Components Guide</p>"},{"location":"cheat-sheet/#theorists","title":"Theorists","text":"<p>Theorist Overview</p>"},{"location":"cheat-sheet/#fit-predict","title":"Fit &amp; Predict","text":"<pre><code>from autora.theorist.bms import BMSRegressor\n\n# declare theorist\ntheorist = BMSRegressor(epochs=100)\n\n# fit theorist to data\nmodel = theorist.fit(conditions, observations)\n\n# predict new observations\nobservations = theorist.predict(conditions)\n</code></pre>"},{"location":"cheat-sheet/#write-custom-theorist","title":"Write Custom Theorist","text":"<p>Custom Theorist Guide</p> <pre><code>from sklearn.base import BaseEstimator\n\nclass LogisticRegressor(BaseEstimator):\n    def __init__(self, *args, **kwargs):\n        self.model = MyTheoristMethod(*args, **kwargs)  \n\n    def fit(self, conditions, observations):\n        self.model.fit(conditions, observations)\n        return self\n\n    def predict(self, conditions):\n        return self.model.predict(observations) \n</code></pre> <p><code>conditions</code> should be a pandas DataFrame with columns corresponding to the independent variables.</p> <p><code>observations</code> should be a pandas DataFrame with columns corresponding to the dependent variables.</p>"},{"location":"cheat-sheet/#experimentalists","title":"Experimentalists","text":"<p>Experimentalist Overview</p>"},{"location":"cheat-sheet/#generate-conditions","title":"Generate Conditions","text":"<pre><code>from autora.experimentalist.random import random_pool\n\nconditions = random_pool(variables, num_samples=10)\n</code></pre>"},{"location":"cheat-sheet/#write-custom-experimentalist","title":"Write Custom Experimentalist","text":"<p>Custom Experimentalist Guide</p> <pre><code>def my_experimentalist(allowed_conditions, num_samples):\n    # ...\n    return selected_conditions\n</code></pre> <p><code>conditions</code> should be a pandas DataFrame with columns corresponding to the independent variables.</p>"},{"location":"cheat-sheet/#experiment-runners","title":"Experiment Runners","text":""},{"location":"cheat-sheet/#run-experiment","title":"Run Experiment","text":"<pre><code>experiment_runner = weber_fechner_law()\nexperiment_data = experiment_runner.run(conditions)\n</code></pre> <p><code>conditions</code> should be a pandas DataFrame with columns corresponding to the independent variables.</p> <p><code>experiment_data</code> should be a pandas DataFrame with columns corresponding to the dependent variables.</p>"},{"location":"cheat-sheet/#using-synthetic-experiment-runners","title":"Using Synthetic Experiment Runners","text":"<p>Equation Runner Example: <pre><code>from autora.experiment_runner.synthetic.abstract.equation import equation_experiment\nfrom sympy import symbols\nimport numpy as np\n\nx, y = symbols(\"x y\")\nexpr = x ** 2 - y ** 2\n\nexperiment = equation_experiment(expr)\n\ntest_input = np.array([[1, 1], [2 ,2], [2 ,3]])\n\nexperiment.experiment_runner(test_input)\n</code></pre></p> <p>Weber-Fechner Example:</p> <pre><code># synthetic experiment from autora inventory\nfrom autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\n\nsynthetic_runner = weber_fechner_law(constant=3)\n\nvariables = synthetic_runner.variables\nconditions = synthetic_runner.domain()\nexperiment_data = synthetic_runner.run(conditions, added_noise=0.01)\n</code></pre>"},{"location":"cheat-sheet/#using-behavioral-experiment-runners","title":"Using Behavioral Experiment Runners","text":"<p>Example Study Guide</p>"},{"location":"cheat-sheet/#initializing-and-running-firebase-runner","title":"Initializing and Running Firebase Runner","text":"<pre><code>firebase_credentials = {\n  \"type\": \"service_account\",\n  \"project_id\": \"closed-loop-study\",\n  \"private_key_id\": \"YOURKEYID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nYOURCREDENTIALS\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"firebase-adminsdk-y7hnh@closed-loop-study.iam.gserviceaccount.com\",\n  \"client_id\": \"YOURLIENTID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-y7hnh%40closed-loop-study.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n\nexperiment_runner = firebase_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=5,\n    sleep_time=3)\n\ndata_raw = experiment_runner(conditions_to_send)\n</code></pre>"},{"location":"cheat-sheet/#initializing-and-running-prolific-runner","title":"Initializing and Running Prolific Runner","text":"<pre><code>sleep_time = 30\nstudy_name = 'my autora experiment'\nstudy_description= 'Psychophysics Study'\nstudy_url = 'https://closed-loop-study.web.app/'\nstudy_completion_time = 5\nprolific_token = 'my prolific token'\ncompletion_code = 'my completion code'\n\nexperiment_runner = firebase_prolific_runner(\n            firebase_credentials=firebase_credentials,\n            sleep_time=sleep_time,\n            study_name=study_name,\n            study_description=study_description,\n            study_url=study_url,\n            study_completion_time=study_completion_time,\n            prolific_token=prolific_token,\n            completion_code=completion_code,\n        )\n</code></pre>"},{"location":"cheat-sheet/#state","title":"State","text":"<p>State Guide</p>"},{"location":"cheat-sheet/#defining-standard-state","title":"Defining Standard State","text":"<pre><code>from autora.state import StandardState\nstate = StandardState(\n    variables=variables,\n)\n</code></pre>"},{"location":"cheat-sheet/#defining-custom-state","title":"Defining Custom State","text":"<pre><code>from autora.state import StandardState\nfrom dataclasses import dataclass, field\n\n@dataclass(frozen=True)\nclass MyCustomState(StandardState):\n    additional_field:  int = field(\n        default_factory=list,\n        metadata={\"delta\": \"extend\"},\n    )\n\n# initialize the state:\nstate = MyCustomState(variables=variables)\n</code></pre>"},{"location":"cheat-sheet/#retrieving-data-from-state","title":"Retrieving Data From State","text":""},{"location":"cheat-sheet/#conditions","title":"Conditions","text":"<pre><code>conditions = state.conditions\n</code></pre> <p>or </p> <pre><code>ivs = [iv.name for iv in variables.independent_variables]\nconditions = state.experiment_data[ivs]\n</code></pre>"},{"location":"cheat-sheet/#experiment-data","title":"Experiment Data","text":"<pre><code>experiment_data = state.experiment_data\n</code></pre>"},{"location":"cheat-sheet/#observations","title":"Observations","text":"<pre><code>experiment_data = state.experiment_data\n\ndvs = [dv.name for dv in variables.dependent_variables]\nobservations = experiment_data[dvs]\n</code></pre>"},{"location":"cheat-sheet/#models","title":"Models","text":"<pre><code>last_model = state.model[-1]\n</code></pre>"},{"location":"cheat-sheet/#defining-state-wrappers","title":"Defining State Wrappers","text":""},{"location":"cheat-sheet/#theorist-wrapper","title":"Theorist Wrapper","text":"<pre><code>from autora.state import on_state, Delta\n\n@on_state()\ndef theorist_on_state(experiment_data, variables):\n    ivs = [iv.name for iv in variables.independent_variables]\n    dvs = [dv.name for dv in variables.dependent_variables]\n    x = experiment_data[ivs]\n    y = experiment_data[dvs]\n    return Delta(models=[my_theorist.fit(x, y)])\n</code></pre>"},{"location":"cheat-sheet/#experimentalist-wrapper","title":"Experimentalist Wrapper","text":"<pre><code>from autora.state import on_state, Delta\n\n@on_state()\ndef experimentalist_on_state(allowed_conditions, num_samples):\n    return Delta(conditions=my_experimentalist(allowed_conditions, num_samples))\n</code></pre>"},{"location":"cheat-sheet/#experiment-runner-wrapper","title":"Experiment Runner Wrapper","text":"<pre><code>from autora.state import on_state, Delta\n\non_state()\ndef experiment_runner_on_state(conditions, added_noise):\n    return Delta(experiment_data=my_experiment_runner(conditions, added_noise))\n</code></pre>"},{"location":"cheat-sheet/#calling-state-wrappers","title":"Calling State Wrappers","text":"<pre><code>state = runner_on_state(state)\n</code></pre> <p>Warning</p> <p>When adding your own input arguments to the wrapper, be sure to call the wrapper with all arguments specified in the function signature, e.g., for  <pre><code>@on_state()\ndef experimentalist_on_state(conditions, num_samples):\n    return Delta(conditions=my_experimentalist(allowed_conditions, num_samples))\n</code></pre> <code>conditions</code> is a variable retrieved from the state and <code>num_samples</code> is a custom argument. Thus, you must call the wrapper with  <pre><code>experimentalist_on_state(state, num_samples=num_samples)\n</code></pre> Note that <code>experimentalist_on_state(state, num_samples)</code> will throw an error.</p>"},{"location":"cheat-sheet/#running-a-basic-workflow","title":"Running a Basic Workflow","text":"<p>Workflow Guide</p>"},{"location":"cheat-sheet/#without-state-wrappers","title":"Without State Wrappers","text":"<pre><code>conditions = initial_experimentalist(state.variables)\n\nfor cycle in range(num_cycles):\n    observations = experiment_runner(conditions, added_noise=1.0)\n    model = theorist(conditions, observations)\n    conditions = experimentalist(model, conditions, observations, num_samples=10)\n</code></pre>"},{"location":"cheat-sheet/#with-state-wrappers","title":"With State Wrappers","text":"<pre><code>state = initial_experimentalist_on_state(state)\n\nfor cycle in range(num_cycles):\n    state = experiment_runner_on_state(state, num_samples=10)\n    state = theorist_on_state(state)\n    state = experimentalist_on_state(state, model=state.model[-1])\n</code></pre>"},{"location":"installation/","title":"Installation Guide","text":"<p>To install and use AutoRA you need:</p> <ul> <li><code>Python</code> (version \"&gt;=3.8,&lt;4\") and</li> <li>the <code>autora</code> package, including required dependencies specified in the <code>pyproject.toml</code> file.</li> </ul>"},{"location":"installation/#step-1-install-python","title":"Step 1: Install <code>python</code>","text":"<p>You can install <code>python</code>:</p> <ul> <li>Using the instructions at python.org, or</li> <li>Using a package manager, e.g.   homebrew,    pyenv,   asdf,    rtx,   winget.</li> </ul> <p>If successful, you should be able to run python in your terminal emulator like this: <pre><code>python\n</code></pre></p> <p>...and see some output like this: <pre><code>Python 3.11.3 (main, Apr  7 2023, 20:13:31) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n</code></pre></p>"},{"location":"installation/#step-2-install-autora","title":"Step 2: Install <code>autora</code>","text":"<p>We recommended using a <code>Python</code> environment manager like <code>virtualenv</code>. You may refer to the Development Guide on how to set up a virtual environment.  </p> <p>Before installing the PyPI <code>autora</code> package, you may activate your environment. To install the PyPI <code>autora</code> package, run the following command:</p> <pre><code>pip install \"autora\"\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.variable import VariableCollection\"\n</code></pre></p> <p>In using AutoRA, it is helpful to be aware of its structure, which is described next.</p>"},{"location":"installation/#step-3-install-optional-dependencies","title":"Step 3: Install Optional Dependencies","text":"<p><code>autora</code> is organized into one \"parent\" and many \"child\" packages.</p> <p></p> <p>The <code>autora</code> \"parent\" package depends on core packages, such as <code>autora-core</code> and <code>autora-synthetic</code>, which are automatically installed with <code>autora</code>. </p> <p>However, the <code>autora</code> ecosystem also includes vetted modules (child packages) as optional dependencies which users can choose to install. Below, we provide a list of all optional dependencies that are currently vetted by the <code>autora</code> core team.</p> <p>To install any (combination) of optional dependencies, users should run the relevant analogue of the following command, with the name in brackets matching the name as specified in the parent <code>pyproject.toml</code> file:</p> <pre><code>pip install -U \"autora[desired-dependency]\"\n</code></pre> <p>For example, to install one of the Theorists, such as the Bayesian Machine Scientist (BMS) from the package <code>autora-theorist-bms</code>, a user should run:</p> <pre><code>pip install -U \"autora[theorist-bms]\"\n</code></pre> <p>To check that installation was successful, a user can try importing one of the main classes of the corresponding child package. For BMS, such a check would be: <pre><code>python -c \"from autora.theorist.bms import BMSRegressor; BMSRegressor()\"\n</code></pre></p>"},{"location":"installation/#list-of-vetted-dependencies","title":"List of Vetted Dependencies","text":"<p>For a complete list of dependencies that have been vetted by the core <code>autora</code> team, see the <code>[project.optional-dependencies]</code> section of the <code>pyproject.toml</code> file in the parent <code>autora</code> package. For your convenience, we provide a list of vetted packages along with their description below. </p> <p>Required dependencies are included in the installation of the <code>autora</code> package. Optional dependencies must be separately installed as outlined above.</p> Dependency Name Dependency Type Feature Type Brief Description Documentation theorist-bms Optional Theorist Equation Discovery Method Using the Bayesian Machine Scientist (Guimer\u00e0 et al. 2020) \ud83d\udd17 theorist-bsr Optional Theorist Equation Discovery Method Using the Bayesian Symbolic Regression (Jin et al. 2020) \ud83d\udd17 theorist-darts Optional Theorist Equation Discovery Method using Differentiable Architecture Search (Liu et al., 2018; Musslick et al., 2020 ) \ud83d\udd17 experimentalist-falsification Optional Experimentalist Identifies experimental conditions method that maximize predicted model loss (Musslick et al., 2023) \ud83d\udd17 experimentalist-inequality Required Experimentalist Identifies experimental conditions from a candidate pool based on their inequality to a reference pool \ud83d\udd17 experimentalist-mixture Required Experimentalist Identifies experimental conditions based on a weighted score of multiple experimentalists. \ud83d\udd17 experimentalist-model-disagreement Required Experimentalist Identifies experimental conditions for which a list of models make different predictions \ud83d\udd17 experimentalist-nearest-value Required Experimentalist Identifies experimental conditions from a candidate pool that are nearest to experimental conditions in a reference pool. \ud83d\udd17 experimentalist-novelty Required Experimentalist Identifies experimental conditions from a candidate pool that are most novel with respect to a reference pool. \ud83d\udd17 experimentalist-uncertainty Optional Experimentalist Identifies experimental conditions based on uncertainty sampling. \ud83d\udd17 autora-experiment-runner-firebase-prolific Optional Experiment Runner This experiment runner provides functionality for running behavioral experiments hosted on Firebase using human participants from Prolific. \ud83d\udd17 experiment-runner-experimentation-manager-firebase Optional Experiment Runner (Experimentation Manager) This manager provides functionality to manage communication of conditions and observation between AutoRA and a behavioral experiment hosted on Firebase. \ud83d\udd17 autora-experiment-runner-recruitment-manager-prolific Optional Experiment Runner (Recruitment Manager) This manager provides functionality for recruiting human participants from Prolific. \ud83d\udd17"},{"location":"installation/#list-of-bundled-dependencies","title":"List of Bundled Dependencies","text":"<p><code>autora</code> also provides the ability to install some of these optional dependencies in a bundle. These bundles can also be found in the <code>[project.optional-dependencies]</code> section of the <code>pyproject.toml</code> file in the parent <code>autora</code> package.</p> <p>You can install these bundles just like any other package. For example, to install the <code>all-theorists</code> bundle:</p> <pre><code>pip install -U \"autora[all-theorists]\"\n</code></pre> <p>For your convenience, we provide a list of vetted bundled packages and what components they include below.</p> Bundle Name Components Included all The theorist, experimentalist, and experiment runner bundles all-theorists theorist-bms, theorist-bsr, theorist-darts all-experimentalists experimentalist-falsification, experimentalist-inequality, experimentalist-mixture, experimentalist-model-disagreement, experimentalist-nearest-value, experimentalist-novelty, experimentalist-uncertainty all-experiment_runners autora-experiment-runner-firebase-prolific all-experiment-runner-experimentation-managers experiment-runner-experimentation-manager-firebase all-experiment-runner-recruitment-managers autora-experiment-runner-recruitment-manager-prolific"},{"location":"terminology/","title":"AutoRA Terminology","text":"<p>The following table includes naming conventions used throughout AutoRA.</p> Term Description Relevant Modules State Object representing data from an experiment, like the conditions, observed experiment data and models. The State also includes rules on how to update those data if new data are provided using the \"Delta mechanism\". Core Delta An object with new data that can be added to a State, returning a new State which includes the new data. Core StandardState An optional default State that has the following fields: <code>variables</code>, <code>conditions</code>, <code>experiment_data</code>, <code>models</code>. Core Variables A State field (<code>state.variables</code>) that holds experimental variables, which are defined according to name, type, units, allowed values, and range. Experimentalists, Experiment Runners, Theorists Independent Variable An experimental variable that the researcher intends to manipulate or change to observe its effect. A parameteriation of independent variables is called a condition. Experimentalists, Experiment Runners, Theorists Dependent Variable An experimental variable that that is measured or observed in response to changes in the independent variable. Values of independent variables are referred to as observations and are part of the experimental data. Experimentalists, Experiment Runners, Theorists VariableCollection Immutable metadata about dependent variables, independent variables, and covariates. Experimentalists, Experiment Runners, Theorists Conditions A State field (<code>state.conditions</code>) that defines what observations should be collected according to a specific combination of values of the independent variables Experimentalists, Experiment Runners, Theorists Experiment Data A State field (<code>state.experiment_data</code>) that holds specified conditions as well as the corresponding observations. Experiment Runners, Theorists Model A State field (<code>state.models</code>) that holds the the collection of best fit models produced by theorists. Theorists, Experimentalists Components The functions that can act on the State (e.g., experimentalists, experiment runners, theorists). Experimentalists, Experiment Runners, Theorists Experimentalist A component that outputs new conditions, which are intended to yield novel observations. Experimentalists Theorist A component that takes in the full collection of conditions and observations and outputs models that link the two. Theorists Experiment Runner A component that takes in conditions and collects corresponding observations. Experiment Runners Wrapper Special functions that make the components of AutoRA able to operate on State objects. Experimentalists, Experiment Runners, Theorists Workflow A collection of tools that enable closed-loop empirical research with the AutoRA framework. Experimentalists, Experiment Runners, Theorists"},{"location":"abstract-equation/docs/","title":"autora-synthetic-abstract-equation","text":"<p>Initializes an autora experiment from a sympy equation (https://www.sympy.org/en/index.html)</p>"},{"location":"abstract-equation/docs/Basic%20Usage/","title":"Basic Equation","text":"<p>First install the package via pip:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install \"autora-synthetic-abstract-equation\"\n</pre> !pip install \"autora-synthetic-abstract-equation\" <p>Then import the <code>sympy_experiment</code> function to your project</p> In\u00a0[2]: Copied! <pre>from autora.experiment_runner.synthetic.abstract.equation import equation_experiment\n</pre> from autora.experiment_runner.synthetic.abstract.equation import equation_experiment <p>The package comes with sympy as dependency, so we can import functionality from sympy as well. Let's create a simple expression with two variables: <code>x**2 - y**2</code></p> In\u00a0[8]: Copied! <pre># First, import sympy\nfrom sympy import symbols\n\n# Declare our variables\nx, y = symbols(\"x y\")\n\n# Declare the expression\nexpr = x ** 2 - y ** 2\n</pre> # First, import sympy from sympy import symbols  # Declare our variables x, y = symbols(\"x y\")  # Declare the expression expr = x ** 2 - y ** 2 <p>We can use the expression to create an autora experiment:</p> In\u00a0[9]: Copied! <pre>experiment = equation_experiment(expr)\n</pre> experiment = equation_experiment(expr) <p>Let's try some values for the experiment. We can use numpy arrays or Pandas Dataframes as inputs. Pandas Dataframes have the advantage that we can name the columns according to the variable declaration to not confuse the order. If we use numpy arrays, the order is interpreted alphabetically</p> In\u00a0[10]: Copied! <pre># First, import numpy\nimport numpy as np\n\n# Declare a test input:\ntest_input = np.array([[1, 1], [2 ,2], [2 ,3]])\n\n# Run the experiment with this input:\nexperiment.experiment_runner(test_input)\n</pre> # First, import numpy import numpy as np  # Declare a test input: test_input = np.array([[1, 1], [2 ,2], [2 ,3]])  # Run the experiment with this input: experiment.experiment_runner(test_input) <pre>/usr/local/lib/python3.10/dist-packages/autora/experiment_runner/synthetic/abstract/sympy_equation/__init__.py:148: RuntimeWarning: Unnamed data is used. Arguments will be sorted alphabetically. Consider using a Pandas DataFrame with named columns for better clarity and ease of use.\n  warnings.warn(\n</pre> Out[10]: x y observations 0 1.000305 0.998960 0.002688 1 2.000750 2.000941 -0.000761 2 1.998049 2.998698 -4.999989 <p>Note, that there is some noise added to the input and that using numpy arrays will result in a warning. Let's rewrite the expression to showcase, why there is a warning and also turn off the noise</p> In\u00a0[11]: Copied! <pre># Rearange the expression\nexpr = y ** 2 - x ** 2\n\n# Reinitialize the experiment without noise\nexperiment = equation_experiment(expr, added_noise=0)\n\n# Run the experiment with the same input:\nexperiment.experiment_runner(test_input)\n</pre> # Rearange the expression expr = y ** 2 - x ** 2  # Reinitialize the experiment without noise experiment = equation_experiment(expr, added_noise=0)  # Run the experiment with the same input: experiment.experiment_runner(test_input) <pre>/usr/local/lib/python3.10/dist-packages/autora/experiment_runner/synthetic/abstract/sympy_equation/__init__.py:148: RuntimeWarning: Unnamed data is used. Arguments will be sorted alphabetically. Consider using a Pandas DataFrame with named columns for better clarity and ease of use.\n  warnings.warn(\n</pre> Out[11]: x y observations 0 1.0 1.0 0.0 1 2.0 2.0 0.0 2 2.0 3.0 5.0 <p>Allthough we reordered the expression, the input arguments still are interpreted in alphabetical order. To be save, we can also use Pandas Dataframe instead of arrays:</p> In\u00a0[13]: Copied! <pre># Import pandas\nimport pandas as pd\n\ntest_input_pandas = pd.DataFrame({'x': [1, 2, 2], 'y': [1, 2, 3]})\n\nexperiment.experiment_runner(test_input_pandas)\n</pre> # Import pandas import pandas as pd  test_input_pandas = pd.DataFrame({'x': [1, 2, 2], 'y': [1, 2, 3]})  experiment.experiment_runner(test_input_pandas) Out[13]: x y observations 0 1.0 1.0 0.0 1 2.0 2.0 0.0 2 2.0 3.0 5.0 <p>To use addidational functionality, we can also add a domain for the independent variables to our experiment. To do this, we need to describe the indpendet variables:</p> In\u00a0[15]: Copied! <pre># Import from autora\nfrom autora.variable import IV\n\n# Describe the variables\nvariable_x = IV(name=\"x\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10))\nvariable_y = IV(name=\"y\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10))\n\n# Reinitialize the experiment:\nexperiment = equation_experiment(expr, [variable_x, variable_y])\n</pre> # Import from autora from autora.variable import IV  # Describe the variables variable_x = IV(name=\"x\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10)) variable_y = IV(name=\"y\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10))  # Reinitialize the experiment: experiment = equation_experiment(expr, [variable_x, variable_y]) <p>Now we can plot the ground truth</p> In\u00a0[16]: Copied! <pre>experiment.plotter()\n</pre> experiment.plotter() <pre>/usr/local/lib/python3.10/dist-packages/autora/experiment_runner/synthetic/abstract/sympy_equation/__init__.py:148: RuntimeWarning: Unnamed data is used. Arguments will be sorted alphabetically. Consider using a Pandas DataFrame with named columns for better clarity and ease of use.\n  warnings.warn(\n</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre>"},{"location":"abstract-equation/docs/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"abstract-equation/docs/Equation%20Sampler/","title":"Equation Sampling","text":"<p>First install the packages via pip:</p> In\u00a0[1]: Copied! <pre>!pip install \"autora-synthetic-abstract-equation\"\n!pip install \"equation-sampler\"\n</pre> !pip install \"autora-synthetic-abstract-equation\" !pip install \"equation-sampler\" <pre>Collecting autora-synthetic-abstract-sympy-equation\n  Downloading autora_synthetic_abstract_sympy_equation-0.0.1-py3-none-any.whl (7.4 kB)\nCollecting autora-core (from autora-synthetic-abstract-sympy-equation)\n  Downloading autora_core-3.3.0-py3-none-any.whl (28 kB)\nCollecting autora-synthetic (from autora-synthetic-abstract-sympy-equation)\n  Downloading autora_synthetic-1.0.3-py3-none-any.whl (13 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from autora-synthetic-abstract-sympy-equation) (1.2.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from autora-synthetic-abstract-sympy-equation) (1.11.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autora-core-&gt;autora-synthetic-abstract-sympy-equation) (1.22.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autora-core-&gt;autora-synthetic-abstract-sympy-equation) (3.7.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from autora-core-&gt;autora-synthetic-abstract-sympy-equation) (1.5.3)\nRequirement already satisfied: scipy&gt;=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;autora-synthetic-abstract-sympy-equation) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;autora-synthetic-abstract-sympy-equation) (1.3.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;autora-synthetic-abstract-sympy-equation) (3.2.0)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;autora-synthetic-abstract-sympy-equation) (1.3.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (1.1.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (4.41.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (23.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (3.1.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (2022.7.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;autora-core-&gt;autora-synthetic-abstract-sympy-equation) (1.16.0)\nInstalling collected packages: autora-core, autora-synthetic, autora-synthetic-abstract-sympy-equation\nSuccessfully installed autora-core-3.3.0 autora-synthetic-1.0.3 autora-synthetic-abstract-sympy-equation-0.0.1\nCollecting equation-sampler\n  Downloading equation_sampler-0.0.4-py3-none-any.whl (11 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from equation-sampler) (1.22.4)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from equation-sampler) (1.11.1)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;equation-sampler) (1.3.0)\nInstalling collected packages: equation-sampler\nSuccessfully installed equation-sampler-0.0.4\n</pre> <p>Then import the <code>sample_equations, to_sympy</code> function to your project</p> In\u00a0[2]: Copied! <pre>from equation_sampler import sample_equations, to_sympy\n</pre> from equation_sampler import sample_equations, to_sympy <p>Let's generate a sample equation using the Equation Sampler library. Please note that the Equation Sampler offers various settings for sampling, including priors. In this example, we will focus on using basic functionality. For more detailed information and options, you can refer to the Equation Sampler's documentation.</p> In\u00a0[35]: Copied! <pre># First, we declare the operator and function space\n\nfunctions = [ \"sin\", \"cos\",\"tan\",\"exp\",\"log\",\"sqrt\",\"abs\"]\n\noperators = [\"+\",\"-\",\"*\",\"/\",\"^\"]\n\n# We also use a seed to get reproducable results:\nimport numpy as np\nnp.random.seed(42)\n\n# then we can sample an equation:\nequations = sample_equations(num_samples=4, max_depth=4, max_num_variables=2,\n    max_num_constants=1,\n    function_space=functions,\n    operation_space=operators, verbose=False)\n\n# Use a helper function to transform the equations from the sample_equation into sympy equations\nsympy_equations_and_values = to_sympy(equations, functions, operators)\n</pre>  # First, we declare the operator and function space  functions = [ \"sin\", \"cos\",\"tan\",\"exp\",\"log\",\"sqrt\",\"abs\"]  operators = [\"+\",\"-\",\"*\",\"/\",\"^\"]  # We also use a seed to get reproducable results: import numpy as np np.random.seed(42)  # then we can sample an equation: equations = sample_equations(num_samples=4, max_depth=4, max_num_variables=2,     max_num_constants=1,     function_space=functions,     operation_space=operators, verbose=False)  # Use a helper function to transform the equations from the sample_equation into sympy equations sympy_equations_and_values = to_sympy(equations, functions, operators) <pre>0 equations generated\nall equations generated\n</pre> <p>Let's look at the equations:</p> In\u00a0[\u00a0]: Copied! <pre>sympy_equations_and_values\n</pre> sympy_equations_and_values <p>There are added values, but we are only interested in the equations themselves:</p> In\u00a0[36]: Copied! <pre>sympy_equations = sympy_equations_and_values[0]\nsympy_equations\n</pre> sympy_equations = sympy_equations_and_values[0] sympy_equations Out[36]: <pre>[Abs(x_2/x_1), tan(exp(sin(x_1))), c_1*cos(x_1), x_1*x_2]</pre> <p>And then we can use the expression to create an autora experiment</p> In\u00a0[38]: Copied! <pre>from autora.experiment_runner.synthetic.abstract.equation import equation_experiment\n\n# Let's pick the first one\nexpr = sympy_equations[0]\n\nexperiment = equation_experiment(expr)\n</pre> from autora.experiment_runner.synthetic.abstract.equation import equation_experiment  # Let's pick the first one expr = sympy_equations[0]  experiment = equation_experiment(expr) <p>Let's try some values for the experiment.</p> In\u00a0[39]: Copied! <pre># First, import numpy\nimport numpy as np\n\n# Declare a test input:\ntest_input = np.array([[1, 2],[3, 4],[5, 6],[7, 8],[9, 10]])\n\n# Run the experiment with this input:\nexperiment.experiment_runner(test_input)\n</pre> # First, import numpy import numpy as np  # Declare a test input: test_input = np.array([[1, 2],[3, 4],[5, 6],[7, 8],[9, 10]])  # Run the experiment with this input: experiment.experiment_runner(test_input) Out[39]: x_1 x_2 observations 0 1.000305 1.998960 1.998351 1 3.000750 4.000941 1.333313 2 4.998049 5.998698 1.200208 3 7.000128 7.999684 1.142791 4 8.999983 9.999147 1.111018 <p>To use the plotting we have to define a domain.</p> In\u00a0[42]: Copied! <pre># Import from autora\nfrom autora.variable import IV\n\n# Describe the variables\nvariable_x_1 = IV(name=\"x_1\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10))\nvariable_x_2 = IV(name=\"x_2\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10))\n\n# Reinitialize the experiment:\nexperiment = equation_experiment(expr, [variable_x_1, variable_x_2])\n</pre> # Import from autora from autora.variable import IV  # Describe the variables variable_x_1 = IV(name=\"x_1\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10)) variable_x_2 = IV(name=\"x_2\", allowed_values=np.linspace(-10, 10, 100), value_range=(-10, 10))  # Reinitialize the experiment: experiment = equation_experiment(expr, [variable_x_1, variable_x_2]) <p>Now we can plot the ground truth</p> In\u00a0[43]: Copied! <pre>experiment.plotter()\n</pre> experiment.plotter() <pre>/usr/local/lib/python3.10/dist-packages/autora/experiment_runner/synthetic/abstract/sympy_equation/__init__.py:148: RuntimeWarning: Unnamed data is used. Arguments will be sorted alphabetically. Consider using a Pandas DataFrame with named columns for better clarity and ease of use.\n  warnings.warn(\n</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre>"},{"location":"abstract-equation/docs/Equation%20Sampler/#equation-samples","title":"Equation Samples\u00b6","text":"<p>Here, we use an equation sampler to sample random equations and then use them as AutoRA experiment runners. This can be used in a closed loop to benchmark or train novel AutoRA Experimentalists or AutoRA Theorists.</p>"},{"location":"abstract-equation/docs/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*autora-synthetic-abstract-sympy-equation is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experiment-runner-synthetic-autora-synthetic-abstract-equation\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experiment_runner.synthetic.autora_synthetic_abstract_equation import equation_experiment\"\n</code></pre></p>"},{"location":"contribute/","title":"Contributor Guide","text":"<p>The AutoRA project is a collection of packages which together form a framework for closed-loop empirical research. We invite contributions to all parts of the project, including the \"core\" packages, and the modules. Below is an overview of the project structure, along with brief mention of the style guide we follow as well as pointers to more detailed contribution guides for each part of the project.</p> <p>Hint</p> <p>If you run into any issues or have any questions regarding a contribution to AutoRA, please reach out to us on the AutoRA forum. We look forward to hearing from you!</p>"},{"location":"contribute/#project-structure","title":"Project Structure","text":"<p>Contributions to AutoRA are organized into one \"parent\" and many \"child\" packages.  Child packages are generally maintained by individual contributors. The parent package, along with some other  core packages, is maintained by the Autonomous Empirical Research Group,  as well as external contributors.</p> <p></p> <p><code>autora</code> is the parent package which end users are expected to install. The parent depends on core packages, such as <code>autora-core</code> and <code>autora-synthetic</code>. It also includes vetted modules (child packages) as optional dependencies which users can choose  to install. </p> <p>You may contribute to any of the core packages or develop your own module as a stand-alone child package (see below).    </p>"},{"location":"contribute/#style-guide","title":"Style Guide","text":""},{"location":"contribute/#code-style","title":"Code Style","text":"<p>In general, AutoRA follows the PEP 8 \u2013 Style Guide for Python Code. We particularly encourage the following conventions:</p> <ul> <li>Snake case for variables and modules: <code>example_name</code>, <code>example_module.py</code></li> <li>Camel case for class names: <code>ExampleClass</code></li> <li>Camel case with spaces for Jupyter notebooks: <code>Example Notebook.ipynb</code></li> <li>Write docstrings for all public modules, functions, classes, methods, and at the top of each file.</li> </ul>"},{"location":"contribute/#documentation-style","title":"Documentation Style","text":"<p>For documentation, AutoRA adheres to the maxim, \"Everything should be made as simple as possible, but no simpler.\" That is, we strive to make documentation clear and comprehensive, but as concise and digestible as possible. We also encourage formatting and hyperlinking that facilitate understanding and make navigation of the docs intuitive. Finally, we encourage a strong form of title case for headings \u2014 that is, for all titles and subtitles, the first letter of each word should be capitalized, such as in the following example: This Is An Example Title</p>"},{"location":"contribute/#module-contributions","title":"Module Contributions","text":"<p>Modules include theorists, experimentalists, experiment runners, or other functionalities not covered by the core packages.  All modules are child packages and can become optional dependencies of the <code>autora</code> parent package. Modules packages are  owned and maintained by you, the contributor, which provides several advantages:</p> <ul> <li>Easy setup: We provide simple templates for modules, which you can use to get started quickly</li> <li>Independence: You can develop and maintain your package independently of other child packages (and thereby avoid dependency conflicts)</li> <li>Ownership: You can publish your package on PyPI or Conda, use it in other projects, and get credit for its use. </li> </ul> <p>For details on how to submit child packages  for inclusion in <code>autora</code>, see the module contributor guide. Feel free to post questions and feedback regarding module contributions on the  AutoRA forum.</p>"},{"location":"contribute/#core-contributions","title":"Core Contributions","text":"<p>The following packages are considered core packages, and are actively maintained by the Autonomous Empirical Research Group:</p> <ul> <li> <p>autora-core <code>https://github.com/autoresearch/autora-core</code> This package includes fundamental utilities and building blocks for all the other packages. This includes basic utilities for managing the workflow of closed-loop research processes, e.g., coordinating workflows between the theorists, experimentalists, and experiment runners. The <code>autora-core</code> package is always installed when a user installs <code>autora</code> and can be  a dependency of other child packages.</p> </li> <li> <p>autora-synthetic <code>https://github.com/autoresearch/autora-synthetic</code>: This package includes a number of ground-truth models from different scientific disciplines that can be used for benchmarking automated scientific discovery. If you seek to contribute a scientific model, please see the core contributor guide for details.   </p> </li> </ul> <p>We welcome contributions to these packages in the form of pull requests, bug reports, and feature requests. For more details, see the core contributor guide. Feel free to ask any questions or provide any feedback regarding core contributions in the  AutoRA forum.</p> <p>For core contributions, including contributions to <code>autora-synthetic</code>, it is possible to set up your python environment in many different ways.  One setup which works for us is described in the setup guide.</p> <p>Hint</p> <p>If you would like to become actively involved in the development and maintenance of core AutoRA packages,  we welcome you to join the Autonomous Empirical Research Group.</p>"},{"location":"contribute/Notebook/","title":"Notebooks","text":"<p>You can use Jupyter Notebooks as part of the documentation for your contribution.</p> In\u00a0[\u00a0]: Copied!"},{"location":"contribute/Notebook/#notebooks","title":"Notebooks\u00b6","text":""},{"location":"contribute/Notebook/#including-images","title":"Including Images\u00b6","text":"<p>When using images, ensure they are \"attached\" to cells, rather than being imported from a path. The image should appear like this in the markdown cell:</p> <pre>![Description](attachment:filename.png)\n</pre> <p>To attach an image, you can open the notebook in the Jupyter notebook browser.</p> <p>For example, to open the <code>docs/contribute/Notebook.ipynb</code> file, you can run:</p> <pre>jupyter notebook \"docs/contribute/Notebook.ipynb\"\n</pre> <p>Then start editing the markdown cell, and at the position where you want the image, open the menu Edit &gt; Insert Image:</p> <p></p>"},{"location":"contribute/core/","title":"Contributions To Core Packages","text":"<p>Core contributions are changes to AutoRA which aren't experimentalists, (synthetic) experiment runners, or theorists.  The primary purpose of the core is to provide utilities for:</p> <ul> <li>describing experiments and handling workflows (in the <code>autora-core</code> package</li> <li>run synthetic experiments (currently in the <code>autora-synthetic</code> package. Synthetic experiment runners may be submitted as pull requests to the      <code>autora-synthetic</code> package, providing they      require no additional dependencies. However, if your contribution requires additional dependencies, you can submit it as a full package following      the module contribution guide.</li> </ul>"},{"location":"contribute/core/#how-to-contribute","title":"How to Contribute","text":"<p>In order to make a contribution to autora core, you may start by cloning the repository: <pre><code>git clone https://github.com/AutoResearch/autora-core.git\n</code></pre></p> <p>You should then follow the Development Setup Guide, to set up your environment. </p> <p>Suggested changes to the core should be submitted as follows, depending on their content:</p> <ul> <li>For fixes or new features closely associated with existing core functionality: pull request to the existing    core package</li> <li>For new features which don't fit into the current module structure, or which are experimental and could lead to    instability for users: as new namespace packages.</li> </ul> <p>Success</p> <p>Reach out to the core team about new core contributions to discuss how best to incorporate them by posting your  idea on the discussions page.</p>"},{"location":"contribute/core/#development-requirements","title":"Development Requirements","text":"<p>Core packages should as a minimum:</p> <ul> <li>Follow standard python coding guidelines including PEP8</li> <li>Run under all minor versions of python (e.g. 3.8, 3.9) allowed in    <code>autora-core</code></li> <li>Be compatible with all current AutoRA packages</li> <li>Have comprehensive test suites</li> <li>Use the linters and checkers defined in the <code>autora-core</code> .pre-commit-config.yaml (see pre-commit hooks)</li> </ul>"},{"location":"contribute/pre-commit-hooks/","title":"Pre-Commit Hooks","text":"<p>We use <code>pre-commit</code> to manage pre-commit hooks. </p> <p>Pre-commit hooks are programs which run before each git commit, and can read and potentially modify the files which are to be committed. </p> <p>We use pre-commit hooks to: - enforce coding guidelines, including the <code>python</code> style-guide PEP8 (<code>black</code> and <code>flake8</code>), - to check the order of <code>import</code> statements (<code>isort</code>), - to check the types of <code>python</code> objects (<code>mypy</code>).</p> <p>The hooks and their settings are specified in the <code>.pre-commit-config.yaml</code> in each repository.</p>"},{"location":"contribute/pre-commit-hooks/#handling-pre-commit-hook-errors","title":"Handling Pre-Commit Hook Errors","text":"<p>If your <code>git commit</code> fails because of the pre-commit hook, then you should:</p> <ol> <li> <p>Run the pre-commit hooks on the files which you have staged, by running the following command in your terminal:      <pre><code>$ pre-commit run\n</code></pre></p> </li> <li> <p>Inspect the output. It might look like this:    <pre><code>$ pre-commit run\nblack....................Passed\nisort....................Passed\nflake8...................Passed\nmypy.....................Failed\n- hook id: mypy\n- exit code: 1\n\nexample.py:33: error: Need type annotation for \"data\" (hint: \"data: Dict[&lt;type&gt;, &lt;type&gt;] = ...\")\nFound 1 errors in 1 files (checked 10 source files)\n</code></pre></p> </li> <li>Fix any errors which are reported.    Important: Once you've changed the code, re-stage the files it to Git.     This might mean un-staging changes and then adding them again.</li> <li>If you have trouble:</li> <li>Do a web-search to see if someone else had a similar error in the past.</li> <li>Check that the tests you've written work correctly.</li> <li>Check that there aren't any other obvious errors with the code.</li> <li>If you've done all of that, and you still can't fix the problem, get help from someone else on the team.</li> <li>Repeat 1-4 until all hooks return \"passed\", e.g.    <pre><code>$ pre-commit run\nblack....................Passed\nisort....................Passed\nflake8...................Passed\nmypy.....................Passed\n</code></pre></li> </ol> <p>It's easiest to solve these kinds of problems if you make small commits, often.  </p>"},{"location":"contribute/setup/","title":"Setup Guide","text":"<p>It's possible to set up your python environment in many different ways. </p> <p>To use the AutoRA package you need:</p> <ul> <li><code>python</code> and </li> <li>packages as specified in the <code>pyproject.toml</code> file.</li> </ul> <p>To develop the AutoRA package, you also need:</p> <ul> <li><code>git</code>, the source control tool,</li> <li><code>pre-commit</code> which is used for handling git pre-commit hooks.</li> </ul> <p>You should also consider using an IDE. We recommend: </p> <ul> <li>PyCharm. This is a <code>python</code>-specific integrated development environment which comes with useful tools    for changing the structure of <code>python</code> code, running tests, etc. </li> <li>Visual Studio Code. This is a powerful general text editor with plugins to support <code>python</code> development.</li> </ul> <p>The following sections describe how to install and configure the recommended setup for developing AutoRA.</p> <p>Tip</p> <p>It is helpful to be familiar with the command line for your operating system. The topics required are covered in:</p> <ul> <li>macOS: Joe Kissell. Take Control of the Mac Command Line with Terminal, 3rd Edition. Take Control Books, 2022. Chapters Read Me First through Bring the Command Line Into The Real World.</li> <li>Linux: William E. Shotts. The Linux Command Line: a Complete Introduction. 2nd edition.. No Starch Press, 2019. Parts I: Learning the Shell and II: Configuration and the Environment.</li> </ul>"},{"location":"contribute/setup/#development-setup","title":"Development Setup","text":""},{"location":"contribute/setup/#clone-the-repository","title":"Clone The Repository","text":"<p>The easiest way to clone the repo is to go to the repository page on GitHub and click the \"&lt;&gt; Code\" button and follow the prompts. </p> <p>Hint</p> <p>We recommend using:</p> <ul> <li>the GitHub Desktop Application on macOS or Windows, or </li> <li>the GitHub command line utility on Linux.</li> </ul>"},{"location":"contribute/setup/#install-python","title":"Install <code>Python</code>","text":"<p>Success</p> <p>All contributions to the AutoRA core packages should work under python 3.8, so we recommend using that version  for development.</p> <p>You can install python:</p> <ul> <li>Using the instructions at python.org, or</li> <li>Using a package manager, e.g.   homebrew,    pyenv,   asdf,    rtx,   winget.</li> </ul> <p>If successful, you should be able to run python in your terminal emulator like this: <pre><code>python\n</code></pre></p> <p>...and see some output like this: <pre><code>Python 3.11.3 (main, Apr  7 2023, 20:13:31) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n</code></pre></p>"},{"location":"contribute/setup/#create-a-virtual-environment","title":"Create A Virtual Environment","text":"<p>Success</p> <p>We recommend setting up your development environment using a manager like <code>venv</code>, which creates isolated python  environments. Other environment managers, like  virtualenv, pipenv, virtualenvwrapper,  hatch,  poetry,  are available and will likely work, but will have different syntax to the syntax shown here. </p> <p>Our packages are set up using <code>virtualenv</code> with <code>pip</code> </p> <p>In the <code>&lt;project directory&gt;</code>, run the following command to create a new virtual environment in the <code>.venv</code> directory</p> <pre><code>python3 -m \"venv\" \".venv\" \n</code></pre> <p>Hint</p> <p>If you have multiple Python versions installed on your system, it may be necessary to specify the Python version when creating a virtual environment. For example, run the following command to specify Python 3.8 for the virtual environment.  <pre><code>python3.8 -m \"venv\" \".venv\" \n</code></pre></p> <p>Activate it by running <pre><code>source \".venv/bin/activate\"\n</code></pre></p>"},{"location":"contribute/setup/#install-dependencies","title":"Install Dependencies","text":"<p>Upgrade pip: <pre><code>pip install --upgrade pip\n</code></pre></p> <p>Install the current project development dependencies: <pre><code>pip install --upgrade --editable \".[dev]\"\n</code></pre></p> <p>Your IDE may have special support for python environments. For IDE-specific setup, see:</p> <ul> <li>PyCharm Documentation</li> <li>VSCode Documentation</li> </ul>"},{"location":"contribute/setup/#activating-and-using-the-environment","title":"Activating And Using The Environment","text":"<p>To run interactive commands, you can activate the virtualenv environment. From the <code>&lt;project directory&gt;</code>  directory, run:</p> <pre><code>source \".venv/bin/activate\"\n</code></pre> <p>This spawns a new shell where you have access to the <code>python</code> and all the packages installed using <code>pip install</code>. You  should see the prompt change:</p> <pre><code>% source .venv/bin/activate\n(.venv) % \n</code></pre> <p>If you execute <code>python</code> and then <code>import numpy</code>, you should be able to see that <code>numpy</code> has been imported from the  <code>.venv</code> environment:</p> <pre><code>(.venv) % python\nPython 3.8.16 (default, Dec 15 2022, 14:31:45) \n[Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; numpy\n&lt;module 'numpy' from '/Users/me/Developer/autora/.venv/lib/python3.8/site-packages/numpy/__init__.py'&gt;\n&gt;&gt;&gt; exit()\n(.venv) %\n</code></pre> <p>You should be able to check that the current project works by running the tests: <pre><code>pytest\n</code></pre></p> <p>It should return something like:</p> <pre><code>% pytest\n.\n--------------------------------\nRan 1 test in 0.000s\n\nOK\n</code></pre> <p>Hint</p> <p>To deactivate the <code>virtualenv</code> environment, <code>deactivate</code> it. This should return you to your original prompt, as follows: <pre><code>(venv) % deactivate\n% \n</code></pre></p>"},{"location":"contribute/setup/#running-code-non-interactively","title":"Running Code Non-Interactively","text":"<p>You can run python programs without activating the environment, by using <code>/path/to/python run {command}</code>. For example, to run unittests tests, execute:</p> <pre><code>.venv/bin/python -m pytest\n</code></pre> <p>It should return something like:</p> <pre><code>% .venv/bin/python -m pytest\n.\n--------------------------------\nRan 1 test in 0.000s\n\nOK\n</code></pre>"},{"location":"contribute/setup/#pre-commit-hooks","title":"Pre-Commit Hooks","text":"<p>If you wish to commit to the repository, you should install and activate <code>pre-commit</code> as follows.  <pre><code>pip install pre-commit\npre-commit install\n</code></pre></p> <p>You can run the pre-commit hooks manually by calling: <pre><code>pre-commit run --all-files\n</code></pre></p> <p>For more information on pre-commit hooks, see Pre-Commit-Hooks</p>"},{"location":"contribute/modules/","title":"Module Contributions","text":"<p>Theorists, experimentalists, experiment runners and other novel functionalities are implemented as \"child\" packages.  They are based on either</p> <ul> <li>the cookiecutter template (recommended), or</li> <li>the unguided template.</li> </ul> <p>Hint</p> <p>The easiest way to contribute a new child package for an experimentalist, experiment runner or theorist, start from the cookiecutter template.</p> <p>Success</p> <p>New synthetic experiment runners may be submitted as pull requests to the  <code>autora-synthetic</code> package, providing they  require no additional dependencies. This is meant to simplify small contributions.  However, if your contribution requires additional dependencies, you can submit it as a full package following  this guide. </p>"},{"location":"contribute/modules/#implement-your-module","title":"Implement Your Module","text":"<p>After setting up your repository and linking it to your GitHub account, you can start implementing your module.</p>"},{"location":"contribute/modules/#implement-your-code","title":"Implement Your Code","text":"<p>You may implement your code in the <code>__init__.py</code> located in the respective feature folder in <code>src/autora</code>.</p> <p>Please refer to the following guides on implementing</p> <ul> <li>theorists</li> <li>experimentalists</li> <li>experiment runners</li> </ul> <p>If the feature you seek to implement does not fit in any of these categories, then  you can create folders for new categories. If you are unsure how to proceed, you are always welcome  to ask for help in the AutoRA forum.</p>"},{"location":"contribute/modules/#add-tests-optional","title":"Add Tests (Optional)","text":"<p>It is highly encouraged to add unit tests to ensure your code is working as intended. These can be doctests or test cases in <code>tests/test_your_contribution_name.py</code>. For example, if you are implementing a sampler experimentalist, you may rename and modify the  <code>tests/test_experimentalist_sampler_example.py</code>.</p> <p>Note: Tests are required for your module to become part of the main  autora package. However, regardless of whether you choose to implement tests,  you will still be able to install your package separately, in addition to <code>autora</code>. </p>"},{"location":"contribute/modules/#add-documentation-optional","title":"Add Documentation (Optional)","text":"<p>It is highly encouraged that you add documentation of your package in <code>docs/index.md</code>. You can also add new or delete unnecessary pages  in the <code>docs</code> folder. However, you structure your documentation, be sure that structure is reflected in the <code>mkdocs.yml</code> file.</p> <p>Hint</p> <p>When naming your documentation files, make sure to use <code>-</code> instead of <code>_</code> in the file name. For example,  <code>random-sampler.md</code> instead of <code>random_sampler.md</code>. This style is in alignment with the AutoRA documentation.  </p> <p>You are also encouraged to describe basic usage of your module in the  python notebook <code>Basic Usage.ipynb</code> in the <code>docs</code> folder. Finally, you can outline the basic setup of your module in  the <code>docs/quickstart.md</code> file.</p> <p>Note: Documentation is required for your module to become part of the main  autora package. However, regardless of whether you choose to write documentation, you will still be able to install your package separately, in addition to <code>autora</code>.</p>"},{"location":"contribute/modules/#add-dependencies","title":"Add Dependencies","text":"<p>In the <code>pyproject.toml</code> file, add the new dependencies under <code>dependencies</code>.</p> <p>Install the added dependencies <pre><code>pip install -e \".[dev]\"\n</code></pre></p>"},{"location":"contribute/modules/#publish-your-module","title":"Publish Your Module","text":"<p>There are several ways to publish your package, depending on how you set up your repository.</p>"},{"location":"contribute/modules/#publishing-via-github-actions","title":"Publishing Via GitHub Actions","text":"<p>If you used the cookiecutter template with the advanced setup, and uploaded your repository to GitHub, then you can use GitHub Actions to automatically publish your package to PyPI or Conda. </p> <p>Note, if your repository is part of the AutoResearch Organization you can skip the step below for creating a new secret in your repository.</p> <ol> <li> <p>Add an API token to the GitHub Secrets</p> <ul> <li>Create a PyPI account if you don't have one already.</li> <li>Once you have an account, generate an API token for your account.</li> <li>In your GitHub repository, go to <code>Settings</code>.</li> <li>Under <code>Secrets and variables</code> in the left-hand menu, select <code>Actions</code>. </li> <li>Create a new secret named <code>PYPI_API_TOKEN</code> and paste in your PyPI API token as the value.</li> </ul> </li> <li> <p>Create a new release</p> <ul> <li>Follow the steps outlined in the GitHub documentation for creating a new release. </li> <li>Once you create a new release, the GitHub Action will automatically trigger, and your package will be built and published to PyPI using the provided API token.</li> </ul> </li> </ol>"},{"location":"contribute/modules/#manually-publishing","title":"Manually Publishing","text":"<p>If you used the unguided template, or you want to manually publish your package, you can follow step 7 in this guide.</p> <p>Once you've published your module, you should take some time to celebrate and announce your contribution in the  AutoRA forum.</p>"},{"location":"contribute/modules/#incorporate-your-module-into-the-autora-parent-package","title":"Incorporate Your Module Into The AutoRA Parent Package","text":"<p>Once your package is working and published, you can make a pull request on <code>autora</code> to have it vetted and added to the \"parent\" package. Note, if you are not a member of the AutoResearch organization on GitHub, you will need to create a fork of the repository for the parent package and submit your pull request via that fork. If you are a member, you can create a pull request from a branch created directly from the parent package repository. Steps for creating a new branch to add your module are specified below.</p> <p>Success</p> <p>In order for your package to be included in the parent package, it must meet the following criteria:</p> <ul> <li>have basic documentation in <code>docs/index.md</code></li> <li>have a basic python notebook exposing how to use the module in <code>docs/Basic Usage.ipynb</code></li> <li>have basic tests in <code>tests/</code></li> <li>be published via PyPI or Conda</li> <li>be compatible with the current version of the parent package</li> <li>follow standard python coding guidelines including PEP8</li> <li>the repository in which your package is hosted must be public</li> </ul> <p>The following demonstrates how to add a package published under <code>autora-theorist-example</code> in PyPI in the GitHub  repository <code>example-contributor/contributor-theorist</code>.</p>"},{"location":"contribute/modules/#install-the-parent-package-in-development-mode","title":"Install The Parent Package In Development Mode","text":"<p>Success</p> <p>We recommend setting up your development environment using a manager like <code>venv</code>, which creates isolated python  environments. Other environment managers, such as  virtualenv, pipenv, virtualenvwrapper,  hatch,  poetry,  are available and will likely work, but will have syntax different to that shown here. </p> <p>Run the following command to create a new virtual environment in the <code>.venv</code> directory</p> <pre><code>python3 -m \"venv\" \".venv\" \n</code></pre> <p>Hint</p> <p>If you have multiple Python versions installed on your system, it may be necessary to specify the Python version when creating a virtual environment. For example, run the following command to specify Python 3.8 for the virtual environment.  <pre><code>python3.8 -m \"venv\" \".venv\" \n</code></pre></p> <p>Activate it by running <pre><code>source \".venv/bin/activate\"\n</code></pre></p> <p>Use <code>pip install</code> to install the current project (<code>\".\"</code>) in editable mode (<code>-e</code>) with dev-dependencies (<code>[dev]</code>): <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Check that the documentation builds correctly by running: <pre><code>mkdocs serve\n</code></pre></p> <p>... then viewing the documentation using the link in your terminal.</p>"},{"location":"contribute/modules/#create-a-new-branch-of-the-parent-package","title":"Create A New Branch Of The Parent Package","text":"<p>Once you've successfully installed the parent package in development mode, you can begin the process of adding your contribution by creating a new branch off of the <code>main</code> branch. You should name your branch according to the name of your contribution. In the example we're using here, the branch would be called <code>feat/contributor-theorist</code>. After creating your branch, you can start making the modifications specified below. </p>"},{"location":"contribute/modules/#add-the-package-as-an-optional-dependency","title":"Add The Package As An Optional Dependency","text":"<p>In the <code>pyorject.toml</code> file add an optional dependency for the package in the <code>[project.optional-dependencies]</code> section:</p> <pre><code>theorist-example = [\"autora-theorist-example==1.0.0\"]\n</code></pre> <p>Success</p> <p>Ensure you include the version number.</p> <p>Add the <code>theorist-example</code> to be part of the <code>all-theorists</code> dependency: <pre><code>all-theorists = [\n    ...\n    \"autora[theorist-example]\",\n    ...\n]\n</code></pre></p> <p>Update the environment:</p> <pre><code>pip install -U -e \".[dev]\"\n</code></pre> <p>... and check that your package is still importable and works as expected.</p>"},{"location":"contribute/modules/#import-documentation-from-the-package-repository","title":"Import Documentation From The Package Repository","text":"<p>Import the documentation in the <code>mkdocs.yml</code> file: <pre><code>plugins:\n  multirepo:\n    nav_repos:\n      ...\n      - name: theorist-example\n        import_url: \"https://github.com/example-contributor/contributor-theorist/?branch=v1.0.0\"\n        imports: [ \"src/\" ]\n  ...\n  mkdocstrings:\n    handlers:\n      python:\n        paths: [\n          ...,\n          \"./temp_dir/theorist-example/src/\"\n        ]\n...\n- User Guide:\n  - Theorists:\n    - Home: 'theorist/index.md'\n    ...\n    - Example Theorist: '!import https://github.com/example-contributor/contributor-theorist/?branch=v1.0.0&amp;extra_imports=[\"mkdocs/base.yml\"]'\n    ...\n</code></pre></p> <p>Success</p> <p>Ensure you include the version number in the <code>!import</code> string after <code>?branch=</code>. Ensure that the commit you want  to submit has a tag with the correct version number in the correct format.</p> <p>Check that the documentation builds correctly by running: <pre><code>mkdocs serve\n</code></pre></p> <p>... then view the documentation using the link in your terminal. Check that your new documentation is included in  the right place and renders correctly.</p>"},{"location":"contribute/modules/#updating-your-module","title":"Updating Your Module","text":"<p>Warning</p> <p>Please note, that packages need to be vetted each time they are updated.</p> <p>In the <code>[project.optional-dependencies]</code> section of the <code>pyproject.toml</code> file, update the version number: <pre><code>theorist-example = [\"autora-theorist-example==1.1.0\"]\n</code></pre></p> <p>Also update the version number in the <code>mkdocs.yml</code>:  <pre><code>plugins:\n  multirepo:\n    nav_repos:\n      ...\n      - name: theorist-example\n        import_url: \"https://github.com/example-contributor/contributor-theorist/?branch=v1.1.0\"\n        imports: [ \"src/\" ]\n...\n- User Guide:\n  - Theorists:\n    ...\n    - Example Theorist: '!import https://github.com/example-contributor/contributor-theorist/?branch=v1.1.0&amp;extra_imports=[\"mkdocs/base.yml\"]'\n    ...\n</code></pre></p> <p>Next, update the environment: <pre><code>pip install -U -e \".[dev]\"\n</code></pre></p> <p>... and check that your package is still importable and works as expected.</p> <p>Check that the documentation builds correctly by running: <pre><code>mkdocs serve\n</code></pre></p> <p>... then view the documentation using the link in your terminal. Check that your new documentation is included in  the right place and renders correctly.</p> <p>Once everything is working locally, make a new PR on GitHub.com with your  changes. Include: </p> <ul> <li>a description of the changes to the package, and </li> <li>a link to your release notes. </li> </ul> <p>Request a review from someone in the core team and wait for their feedback!</p> <p>Note, whenever you update and release a new version of your module, you will need to add the new version number in the places described above and create a new PR to have it included in <code>autora</code>.</p>"},{"location":"contribute/modules/experiment-runner/","title":"Contribute An Experiment Runner","text":"<p>AutoRA experiment runners are designed to generate observations. They encompass a range of tools for conducting both synthetic and real experiments. By combining observations with corresponding experimental conditions, they provide inputs necessary for an AutoRA theorist to perform model discovery. </p> <p>Experiment runners can be implemented as synthetic runners: To contribute a synthetic experiment runner follow the core contribution guide.</p> <p>Contributions may be complete experiment runners, which are functions that return observations, or tools that help automate experiments. Examples of such tools that are already implemented include a recruitment manager for recruiting participants on Prolific and an experimentation manager for executing online experiments with Firebase.</p>"},{"location":"contribute/modules/experiment-runner/#repository-setup","title":"Repository Setup","text":"<p>For non-synthetic experiment runners, we recommend using the cookiecutter template to set up a repository for your experiment runner. Alternatively, you can use the  unguided template. If you choose the cookiecutter template, you can set up your repository using</p> <pre><code>cookiecutter https://github.com/AutoResearch/autora-template-cookiecutter\n</code></pre> <p>Make sure to select the <code>experiment runner</code> option when prompted. If you want to design a tool to recruit participants, choose the <code>recruitment manager</code> option. If you want to design a tool to conduct experiments, choose the <code>experimentation manager</code> option. You can also design a custom tool and name it yourself. You can skip all prompts pertaining to other modules  (e.g., experimentalists) by pressing enter.</p>"},{"location":"contribute/modules/experiment-runner/#implementation","title":"Implementation","text":"<p>To implement a complete experiment runner, be sure to define a function that returns observations. To get an idea for tools that help automate experiments, see the list of tools that are already implemented.</p>"},{"location":"contribute/modules/experiment-runner/#next-steps-testing-documentation-publishing","title":"Next Steps: Testing, Documentation, Publishing","text":"<p>For more information on how to test, document, and publish your experiment runner, please refer to the  general guideline for module contributions . </p>"},{"location":"contribute/modules/experimentalist/","title":"Contribute An Experimentalist","text":"<p>AutoRA experimentalists are meant to return novel experimental conditions based on prior experimental conditions, prior observations, and/or prior models. Such conditions may serve as a basis for new, informative experiments conducted  by an experiment runner. Experimentalists are generally implemented as functions that can be integrated into an  Experimentalist Pipeline.</p> <p></p>"},{"location":"contribute/modules/experimentalist/#repository-setup","title":"Repository Setup","text":"<p>We recommend using the cookiecutter template to set up a repository for your experimentalist. Alternatively, you can use the  unguided template. If you choose the cookiecutter template, you can set up your repository using</p> <pre><code>cookiecutter https://github.com/AutoResearch/autora-template-cookiecutter\n</code></pre> <p>Make sure to select the <code>experimentalist</code> option when prompted. You can skip all other prompts pertaining to other modules  (e.g., experiment runners) by pressing enter.</p>"},{"location":"contribute/modules/experimentalist/#implementation","title":"Implementation","text":"<p>For an experimentalist, you should implement a function that returns a set of experimental conditions. This set may be a <code>pandas</code> data frame, <code>numpy</code> array, iterator variable or other data format. </p> <p>Hint</p> <p>We generally recommend using pandas data frames as outputs in which</p> <p>columns correspond to the independent variables of an experiment. </p> <p>Once you've created your repository, you can implement your experimentalist by editing the  <code>__init__.py</code> file in  <code>src/autora/experimentalist/name_of_your_experimentalist/</code>.  You may also add additional files to this directory if needed.  It is important that the <code>__init__.py</code> file contains a function called  <code>name_of_your_experimentalist</code>  which returns a set of experimental conditions (e.g., as a numpy array).</p> <p>The following example <code>__init__.py</code> illustrates the implementation of a simple experimentalist that uniformly samples without replacement from a pool of candidate conditions.</p> <pre><code>\"\"\"\nExample Experimentalist\n\"\"\"\n\nimport random\nimport pandas as pd\nimport numpy as np\nfrom typing import Iterable, Union\n\ndef random_sample(conditions: Union[pd.DataFrame, np.ndarray], \n              num_samples: int = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Uniform random sampling without replacement from a pool of conditions.\n    Args:\n        conditions: Pool of conditions\n        num_samples: number of samples to collect\n\n    Returns: Sampled pool of conditions\n\n    \"\"\"\n\n    if isinstance(conditions, pd.DataFrame):\n        # Randomly sample N rows from DataFrame\n        sampled_data = conditions.sample(n=num_samples)\n        return sampled_data\n\n    elif isinstance(conditions, np.ndarray):\n        # Randomly sample N rows from NumPy array\n        if num_samples &gt; conditions.shape[0]:\n            raise ValueError(\"num_samples cannot be greater than the number of rows in the array.\")\n        indices = np.random.choice(conditions.shape[0], size=num_samples, replace=False)\n        sampled_conditions = conditions[indices]\n        return sampled_conditions\n</code></pre>"},{"location":"contribute/modules/experimentalist/#next-steps-testing-documentation-publishing","title":"Next Steps: Testing, Documentation, Publishing","text":"<p>For more information on how to test, document, and publish your experimentalist, please refer to the  general guideline for module contributions . </p>"},{"location":"contribute/modules/theorist/","title":"Contribute A Theorist","text":"<p>AutoRA theorists are meant to return scientific models describing the relationship between experimental conditions and observations. Such models may take the form of a simple linear regression, non-linear equations, causal graphs,  a more complex neural network, or other models which</p> <ul> <li>can be identified based on data (and prior knowledge)</li> <li>can be used to make novel predictions about observations given experimental conditions.</li> </ul> <p></p> <p>All theorists are implemented as <code>sklearn</code> regressors. They are fitted based on experimental conditions and respective observations, and can be used to predict observations for new experimental conditions.</p>"},{"location":"contribute/modules/theorist/#repository-setup","title":"Repository Setup","text":"<p>We recommend using the cookiecutter template to set up a repository for your theorist. Alternatively, you use the  unguided template. If you are using the cookiecutter template, you can set up your repository using</p> <pre><code>cookiecutter https://github.com/AutoResearch/autora-template-cookiecutter\n</code></pre> <p>Make sure to select the <code>theorist</code> option when prompted. You can skip all other prompts pertaining to other modules  (e.g., experimentalists) by pressing enter.</p>"},{"location":"contribute/modules/theorist/#implementation","title":"Implementation","text":"<p>Once you've created your repository, you can implement your theorist by editing the <code>__init__.py</code>  file in  <code>src/autora/theorist/name_of_your_theorist/</code>. You may also add additional files to this directory if needed.  It is important that the <code>__init__.py</code> file contains a class called <code>NameOfYourTheorist</code> which  inherits from <code>sklearn.base.BaseEstimator</code> and implements the following methods:</p> <ul> <li><code>fit(self, conditions, observations)</code></li> <li><code>predict(self, conditions)</code></li> </ul> <p>See the sklearn documentation for more information on  how to implement the methods. The following example <code>__init__.py</code> illustrates the implementation  of a simple theorist that fits a polynomial function to the data:</p> <pre><code>\"\"\"\nExample Theorist\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom typing import Union\nfrom sklearn.base import BaseEstimator\n\n\nclass ExampleRegressor(BaseEstimator):\n    \"\"\"\n    This theorist fits a polynomial function to the data.\n    \"\"\"\n\n    def __init__(self, degree: int = 2):\n        self.degree = degree\n\n    def fit(self, conditions: Union[pd.DataFrame, np.ndarray],\n            observations: Union[pd.DataFrame, np.ndarray]):\n\n        # fit polynomial function: observations ~ conditions \n        self.coeff = np.polyfit(conditions, observations, deg = 2)\n        self.polynomial = np.poly1d(self.coeff)\n        pass\n\n    def predict(self, conditions):\n\n        return self.polynomial(conditions)\n</code></pre> <p>Note, however, that it is best practice to make sure the conditions are compatible with the <code>polyfit</code>. In this case, we will make sure to add some checks:</p> <pre><code>\"\"\"\nExample Theorist\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom typing import Union\nfrom sklearn.base import BaseEstimator\n\n\nclass ExampleRegressor(BaseEstimator):\n    \"\"\"\n    This theorist fits a polynomial function to the data.\n    \"\"\"\n\n    def __init__(self, degree: int = 2):\n        self.degree = degree\n\n    def fit(self, conditions: Union[pd.DataFrame, np.ndarray],\n            observations: Union[pd.DataFrame, np.ndarray]):\n\n        # polyfit expects a 1D array, convert pandas data frame to 1D vector\n        if isinstance(conditions, pd.DataFrame):\n          conditions = conditions.squeeze()\n\n        # polyfit expects a 1D array, flatten nd array\n        if isinstance(conditions, np.ndarray) and conditions.ndim &gt; 1:\n            conditions = conditions.flatten()\n\n        # fit polynomial function: observations ~ conditions \n        self.coeff = np.polyfit(conditions, observations, deg = 2)\n        self.polynomial = np.poly1d(self.coeff)\n        pass\n\n    def predict(self, conditions):\n\n        # polyfit expects a 1D array, convert pandas data frame to 1D vector\n        if isinstance(conditions, pd.DataFrame):\n          conditions = conditions.squeeze()\n\n        # polyfit expects a 1D array, flatten nd array\n        if isinstance(conditions, np.ndarray) and conditions.ndim &gt; 1:\n            conditions = conditions.flatten()\n\n        return self.polynomial(conditions)\n</code></pre>"},{"location":"contribute/modules/theorist/#important-considerations-for-sklearn-baseestimators","title":"Important Considerations for <code>sklearn</code> BaseEstimators","text":"<p>When working with <code>sklearn</code>'s <code>BaseEstimator</code>, it's crucial to ensure that any arguments passed to the <code>__init__</code> function of a derived class are assigned as instance attributes. This is a requirement in sklearn to maintain consistency and functionality across its estimators. </p> <p>For instance, the following code will raise an error because <code>input_argument</code> does exist as an input to the <code>init</code>-method but not as a class member:</p> <p><pre><code>from sklearn.base import BaseEstimator\n\n\nclass ExampleRegressor(BaseEstimator):\n    def __init__(self, input_argument):\n        print(input_argument)\n\n\ntheorist = ExampleRegressor('test')\n\nprint(theorist)\n</code></pre> To avoid this issue, <code>input_argument</code> should be explicitly set as a class attribute, like this:</p> <pre><code>from sklearn.base import BaseEstimator\n\n\nclass ExampleRegressor(BaseEstimator):\n    def __init__(self, input_argument):\n        self.input_argument = 'something arbitrary'\n        print(input_argument)\n\n\ntheorist = ExampleRegressor('test')\n\nprint(theorist)\n</code></pre> <p>By assigning arguments as instance attributes (e.g., <code>self.input_argument</code>), your class will behave as expected within the <code>sklearn</code> framework.</p>"},{"location":"contribute/modules/theorist/#next-steps-testing-documentation-publishing","title":"Next Steps: Testing, Documentation, Publishing","text":"<p>For more information on how to test, document, and publish your theorist, please refer to the  general guideline for module contributions . </p>"},{"location":"core/docs/","title":"Core Functionality","text":"<p>AutoRA includes core functionality for running AutoRA experiments organized into these submodules:</p> <ul> <li><code>autora.state</code>, which underpins the unified <code>State</code> interface for writing experimentalists, experiment runners and    theorists</li> <li><code>autora.serializer</code>, utilities for saving and loading <code>States</code></li> <li><code>autora.workflow</code>, command line tools for running experimentalists, experiment runners and theorists</li> <li><code>autora.variable</code>, for representing experimental metadata describing the type and domain of variables</li> <li><code>autora.utils</code>, utilities and helper functions not linked to any specific core functionality  </li> </ul> <p>It also provides some basic experimentalists in the <code>autora.experimentalist</code> submodule. However, most  genuinely useful experimentalists and theorists are provided as optional dependencies to the <code>autora</code> package.</p>"},{"location":"core/docs/The%20State%20Mechanism/","title":"Overview","text":"<p>A <code>State</code> is an object representing data from an experiment, like the conditions, observed experiment data and models. In the AutoRA framework, experimentalists, experiment runners and theorists are functions which</p> <ul> <li>operate on <code>States</code> and</li> <li>return <code>States</code>.</li> </ul> <p>The <code>autora.state</code> submodule provides classes and functions to help build these functions.</p> In\u00a0[\u00a0]: Copied! <pre>from dataclasses import dataclass, field\n\nimport numpy as np\nimport pandas as pd\nimport autora.state\nfrom autora.variable import VariableCollection, Variable\n</pre> from dataclasses import dataclass, field  import numpy as np import pandas as pd import autora.state from autora.variable import VariableCollection, Variable In\u00a0[\u00a0]: Copied! <pre>@dataclass(frozen=True)\nclass BasicState(autora.state.State):\n   data: pd.DataFrame = field(default_factory=pd.DataFrame, metadata={\"delta\": \"extend\"})\n   \ns = BasicState()\n</pre> @dataclass(frozen=True) class BasicState(autora.state.State):    data: pd.DataFrame = field(default_factory=pd.DataFrame, metadata={\"delta\": \"extend\"})     s = BasicState() <p>Because it is a python dataclass, the <code>State</code> fields can be accessed using attribute notation, for example:</p> In\u00a0[\u00a0]: Copied! <pre>s.data  # an empty DataFrame with a column \"x\"\n</pre> s.data  # an empty DataFrame with a column \"x\" Out[\u00a0]: <p><code>State</code> objects can be updated by adding <code>Delta</code> objects. A <code>Delta</code> represents new data, and is combined with the existing data in the <code>State</code> object. The <code>State</code> itself is immutable by design, so adding a <code>Delta</code> to it creates a new <code>State</code>.</p> In\u00a0[\u00a0]: Copied! <pre>s + autora.state.Delta(data=pd.DataFrame({\"x\":[1], \"y\":[1]}))\n</pre> s + autora.state.Delta(data=pd.DataFrame({\"x\":[1], \"y\":[1]})) Out[\u00a0]: <pre>BasicState(data=   x  y\n0  1  1)</pre> <p>When carrying out this \"addition\", <code>s</code>:</p> <ul> <li><p>inspects the <code>Delta</code> it has been passed and finds any field names matching fields on <code>s</code>, in this case <code>data</code>.</p> </li> <li><p>For each matching field it combines the data in a way determined by the field's metadata. The key options are:</p> <ul> <li>\"replace\" means that the data in the <code>Delta</code> object completely replace the data in the <code>State</code>,</li> <li>\"extend\" means that the data in the <code>Delta</code> object are combined \u2013 for pandas DataFrames this means that the new data are concatenated to the bottom of the existing DataFrame.</li> </ul> <p>For full details on which options are available, see the documentation for the <code>autora.state</code> module.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>(s + \n autora.state.Delta(data=pd.DataFrame({\"x\":[1], \"y\":[1]})) + \n autora.state.Delta(data=pd.DataFrame({\"x\":[2], \"y\":[2]}))\n ).data  # Access just the experiment_data on the updated State\n</pre> (s +   autora.state.Delta(data=pd.DataFrame({\"x\":[1], \"y\":[1]})) +   autora.state.Delta(data=pd.DataFrame({\"x\":[2], \"y\":[2]}))  ).data  # Access just the experiment_data on the updated State Out[\u00a0]: x y 0 1 1 1 2 2 In\u00a0[\u00a0]: Copied! <pre>s_0 = autora.state.StandardState(\n    variables=VariableCollection(\n        independent_variables=[Variable(\"x\", value_range=(-10, 10))],\n        dependent_variables=[Variable(\"y\")]\n    ),\n    conditions=pd.DataFrame({\"x\":[]}),\n    experiment_data=pd.DataFrame({\"x\":[], \"y\":[]}),\n    models=[]\n)\n</pre> s_0 = autora.state.StandardState(     variables=VariableCollection(         independent_variables=[Variable(\"x\", value_range=(-10, 10))],         dependent_variables=[Variable(\"y\")]     ),     conditions=pd.DataFrame({\"x\":[]}),     experiment_data=pd.DataFrame({\"x\":[], \"y\":[]}),     models=[] ) In\u00a0[\u00a0]: Copied! <pre>def generate_conditions(variables, num_samples=5, random_state=42):\n    rng = np.random.default_rng(random_state)               # Initialize a random number generator\n    conditions = pd.DataFrame()                             # Create a DataFrame to hold the results  \n    for iv in variables.independent_variables:              # Loop through the independent variables\n        c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range\n        conditions[iv.name] = c                             #  - Save the new values to the DataFrame\n    return conditions\n</pre> def generate_conditions(variables, num_samples=5, random_state=42):     rng = np.random.default_rng(random_state)               # Initialize a random number generator     conditions = pd.DataFrame()                             # Create a DataFrame to hold the results       for iv in variables.independent_variables:              # Loop through the independent variables         c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range         conditions[iv.name] = c                             #  - Save the new values to the DataFrame     return conditions <p>We'll look at each of the ways you can make this into a function of the required form.</p> In\u00a0[\u00a0]: Copied! <pre>@autora.state.on_state(output=[\"conditions\"])\ndef generate_conditions(variables, num_samples=5, random_state=42):\n    rng = np.random.default_rng(random_state)               # Initialize a random number generator\n    conditions = pd.DataFrame()                             # Create a DataFrame to hold the results  \n    for iv in variables.independent_variables:              # Loop through the independent variables\n        c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range\n        conditions[iv.name] = c                             #  - Save the new values to the DataFrame\n    return conditions\n\n# Example\ngenerate_conditions(s_0)\n</pre> @autora.state.on_state(output=[\"conditions\"]) def generate_conditions(variables, num_samples=5, random_state=42):     rng = np.random.default_rng(random_state)               # Initialize a random number generator     conditions = pd.DataFrame()                             # Create a DataFrame to hold the results       for iv in variables.independent_variables:              # Loop through the independent variables         c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range         conditions[iv.name] = c                             #  - Save the new values to the DataFrame     return conditions  # Example generate_conditions(s_0) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-10, 10), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.479121\n1 -1.222431\n2  7.171958\n3  3.947361\n4 -8.116453, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> <p>Fully equivalently, you can modify <code>generate_conditions</code> to return a Delta of values with the appropriate field names from <code>State</code>:</p> In\u00a0[\u00a0]: Copied! <pre>@autora.state.on_state\ndef generate_conditions(variables, num_samples=5, random_state=42):\n    rng = np.random.default_rng(random_state)               # Initialize a random number generator\n    conditions = pd.DataFrame()                             # Create a DataFrame to hold the results  \n    for iv in variables.independent_variables:              # Loop through the independent variables\n        c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range\n        conditions[iv.name] = c                             #  - Save the new values to the DataFrame\n    return autora.state.Delta(conditions=conditions)        # Return a Delta with the appropriate names\n    # return {\"conditions\": conditions}                     # Returning a dictionary is equivalent\n\n# Example\ngenerate_conditions(s_0)\n</pre> @autora.state.on_state def generate_conditions(variables, num_samples=5, random_state=42):     rng = np.random.default_rng(random_state)               # Initialize a random number generator     conditions = pd.DataFrame()                             # Create a DataFrame to hold the results       for iv in variables.independent_variables:              # Loop through the independent variables         c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range         conditions[iv.name] = c                             #  - Save the new values to the DataFrame     return autora.state.Delta(conditions=conditions)        # Return a Delta with the appropriate names     # return {\"conditions\": conditions}                     # Returning a dictionary is equivalent  # Example generate_conditions(s_0) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-10, 10), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.479121\n1 -1.222431\n2  7.171958\n3  3.947361\n4 -8.116453, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>def generate_conditions_inner(variables, num_samples=5, random_state=42):\n    rng = np.random.default_rng(random_state)               # Initialize a random number generator\n    result = pd.DataFrame()                             # Create a DataFrame to hold the results  \n    for iv in variables.independent_variables:              # Loop through the independent variables\n        c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range\n        result[iv.name] = c                             #  - Save the new values to the DataFrame\n    return result\n\ngenerate_conditions = autora.state.on_state(generate_conditions_inner, output=[\"conditions\"])\n\n# Example\ngenerate_conditions(s_0, random_state=180)\n</pre> def generate_conditions_inner(variables, num_samples=5, random_state=42):     rng = np.random.default_rng(random_state)               # Initialize a random number generator     result = pd.DataFrame()                             # Create a DataFrame to hold the results       for iv in variables.independent_variables:              # Loop through the independent variables         c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range         result[iv.name] = c                             #  - Save the new values to the DataFrame     return result  generate_conditions = autora.state.on_state(generate_conditions_inner, output=[\"conditions\"])  # Example generate_conditions(s_0, random_state=180) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-10, 10), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  1.521127\n1  3.362120\n2  1.065391\n3 -5.844244\n4 -6.444732, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> <p>During the <code>generate_conditions(s_0, random_state=180)</code> call, <code>autora.state.on_state</code> does the following:</p> <ul> <li>Inspects the signature of <code>generate_conditions_inner</code> to see which variables are required \u2013 in this case:<ul> <li><code>variables</code>,</li> <li><code>num_samples</code> and</li> <li><code>random_state</code>.</li> </ul> </li> <li>Looks for fields with those names on <code>s_0</code>:<ul> <li>Finds a field called <code>variables</code>.</li> </ul> </li> <li>Calls <code>generate_conditions_inner</code> with those fields as arguments, plus any arguments specified in the <code>generate_conditions</code> call (here just <code>random_state</code>)</li> <li>Converts the returned value <code>result</code> into <code>Delta(conditions=result)</code> using the name specified in <code>output=[\"conditions\"]</code></li> <li>Returns <code>s_0 + Delta(conditions=result)</code></li> </ul> <p>Fully equivalently to using the <code>autora.state.on_state</code> wrapper, you can construct a function which takes and returns <code>State</code> objects.</p> In\u00a0[\u00a0]: Copied! <pre>def generate_conditions(state: autora.state.StandardState, num_samples=5, random_state=42):\n    rng = np.random.default_rng(random_state)               # Initialize a random number generator\n    conditions = pd.DataFrame()                             # Create a DataFrame to hold the results  \n    for iv in state.variables.independent_variables:        # Loop through the independent variables\n        c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range\n        conditions[iv.name] = c                             #  - Save the new values to the DataFrame\n    delta = autora.state.Delta(conditions=conditions)       # Construct a new Delta representing the updated data\n    new_state = state + delta                               # Construct a new state, \"adding\" the Delta\n    return new_state\n\n# Example\ngenerate_conditions(s_0)\n</pre> def generate_conditions(state: autora.state.StandardState, num_samples=5, random_state=42):     rng = np.random.default_rng(random_state)               # Initialize a random number generator     conditions = pd.DataFrame()                             # Create a DataFrame to hold the results       for iv in state.variables.independent_variables:        # Loop through the independent variables         c = rng.uniform(*iv.value_range, size=num_samples)  #  - Generate a uniform sample from the range         conditions[iv.name] = c                             #  - Save the new values to the DataFrame     delta = autora.state.Delta(conditions=conditions)       # Construct a new Delta representing the updated data     new_state = state + delta                               # Construct a new state, \"adding\" the Delta     return new_state  # Example generate_conditions(s_0) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-10, 10), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.479121\n1 -1.222431\n2  7.171958\n3  3.947361\n4 -8.116453, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\n\nestimator = LinearRegression(fit_intercept=True)       # Initialize the regressor with all its parameters\ntheorist = autora.state.estimator_on_state(estimator)  # Wrap the estimator\n\n\n# Example\nvariables = s_0.variables          # Reuse the variables from before \nxs = np.linspace(-10, 10, 101)     # Make an array of x-values \nnoise = np.random.default_rng(179).normal(0., 0.5, xs.shape)  # Gaussian noise\nys = (3.5 * xs + 2. + noise)       # Calculate y = 3.5 x + 2 + noise  \n\ns_1 = autora.state.StandardState(  # Initialize the State with those data\n    variables=variables,\n    experiment_data=pd.DataFrame({\"x\":xs, \"y\":ys}),\n)\ns_1_prime = theorist(s_1)         # Run the theorist\nprint(f\"Returned models: \"\n      f\"{s_1_prime.models}\")      \nprint(f\"Last model's coefficients: \"\n      f\"y = {s_1_prime.models[-1].coef_[0]} x + {s_1_prime.models[-1].intercept_}\")\n</pre> from sklearn.linear_model import LinearRegression   estimator = LinearRegression(fit_intercept=True)       # Initialize the regressor with all its parameters theorist = autora.state.estimator_on_state(estimator)  # Wrap the estimator   # Example variables = s_0.variables          # Reuse the variables from before  xs = np.linspace(-10, 10, 101)     # Make an array of x-values  noise = np.random.default_rng(179).normal(0., 0.5, xs.shape)  # Gaussian noise ys = (3.5 * xs + 2. + noise)       # Calculate y = 3.5 x + 2 + noise    s_1 = autora.state.StandardState(  # Initialize the State with those data     variables=variables,     experiment_data=pd.DataFrame({\"x\":xs, \"y\":ys}), ) s_1_prime = theorist(s_1)         # Run the theorist print(f\"Returned models: \"       f\"{s_1_prime.models}\")       print(f\"Last model's coefficients: \"       f\"y = {s_1_prime.models[-1].coef_[0]} x + {s_1_prime.models[-1].intercept_}\") <pre>Returned models: [LinearRegression()]\nLast model's coefficients: y = [3.49729147] x + [1.99930059]\n</pre> <p>During the <code>theorist(s_1)</code> call, <code>autora.state.estimator_on_state</code> does the following:</p> <ul> <li>Gets the names of the independent and dependent variables from the <code>s_1.variables</code></li> <li>Gathers the values of those variables from <code>s_1.experiment_data</code></li> <li>Passes those values to the <code>LinearRegression().fit(x, y)</code> method</li> <li>Constructs <code>Delta(models=[LinearRegression()])</code> with the fitted regressor</li> <li>Returns <code>s_1 + Delta(models=[LinearRegression()])</code></li> </ul>"},{"location":"core/docs/The%20State%20Mechanism/#the-state-mechanism","title":"The <code>State</code> mechanism\u00b6","text":""},{"location":"core/docs/The%20State%20Mechanism/#core-principle-every-procedure-accepts-a-state-and-returns-a-state","title":"Core Principle: every procedure accepts a <code>State</code> and returns a <code>State</code>\u00b6","text":"<p>The AutoRA <code>State</code> mechanism is an implementation of the functional programming paradigm. It distinguishes between:</p> <ul> <li>Data \u2013 stored as an immutable <code>State</code></li> <li>Procedures \u2013 functions which act on <code>State</code> objects to add new data and return a new <code>State</code>.</li> </ul> <p>Procedures generate data. Some common procedures which appear in AutoRA experiments, and the data they produce are:</p> Procedure Data Experimentalist Conditions Experiment Runner Experiment Data Theorist Model <p>The data produced by each procedure $f$ can be seen as additions to the existing data. Each procedure $f$:</p> <ul> <li>Takes in existing Data in a <code>State</code> $S$</li> <li>Adds new data $\\Delta S$</li> <li>Returns an updated <code>State</code> $S^\\prime$</li> </ul> <p>$$ \\begin{aligned} f(S) &amp;= S + \\Delta S \\\\      &amp;= S^\\prime \\end{aligned} $$</p> <p>AutoRA includes:</p> <ul> <li>Classes to represent the Data $S$ \u2013 the <code>State</code> object (and the derived <code>StandardState</code> \u2013 a pre-defined version with the common fields needed for cyclical experiments)</li> <li>Functions to make it easier to write procedures of the form $f(S) = S^\\prime$</li> </ul>"},{"location":"core/docs/The%20State%20Mechanism/#state-objects","title":"<code>State</code> objects\u00b6","text":"<p><code>State</code> objects contain metadata describing an experiment, and the data gathered during an experiment. Any <code>State</code> object used in an AutoRA cycle will be a subclass of the <code>autora.state.State</code>, with the necessary fields specified. (The <code>autora.state.StandardState</code> provides some sensible defaults.)</p>"},{"location":"core/docs/The%20State%20Mechanism/#standardstate","title":"<code>StandardState</code>\u00b6","text":"<p>For typical AutoRA experiments, you can use the <code>autora.state.StandardState</code> object, which has fields for variables, conditions, experiment data and models. You can initialize a <code>StandardState</code> object like this:</p>"},{"location":"core/docs/The%20State%20Mechanism/#making-a-function-of-the-correct-form","title":"Making a function of the correct form\u00b6","text":"<p>There are several equivalent ways to make a function of the form $f(S) = S^\\prime$. These are (from simplest but most restrictive, to most complex but with the greatest flexibility):</p> <ul> <li>Use the <code>autora.state.on_state</code> decorator</li> <li>Modify <code>generate_conditions</code> to accept a <code>StandardState</code> and update this with a <code>Delta</code></li> </ul> <p>There are also special cases, like the <code>autora.state.estimator_on_state</code> wrapper for <code>scikit-learn</code> estimators.</p> <p>Say you have a function to generate new experimental conditions, given some variables.</p>"},{"location":"core/docs/The%20State%20Mechanism/#use-the-autorastateon_state-decorator","title":"Use the <code>autora.state.on_state</code> decorator\u00b6","text":"<p><code>autora.state.on_state</code> is a wrapper for functions which allows them to accept <code>State</code> objects as the first argument.</p> <p>The most concise way to use it is as a decorator on the function where it is defined. You can specify how the returned values should be mapped to fields on the <code>State</code> using the <code>@autora.state.on_state(output=...)</code> argument.</p>"},{"location":"core/docs/The%20State%20Mechanism/#deep-dive-autorastate_on_state","title":"Deep dive: <code>autora.state_on_state</code>\u00b6","text":"<p>The decorator notation is equivalent to the following:</p>"},{"location":"core/docs/The%20State%20Mechanism/#modify-generate_conditions-to-accept-a-standardstate-and-update-this-with-a-delta","title":"Modify <code>generate_conditions</code> to accept a <code>StandardState</code> and update this with a <code>Delta</code>\u00b6","text":""},{"location":"core/docs/The%20State%20Mechanism/#special-case-autorastateestimator_on_state-for-scikit-learn-estimators","title":"Special case: <code>autora.state.estimator_on_state</code> for <code>scikit-learn</code> estimators\u00b6","text":"<p>The \"theorist\" component in an AutoRA cycle is often a <code>scikit-learn</code> compatible estimator which implements a curve fitting function like a linear, logistic or symbolic regression. <code>scikit-learn</code> estimators are classes, and they have a specific wrapper: <code>autora.state.estimator_on_state</code>, used as follows:</p>"},{"location":"core/docs/Variable/","title":"AutoRA Variables","text":"In\u00a0[\u00a0]: Copied! <pre>from autora.variable import Variable\n\nx1 = Variable(\n    name=\"x1\",\n)\nx2 = Variable(\n    name=\"x2\",\n)\ny = Variable(\n    name=\"y\",\n)\n</pre> from autora.variable import Variable  x1 = Variable(     name=\"x1\", ) x2 = Variable(     name=\"x2\", ) y = Variable(     name=\"y\", ) <p>A group of <code>Variables</code> representing the domain of an experiment is a <code>autora.variable.VariableCollection</code>.</p> <p>They can be initialized as follows:</p> In\u00a0[\u00a0]: Copied! <pre>from autora.variable import VariableCollection\n\nVariableCollection(\n    independent_variables=[x1, x2],\n    dependent_variables=[y]\n)\n</pre> from autora.variable import VariableCollection  VariableCollection(     independent_variables=[x1, x2],     dependent_variables=[y] ) Out[\u00a0]: <pre>VariableCollection(independent_variables=[Variable(name='x1', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False), Variable(name='x2', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[])</pre> <p>For the full list of arguments, see the documentation in the <code>autora.variable</code> submodule.</p> <p>Some functions included in AutoRA use specific values stored on the Variable objects. For instance, the <code>autora.experimentalist.grid.pool</code> uses the <code>allowed_values</code> field to create a grid of conditions:</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.grid import grid_pool\n\ngrid_pool(\n    VariableCollection(independent_variables=[\n        Variable(name=\"x1\", allowed_values=[-1, -2, -3]),\n        Variable(name=\"x2\", allowed_values=[11, 12, 13])\n    ])\n)\n</pre> from autora.experimentalist.grid import grid_pool  grid_pool(     VariableCollection(independent_variables=[         Variable(name=\"x1\", allowed_values=[-1, -2, -3]),         Variable(name=\"x2\", allowed_values=[11, 12, 13])     ]) ) Out[\u00a0]: x1 x2 0 -1 11 1 -1 12 2 -1 13 3 -2 11 4 -2 12 5 -2 13 6 -3 11 7 -3 12 8 -3 13 <p>The <code>autora.experimentalist.random.pool</code> uses the <code>value_range</code> field to sample conditions:</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\n\nrandom_pool(\n    VariableCollection(independent_variables=[\n        Variable(name=\"x1\", value_range=(-3, 3)),\n        Variable(name=\"x2\", value_range=(101, 102))\n    ]), \n    random_state=180\n)\n</pre> from autora.experimentalist.random import random_pool  random_pool(     VariableCollection(independent_variables=[         Variable(name=\"x1\", value_range=(-3, 3)),         Variable(name=\"x2\", value_range=(101, 102))     ]),      random_state=180 ) Out[\u00a0]: x1 x2 0 0.456338 101.527294 1 1.008636 101.297280 2 0.319617 101.962166 3 -1.753273 101.859696 4 -1.933420 101.201565 <p>The <code>autora.state.estimator_from_state</code> function uses the <code>names</code> of the variables to pass the correct columns to a <code>scikit-learn</code> compatible estimator for curve fitting.</p> <p>Check the documentation for any functions you are using to determine whether you need to include specific metadata.</p>"},{"location":"core/docs/Variable/#autoravariable-variable-and-variablecollection","title":"<code>autora.variable</code>: <code>Variable</code> and <code>VariableCollection</code>\u00b6","text":"<p><code>autora.variable.Variable</code> represents an experimental variable:</p> <ul> <li>an independent variable, or</li> <li>dependent variable.</li> </ul> <p>They can be initialized as follows:</p>"},{"location":"core/docs/cli/","title":"Command Line Interface","text":"<p>Different parts of AutoRA experiments can require very different computational resources. For instance:</p> <ul> <li>The theorist and experimentalist might require training or use of neural networks, and benefit from high    performance computing (HPC) resources for short bursts \u2013 minutes or hours.</li> <li>The experiment runner might post an experiment using a service like \"Prolific\" and poll every few minutes for    hours, days or week until the experimental data are gathered.</li> </ul> <p>Running the experiment runner with the same resources as the theorist and experimentalist in this case would be  wasteful, and may be prohibitively expensive.</p> <p>To solve this problem, AutoRA comes with a command line interface (CLI). This can be used with HPC schedulers like  SLURM to run different steps in the cycle with different resources.</p> <p>You can use the CLI if the following conditions are true:</p> <ol> <li>Every part of <code>s</code> can be successfully pickled.</li> <li>You can write each step of your experiment as a single importable function which operates on a state and returns     a state:     <pre><code>from example.lib import initial_state, experimentalist, experiment_runner, theorist\ns = initial_state()\nfor i in range(3):\n    s = experimentalist(s)\n    s = experiment_runner(s)\n    s = theorist(s)\n</code></pre></li> </ol> <p>Contents of this section:</p> <ul> <li>\"Basic Usage\": Basic usage of the CLI</li> <li>\"Usage with Cylc workflow manager\": Example using the Cylc workflow manager (which    handles cyclical processes)</li> <li>\"Usage with Cylc workflow manager and Slurm\": Example using the Cylc    workflow manager and the SLURM scheduler with different resources for each step in the cycle.    </li> </ul>"},{"location":"core/docs/cli/basic-usage/","title":"Command Line Interface Overview","text":"<p>The command line interface allows us to load and save <code>States</code> and run arbitrary functions on them.</p>"},{"location":"core/docs/cli/basic-usage/#setup","title":"Setup","text":"<p>To use the command line, we first define a package <code>example</code> containing the functions we want to run on the State:</p> example/lib.py<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.state import StandardState, estimator_on_state, on_state\nfrom autora.variable import Variable, VariableCollection\n\nrng = np.random.default_rng()\n\n\ndef initial_state(_):\n    state = StandardState(\n        variables=VariableCollection(\n            independent_variables=[\n                Variable(name=\"x\", allowed_values=np.linspace(-10, +10, 1001))\n            ],\n            dependent_variables=[Variable(name=\"y\")],\n            covariates=[],\n        ),\n        conditions=None,\n        experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),\n        models=[],\n    )\n    return state\n\n\n@on_state(output=[\"conditions\"])\ndef experimentalist(variables):\n    conditions: pd.DataFrame = grid_pool(variables)\n    selected_conditions = conditions.sample(10, random_state=rng)\n    return selected_conditions\n\n\ncoefs = [2.0, 3.0, 1.0]\nnoise_std = 10.0\n\n\ndef ground_truth(x, coefs_=coefs):\n    return coefs_[0] * x**2.0 + coefs_[1] * x + coefs_[2]\n\n\n@on_state(output=[\"experiment_data\"])\ndef experiment_runner(conditions, coefs_=coefs, noise_std_=noise_std, rng=rng):\n    experiment_data = conditions.assign(\n        y=(\n            ground_truth(conditions[\"x\"], coefs_=coefs_)\n            + rng.normal(0.0, noise_std_, size=conditions[\"x\"].shape)\n        )\n    )\n    return experiment_data\n\n\ntheorist = estimator_on_state(\n    GridSearchCV(\n        make_pipeline(PolynomialFeatures(), LinearRegression()),\n        param_grid={\"polynomialfeatures__degree\": [0, 1, 2, 3, 4]},\n        scoring=\"r2\",\n    )\n)\n</code></pre> example/__init__.py<pre><code># This __init__.py file is turns the `example` directory into a python package, allowing\n# the other files plot.py and lib.py to import functions and variables from each other.\n</code></pre> <p>We can run the pipeline of initialization, condition generation, experiment and theory building as follows.</p> <p>First we create an initial state file:</p> <pre><code>python -m autora.workflow example.lib.initial_state --out-path initial.pkl\n</code></pre> <p>Next we run the condition generation:</p> <pre><code>python -m autora.workflow example.lib.experimentalist --in-path initial.pkl --out-path conditions.pkl\n</code></pre> <p>We run the experiment:</p> <pre><code>python -m autora.workflow example.lib.experiment_runner --in-path conditions.pkl --out-path experiment_data.pkl\n</code></pre> <p>And then the theorist:</p> <pre><code>python -m autora.workflow example.lib.theorist --in-path experiment_data.pkl --out-path model.pkl\n</code></pre> <p>We can interrogate the results by loading them into the current session.</p> <pre><code>#!/usr/bin/env python\nfrom autora.workflow.__main__ import load_state\nstate = load_state(\"model.pkl\")\nprint(state)\n# state = \n# StandardState(\n#     variables=VariableCollection(\n#         independent_variables=[\n#             Variable(name='x',\n#                      value_range=None, \n#                      allowed_values=array([-10.  ,  -9.98,  -9.96, ...,   9.96,   9.98,  10.  ]), \n#                      units='', \n#                      type=&lt;ValueType.REAL: 'real'&gt;, \n#                      variable_label='', \n#                      rescale=1, \n#                      is_covariate=False)\n#             ], \n#         dependent_variables=[\n#             Variable(name='y', \n#                      value_range=None, \n#                      allowed_values=None, \n#                      units='', \n#                      type=&lt;ValueType.REAL: 'real'&gt;, \n#                      variable_label='', \n#                      rescale=1, \n#                      is_covariate=False)\n#         ], \n#         covariates=[]\n#     ), \n#     conditions=        x\n#                342 -3.16\n#                869  7.38\n#                732  4.64\n#                387 -2.26\n#                919  8.38\n#                949  8.98\n#                539  0.78\n#                563  1.26\n#                855  7.10\n#                772  5.44, \n#     experiment_data=      x           y\n#                     0 -3.16    1.257587\n#                     1  7.38  153.259915\n#                     2  4.64   54.291348\n#                     3 -2.26   10.374509\n#                     4  8.38  155.483778\n#                     5  8.98  183.774472\n#                     6  0.78    3.154024\n#                     7  1.26   14.033608\n#                     8  7.10  103.032008\n#                     9  5.44   94.629911, \n#     models=[\n#         GridSearchCV(\n#             estimator=Pipeline(steps=[\n#                 ('polynomialfeatures', PolynomialFeatures()),\n#                 ('linearregression', LinearRegression())]),\n#             param_grid={'polynomialfeatures__degree': [0, 1, 2, 3, 4]}, \n#             scoring='r2'\n#         )\n#     ]\n# )\n</code></pre> <p>For instance, we can plot the results. We define another script in the <code>example</code> package:</p> example/plot.py<pre><code>#!/usr/bin/env python\nimport pathlib\n\nimport numpy as np\nimport pandas as pd\nimport typer\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\n\nfrom autora.state import StandardState\nfrom autora.workflow.__main__ import load_state\n\nfrom .lib import ground_truth, noise_std\n\n\ndef plot_results(state: StandardState):\n    x = np.linspace(-10, 10, 100).reshape((-1, 1))\n    plt.plot(x, ground_truth(x), label=\"ground_truth\", c=\"orange\")\n    plt.fill_between(\n        x.flatten(),\n        ground_truth(x).flatten() + noise_std,\n        ground_truth(x).flatten() - noise_std,\n        alpha=0.3,\n        color=\"orange\",\n    )\n\n    assert isinstance(state.experiment_data, pd.DataFrame)\n    xi, yi = state.experiment_data[\"x\"], state.experiment_data[\"y\"]\n    plt.scatter(xi, yi, label=\"observations\")\n\n    assert isinstance(state.models[-1], GridSearchCV)\n    plt.plot(x, state.models[-1].predict(x), label=\"model\")\n\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    plt.legend()\n    plt.show()\n\n\ndef main(filename: pathlib.Path):\n    state = load_state(filename)\n    assert isinstance(state, StandardState)\n    plot_results(state)\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <p>... and invoke it on the command line:</p> <pre><code>python -m example.plot model.pkl\n</code></pre> <p></p> <p>If we instead run the experiment for 4 cycles, we can get results closer to the ground truth.</p> <pre><code>set -x  # echo each command \n\npython -m autora.workflow example.lib.initial_state --out-path \"result.pkl\"\n\nfor i in {1..4}\ndo\n    python -m autora.workflow example.lib.experimentalist --in-path \"result.pkl\" --out-path \"result.pkl\"\n    python -m autora.workflow example.lib.experiment_runner --in-path \"result.pkl\" --out-path \"result.pkl\"\n    python -m autora.workflow example.lib.theorist --in-path \"result.pkl\" --out-path \"result.pkl\"\ndone\n\npython example.plot result.pkl\n</code></pre> <p></p>"},{"location":"core/docs/cli/basic-usage/example/__init__/","title":"init","text":"<p>This init.py file is turns the <code>example</code> directory into a python package, allowing the other files plot.py and lib.py to import functions and variables from each other.</p>"},{"location":"core/docs/cli/basic-usage/example/lib/","title":"Lib","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n</pre> import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.model_selection import GridSearchCV from sklearn.pipeline import make_pipeline from sklearn.preprocessing import PolynomialFeatures In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.grid import grid_pool\nfrom autora.state import StandardState, estimator_on_state, on_state\nfrom autora.variable import Variable, VariableCollection\n</pre> from autora.experimentalist.grid import grid_pool from autora.state import StandardState, estimator_on_state, on_state from autora.variable import Variable, VariableCollection In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng()\n</pre> rng = np.random.default_rng() In\u00a0[\u00a0]: Copied! <pre>def initial_state(_):\n    state = StandardState(\n        variables=VariableCollection(\n            independent_variables=[\n                Variable(name=\"x\", allowed_values=np.linspace(-10, +10, 1001))\n            ],\n            dependent_variables=[Variable(name=\"y\")],\n            covariates=[],\n        ),\n        conditions=None,\n        experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),\n        models=[],\n    )\n    return state\n</pre> def initial_state(_):     state = StandardState(         variables=VariableCollection(             independent_variables=[                 Variable(name=\"x\", allowed_values=np.linspace(-10, +10, 1001))             ],             dependent_variables=[Variable(name=\"y\")],             covariates=[],         ),         conditions=None,         experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),         models=[],     )     return state In\u00a0[\u00a0]: Copied! <pre>@on_state(output=[\"conditions\"])\ndef experimentalist(variables):\n    conditions: pd.DataFrame = grid_pool(variables)\n    selected_conditions = conditions.sample(10, random_state=rng)\n    return selected_conditions\n</pre> @on_state(output=[\"conditions\"]) def experimentalist(variables):     conditions: pd.DataFrame = grid_pool(variables)     selected_conditions = conditions.sample(10, random_state=rng)     return selected_conditions In\u00a0[\u00a0]: Copied! <pre>coefs = [2.0, 3.0, 1.0]\nnoise_std = 10.0\n</pre> coefs = [2.0, 3.0, 1.0] noise_std = 10.0 In\u00a0[\u00a0]: Copied! <pre>def ground_truth(x, coefs_=coefs):\n    return coefs_[0] * x**2.0 + coefs_[1] * x + coefs_[2]\n</pre> def ground_truth(x, coefs_=coefs):     return coefs_[0] * x**2.0 + coefs_[1] * x + coefs_[2] In\u00a0[\u00a0]: Copied! <pre>@on_state(output=[\"experiment_data\"])\ndef experiment_runner(conditions, coefs_=coefs, noise_std_=noise_std, rng=rng):\n    experiment_data = conditions.assign(\n        y=(\n            ground_truth(conditions[\"x\"], coefs_=coefs_)\n            + rng.normal(0.0, noise_std_, size=conditions[\"x\"].shape)\n        )\n    )\n    return experiment_data\n</pre> @on_state(output=[\"experiment_data\"]) def experiment_runner(conditions, coefs_=coefs, noise_std_=noise_std, rng=rng):     experiment_data = conditions.assign(         y=(             ground_truth(conditions[\"x\"], coefs_=coefs_)             + rng.normal(0.0, noise_std_, size=conditions[\"x\"].shape)         )     )     return experiment_data In\u00a0[\u00a0]: Copied! <pre>theorist = estimator_on_state(\n    GridSearchCV(\n        make_pipeline(PolynomialFeatures(), LinearRegression()),\n        param_grid={\"polynomialfeatures__degree\": [0, 1, 2, 3, 4]},\n        scoring=\"r2\",\n    )\n)\n</pre> theorist = estimator_on_state(     GridSearchCV(         make_pipeline(PolynomialFeatures(), LinearRegression()),         param_grid={\"polynomialfeatures__degree\": [0, 1, 2, 3, 4]},         scoring=\"r2\",     ) )"},{"location":"core/docs/cli/basic-usage/example/plot/","title":"Plot","text":"In\u00a0[\u00a0]: Copied! <pre>import pathlib\n</pre> import pathlib In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport typer\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\n</pre> import numpy as np import pandas as pd import typer from matplotlib import pyplot as plt from sklearn.model_selection import GridSearchCV In\u00a0[\u00a0]: Copied! <pre>from autora.state import StandardState\nfrom autora.workflow.__main__ import load_state\n</pre> from autora.state import StandardState from autora.workflow.__main__ import load_state In\u00a0[\u00a0]: Copied! <pre>from .lib import ground_truth, noise_std\n</pre> from .lib import ground_truth, noise_std In\u00a0[\u00a0]: Copied! <pre>def plot_results(state: StandardState):\n    x = np.linspace(-10, 10, 100).reshape((-1, 1))\n    plt.plot(x, ground_truth(x), label=\"ground_truth\", c=\"orange\")\n    plt.fill_between(\n        x.flatten(),\n        ground_truth(x).flatten() + noise_std,\n        ground_truth(x).flatten() - noise_std,\n        alpha=0.3,\n        color=\"orange\",\n    )\n\n    assert isinstance(state.experiment_data, pd.DataFrame)\n    xi, yi = state.experiment_data[\"x\"], state.experiment_data[\"y\"]\n    plt.scatter(xi, yi, label=\"observations\")\n\n    assert isinstance(state.models[-1], GridSearchCV)\n    plt.plot(x, state.models[-1].predict(x), label=\"model\")\n\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    plt.legend()\n    plt.show()\n</pre> def plot_results(state: StandardState):     x = np.linspace(-10, 10, 100).reshape((-1, 1))     plt.plot(x, ground_truth(x), label=\"ground_truth\", c=\"orange\")     plt.fill_between(         x.flatten(),         ground_truth(x).flatten() + noise_std,         ground_truth(x).flatten() - noise_std,         alpha=0.3,         color=\"orange\",     )      assert isinstance(state.experiment_data, pd.DataFrame)     xi, yi = state.experiment_data[\"x\"], state.experiment_data[\"y\"]     plt.scatter(xi, yi, label=\"observations\")      assert isinstance(state.models[-1], GridSearchCV)     plt.plot(x, state.models[-1].predict(x), label=\"model\")      plt.xlabel(\"x\")     plt.ylabel(\"y\")      plt.legend()     plt.show() In\u00a0[\u00a0]: Copied! <pre>def main(filename: pathlib.Path):\n    state = load_state(filename)\n    assert isinstance(state, StandardState)\n    plot_results(state)\n</pre> def main(filename: pathlib.Path):     state = load_state(filename)     assert isinstance(state, StandardState)     plot_results(state) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    typer.run(main)\n</pre> if __name__ == \"__main__\":     typer.run(main)"},{"location":"core/docs/cli/cylc-pip/","title":"Usage with Cylc workflow manager","text":"<p>The command line interface can be used with workflow managers like cylc in virtualenv environments.</p> <p>Note</p> <p>This page covers basic usage of cylc. For usage with Slurm, see cylc-slurm-pip </p>"},{"location":"core/docs/cli/cylc-pip/#prerequisites","title":"Prerequisites","text":"<p>This example requires:</p> <ul> <li>familiarity with and a working installation of <code>cylc</code> (e.g. by going through the   tutorial)</li> <li><code>virtualenv</code></li> <li><code>python3.8</code> (so you can run <code>virtualenv venv -p python3.8</code>)</li> </ul> <p>A new environment will be created during the setup phase of the <code>cylc</code> workflow run.</p>"},{"location":"core/docs/cli/cylc-pip/#setup","title":"Setup","text":"<p>To initialize the workflow, we define a file in the<code>lib/python</code> directory  (a cylc convention) with the code for the experiment:  <code>lib/python/components.py</code>, including all the required functions. </p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.state import StandardState, estimator_on_state, on_state\nfrom autora.variable import Variable, VariableCollection\n\n\ndef initial_state(_):\n    state = StandardState(\n        variables=VariableCollection(\n            independent_variables=[Variable(name=\"x\", allowed_values=range(100))],\n            dependent_variables=[Variable(name=\"y\")],\n            covariates=[],\n        ),\n        conditions=None,\n        experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),\n        models=[],\n    )\n    return state\n\n\nexperimentalist = on_state(grid_pool, output=[\"conditions\"])\n\nexperiment_runner = on_state(\n    lambda conditions: conditions.assign(y=2 * conditions[\"x\"] + 0.5),\n    output=[\"experiment_data\"],\n)\n\ntheorist = estimator_on_state(LinearRegression(fit_intercept=True))\n</code></pre> <p>These functions will be called in turn by the <code>autora.workflow</code> script.</p> <p>The <code>flow.cylc</code> file defines the workflow.</p> <pre><code>[scheduling]\n    cycling mode = integer\n    initial cycle point = 0\n    final cycle point = 5\n    [[graph]]\n        R1/0 = \"\"\"\n        setup_python =&gt; initial_state\n        \"\"\"\n        R1/1 = \"\"\"\n            initial_state[^] =&gt; experimentalist =&gt; experiment_runner =&gt; theorist\n        \"\"\"\n        2/P1 = \"\"\"\n            theorist[-P1] =&gt; experimentalist =&gt; experiment_runner =&gt; theorist\n        \"\"\"\n\n[runtime]\n    [[setup_python]]\n        script = \"\"\"\n            virtualenv \"$CYLC_WORKFLOW_SHARE_DIR/env\" -p python3.8\n            source \"$CYLC_WORKFLOW_SHARE_DIR/env/bin/activate\"\n            pip install --upgrade pip\n            pip install -r \"$CYLC_WORKFLOW_RUN_DIR/requirements.txt\"\n        \"\"\"\n    [[initial_state]]\n    script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python \\\n                -m autora.workflow \\\n                components.initial_state \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/result\"\n        \"\"\"\n    [[experimentalist]]\n        script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python -m autora.workflow \\\n                components.experimentalist \\\n                --in-path \"$CYLC_WORKFLOW_SHARE_DIR/$((CYLC_TASK_CYCLE_POINT - 1))/result\" \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/conditions\"\n        \"\"\"\n    [[experiment_runner]]\n        script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python -m autora.workflow \\\n                components.experiment_runner \\\n                --in-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/conditions\" \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/data\"\n        \"\"\"\n    [[theorist]]\n        script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python -m autora.workflow \\\n                components.theorist \\\n                --in-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/data\" \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/result\"\n        \"\"\"\n</code></pre> <p>Note that the first step \u2013 <code>setup_python</code> \u2013 initializes a new virtual environment for python, using the requirements  file. In this example, we require the following requirements, but yours will likely be different:</p> <pre><code>autora-core&gt;=4.0.0\nautora-workflow&gt;=v0.5.0\ncylc-flow==8.1.4\ncylc-uiserver==1.2.2\n</code></pre>"},{"location":"core/docs/cli/cylc-pip/#execution","title":"Execution","text":"<p>We can call the <code>cylc</code> command line interface as follows, in a shell session:</p> <p>First, we validate the <code>flow.cylc</code> file: <pre><code>cylc validate .\n</code></pre></p> <p>We install the workflow: <pre><code>cylc install .\n</code></pre></p> <p>We tell cylc to play the workflow: <pre><code>cylc play \"cylc-pip\"\n</code></pre></p> <p>(As a shortcut for \"validate, install and play\", use <code>cylc vip .</code>)</p> <p>We can view the workflow running in the graphical user interface (GUI): <pre><code>cylc gui\n</code></pre></p> <p>... or the text user interface (TUI): <pre><code>cylc tui \"cylc-pip\"\n</code></pre></p>"},{"location":"core/docs/cli/cylc-pip/#results","title":"Results","text":"<p>We can load and interrogate the results as follows:</p> <pre><code>from autora.serializer import load_state\n\nstate = load_state(\"~/cylc-run/cylc-pip/runN/share/result\")\nprint(state)\n</code></pre>"},{"location":"core/docs/cli/cylc-pip/lib/python/components/","title":"Components","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n</pre> import pandas as pd from sklearn.linear_model import LinearRegression In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.grid import grid_pool\nfrom autora.state import StandardState, estimator_on_state, on_state\nfrom autora.variable import Variable, VariableCollection\n</pre> from autora.experimentalist.grid import grid_pool from autora.state import StandardState, estimator_on_state, on_state from autora.variable import Variable, VariableCollection In\u00a0[\u00a0]: Copied! <pre>def initial_state(_):\n    state = StandardState(\n        variables=VariableCollection(\n            independent_variables=[Variable(name=\"x\", allowed_values=range(100))],\n            dependent_variables=[Variable(name=\"y\")],\n            covariates=[],\n        ),\n        conditions=None,\n        experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),\n        models=[],\n    )\n    return state\n</pre> def initial_state(_):     state = StandardState(         variables=VariableCollection(             independent_variables=[Variable(name=\"x\", allowed_values=range(100))],             dependent_variables=[Variable(name=\"y\")],             covariates=[],         ),         conditions=None,         experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),         models=[],     )     return state In\u00a0[\u00a0]: Copied! <pre>experimentalist = on_state(grid_pool, output=[\"conditions\"])\n</pre> experimentalist = on_state(grid_pool, output=[\"conditions\"]) In\u00a0[\u00a0]: Copied! <pre>experiment_runner = on_state(\n    lambda conditions: conditions.assign(y=2 * conditions[\"x\"] + 0.5),\n    output=[\"experiment_data\"],\n)\n</pre> experiment_runner = on_state(     lambda conditions: conditions.assign(y=2 * conditions[\"x\"] + 0.5),     output=[\"experiment_data\"], ) In\u00a0[\u00a0]: Copied! <pre>theorist = estimator_on_state(LinearRegression(fit_intercept=True))\n</pre> theorist = estimator_on_state(LinearRegression(fit_intercept=True))"},{"location":"core/docs/cli/cylc-slurm-pip/","title":"Usage with Cylc workflow manager and Slurm","text":"<p>The command line interface can be used with cylc in environments which use a scheduler like slurm.</p>"},{"location":"core/docs/cli/cylc-slurm-pip/#prerequisites","title":"Prerequisites","text":"<p>This example requires:</p> <ul> <li><code>slurm</code>, e.g. on a high performance computing cluster.</li> <li>familiarity with and a working installation of <code>cylc</code> (e.g. by going through the   tutorial)</li> <li><code>virtualenv</code></li> <li><code>python</code> (so you can run <code>virtualenv venv -p python</code>)</li> </ul> <p>A new environment will be created during the setup phase of the <code>cylc</code> workflow run.</p> <p>Cylc requires a site-specific setup when using a scheduler like slurm. See the cylc documentation for a guide on setting up cylc on your platform. For Oscar at Brown University, we can use the following configuration in  <code>./global.cylc</code></p> <pre><code>[platforms]\n    [[oscar]]\n        hosts = localhost\n        install target = localhost\n        job runner = slurm\n        retrieve job logs = True\n        global init-script = \"\"\"\n            module load python/3.9.0\n        \"\"\"\n</code></pre>"},{"location":"core/docs/cli/cylc-slurm-pip/#setup","title":"Setup","text":"<p>To initialize the workflow, we define a file in the<code>lib/python</code> directory  (a cylc convention) with the code for the experiment:  <code>lib/python/runner.py</code>, including all the required functions. </p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.state import StandardState, estimator_on_state, on_state\nfrom autora.variable import Variable, VariableCollection\n\n\ndef initial_state(_):\n    state = StandardState(\n        variables=VariableCollection(\n            independent_variables=[Variable(name=\"x\", allowed_values=range(100))],\n            dependent_variables=[Variable(name=\"y\")],\n            covariates=[],\n        ),\n        conditions=None,\n        experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),\n        models=[],\n    )\n    return state\n\n\nexperimentalist = on_state(grid_pool, output=[\"conditions\"])\n\nexperiment_runner = on_state(\n    lambda conditions: conditions.assign(y=2 * conditions[\"x\"] + 0.5),\n    output=[\"experiment_data\"],\n)\n\ntheorist = estimator_on_state(LinearRegression(fit_intercept=True))\n</code></pre> <p>These functions will be called in turn by the <code>autora.workflow</code> script.</p> <p>The <code>flow.cylc</code> file defines the workflow.</p> <pre><code>[scheduling]\n    cycling mode = integer\n    initial cycle point = 0\n    final cycle point = 5\n    [[graph]]\n        R1/0 = \"\"\"\n        setup_python =&gt; initial_state\n        \"\"\"\n        R1/1 = \"\"\"\n            initial_state[^] =&gt; experimentalist =&gt; experiment_runner =&gt; theorist\n        \"\"\"\n        2/P1 = \"\"\"\n            theorist[-P1] =&gt; experimentalist =&gt; experiment_runner =&gt; theorist\n        \"\"\"\n\n[runtime]\n    [[setup_python]]\n        script = \"\"\"\n            virtualenv \"$CYLC_WORKFLOW_SHARE_DIR/env\" -p python\n            source \"$CYLC_WORKFLOW_SHARE_DIR/env/bin/activate\"\n            pip install --upgrade pip\n            pip install -r \"$CYLC_WORKFLOW_RUN_DIR/requirements.txt\"\n        \"\"\"\n        platform = oscar\n        execution time limit = PT20M\n\n    [[initial_state]]\n    script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python \\\n                -m autora.workflow \\\n                runner.initial_state \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/result\"\n        \"\"\"\n        platform = oscar\n\n    [[experimentalist]]\n        script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python -m autora.workflow \\\n                runner.experimentalist \\\n                --in-path \"$CYLC_WORKFLOW_SHARE_DIR/$((CYLC_TASK_CYCLE_POINT - 1))/result\" \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/conditions\"\n        \"\"\"\n        platform = oscar\n\n    [[experiment_runner]]\n        script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python -m autora.workflow \\\n                runner.experiment_runner \\\n                --in-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/conditions\" \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/data\"\n        \"\"\"\n        platform = oscar\n\n    [[theorist]]\n        script = \"\"\"\n            $CYLC_WORKFLOW_SHARE_DIR/env/bin/python -m autora.workflow \\\n                runner.theorist \\\n                --in-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/data\" \\\n                --out-path \"$CYLC_WORKFLOW_SHARE_DIR/$CYLC_TASK_CYCLE_POINT/result\"\n        \"\"\"\n        platform = oscar\n        execution time limit = PT1H\n        [[[directives]]]\n            --partition = gpu\n            --gres = gpu:1\n</code></pre>"},{"location":"core/docs/cli/cylc-slurm-pip/#execution","title":"Execution","text":"<p>We can call the <code>cylc</code> command line interface as follows, in a shell session:</p> <p>Validate, install and play the flow: First, we validate the <code>flow.cylc</code> file: <pre><code>cylc vip .\n</code></pre></p> <p>We can view the workflow running in the graphical user interface (GUI): <pre><code>cylc gui\n</code></pre></p>"},{"location":"core/docs/cli/cylc-slurm-pip/#results","title":"Results","text":"<p>We can load and interrogate the resulting object in Python as follows:</p> <pre><code>from autora.serializer import load_state\n\nstate = load_state(\"~/cylc-run/cylc-slurm-pip/runN/share/result\")\nprint(state)\n</code></pre>"},{"location":"core/docs/cli/cylc-slurm-pip/lib/python/runner/","title":"Runner","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\n</pre> import pandas as pd from sklearn.linear_model import LinearRegression In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.grid import grid_pool\nfrom autora.state import StandardState, estimator_on_state, on_state\nfrom autora.variable import Variable, VariableCollection\n</pre> from autora.experimentalist.grid import grid_pool from autora.state import StandardState, estimator_on_state, on_state from autora.variable import Variable, VariableCollection In\u00a0[\u00a0]: Copied! <pre>def initial_state(_):\n    state = StandardState(\n        variables=VariableCollection(\n            independent_variables=[Variable(name=\"x\", allowed_values=range(100))],\n            dependent_variables=[Variable(name=\"y\")],\n            covariates=[],\n        ),\n        conditions=None,\n        experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),\n        models=[],\n    )\n    return state\n</pre> def initial_state(_):     state = StandardState(         variables=VariableCollection(             independent_variables=[Variable(name=\"x\", allowed_values=range(100))],             dependent_variables=[Variable(name=\"y\")],             covariates=[],         ),         conditions=None,         experiment_data=pd.DataFrame({\"x\": [], \"y\": []}),         models=[],     )     return state In\u00a0[\u00a0]: Copied! <pre>experimentalist = on_state(grid_pool, output=[\"conditions\"])\n</pre> experimentalist = on_state(grid_pool, output=[\"conditions\"]) In\u00a0[\u00a0]: Copied! <pre>experiment_runner = on_state(\n    lambda conditions: conditions.assign(y=2 * conditions[\"x\"] + 0.5),\n    output=[\"experiment_data\"],\n)\n</pre> experiment_runner = on_state(     lambda conditions: conditions.assign(y=2 * conditions[\"x\"] + 0.5),     output=[\"experiment_data\"], ) In\u00a0[\u00a0]: Copied! <pre>theorist = estimator_on_state(LinearRegression(fit_intercept=True))\n</pre> theorist = estimator_on_state(LinearRegression(fit_intercept=True))"},{"location":"core/docs/cycle/Basic%20Introduction%20to%20Functions%20and%20States/","title":"Basic Usage","text":"<p>Using the functions and objects in <code>autora.state</code>, we can build flexible pipelines and cycles which operate on state objects.</p> <p>.</p> <ul> <li>A new state at some point $i+1$ is $$S_{i+1} = S_i + \\Delta S_{i+1}$$</li> <li>The cycle state after $n$ steps is thus $$S_n = S_{0} +  \\sum^{n}_{i=1} \\Delta S_{i}$$</li> </ul> <p>To represent $S$ and $\\Delta S$ in code, you can use <code>autora.state.State</code> and <code>autora.state.Delta</code> respectively. To operate on these, we define functions.</p> <ul> <li><p>Each operation in an AER cycle (theorist, experimentalist, experiment_runner, etc.) is implemented as a function with $n$ arguments $s_j$ which are members of $S$ and $m$ others $a_k$ which are not. $$ f(s_0, ..., s_n, a_0, ..., a_m) \\rightarrow \\Delta S_{i+1}$$</p> </li> <li><p>There is a wrapper function $w$ (<code>autora.state.wrap_to_use_state</code>) which changes the signature of $f$ to require $S$ and aggregates the resulting $\\Delta S_{i+1}$ $$w\\left[f(s_0, ..., s_n, a_0, ..., a_m) \\rightarrow \\Delta S_{i+1}\\right] \\rightarrow \\left[ f^\\prime(S_i, a_0, ..., a_m) \\rightarrow S_{i} + \\Delta S_{i+1} = S_{i+1}\\right]$$</p> </li> <li><p>Assuming that the other arguments $a_k$ are provided by partial evaluation of the $f^\\prime$, the full AER cycle can then be represented as: $$S_n = f_n^\\prime(...f_2^\\prime(f_1^\\prime(S_0)))$$</p> </li> </ul> <p>There are additional helper functions to wrap common experimentalists, experiment runners and theorists so that we can define a full AER cycle using python notation as shown in the following example.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.state import StandardState\nfrom autora.variable import VariableCollection, Variable\n\ns_0 = StandardState(\n    variables=VariableCollection(\n        independent_variables=[Variable(\"x\", value_range=(-10, 10))],\n        dependent_variables=[Variable(\"y\")]\n    )\n)\n</pre> from autora.state import StandardState from autora.variable import VariableCollection, Variable  s_0 = StandardState(     variables=VariableCollection(         independent_variables=[Variable(\"x\", value_range=(-10, 10))],         dependent_variables=[Variable(\"y\")]     ) ) <p>Specify the experimentalist. Use a standard function <code>random_pool</code>. This gets 5 independent random samples (by default, configurable using an argument) from the value_range of the independent variables, and returns them in a DataFrame. To make this work as a function on the State objects, we wrap it in the <code>on_state</code> function and determine the state field it will operate on, namely <code>conditions</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\nfrom autora.state import on_state\n\nexperimentalist = on_state(function=random_pool, output=[\"conditions\"])\ns_1 = experimentalist(s_0, random_state=42)\ns_1\n</pre> from autora.experimentalist.random import random_pool from autora.state import on_state  experimentalist = on_state(function=random_pool, output=[\"conditions\"]) s_1 = experimentalist(s_0, random_state=42) s_1 Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-10, 10), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.479121\n1 -1.222431\n2  7.171958\n3  3.947361\n4 -8.116453, experiment_data=None, models=[])</pre> <p>Specify the experiment runner with the state field it will operate on, namely <code>experiment_data</code>. This experiment runner calculates a linear function, adds noise, assigns the value to the <code>y</code> column in a new DataFrame.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.state import on_state\nimport numpy as np\nimport pandas as pd\n\ndef experiment_runner(conditions: pd.DataFrame, c=[2, 4], random_state = None):\n    rng = np.random.default_rng(random_state)\n    x = conditions[\"x\"]\n    noise = rng.normal(0, 1, len(x))\n    y = c[0] + (c[1] * x) + noise\n    observations = conditions.assign(y = y)\n    return observations\n\nexperiment_runner = on_state(function=experiment_runner, output=[\"experiment_data\"])\ns_2 = experiment_runner(s_1, random_state=43)\ns_2\n</pre> from autora.state import on_state import numpy as np import pandas as pd  def experiment_runner(conditions: pd.DataFrame, c=[2, 4], random_state = None):     rng = np.random.default_rng(random_state)     x = conditions[\"x\"]     noise = rng.normal(0, 1, len(x))     y = c[0] + (c[1] * x) + noise     observations = conditions.assign(y = y)     return observations  experiment_runner = on_state(function=experiment_runner, output=[\"experiment_data\"]) s_2 = experiment_runner(s_1, random_state=43) s_2 <p>Specify a theorist, using a standard LinearRegression from scikit-learn. We do not need to define the state field that the theorists will operate on - it will automatically operate on the <code>models</code> field.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\nfrom autora.state import estimator_on_state\n\ntheorist = estimator_on_state(LinearRegression(fit_intercept=True))\ns_3 = theorist(experiment_runner(experimentalist(s_2)))\ns_3\n</pre> from sklearn.linear_model import LinearRegression from autora.state import estimator_on_state  theorist = estimator_on_state(LinearRegression(fit_intercept=True)) s_3 = theorist(experiment_runner(experimentalist(s_2))) s_3 Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-10, 10), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  0.785198\n1  9.834543\n2  0.616326\n3 -4.376617\n4 -3.698967, experiment_data=          x          y\n0  5.479121  24.160713\n1 -1.222431  -2.211546\n2  7.171958  30.102304\n3  3.947361  16.880769\n4 -8.116453 -32.457650\n5  0.785198   3.193693\n6  9.834543  41.207621\n7  0.616326   3.879125\n8 -4.376617 -14.668082\n9 -3.698967 -11.416276, models=[LinearRegression()])</pre> <p>If we like, we can run the experimentalist, experiment_runner and theorist ten times.</p> In\u00a0[\u00a0]: Copied! <pre>s_ = s_0\nfor i in range(10):\n    s_ = experimentalist(s_, random_state=180+i)\n    s_ = experiment_runner(s_, random_state=2*180+i)\n    s_ = theorist(s_)\n</pre> s_ = s_0 for i in range(10):     s_ = experimentalist(s_, random_state=180+i)     s_ = experiment_runner(s_, random_state=2*180+i)     s_ = theorist(s_) <p>The experiment_data has 50 entries (10 cycles and 5 samples per cycle):</p> In\u00a0[\u00a0]: Copied! <pre>s_.experiment_data\n</pre> s_.experiment_data Out[\u00a0]: x y 0 1.521127 8.997542 1 3.362120 15.339784 2 1.065391 5.938495 3 -5.844244 -21.453802 4 -6.444732 -24.975886 5 5.724585 24.929289 6 1.781805 9.555725 7 -1.015081 -2.632280 8 2.044083 12.001204 9 7.709324 30.806166 10 -6.680454 -24.846327 11 -3.630735 -11.346701 12 -0.498322 1.794183 13 -4.043702 -15.594289 14 5.772865 25.094876 15 9.028931 37.677228 16 8.052637 34.472556 17 3.774115 16.791553 18 -8.405662 -31.734315 19 5.433506 22.975112 20 -9.644367 -36.919598 21 1.673131 7.548614 22 7.600316 32.294054 23 4.354666 20.998850 24 6.047273 26.670616 25 -5.608438 -20.570161 26 0.733890 5.029705 27 -2.781912 -9.190651 28 -2.308464 -6.179939 29 -3.547105 -12.875100 30 0.945089 6.013183 31 2.694897 14.141356 32 7.445893 31.312279 33 4.423105 19.647015 34 2.200961 11.587911 35 -4.915881 -17.061782 36 -2.997968 -10.397403 37 0.099454 4.949820 38 -3.924786 -13.532503 39 7.050950 31.085545 40 -8.077780 -31.084307 41 4.391481 17.991533 42 6.749162 30.242121 43 2.246804 10.411612 44 4.477989 19.571584 45 -0.262734 1.181040 46 -7.187250 -26.718313 47 -0.790985 0.058681 48 6.545334 27.510641 49 -7.185274 -26.510872 <p>The fitted coefficients are close to the original intercept = 2, gradient = 4</p> In\u00a0[\u00a0]: Copied! <pre>print(s_.models[-1].intercept_, s_.models[-1].coef_)\n</pre> print(s_.models[-1].intercept_, s_.models[-1].coef_)  <pre>[2.08476524] [[4.00471062]]\n</pre>"},{"location":"core/docs/cycle/Basic%20Introduction%20to%20Functions%20and%20States/#basic-introduction-to-functions-and-states","title":"Basic Introduction to Functions and States\u00b6","text":""},{"location":"core/docs/cycle/Basic%20Introduction%20to%20Functions%20and%20States/#theoretical-overview","title":"Theoretical Overview\u00b6","text":"<p>The fundamental idea is this:</p> <ul> <li>We define a \"state\" object $S$ which can be modified by components of <code>autora</code> (theorist, experimentalist, experiment_runner), $\\Delta S$.</li> <li>A new state at some point $i+1$ is $$S_{i+1} = S_i + \\Delta S_{i+1}$$</li> <li>The cycle state after $n$ steps is thus $$S_n = S_{0} +  \\sum^{n}_{i=1} \\Delta S_{i}$$</li> </ul> <p>To represent $S$ in code, you can use <code>autora.state.State</code>. To operate on these, we define functions.</p> <ul> <li><p>Each component in an <code>autora</code> cycle (theorist, experimentalist, experiment_runner, etc.) is implemented as a function with $n$ arguments $s_j$ which are members of $S$ and $m$ others $a_k$ which are not. $$ f(s_0, ..., s_n, a_0, ..., a_m) \\rightarrow \\Delta S_{i+1}$$</p> </li> <li><p>There is a wrapper function $w$ (<code>autora.state.wrap_to_use_state</code>) which changes the signature of $f$ to require $S$ and aggregates the resulting $\\Delta S_{i+1}$ $$w\\left[f(s_0, ..., s_n, a_0, ..., a_m) \\rightarrow \\Delta S_{i+1}\\right] \\rightarrow \\left[ f^\\prime(S_i, a_0, ..., a_m) \\rightarrow S_{i} + \\Delta S_{i+1} = S_{i+1}\\right]$$</p> </li> <li><p>Assuming that the other arguments $a_k$ are provided by partial evaluation of the $f^\\prime$, the full <code>autora</code> cycle can then be represented as: $$S_n = f_n^\\prime(...f_2^\\prime(f_1^\\prime(S_0)))$$</p> </li> </ul> <p>There are additional helper functions to wrap common experimentalists, experiment runners and theorists so that we can define a full <code>autora</code> cycle using python notation as shown in the following example.</p>"},{"location":"core/docs/cycle/Basic%20Introduction%20to%20Functions%20and%20States/#example","title":"Example\u00b6","text":"<p>First initialize the State. In this case, we use the pre-defined <code>StandardState</code> which implements the standard <code>autora</code> naming convention. There are two variables <code>x</code> with a range [-10, 10] and <code>y</code> with an unspecified range.</p>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/","title":"Combining Experimentalists with States","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import List, Optional\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom autora.variable import VariableCollection, Variable\n</pre> from typing import List, Optional  import numpy as np import pandas as pd from matplotlib import pyplot as plt  from autora.variable import VariableCollection, Variable In\u00a0[\u00a0]: Copied! <pre>conditions_ = pd.DataFrame({\"x1\": np.linspace(-3, 3, 7), \"x2\": np.linspace(-1, 5, 7)})\nconditions_\n</pre> conditions_ = pd.DataFrame({\"x1\": np.linspace(-3, 3, 7), \"x2\": np.linspace(-1, 5, 7)}) conditions_ Out[\u00a0]: x1 x2 0 -3.0 -1.0 1 -2.0 0.0 2 -1.0 1.0 3 0.0 2.0 4 1.0 3.0 5 2.0 4.0 6 3.0 5.0 In\u00a0[\u00a0]: Copied! <pre>def avoid_negative(conditions: pd.DataFrame):\n    downvotes = (conditions_ &lt; 0).sum(axis=1)\n    with_votes = pd.DataFrame.assign(conditions, downvotes=downvotes)\n    with_votes_sorted = with_votes.sort_values(by=\"downvotes\", ascending=True)\n    return with_votes_sorted\n\navoid_negative(conditions_)\n</pre> def avoid_negative(conditions: pd.DataFrame):     downvotes = (conditions_ &lt; 0).sum(axis=1)     with_votes = pd.DataFrame.assign(conditions, downvotes=downvotes)     with_votes_sorted = with_votes.sort_values(by=\"downvotes\", ascending=True)     return with_votes_sorted  avoid_negative(conditions_) Out[\u00a0]: x1 x2 downvotes 3 0.0 2.0 0 4 1.0 3.0 0 5 2.0 4.0 0 6 3.0 5.0 0 1 -2.0 0.0 1 2 -1.0 1.0 1 0 -3.0 -1.0 2 In\u00a0[\u00a0]: Copied! <pre>def avoid_even_function(x):\n    y = 1 - np.minimum(np.mod(x, 2), np.mod(-x, 2))\n    return y\n\nx = np.linspace(-1, 4, 101)\nplt.plot(x, avoid_even_function(x))\nplt.title(\"Avoid-even function\")\n</pre> def avoid_even_function(x):     y = 1 - np.minimum(np.mod(x, 2), np.mod(-x, 2))     return y  x = np.linspace(-1, 4, 101) plt.plot(x, avoid_even_function(x)) plt.title(\"Avoid-even function\")  Out[\u00a0]: <pre>Text(0.5, 1.0, 'Avoid-even function')</pre> In\u00a0[\u00a0]: Copied! <pre>def avoid_even(conditions: pd.DataFrame):\n    downvotes = avoid_even_function(conditions_).sum(axis=1)\n    with_votes = pd.DataFrame.assign(conditions, downvotes=downvotes)\n    with_votes_sorted = with_votes.sort_values(by=\"downvotes\", ascending=True)\n    return with_votes_sorted\n\navoid_even(conditions_)\n</pre> def avoid_even(conditions: pd.DataFrame):     downvotes = avoid_even_function(conditions_).sum(axis=1)     with_votes = pd.DataFrame.assign(conditions, downvotes=downvotes)     with_votes_sorted = with_votes.sort_values(by=\"downvotes\", ascending=True)     return with_votes_sorted  avoid_even(conditions_)  Out[\u00a0]: x1 x2 downvotes 0 -3.0 -1.0 0.0 2 -1.0 1.0 0.0 4 1.0 3.0 0.0 6 3.0 5.0 0.0 1 -2.0 0.0 2.0 3 0.0 2.0 2.0 5 2.0 4.0 2.0 In\u00a0[\u00a0]: Copied! <pre>def combine_downvotes(conditions, *arrays: pd.DataFrame):\n    result = conditions.copy()\n    for i, a in enumerate(arrays):\n        a_name = a.attrs.get(\"name\", i)\n        result[f\"{a_name}.downvotes\"] = a.downvotes\n    result[\"downvotes\"] = result.loc[:,result.columns.str.contains('.*\\.downvotes')].sum(axis=1)\n    return result\n\ncombine_downvotes(\n    conditions_,\n    conditions_.assign(downvotes=1),\n    conditions_.assign(downvotes=[0, 1, 2, 3, 4, 5, 6]).sample(frac=1)\n)\n</pre> def combine_downvotes(conditions, *arrays: pd.DataFrame):     result = conditions.copy()     for i, a in enumerate(arrays):         a_name = a.attrs.get(\"name\", i)         result[f\"{a_name}.downvotes\"] = a.downvotes     result[\"downvotes\"] = result.loc[:,result.columns.str.contains('.*\\.downvotes')].sum(axis=1)     return result  combine_downvotes(     conditions_,     conditions_.assign(downvotes=1),     conditions_.assign(downvotes=[0, 1, 2, 3, 4, 5, 6]).sample(frac=1) ) Out[\u00a0]: x1 x2 0.downvotes 1.downvotes downvotes 0 -3.0 -1.0 1 0 1 1 -2.0 0.0 1 1 2 2 -1.0 1.0 1 2 3 3 0.0 2.0 1 3 4 4 1.0 3.0 1 4 5 5 2.0 4.0 1 5 6 6 3.0 5.0 1 6 7 In\u00a0[\u00a0]: Copied! <pre>def downvote_order(conditions: pd.DataFrame, experimentalists: List):\n    downvoted_conditions = []\n    for e in experimentalists:\n        new_downvoted_conditions = e(conditions)\n        new_downvoted_conditions.attrs[\"name\"] = e.__name__\n        downvoted_conditions.append(new_downvoted_conditions)\n    result = combine_downvotes(conditions, *downvoted_conditions)\n    result = result.sort_values(by=\"downvotes\", ascending=True)\n    return result\n\ndownvote_order(conditions_, experimentalists=[])\n</pre> def downvote_order(conditions: pd.DataFrame, experimentalists: List):     downvoted_conditions = []     for e in experimentalists:         new_downvoted_conditions = e(conditions)         new_downvoted_conditions.attrs[\"name\"] = e.__name__         downvoted_conditions.append(new_downvoted_conditions)     result = combine_downvotes(conditions, *downvoted_conditions)     result = result.sort_values(by=\"downvotes\", ascending=True)     return result  downvote_order(conditions_, experimentalists=[]) Out[\u00a0]: x1 x2 downvotes 0 -3.0 -1.0 0.0 1 -2.0 0.0 0.0 2 -1.0 1.0 0.0 3 0.0 2.0 0.0 4 1.0 3.0 0.0 5 2.0 4.0 0.0 6 3.0 5.0 0.0 In\u00a0[\u00a0]: Copied! <pre>downvote_order(conditions_, experimentalists=[avoid_negative])\n</pre> downvote_order(conditions_, experimentalists=[avoid_negative]) Out[\u00a0]: x1 x2 avoid_negative.downvotes downvotes 3 0.0 2.0 0 0 4 1.0 3.0 0 0 5 2.0 4.0 0 0 6 3.0 5.0 0 0 1 -2.0 0.0 1 1 2 -1.0 1.0 1 1 0 -3.0 -1.0 2 2 In\u00a0[\u00a0]: Copied! <pre>downvote_order(conditions_, experimentalists=[avoid_negative, avoid_even])\n</pre> downvote_order(conditions_, experimentalists=[avoid_negative, avoid_even])  Out[\u00a0]: x1 x2 avoid_negative.downvotes avoid_even.downvotes downvotes 4 1.0 3.0 0 0.0 0.0 6 3.0 5.0 0 0.0 0.0 2 -1.0 1.0 1 0.0 1.0 0 -3.0 -1.0 2 0.0 2.0 3 0.0 2.0 0 2.0 2.0 5 2.0 4.0 0 2.0 2.0 1 -2.0 0.0 1 2.0 3.0 <p>Adding this dataframe to a State object:</p> In\u00a0[\u00a0]: Copied! <pre>from autora.state import Delta, on_state, State, StandardState, inputs_from_state\n\ns = StandardState() + Delta(conditions=downvote_order(conditions_, experimentalists=[avoid_negative, avoid_even]))\ns.conditions\n</pre> from autora.state import Delta, on_state, State, StandardState, inputs_from_state  s = StandardState() + Delta(conditions=downvote_order(conditions_, experimentalists=[avoid_negative, avoid_even])) s.conditions Out[\u00a0]: x1 x2 avoid_negative.downvotes avoid_even.downvotes downvotes 4 1.0 3.0 0 0.0 0.0 6 3.0 5.0 0 0.0 0.0 2 -1.0 1.0 1 0.0 1.0 0 -3.0 -1.0 2 0.0 2.0 3 0.0 2.0 0 2.0 2.0 5 2.0 4.0 0 2.0 2.0 1 -2.0 0.0 1 2.0 3.0 In\u00a0[\u00a0]: Copied! <pre>def avoid_negative_separate(conditions: pd.DataFrame):\n    downvotes = (conditions_ &lt; 0).sum(axis=1).sort_values(ascending=True)\n    conditions_sorted = pd.DataFrame(conditions, index=downvotes.index)\n    return {\"conditions\": conditions_sorted, \"downvotes\": downvotes}\n\navoid_negative_separate(conditions_)[\"conditions\"]\n</pre> def avoid_negative_separate(conditions: pd.DataFrame):     downvotes = (conditions_ &lt; 0).sum(axis=1).sort_values(ascending=True)     conditions_sorted = pd.DataFrame(conditions, index=downvotes.index)     return {\"conditions\": conditions_sorted, \"downvotes\": downvotes}  avoid_negative_separate(conditions_)[\"conditions\"] Out[\u00a0]: x1 x2 3 0.0 2.0 4 1.0 3.0 5 2.0 4.0 6 3.0 5.0 1 -2.0 0.0 2 -1.0 1.0 0 -3.0 -1.0 In\u00a0[\u00a0]: Copied! <pre>def avoid_even_separate(conditions: pd.DataFrame):\n    downvotes = avoid_even_function(conditions_).sum(axis=1).sort_values(ascending=True)\n    conditions_sorted = pd.DataFrame(conditions, index=downvotes.index)\n    return {\"conditions\": conditions_sorted, \"downvotes\": downvotes}\n\navoid_even_separate(conditions_)[\"conditions\"], avoid_even_separate(conditions_)[\"downvotes\"]\n</pre> def avoid_even_separate(conditions: pd.DataFrame):     downvotes = avoid_even_function(conditions_).sum(axis=1).sort_values(ascending=True)     conditions_sorted = pd.DataFrame(conditions, index=downvotes.index)     return {\"conditions\": conditions_sorted, \"downvotes\": downvotes}  avoid_even_separate(conditions_)[\"conditions\"], avoid_even_separate(conditions_)[\"downvotes\"] Out[\u00a0]: <pre>(    x1   x2\n 0 -3.0 -1.0\n 2 -1.0  1.0\n 4  1.0  3.0\n 6  3.0  5.0\n 1 -2.0  0.0\n 3  0.0  2.0\n 5  2.0  4.0,\n 0    0.0\n 2    0.0\n 4    0.0\n 6    0.0\n 1    2.0\n 3    2.0\n 5    2.0\n dtype: float64)</pre> In\u00a0[\u00a0]: Copied! <pre>def downvote_order_separate(conditions: pd.DataFrame, experimentalists: List):\n    downvote_arrays = {\"initial\": pd.Series(0, index=conditions.index)}\n    for e in experimentalists:\n        downvote_arrays[e.__name__] = e(conditions)[\"downvotes\"]\n    combined_downvotes = pd.DataFrame(downvote_arrays)\n    combined_downvotes[\"total\"] = combined_downvotes.sum(axis=1)\n    combined_downvotes_sorted = combined_downvotes.sort_values(by=\"total\", ascending=True)\n    conditions_sorted = pd.DataFrame(conditions, index=combined_downvotes_sorted.index)\n    return {\"conditions\": conditions_sorted, \"downvotes\": combined_downvotes_sorted}\n\ndownvote_order_separate(conditions_, experimentalists=[])\n</pre> def downvote_order_separate(conditions: pd.DataFrame, experimentalists: List):     downvote_arrays = {\"initial\": pd.Series(0, index=conditions.index)}     for e in experimentalists:         downvote_arrays[e.__name__] = e(conditions)[\"downvotes\"]     combined_downvotes = pd.DataFrame(downvote_arrays)     combined_downvotes[\"total\"] = combined_downvotes.sum(axis=1)     combined_downvotes_sorted = combined_downvotes.sort_values(by=\"total\", ascending=True)     conditions_sorted = pd.DataFrame(conditions, index=combined_downvotes_sorted.index)     return {\"conditions\": conditions_sorted, \"downvotes\": combined_downvotes_sorted}  downvote_order_separate(conditions_, experimentalists=[]) Out[\u00a0]: <pre>{'conditions':     x1   x2\n 0 -3.0 -1.0\n 1 -2.0  0.0\n 2 -1.0  1.0\n 3  0.0  2.0\n 4  1.0  3.0\n 5  2.0  4.0\n 6  3.0  5.0,\n 'downvotes':    initial  total\n 0        0      0\n 1        0      0\n 2        0      0\n 3        0      0\n 4        0      0\n 5        0      0\n 6        0      0}</pre> In\u00a0[\u00a0]: Copied! <pre>results = downvote_order_separate(conditions_, experimentalists=[avoid_even_separate, avoid_negative_separate])\n\npd.DataFrame.join(results[\"conditions\"], results[\"downvotes\"]).sort_index()\n</pre> results = downvote_order_separate(conditions_, experimentalists=[avoid_even_separate, avoid_negative_separate])  pd.DataFrame.join(results[\"conditions\"], results[\"downvotes\"]).sort_index() Out[\u00a0]: x1 x2 initial avoid_even_separate avoid_negative_separate total 0 -3.0 -1.0 0 0.0 2 2.0 1 -2.0 0.0 0 2.0 1 3.0 2 -1.0 1.0 0 0.0 1 1.0 3 0.0 2.0 0 2.0 0 2.0 4 1.0 3.0 0 0.0 0 0.0 5 2.0 4.0 0 2.0 0 2.0 6 3.0 5.0 0 0.0 0 0.0 In\u00a0[\u00a0]: Copied! <pre>def avoid_repeat(conditions, experiment_data: pd.DataFrame, variables: VariableCollection):\n    iv_column_names = [v.name for v in variables.independent_variables]\n    count_already_seen = pd.Series(experiment_data.groupby(iv_column_names).size(), name=\"downvotes\")\n    conditions = pd.DataFrame.join(conditions, count_already_seen, on=iv_column_names).fillna(0)\n    return {\"conditions\": conditions, \"already_seen\": count_already_seen}\n\nexperiment_data_ = pd.DataFrame(dict(x1=[-3, 3, -3], x2=[-1, 5, -1]))\nvariables_ = VariableCollection(independent_variables=[Variable(\"x1\"), Variable(\"x2\")])\n\navoid_repeat(\n    conditions=conditions_,\n    experiment_data=experiment_data_,\n    variables=variables_\n)[\"conditions\"]\n</pre> def avoid_repeat(conditions, experiment_data: pd.DataFrame, variables: VariableCollection):     iv_column_names = [v.name for v in variables.independent_variables]     count_already_seen = pd.Series(experiment_data.groupby(iv_column_names).size(), name=\"downvotes\")     conditions = pd.DataFrame.join(conditions, count_already_seen, on=iv_column_names).fillna(0)     return {\"conditions\": conditions, \"already_seen\": count_already_seen}  experiment_data_ = pd.DataFrame(dict(x1=[-3, 3, -3], x2=[-1, 5, -1])) variables_ = VariableCollection(independent_variables=[Variable(\"x1\"), Variable(\"x2\")])  avoid_repeat(     conditions=conditions_,     experiment_data=experiment_data_,     variables=variables_ )[\"conditions\"] Out[\u00a0]: x1 x2 downvotes 0 -3.0 -1.0 2.0 1 -2.0 0.0 0.0 2 -1.0 1.0 0.0 3 0.0 2.0 0.0 4 1.0 3.0 0.0 5 2.0 4.0 0.0 6 3.0 5.0 1.0 <p>We wrap the <code>avoid_repeat</code> function with the usual <code>on_state</code> wrapper to make it compatible with the state mechanism. As it already returns a dictionary, we don't need to specify the output names. Then we can the wrapped function on the State object.</p> In\u00a0[\u00a0]: Copied! <pre>avoid_repeat_state = on_state(avoid_repeat)\ns = StandardState(\n    experiment_data=pd.DataFrame(dict(x1=[-3, 3, -3], x2=[-1, 5, -1])),\n    variables=VariableCollection(independent_variables=[Variable(\"x1\"), Variable(\"x2\")])\n)\navoid_repeat_state(s, conditions=conditions_)\n</pre> avoid_repeat_state = on_state(avoid_repeat) s = StandardState(     experiment_data=pd.DataFrame(dict(x1=[-3, 3, -3], x2=[-1, 5, -1])),     variables=VariableCollection(independent_variables=[Variable(\"x1\"), Variable(\"x2\")]) ) avoid_repeat_state(s, conditions=conditions_) <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:249: UserWarning: These fields: ['already_seen'] could not be used to update StandardState, which has these fields &amp; aliases: ['variables', 'conditions', 'experiment_data', 'models']\n  warnings.warn(\n</pre> Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x1', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False), Variable(name='x2', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[], covariates=[]), conditions=    x1   x2  downvotes\n0 -3.0 -1.0        2.0\n1 -2.0  0.0        0.0\n2 -1.0  1.0        0.0\n3  0.0  2.0        0.0\n4  1.0  3.0        0.0\n5  2.0  4.0        0.0\n6  3.0  5.0        1.0, experiment_data=   x1  x2\n0  -3  -1\n1   3   5\n2  -3  -1, models=[])</pre> <p>The way we handle this is to write a function which operates on the State directly, passing it to experimentalists wrapped with <code>on_state</code>, then combine their outputs. This is done as follows if our conditions are returned with the downvotes in the same dataframe:</p> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef combine_downvotes_state(\n    state: State,\n    conditions: pd.DataFrame,\n    experimentalists: List,\n    num_samples: int\n):\n    # iv_column_names = [v.name for v in s.variables.independent_variables]\n    downvoted_conditions = []\n    for e in experimentalists:\n        new_state = e(state, conditions=conditions)\n        this_downvoted_conditions = new_state.conditions\n        this_downvoted_conditions.attrs[\"name\"] = e.__name__\n        downvoted_conditions.append(this_downvoted_conditions)\n    combined_downvotes = combine_downvotes(conditions, *downvoted_conditions)\n    combined_downvotes_sorted_filtered = combined_downvotes\\\n        .sort_values(by=\"downvotes\", ascending=True)\\\n        .iloc[:num_samples]\n\n    d = Delta(conditions=combined_downvotes_sorted_filtered)\n    return d\n\ncombine_downvotes_state(\n    s,\n    conditions=conditions_,\n    experimentalists=[\n        on_state(avoid_repeat),\n        on_state(avoid_negative, output=[\"conditions\"]),\n        on_state(avoid_even, output=[\"conditions\"])\n    ],\n    num_samples=7\n).conditions\n</pre> @on_state() def combine_downvotes_state(     state: State,     conditions: pd.DataFrame,     experimentalists: List,     num_samples: int ):     # iv_column_names = [v.name for v in s.variables.independent_variables]     downvoted_conditions = []     for e in experimentalists:         new_state = e(state, conditions=conditions)         this_downvoted_conditions = new_state.conditions         this_downvoted_conditions.attrs[\"name\"] = e.__name__         downvoted_conditions.append(this_downvoted_conditions)     combined_downvotes = combine_downvotes(conditions, *downvoted_conditions)     combined_downvotes_sorted_filtered = combined_downvotes\\         .sort_values(by=\"downvotes\", ascending=True)\\         .iloc[:num_samples]      d = Delta(conditions=combined_downvotes_sorted_filtered)     return d  combine_downvotes_state(     s,     conditions=conditions_,     experimentalists=[         on_state(avoid_repeat),         on_state(avoid_negative, output=[\"conditions\"]),         on_state(avoid_even, output=[\"conditions\"])     ],     num_samples=7 ).conditions <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:249: UserWarning: These fields: ['already_seen'] could not be used to update StandardState, which has these fields &amp; aliases: ['variables', 'conditions', 'experiment_data', 'models']\n  warnings.warn(\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[32], line 24\n     20     return d\n     22 combine_downvotes_state = on_state(combine_downvotes)\n---&gt; 24 combine_downvotes_state(\n     25     s,\n     26     conditions=conditions_,\n     27     experimentalists=[\n     28         on_state(avoid_repeat),\n     29         on_state(avoid_negative, output=[\"conditions\"]),\n     30         on_state(avoid_even, output=[\"conditions\"])\n     31     ],\n     32     num_samples=7\n     33 ).conditions\n\nFile c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:939, in delta_to_state.&lt;locals&gt;._f(state_, **kwargs)\n    937 @wraps(f)\n    938 def _f(state_: S, **kwargs) -&gt; S:\n--&gt; 939     delta = f(state_, **kwargs)\n    940     assert isinstance(delta, Mapping), (\n    941         \"Output of %s must be a `Delta`, `UserDict`, \" \"or `dict`.\" % f\n    942     )\n    943     new_state = state_ + delta\n\nFile c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:675, in inputs_from_state.&lt;locals&gt;._f(state_, **kwargs)\n    673     arguments_from_state[\"state\"] = state_\n    674 arguments = dict(arguments_from_state, **kwargs)\n--&gt; 675 result = f(**arguments)\n    676 return result\n\nCell In[32], line 14, in combine_downvotes(state, conditions, experimentalists, num_samples)\n     12     this_downvoted_conditions.attrs[\"name\"] = e.__name__\n     13     downvoted_conditions.append(this_downvoted_conditions)\n---&gt; 14 combined_downvotes = combine_downvotes(conditions, *downvoted_conditions)\n     15 combined_downvotes_sorted_filtered = combined_downvotes\\\n     16     .sort_values(by=\"downvotes\", ascending=True)\\\n     17     .iloc[:num_samples]\n     19 d = Delta(conditions=combined_downvotes_sorted_filtered)\n\nCell In[32], line 10, in combine_downvotes(state, conditions, experimentalists, num_samples)\n      8 downvoted_conditions = []\n      9 for e in experimentalists:\n---&gt; 10     new_state = e(state, conditions=conditions)\n     11     this_downvoted_conditions = new_state.conditions\n     12     this_downvoted_conditions.attrs[\"name\"] = e.__name__\n\nTypeError: 'str' object is not callable</pre> In\u00a0[\u00a0]: Copied! <pre>def avoid_repeat_separate(\n    conditions: pd.DataFrame,\n    experiment_data: pd.DataFrame,\n    variables: VariableCollection\n):\n    conditions_with_downvotes = avoid_repeat(\n        conditions=conditions,\n        experiment_data=experiment_data,\n        variables=variables\n    )[\"conditions\"]\n\n    # Now we split up the results\n    iv_column_names = [v.name for v in variables.independent_variables]\n    conditions = conditions_with_downvotes[iv_column_names]\n    downvotes = conditions_with_downvotes[\"downvotes\"]\n\n    return {\"conditions\": conditions, \"downvotes\": downvotes}\n\navoid_repeat_separate(\n    conditions=conditions_,\n    experiment_data=experiment_data_,\n    variables=variables_\n)\n</pre> def avoid_repeat_separate(     conditions: pd.DataFrame,     experiment_data: pd.DataFrame,     variables: VariableCollection ):     conditions_with_downvotes = avoid_repeat(         conditions=conditions,         experiment_data=experiment_data,         variables=variables     )[\"conditions\"]      # Now we split up the results     iv_column_names = [v.name for v in variables.independent_variables]     conditions = conditions_with_downvotes[iv_column_names]     downvotes = conditions_with_downvotes[\"downvotes\"]      return {\"conditions\": conditions, \"downvotes\": downvotes}  avoid_repeat_separate(     conditions=conditions_,     experiment_data=experiment_data_,     variables=variables_ ) Out[\u00a0]: <pre>{'conditions':     x1   x2\n 0 -3.0 -1.0\n 1 -2.0  0.0\n 2 -1.0  1.0\n 3  0.0  2.0\n 4  1.0  3.0\n 5  2.0  4.0\n 6  3.0  5.0,\n 'downvotes': 0    2.0\n 1    0.0\n 2    0.0\n 3    0.0\n 4    0.0\n 5    0.0\n 6    1.0\n Name: downvotes, dtype: float64}</pre> <p>In the aggregation function, we have to gather the \"downvotes\" from the individual experimentalists (having passed them the full state as well as some seed conditions), then combine them, before we can split off the conditions and downvotes for the result object</p> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef combine_downvotes_separate_state(\n    state: State,\n    conditions: pd.DataFrame,\n    experimentalists: List,\n    variables: VariableCollection,\n    num_samples: int\n):\n    # iv_column_names = [v.name for v in s.variables.independent_variables]\n    all_downvotes = []\n    for e in experimentalists:\n        delta = e(state, conditions=conditions)\n        this_downvotes_series = delta[\"downvotes\"]\n        this_downvotes_series.attrs[\"name\"] = e.__name__\n        all_downvotes.append(this_downvotes_series.to_frame(\"downvotes\"))\n    combined_downvotes = combine_downvotes(conditions, *all_downvotes)\n\n    combined_downvotes_sorted_filtered = combined_downvotes\\\n        .sort_values(by=\"downvotes\", ascending=True)\\\n        .iloc[:num_samples]\n\n    iv_column_names = [v.name for v in variables.independent_variables]\n    result_conditions = combined_downvotes_sorted_filtered[iv_column_names]\n    result_downvotes = combined_downvotes_sorted_filtered[\"downvotes\"]\n\n    d = Delta(conditions=result_conditions, downvotes=result_downvotes)\n    return d\n\ncombine_downvotes_separate_state(\n    s,\n    conditions=conditions_,\n    experimentalists=[\n        # Here we have to use `inputs_from_state` but return our dictionary.\n        # There isn't a `downvotes` field we can update,\n        # so if we try to use the state mechanism, we lose the downvotes data\n        inputs_from_state(avoid_repeat_separate),\n        inputs_from_state(avoid_negative_separate),\n        inputs_from_state(avoid_even_separate)\n    ],\n    num_samples=7\n).conditions\n</pre> @on_state() def combine_downvotes_separate_state(     state: State,     conditions: pd.DataFrame,     experimentalists: List,     variables: VariableCollection,     num_samples: int ):     # iv_column_names = [v.name for v in s.variables.independent_variables]     all_downvotes = []     for e in experimentalists:         delta = e(state, conditions=conditions)         this_downvotes_series = delta[\"downvotes\"]         this_downvotes_series.attrs[\"name\"] = e.__name__         all_downvotes.append(this_downvotes_series.to_frame(\"downvotes\"))     combined_downvotes = combine_downvotes(conditions, *all_downvotes)      combined_downvotes_sorted_filtered = combined_downvotes\\         .sort_values(by=\"downvotes\", ascending=True)\\         .iloc[:num_samples]      iv_column_names = [v.name for v in variables.independent_variables]     result_conditions = combined_downvotes_sorted_filtered[iv_column_names]     result_downvotes = combined_downvotes_sorted_filtered[\"downvotes\"]      d = Delta(conditions=result_conditions, downvotes=result_downvotes)     return d  combine_downvotes_separate_state(     s,     conditions=conditions_,     experimentalists=[         # Here we have to use `inputs_from_state` but return our dictionary.         # There isn't a `downvotes` field we can update,         # so if we try to use the state mechanism, we lose the downvotes data         inputs_from_state(avoid_repeat_separate),         inputs_from_state(avoid_negative_separate),         inputs_from_state(avoid_even_separate)     ],     num_samples=7 ).conditions <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:249: UserWarning: These fields: ['downvotes'] could not be used to update StandardState, which has these fields &amp; aliases: ['variables', 'conditions', 'experiment_data', 'models']\n  warnings.warn(\n</pre> Out[\u00a0]: x1 x2 4 1.0 3.0 2 -1.0 1.0 6 3.0 5.0 3 0.0 2.0 5 2.0 4.0 1 -2.0 0.0 0 -3.0 -1.0 In\u00a0[\u00a0]: Copied! <pre>def combine_downvotes(a, b, *arrays):\n    if isinstance(b, pd.Series):\n        new_downvotes = b\n    elif isinstance(b, pd.DataFrame):\n        new_downvotes = b.downvotes\n    if \"downvotes\" in a.columns:\n        result = a.assign(downvotes=a.downvotes + new_downvotes)\n    else:\n        result = a.assign(downvotes=new_downvotes)\n    if len(arrays) == 0:\n        return result\n    else:\n        return combine_downvotes(result, arrays[0], *arrays[1:])\n</pre> def combine_downvotes(a, b, *arrays):     if isinstance(b, pd.Series):         new_downvotes = b     elif isinstance(b, pd.DataFrame):         new_downvotes = b.downvotes     if \"downvotes\" in a.columns:         result = a.assign(downvotes=a.downvotes + new_downvotes)     else:         result = a.assign(downvotes=new_downvotes)     if len(arrays) == 0:         return result     else:         return combine_downvotes(result, arrays[0], *arrays[1:]) <p>If we pass in some conditions with no downvotes (<code>conditions_</code>) and then combine with a DataFrame with constant downvotes <code>conditions_.assign(downvotes=1)</code> we get constant total downvotes:</p> In\u00a0[\u00a0]: Copied! <pre>combine_downvotes(\n    conditions_,\n    conditions_.assign(downvotes=1)\n)\n</pre> combine_downvotes(     conditions_,     conditions_.assign(downvotes=1) ) Out[\u00a0]: x1 x2 downvotes 0 -3.0 -1.0 1 1 -2.0 0.0 1 2 -1.0 1.0 1 3 0.0 2.0 1 4 1.0 3.0 1 5 2.0 4.0 1 6 3.0 5.0 1 <p>We can add another set of downvotes, which are summed with the existing ones:</p> In\u00a0[\u00a0]: Copied! <pre>combine_downvotes(\n    conditions_,\n    conditions_.assign(downvotes=1),\n    conditions_.assign(downvotes=[0, 1, 2, 3, 4, 5, 6]).sample(frac=1)\n)\n</pre> combine_downvotes(     conditions_,     conditions_.assign(downvotes=1),     conditions_.assign(downvotes=[0, 1, 2, 3, 4, 5, 6]).sample(frac=1) ) Out[\u00a0]: x1 x2 downvotes 0 -3.0 -1.0 1 1 -2.0 0.0 2 2 -1.0 1.0 3 3 0.0 2.0 4 4 1.0 3.0 5 5 2.0 4.0 6 6 3.0 5.0 7 <p>Using these, we can build functions which are aware of and add to existing downvotes if they exist.</p> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef avoid_even_chainable(conditions: pd.DataFrame, variables: VariableCollection):\n    iv_names = [v.name for v in variables.independent_variables]\n    downvotes = avoid_even_function(conditions_[iv_names]).sum(axis=1)\n    result = combine_downvotes(conditions, downvotes)\n    result[\"avoid_even.downvotes\"] = downvotes\n    return {\"conditions\": result}\navoid_even_chainable(s, conditions=conditions_)\n</pre> @on_state() def avoid_even_chainable(conditions: pd.DataFrame, variables: VariableCollection):     iv_names = [v.name for v in variables.independent_variables]     downvotes = avoid_even_function(conditions_[iv_names]).sum(axis=1)     result = combine_downvotes(conditions, downvotes)     result[\"avoid_even.downvotes\"] = downvotes     return {\"conditions\": result} avoid_even_chainable(s, conditions=conditions_) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x1', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False), Variable(name='x2', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[], covariates=[]), conditions=    x1   x2  downvotes  avoid_even.downvotes\n0 -3.0 -1.0        0.0                   0.0\n1 -2.0  0.0        2.0                   2.0\n2 -1.0  1.0        0.0                   0.0\n3  0.0  2.0        2.0                   2.0\n4  1.0  3.0        0.0                   0.0\n5  2.0  4.0        2.0                   2.0\n6  3.0  5.0        0.0                   0.0, experiment_data=   x1  x2\n0  -3  -1\n1   3   5\n2  -3  -1, models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef avoid_negative_chainable(conditions: pd.DataFrame, variables: VariableCollection):\n    iv_names = [v.name for v in variables.independent_variables]\n    downvotes = (conditions_[iv_names] &lt; 0).sum(axis=1)\n    result = combine_downvotes(conditions, downvotes)\n    result[\"avoid_negative.downvotes\"] = downvotes\n    return {\"conditions\": result}\navoid_negative_chainable(s, conditions=conditions_)\n</pre> @on_state() def avoid_negative_chainable(conditions: pd.DataFrame, variables: VariableCollection):     iv_names = [v.name for v in variables.independent_variables]     downvotes = (conditions_[iv_names] &lt; 0).sum(axis=1)     result = combine_downvotes(conditions, downvotes)     result[\"avoid_negative.downvotes\"] = downvotes     return {\"conditions\": result} avoid_negative_chainable(s, conditions=conditions_) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x1', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False), Variable(name='x2', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[], covariates=[]), conditions=    x1   x2  downvotes  avoid_negative.downvotes\n0 -3.0 -1.0          2                         2\n1 -2.0  0.0          1                         1\n2 -1.0  1.0          1                         1\n3  0.0  2.0          0                         0\n4  1.0  3.0          0                         0\n5  2.0  4.0          0                         0\n6  3.0  5.0          0                         0, experiment_data=   x1  x2\n0  -3  -1\n1   3   5\n2  -3  -1, models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef avoid_repeat_chainable(\n    conditions: pd.DataFrame,\n    experiment_data: pd.DataFrame,\n    variables: VariableCollection\n):\n    iv_column_names = [v.name for v in variables.independent_variables]\n    count_already_seen = pd.Series(experiment_data.groupby(iv_column_names).size(), name=\"downvotes\")\n    downvotes = pd.DataFrame.join(conditions, count_already_seen, on=iv_column_names).fillna(0)[\"downvotes\"]\n    result = combine_downvotes(conditions, downvotes)\n    result[\"avoid_repeat.downvotes\"] = downvotes\n    return {\"conditions\": result}\n\n\navoid_repeat_chainable(\n    s, conditions=conditions_\n)\n</pre> @on_state() def avoid_repeat_chainable(     conditions: pd.DataFrame,     experiment_data: pd.DataFrame,     variables: VariableCollection ):     iv_column_names = [v.name for v in variables.independent_variables]     count_already_seen = pd.Series(experiment_data.groupby(iv_column_names).size(), name=\"downvotes\")     downvotes = pd.DataFrame.join(conditions, count_already_seen, on=iv_column_names).fillna(0)[\"downvotes\"]     result = combine_downvotes(conditions, downvotes)     result[\"avoid_repeat.downvotes\"] = downvotes     return {\"conditions\": result}   avoid_repeat_chainable(     s, conditions=conditions_ ) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x1', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False), Variable(name='x2', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[], covariates=[]), conditions=    x1   x2  downvotes  avoid_repeat.downvotes\n0 -3.0 -1.0        2.0                     2.0\n1 -2.0  0.0        0.0                     0.0\n2 -1.0  1.0        0.0                     0.0\n3  0.0  2.0        0.0                     0.0\n4  1.0  3.0        0.0                     0.0\n5  2.0  4.0        0.0                     0.0\n6  3.0  5.0        1.0                     1.0, experiment_data=   x1  x2\n0  -3  -1\n1   3   5\n2  -3  -1, models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef sample_downvotes(conditions: pd.DataFrame, num_samples:Optional[int]=None):\n    conditions = conditions.sort_values(by=\"downvotes\").iloc[:num_samples]\n    return Delta(conditions=conditions)\n\nsample_downvotes(\n    avoid_repeat_chainable(s, conditions=conditions_),\n    num_samples=6\n).conditions\n</pre> @on_state() def sample_downvotes(conditions: pd.DataFrame, num_samples:Optional[int]=None):     conditions = conditions.sort_values(by=\"downvotes\").iloc[:num_samples]     return Delta(conditions=conditions)  sample_downvotes(     avoid_repeat_chainable(s, conditions=conditions_),     num_samples=6 ).conditions  Out[\u00a0]: x1 x2 downvotes avoid_repeat.downvotes 1 -2.0 0.0 0.0 0.0 2 -1.0 1.0 0.0 0.0 3 0.0 2.0 0.0 0.0 4 1.0 3.0 0.0 0.0 5 2.0 4.0 0.0 0.0 6 3.0 5.0 1.0 1.0 In\u00a0[\u00a0]: Copied! <pre>s_0 = s + Delta(conditions=conditions_)  # add the seed conditions\ns_1 = avoid_repeat_chainable(s_0)\ns_2 = avoid_even_chainable(s_1)\ns_3 = avoid_negative_chainable(s_2)\ns_4 = sample_downvotes(s_3, num_samples=7)\ns_4.conditions\n</pre> s_0 = s + Delta(conditions=conditions_)  # add the seed conditions s_1 = avoid_repeat_chainable(s_0) s_2 = avoid_even_chainable(s_1) s_3 = avoid_negative_chainable(s_2) s_4 = sample_downvotes(s_3, num_samples=7) s_4.conditions Out[\u00a0]: x1 x2 downvotes avoid_repeat.downvotes avoid_even.downvotes avoid_negative.downvotes 4 1.0 3.0 0.0 0.0 0.0 0 2 -1.0 1.0 1.0 0.0 0.0 1 6 3.0 5.0 1.0 1.0 0.0 0 3 0.0 2.0 2.0 0.0 2.0 0 5 2.0 4.0 2.0 0.0 2.0 0 1 -2.0 0.0 3.0 0.0 2.0 1 0 -3.0 -1.0 4.0 2.0 0.0 2 In\u00a0[\u00a0]: Copied! <pre>s_0 = StandardState(\n    experiment_data=pd.DataFrame({\"x1\":[-10], \"x2\":[-10], \"y\":[-10]})\n)\n</pre> s_0 = StandardState(     experiment_data=pd.DataFrame({\"x1\":[-10], \"x2\":[-10], \"y\":[-10]}) ) <p>... and we add data with extra columns:</p> In\u00a0[\u00a0]: Copied! <pre>new_experiment_data = pd.DataFrame({\"x1\":[5], \"x2\":[5], \"y\":[5], \"new_column\": [15]})\ns_1 = s_0 + Delta(experiment_data=new_experiment_data)\n</pre> new_experiment_data = pd.DataFrame({\"x1\":[5], \"x2\":[5], \"y\":[5], \"new_column\": [15]}) s_1 = s_0 + Delta(experiment_data=new_experiment_data) <p>then the additional columns just get added on the end, and any missing values are replaced by NaNs:</p> In\u00a0[\u00a0]: Copied! <pre>s_1.experiment_data\n</pre> s_1.experiment_data Out[\u00a0]: x1 x2 y new_column 0 -10 -10 -10 NaN 1 5 5 5 15.0"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#building-mixture-experimentalists","title":"Building Mixture Experimentalists\u00b6","text":""},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#introduction","title":"Introduction\u00b6","text":"<p>One thing the State should support is making more complex experimentalists which combine others. One example that has been suggested by the <code>autora</code> group are a \"mixture experimentalist,\" which weights the outputs of other experimentalists.</p> <p>How experimentalists are typically defined has a major impact on whether this kind of mixture experimentalist is easy or hard to implement. Since the research group is currently (August 2023) deciding how experimentalists should generally be defined, now seems a good time to look at the different basic options for standards &amp; conventions.</p> <p>To help the discussion, here we've put together some examples based on some toy experimentalists.</p>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#outline-of-the-open-question","title":"Outline of the Open Question\u00b6","text":"<p>The question has to do with whether \"additional data\" beyond the conditions are included in the same or a different data array. (\"Additional data\" are data which are generated by the experimentalist and potentially needed by another experimentalist down the line, but are not the conditions themselves).</p> <p>The two competing conventions are if an experimentalist returns some extra data:</p> <ul> <li>They are included in the <code>conditions</code> array as additional columns, or</li> <li>They are passed as a different array alongside the <code>conditions</code>.</li> </ul>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#notebook-outline","title":"Notebook Outline\u00b6","text":"<p>The examples are organized as follows:</p> <ul> <li>A combination experimentalist which aggregates additional measures from the component experimentalists.<ul> <li>Where the measure is passed back in the conditions array, or</li> <li>Where the measure is passed back in a separate array</li> </ul> </li> <li>A combination experimentalist where the components need the full State  as they have complex arguments.</li> </ul>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#toy-experimentalists","title":"Toy Experimentalists\u00b6","text":"<p>We're combining experimentalists which samples conditions based on whether they are downvoted (or not) according to some criteria:</p> <ul> <li>The <code>Avoid Negative</code> experimentalist, which downvotes conditions which have negative values (with one downvote per negative value in the conditions $x_i$: if both $x_1$ and $x_2$ are negative, the condition gets 2 downvotes, and so on) and returns all the conditions in the \"preferred\" order (fewest downvotes first),</li> <li>The <code>Avoid Even</code> experimentalist, which downvotes conditions which are closer to even numbers more (with one downvote per even value in the conditions and half a downvote if a condition is $1/2$ away from an even number) and returns all the conditions in the \"preferred\" order,</li> <li>The <code>Avoid Repeat</code> experimentalist, which downvotes conditions which have already been seen based on the number of times a condition has been seen and returns all the conditions in the \"preferred\" order,</li> <li>The <code>Combine Downvotes</code> experimentalist, which sums the downvotes of the others and returns the top $n$ \"preferred\" conditions (with the fewest downvotes); in the case of a tie, it returns conditions in the order of the original conditions list.</li> </ul> <p>We also need to see what happens when we:</p> <ul> <li>Try to extend a dataframe with an extra data frame which has new columns.</li> </ul>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#combination-experimentalist-which-aggregates-measures","title":"Combination Experimentalist which Aggregates Measures\u00b6","text":""},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#returns-an-extended-conditions-array","title":"Returns an extended conditions array\u00b6","text":""},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#return-a-separate-array-of-additional-measures","title":"Return a separate array of additional measures\u00b6","text":"<p>To ensure we don't mix up the order of return values and to facilitate updating the returned values in future without breaking dependents functions when returning multiple objects, we return a structured object \u2013 in this case a simple dictionary of results.</p>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#combination-experimentalist-needing-the-full-state","title":"Combination Experimentalist Needing The Full State\u00b6","text":"<p>In this case, we have at least one component-experimentalist which needs the full state.</p>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#experimentalists-return-combined-results-and-measures","title":"Experimentalists Return Combined Results and Measures\u00b6","text":""},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#experimentalists-return-separate-conditions-and-additional-measures","title":"Experimentalists Return Separate Conditions and Additional Measures\u00b6","text":"<p>If we return separate conditions and measures, then we need to split up the combined downvoted conditions from the downvotes:</p>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#chained-experimentalists","title":"Chained Experimentalists\u00b6","text":"<p>We can also define experimentalists which add their vote to the existing vote, if it exists:</p>"},{"location":"core/docs/cycle/Combining%20Experimentalists%20with%20State/#what-happens-when-we-extend-a-dataframe-with-new-columns-in-the-state-mechanism","title":"What Happens When We Extend a Dataframe With New Columns in the State Mechanism\u00b6","text":"<p>If we have an experiment_data field which has particular columns:</p>"},{"location":"core/docs/cycle/Dynamically%20Extending%20and%20Altering%20the%20State/","title":"Dynamically Altering the State","text":"<p>We can add fields to the <code>State</code> or alter the behaviour of the fields dynamically.</p> <p>Here, we show how to use different experimentalists to sample from a common pool and combine the outputs. We achieve this by adding a <code>pool</code> field to the <code>StandardState</code> and dynamically changing the behaviour of the <code>conditions</code> field so instead of replacing the <code>conditions</code> they get extended.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom autora.variable import VariableCollection, Variable\nfrom autora.state import StandardState\n\ns = StandardState(\n    variables=VariableCollection(independent_variables=[Variable(\"x\", value_range=(-15,15))],\n                                 dependent_variables=[Variable(\"y\")]),\n)\n\ns\n</pre> import numpy as np import pandas as pd from autora.variable import VariableCollection, Variable from autora.state import StandardState  s = StandardState(     variables=VariableCollection(independent_variables=[Variable(\"x\", value_range=(-15,15))],                                  dependent_variables=[Variable(\"y\")]), )  s Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-15, 15), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=None, experiment_data=None, models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>from dataclasses import dataclass, field\n\n@dataclass(frozen=True)\nclass ExtendedStandardState(StandardState):\n    pool: pd.DataFrame = field(\n        default_factory=list,\n        metadata={'delta': 'replace'}\n    )\n\ns = ExtendedStandardState(\n    variables=VariableCollection(independent_variables=[Variable(\"x\", value_range=(-15,15))],\n                                 dependent_variables=[Variable(\"y\")])\n)\n\ns\n</pre> from dataclasses import dataclass, field  @dataclass(frozen=True) class ExtendedStandardState(StandardState):     pool: pd.DataFrame = field(         default_factory=list,         metadata={'delta': 'replace'}     )  s = ExtendedStandardState(     variables=VariableCollection(independent_variables=[Variable(\"x\", value_range=(-15,15))],                                  dependent_variables=[Variable(\"y\")]) )  s Out[\u00a0]: <pre>ExtendedStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-15, 15), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=None, experiment_data=None, models=[], pool=[])</pre> <p>We use <code>random_pool</code> as our pooler and define the output to be the newly created <code>pool</code> field:</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\nfrom autora.state import on_state\n\npool = on_state(random_pool, output=[\"pool\"])\n\ns_1 = pool(s, num_samples=10)\ns_1\n</pre> from autora.experimentalist.random import random_pool from autora.state import on_state  pool = on_state(random_pool, output=[\"pool\"])  s_1 = pool(s, num_samples=10) s_1 Out[\u00a0]: <pre>ExtendedStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-15, 15), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=None, experiment_data=None, models=[], pool=           x\n0  -3.599348\n1 -14.328625\n2 -13.764225\n3   3.656028\n4   2.723904\n5  -7.214785\n6   6.466772\n7   7.363881\n8  13.304111\n9   2.923905)</pre> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_sample\nfrom autora.state import Delta\n\n@on_state\ndef sample(pool, **kwargs):\n    return Delta(conditions=random_sample(pool, **kwargs))\n\ns_2 = sample(s_1, num_samples=5)\ns_2.conditions\n</pre> from autora.experimentalist.random import random_sample from autora.state import Delta  @on_state def sample(pool, **kwargs):     return Delta(conditions=random_sample(pool, **kwargs))  s_2 = sample(s_1, num_samples=5) s_2.conditions Out[\u00a0]: x 0 -3.599348 1 -14.328625 5 -7.214785 9 2.923905 8 13.304111 <p>If we run the sampler on the state again, the conditions get replaced:</p> In\u00a0[\u00a0]: Copied! <pre>s_3 = sample(s_2, num_samples=3)\ns_3.conditions\n</pre> s_3 = sample(s_2, num_samples=3) s_3.conditions Out[\u00a0]: x 2 -13.764225 3 3.656028 9 2.923905 <p>We can change this behaviour, by setting the delta of the state:</p> In\u00a0[\u00a0]: Copied! <pre>s_3.conditions.metadata['delta'] = 'extend'\ns_4 = sample(s_3, num_samples=4)\ns_4.conditions\n</pre> s_3.conditions.metadata['delta'] = 'extend' s_4 = sample(s_3, num_samples=4) s_4.conditions <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 s_3.set_delta('conditions', 'extend')\n      2 s_4 = sample(s_3, num_samples=4)\n      3 s_4.conditions\n\nAttributeError: 'ExtendedStandardState' object has no attribute 'set_delta'</pre> In\u00a0[\u00a0]: Copied! <pre>def cycle(s, i):\n    s = pool(s, num_samples=10)\n    s.set_delta(\"conditions\", \"replace\")\n    s = sample(s, num_samples=2)  # now there are always 2 conditions in the field\n    print(f'cycle {i}, first sample:', s.conditions)\n    s.set_delta(\"conditions\", \"extend\")\n    s = sample(s, num_samples=2)  # now there are 4 conditions in the field\n    print(f'cycle {i}, combined sample:', s.conditions)\n    print()\n    return s\n\nfor i in range(10):\n    s = cycle(s, i)\n</pre> def cycle(s, i):     s = pool(s, num_samples=10)     s.set_delta(\"conditions\", \"replace\")     s = sample(s, num_samples=2)  # now there are always 2 conditions in the field     print(f'cycle {i}, first sample:', s.conditions)     s.set_delta(\"conditions\", \"extend\")     s = sample(s, num_samples=2)  # now there are 4 conditions in the field     print(f'cycle {i}, combined sample:', s.conditions)     print()     return s  for i in range(10):     s = cycle(s, i)  <pre>cycle 0, first sample:           x\n8  6.243647\n6 -2.637910\ncycle 0, combined sample:            x\n0   6.243647\n1  -2.637910\n2  10.854779\n3  -9.031437\n\ncycle 1, first sample:            x\n3  -4.390997\n2 -13.689377\ncycle 1, combined sample:            x\n0  -4.390997\n1 -13.689377\n2   8.103764\n3   0.587679\n\ncycle 2, first sample:            x\n9  13.485559\n5  -8.526151\ncycle 2, combined sample:            x\n0  13.485559\n1  -8.526151\n2   8.072581\n3  12.135963\n\ncycle 3, first sample:            x\n5  13.168087\n7  -4.252829\ncycle 3, combined sample:            x\n0  13.168087\n1  -4.252829\n2   9.545601\n3  13.168087\n\ncycle 4, first sample:            x\n1 -11.610008\n8  -5.419989\ncycle 4, combined sample:            x\n0 -11.610008\n1  -5.419989\n2  10.730890\n3  10.875790\n\ncycle 5, first sample:            x\n0  -0.913466\n6  13.441385\ncycle 5, combined sample:            x\n0  -0.913466\n1  13.441385\n2  -2.473232\n3  -0.913466\n\ncycle 6, first sample:            x\n5  11.894723\n6   4.894433\ncycle 6, combined sample:            x\n0  11.894723\n1   4.894433\n2  -6.875161\n3   0.735716\n\ncycle 7, first sample:           x\n4 -7.696556\n3 -6.535279\ncycle 7, combined sample:           x\n0 -7.696556\n1 -6.535279\n2 -7.981432\n3  5.399625\n\ncycle 8, first sample:           x\n7 -4.805527\n8 -4.611733\ncycle 8, combined sample:           x\n0 -4.805527\n1 -4.611733\n2 -4.611733\n3  2.183176\n\ncycle 9, first sample:           x\n5 -8.000647\n7  4.524020\ncycle 9, combined sample:            x\n0  -8.000647\n1   4.524020\n2  -8.000647\n3 -11.450967\n\n</pre>"},{"location":"core/docs/cycle/Dynamically%20Extending%20and%20Altering%20the%20State/#dynamically-extending-and-altering-the-states","title":"Dynamically Extending And Altering The States\u00b6","text":""},{"location":"core/docs/cycle/Dynamically%20Extending%20and%20Altering%20the%20State/#defining-the-state","title":"Defining The State\u00b6","text":"<p>We use the standard State object bundled with <code>autora</code>: <code>StandardState</code>. This state has four built in fields: <code>variables</code>, <code>conditions</code>, <code>experiment_data</code> and <code>models</code>. We can initialize some (or all) of these fields:</p>"},{"location":"core/docs/cycle/Dynamically%20Extending%20and%20Altering%20the%20State/#adding-pool-to-the-state","title":"Adding Pool To The State\u00b6","text":"<p>First, we add a new field, <code>pool</code> to state <code>s</code>. To do this, we must expand the StandardState class, while adding the field. We want the content of this field to be replaced each time a function writes into the field.</p>"},{"location":"core/docs/cycle/Dynamically%20Extending%20and%20Altering%20the%20State/#defining-the-experimentalists","title":"Defining The Experimentalists\u00b6","text":"<p>Here, we use a random sampler To make it use the pool as input, we wrap them in a function. The output will be written into the conditions field.</p>"},{"location":"core/docs/cycle/Dynamically%20Extending%20and%20Altering%20the%20State/#defining-a-cycle-that-dynamically-alters-the-behaviour-of-a-field","title":"Defining A Cycle That Dynamically Alters The Behaviour Of A Field\u00b6","text":"<p>We can use this to dynamically switch between replacing and extending the field. This is a toy example since we use the same experimentalist twice, but we could also use other sampling strategies and combine the outputs via  this method.</p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/","title":"Overview","text":"<p>Using the functions and objects in <code>autora</code>, we can build flexible pipelines and cycles.</p> <p>We define a two part <code>autora</code> pipeline consisting of an experiment runner and a theorist (we use the seed conditions always).</p> <p>The key part here is that both experiment runner and theorist are functions which:</p> <ul> <li>operate on the <code>State</code>, and</li> <li>return a modified object of the same type <code>State</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom autora.variable import VariableCollection, Variable\nfrom autora.state import StandardState\n\ns = StandardState(\n    variables=VariableCollection(independent_variables=[Variable(\"x\", value_range=(-15,15))],\n                                 dependent_variables=[Variable(\"y\")]),\n    conditions=pd.DataFrame({\"x\": np.linspace(-15,15,101)}),\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]),\n)\n</pre> import numpy as np import pandas as pd from autora.variable import VariableCollection, Variable from autora.state import StandardState  s = StandardState(     variables=VariableCollection(independent_variables=[Variable(\"x\", value_range=(-15,15))],                                  dependent_variables=[Variable(\"y\")]),     conditions=pd.DataFrame({\"x\": np.linspace(-15,15,101)}),     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]), ) In\u00a0[\u00a0]: Copied! <pre>s\n</pre> s Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-15, 15), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=        x\n0   -15.0\n1   -14.7\n2   -14.4\n3   -14.1\n4   -13.8\n..    ...\n96   13.8\n97   14.1\n98   14.4\n99   14.7\n100  15.0\n\n[101 rows x 1 columns], experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> <p>Given this state, we define a two part <code>autora</code> pipeline consisting of an experiment runner and a theorist. We'll just reuse the initial seed <code>conditions</code> in this example.</p> <p>First we define and test the experiment runner.</p> <p>The key part here is that both the experiment runner and the theorist are functions which operate on the <code>State</code>. We use the wrapper function <code>wrap_to_use_state</code> that wraps the experiment_runner and makes it operate on the fields of the <code>State</code> rather than the <code>conditions</code> and <code>experiment_data</code> directly.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.state import on_state\n\ndef ground_truth(x: pd.Series, c=(432, -144, -3, 1)):\n    return c[0] + c[1] * x + c[2] * x**2 + c[3] * x**3\n\ndef experiment_runner(conditions, std=100., random_state=None):\n    \"\"\"Coefs from https://www.maa.org/sites/default/files/0025570x28304.di021116.02p0130a.pdf\"\"\"\n    rng = np.random.default_rng(random_state)\n    x = conditions[\"x\"]\n    noise = rng.normal(0, std, len(x))\n    y = (ground_truth(x) + noise)\n    experiment_data = conditions.assign(y = y)\n    return experiment_data\n\nexperiment_runner = on_state(experiment_runner, output=['experiment_data'])\n</pre> from autora.state import on_state  def ground_truth(x: pd.Series, c=(432, -144, -3, 1)):     return c[0] + c[1] * x + c[2] * x**2 + c[3] * x**3  def experiment_runner(conditions, std=100., random_state=None):     \"\"\"Coefs from https://www.maa.org/sites/default/files/0025570x28304.di021116.02p0130a.pdf\"\"\"     rng = np.random.default_rng(random_state)     x = conditions[\"x\"]     noise = rng.normal(0, std, len(x))     y = (ground_truth(x) + noise)     experiment_data = conditions.assign(y = y)     return experiment_data  experiment_runner = on_state(experiment_runner, output=['experiment_data']) <p>When we run the experiment runner, we can see the updated state object which is returned \u2013 it has new experimental data.</p> In\u00a0[\u00a0]: Copied! <pre>experiment_runner(s, std=1).experiment_data\n</pre> experiment_runner(s, std=1).experiment_data <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> Out[\u00a0]: x y 0 -15.0 -1459.429483 1 -14.7 -1274.322129 2 -14.4 -1101.452213 3 -14.1 -937.643485 4 -13.8 -779.855970 ... ... ... 96 13.8 502.684242 97 14.1 610.424989 98 14.4 722.685211 99 14.7 844.061598 100 15.0 971.162262 <p>101 rows \u00d7 2 columns</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\nfrom autora.state import estimator_on_state\nfrom sklearn.pipeline import make_pipeline as make_theorist_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Completely standard scikit-learn pipeline regressor\nregressor = make_theorist_pipeline(PolynomialFeatures(degree=5), LinearRegression())\ntheorist = estimator_on_state(regressor)\n\ndef get_equation(r):\n    t = r.named_steps['polynomialfeatures'].get_feature_names_out()\n    c = r.named_steps['linearregression'].coef_\n    return pd.DataFrame({\"t\": t, \"coefficient\": c.reshape(t.shape)})\n</pre> from sklearn.linear_model import LinearRegression from autora.state import estimator_on_state from sklearn.pipeline import make_pipeline as make_theorist_pipeline from sklearn.preprocessing import PolynomialFeatures  # Completely standard scikit-learn pipeline regressor regressor = make_theorist_pipeline(PolynomialFeatures(degree=5), LinearRegression()) theorist = estimator_on_state(regressor)  def get_equation(r):     t = r.named_steps['polynomialfeatures'].get_feature_names_out()     c = r.named_steps['linearregression'].coef_     return pd.DataFrame({\"t\": t, \"coefficient\": c.reshape(t.shape)})  In\u00a0[\u00a0]: Copied! <pre>t = theorist(experiment_runner(s, random_state=1))\n</pre> t = theorist(experiment_runner(s, random_state=1)) <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> <p>The fitted coefficients are:</p> In\u00a0[\u00a0]: Copied! <pre>get_equation(t.models[-1])\n</pre> get_equation(t.models[-1]) Out[\u00a0]: t coefficient 0 1 0.000000 1 x -145.723526 2 x^2 -2.909293 3 x^3 1.048788 4 x^4 -0.000242 5 x^5 -0.000252 In\u00a0[\u00a0]: Copied! <pre>def pipeline(state: StandardState, random_state=None) -&gt; StandardState:\n    s_ = state\n    t_ = experiment_runner(s_, random_state=random_state)\n    u_ = theorist(t_)\n    return u_\n</pre> def pipeline(state: StandardState, random_state=None) -&gt; StandardState:     s_ = state     t_ = experiment_runner(s_, random_state=random_state)     u_ = theorist(t_)     return u_ <p>Running this pipeline is the same as running the individual steps \u2013 just pass the state object.</p> In\u00a0[\u00a0]: Copied! <pre>u = pipeline(s, random_state=1)\nget_equation(u.models[-1])\n</pre> u = pipeline(s, random_state=1) get_equation(u.models[-1]) <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> Out[\u00a0]: t coefficient 0 1 0.000000 1 x -145.723526 2 x^2 -2.909293 3 x^3 1.048788 4 x^4 -0.000242 5 x^5 -0.000252 <p>Since the pipeline function operates on the <code>State</code> itself and returns a <code>State</code>, we can chain these pipelines in the same fashion as we chain the theorist and experiment runner:</p> In\u00a0[\u00a0]: Copied! <pre>u_ = pipeline(pipeline(s, random_state=1), random_state=2)\nget_equation(u_.models[-1])\n</pre> u_ = pipeline(pipeline(s, random_state=1), random_state=2) get_equation(u_.models[-1]) <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> Out[\u00a0]: t coefficient 0 1 0.000000 1 x -145.738569 2 x^2 -2.898667 3 x^3 1.042038 4 x^4 -0.000893 5 x^5 -0.000218 <p>To show what's happening, we'll show the data, best fit model and ground truth:</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\n\n\ndef show_best_fit(state):\n    state.experiment_data.plot.scatter(\"x\", \"y\", s=1, alpha=0.5, c=\"gray\")\n\n    observed_x = state.experiment_data[[\"x\"]].sort_values(by=\"x\")\n    observed_x = pd.DataFrame({\"x\": np.linspace(observed_x[\"x\"].min(), observed_x[\"x\"].max(), 101)})\n\n    plt.plot(observed_x, state.models[-1].predict(observed_x), label=\"best fit\")\n    \n    allowed_x = pd.Series(np.linspace(*state.variables.independent_variables[0].value_range, 101), name=\"x\")\n    plt.plot(allowed_x, ground_truth(allowed_x), label=\"ground truth\")\n    \n    plt.legend()\n\ndef show_coefficients(state):\n    return get_equation(state.models[-1])\n\nshow_best_fit(u)\nshow_coefficients(u)\n</pre> from matplotlib import pyplot as plt   def show_best_fit(state):     state.experiment_data.plot.scatter(\"x\", \"y\", s=1, alpha=0.5, c=\"gray\")      observed_x = state.experiment_data[[\"x\"]].sort_values(by=\"x\")     observed_x = pd.DataFrame({\"x\": np.linspace(observed_x[\"x\"].min(), observed_x[\"x\"].max(), 101)})      plt.plot(observed_x, state.models[-1].predict(observed_x), label=\"best fit\")          allowed_x = pd.Series(np.linspace(*state.variables.independent_variables[0].value_range, 101), name=\"x\")     plt.plot(allowed_x, ground_truth(allowed_x), label=\"ground truth\")          plt.legend()  def show_coefficients(state):     return get_equation(state.models[-1])  show_best_fit(u) show_coefficients(u) Out[\u00a0]: t coefficient 0 1 0.000000 1 x -145.738569 2 x^2 -2.898667 3 x^3 1.042038 4 x^4 -0.000893 5 x^5 -0.000218 <p>We can use this pipeline to make a trivial cycle, where we keep on gathering data until we reach 1000 datapoints. Any condition defined on the state object could be used here, though.</p> In\u00a0[\u00a0]: Copied! <pre>v = s\nwhile len(v.experiment_data) &lt; 1_000:  # any condition on the state can be used here.\n    v = pipeline(v)\nshow_best_fit(v)\n</pre> v = s while len(v.experiment_data) &lt; 1_000:  # any condition on the state can be used here.     v = pipeline(v) show_best_fit(v) <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> In\u00a0[\u00a0]: Copied! <pre>def cycle(state: StandardState) -&gt; StandardState:\n    s_ = state\n    while True:\n        s_ = experiment_runner(s_)\n        s_ = theorist(s_)\n        yield s_\n\ncycle_generator = cycle(s)\n\nfor i in range(1000):\n    t = next(cycle_generator)\nshow_best_fit(t)\n</pre> def cycle(state: StandardState) -&gt; StandardState:     s_ = state     while True:         s_ = experiment_runner(s_)         s_ = theorist(s_)         yield s_  cycle_generator = cycle(s)  for i in range(1000):     t = next(cycle_generator) show_best_fit(t) <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> <p>You can also define a cycle (or a sequence of steps) which yield the intermediate results.</p> In\u00a0[\u00a0]: Copied! <pre>v0 = s\ndef cycle(state: StandardState) -&gt; StandardState:\n    s_ = state\n    while True:\n        print(\"#-- running experiment_runner --#\\n\")\n        s_ = experiment_runner(s_)\n        yield s_\n        print(\"#-- running theorist --#\\n\")\n        s_ = theorist(s_)\n        yield s_\n\ncycle_generator = cycle(v0)\n</pre> v0 = s def cycle(state: StandardState) -&gt; StandardState:     s_ = state     while True:         print(\"#-- running experiment_runner --#\\n\")         s_ = experiment_runner(s_)         yield s_         print(\"#-- running theorist --#\\n\")         s_ = theorist(s_)         yield s_  cycle_generator = cycle(v0) In\u00a0[\u00a0]: Copied! <pre>s\n</pre> s Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-15, 15), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=        x\n0   -15.0\n1   -14.7\n2   -14.4\n3   -14.1\n4   -13.8\n..    ...\n96   13.8\n97   14.1\n98   14.4\n99   14.7\n100  15.0\n\n[101 rows x 1 columns], experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> <p>At the outset, we have no model and an emtpy <code>experiment_data</code> dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"{v0.models=}, \\n{v0.experiment_data=}\")\n</pre> print(f\"{v0.models=}, \\n{v0.experiment_data=}\") <pre>v0.models=[], \nv0.experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: []\n</pre> <p>In the first <code>next</code>, we only run the \"experiment_runner\"</p> In\u00a0[\u00a0]: Copied! <pre>v1 = next(cycle_generator)\nprint(f\"{v1.models=}, \\n{v1.experiment_data=}\")\n</pre> v1 = next(cycle_generator) print(f\"{v1.models=}, \\n{v1.experiment_data=}\") <pre>#-- running experiment_runner --#\n\nv1.models=[], \nv1.experiment_data=        x            y\n0   -15.0 -1504.798665\n1   -14.7 -1447.778278\n2   -14.4 -1079.358506\n3   -14.1 -1075.973379\n4   -13.8  -601.183784\n..    ...          ...\n96   13.8   610.172788\n97   14.1   566.573162\n98   14.4   595.721089\n99   14.7   788.030909\n100  15.0  1009.839502\n\n[101 rows x 2 columns]\n</pre> <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre> <p>In the next step, we run the theorist on that data, but we don't add any new data:</p> In\u00a0[\u00a0]: Copied! <pre>v2 = next(cycle_generator)\nprint(f\"{v2.models=}, \\n{v2.experiment_data.shape=}\")\n</pre> v2 = next(cycle_generator) print(f\"{v2.models=}, \\n{v2.experiment_data.shape=}\") <pre>#-- running theorist --#\n\nv2.models=[Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])], \nv2.experiment_data.shape=(101, 2)\n</pre> <p>In the next step, we run the experiment runner again and gather more observations:</p> In\u00a0[\u00a0]: Copied! <pre>v3 = next(cycle_generator)\nprint(f\"{v3.models=}, \\n{v3.experiment_data.shape=}\")\n</pre> v3 = next(cycle_generator) print(f\"{v3.models=}, \\n{v3.experiment_data.shape=}\")  <pre>#-- running theorist --#\n\nv3.models=[Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())]), Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])], \nv3.experiment_data.shape=(202, 2)\n</pre> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\nexperimentalist = on_state(random_pool, output=[\"conditions\"])\nexperimentalist(s)\n</pre> from autora.experimentalist.random import random_pool experimentalist = on_state(random_pool, output=[\"conditions\"]) experimentalist(s) Out[\u00a0]: <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(-15, 15), allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=           x\n0  13.318426\n1   3.322472\n2 -10.317879\n3   6.496320\n4  -2.501831, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])</pre> In\u00a0[\u00a0]: Copied! <pre>u0 = s\nfor i in range(5):\n    u0 = experimentalist(u0, num_samples=10, random_state=42+i)\n    u0 = experiment_runner(u0, random_state=43+i)\n    u0 = theorist(u0)\n    show_best_fit(u0)\n    plt.title(f\"{i=}, {len(u0.experiment_data)=}\")\n</pre> u0 = s for i in range(5):     u0 = experimentalist(u0, num_samples=10, random_state=42+i)     u0 = experiment_runner(u0, random_state=43+i)     u0 = theorist(u0)     show_best_fit(u0)     plt.title(f\"{i=}, {len(u0.experiment_data)=}\") <pre>c:\\Users\\cwill\\GitHub\\virtualEnvs\\autoraEnv\\lib\\site-packages\\autora\\state.py:417: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat((a, b), ignore_index=True)\n</pre>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#linear-and-cyclical-workflows-using-functions-and-states","title":"Linear And Cyclical Workflows Using Functions And States\u00b6","text":""},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#experiment-runner-and-theorist","title":"Experiment Runner And Theorist\u00b6","text":""},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#defining-the-state","title":"Defining The State\u00b6","text":"<p>We use the standard State object bundled with <code>autora</code>: <code>StandardState</code></p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#defining-the-experiment-runner","title":"Defining The Experiment Runner\u00b6","text":"<p>For this example, we'll use a polynomial of degree 3 as our \"ground truth\" function. We're also using pandas DataFrames and Series as our data interchange format.</p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#defining-the-theorist","title":"Defining The Theorist\u00b6","text":"<p>Now we define a theorist, which does a linear regression on the polynomial of degree 5. We define a regressor and a method to return its feature names and coefficients, and then the theorist to handle it. Here, we use a different wrapper <code>estimator_on_state</code> that wraps the regressor and returns a function with the same functionality, but operating on <code>State</code> fields. In this case, we want to use the <code>State</code> field <code>experiment_data</code> and extend the <code>State</code> field <code>models</code>.</p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#directly-chaining-state-based-functions","title":"Directly Chaining State Based Functions\u00b6","text":"<p>Now we run the theorist on the result of the experiment runner (by chaining the two functions).</p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#creating-a-pipeline-with-state-based-functions","title":"Creating A Pipeline With State Based Functions\u00b6","text":"<p>Now we can define the simplest pipeline which runs the experiment runner and theorist in sequence and returns the updated state:</p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#creating-generators-with-state-based-functions","title":"Creating Generators With State Based Functions\u00b6","text":"<p>We can redefine the pipeline as a generator, which can be operated on using iteration tools:</p>"},{"location":"core/docs/cycle/Linear%20and%20Cyclical%20Workflows%20using%20Functions%20and%20States/#adding-the-experimentalist","title":"Adding The Experimentalist\u00b6","text":"<p>Modifying the code to use a custom experimentalist is simple. We define an experimentalist which adds some observations each cycle:</p>"},{"location":"core/docs/experimentalists/grid/","title":"Grid Pooler","text":"<p>Creates exhaustive pool from discrete values using a Cartesian product of sets.</p>"},{"location":"core/docs/experimentalists/grid/#example","title":"Example","text":"<p>To illustrate the concept of an exhaustive pool, let's consider a situation where a certain condition is defined by two variables: \\(x_{1}\\) and \\(x_{2}\\). The variable \\(x_{1}\\) can take on the values of 1, 2, or 3, while \\(x_{2}\\) can adopt the values of 4, 5, or 6.</p> \\(x_{1}\\) \\(x_{2}\\) 1 4 2 5 3 6 <p>This means that there are various combinations that these variables can form, thereby creating a comprehensive set or \"exhaustive pool\" of possibilities.</p> 4 5 6 1 (1,4) (1,5) (1,6) 2 (2,4) (2,5) (2,6) 3 (3,4) (3,5) (3,6)"},{"location":"core/docs/experimentalists/grid/#example-code","title":"Example Code","text":"<pre><code>from autora.experimentalist.grid import grid_pool\nfrom autora.variable import Variable, VariableCollection\n\niv_1 = Variable(allowed_values=[1, 2, 3])\niv_2 = Variable(allowed_values=[4, 5, 6])\nvariables = VariableCollection(independent_variables=[iv_1, iv_2])\n\npool = grid_pool(variables)\n</code></pre>"},{"location":"core/docs/experimentalists/grid/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Grid Pooler is part of the <code>autora-core</code> package and does not need to be installed separately</p> <p>you can import the grid pooler via:</p> <pre><code>from autora.experimentalist.grid import grid_pool\n</code></pre>"},{"location":"core/docs/experimentalists/random/","title":"Random Pooler","text":"<p>Creates combinations from lists of discrete values using random selection.</p>"},{"location":"core/docs/experimentalists/random/#example","title":"Example","text":"<p>To illustrate the concept of a random pool of size 3, let's consider a situation where a certain condition is defined by two variables: \\(x_{1}\\) and \\(x_{2}\\). The variable \\(x_{1}\\) can take on the values of 1, 2, or 3, while \\(x_{2}\\) can take on the values of 4, 5, or 6.</p> \\(x_{1}\\) \\(x_{2}\\) 1 4 2 5 3 6 <p>This means that there are 9 possible combinations for these variables (3x3), from which a random pool of size 3 draws 3 combinations.</p> 4 5 6 1 X (1,5) X 2 X X X 3 (3,4) (3,5) X"},{"location":"core/docs/experimentalists/random/#example-code","title":"Example Code","text":"<pre><code>from autora.experimentalist.random import random_pool\n\npool = random_pool([1, 2, 3], [4, 5, 6], num_samples=3)\n</code></pre>"},{"location":"core/docs/experimentalists/random/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Random Pooler and Sampler are part of the <code>autora-core</code> package and do not need to be installed separately</p> <p>You can import and invoke the pool like this:</p> <pre><code>from autora.variable import VariableCollection, Variable\nfrom autora.experimentalist.random import pool\n\npool(\n    VariableCollection(independent_variables=[Variable(name=\"x\", allowed_values=range(10))]),\n    random_state=1\n)\n</code></pre> <p>You can import the sampler like this:</p> <pre><code>from autora.experimentalist.random import sample\n\nsample([1, 1, 2, 2, 3, 3], num_samples=2)\n</code></pre>"},{"location":"core/docs/pipeline/Experimentalist%20Pipeline%20Examples/","title":"Pipeline","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install autora\n</pre> # Uncomment the following line when running on Google Colab # !pip install autora In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nfrom autora.variable import DV, IV, ValueType, VariableCollection\nfrom autora.experimentalist.pipeline import Pipeline\nfrom autora.experimentalist.pooler.grid import grid_pool\nfrom autora.experimentalist.sampler.random_sampler import random_sample\n</pre> import numpy as np  from autora.variable import DV, IV, ValueType, VariableCollection from autora.experimentalist.pipeline import Pipeline from autora.experimentalist.pooler.grid import grid_pool from autora.experimentalist.sampler.random_sampler import random_sample In\u00a0[\u00a0]: Copied! <pre>def weber_filter(values):\n    return filter(lambda s: s[0] &lt;= s[1], values)\n</pre> def weber_filter(values):     return filter(lambda s: s[0] &lt;= s[1], values) In\u00a0[\u00a0]: Copied! <pre># Specifying  Dependent and Independent Variables\n# Specify independent variables\niv1 = IV(\n    name=\"S1\",\n    allowed_values=np.linspace(0, 5, 5),\n    units=\"intensity\",\n    variable_label=\"Stimulus 1 Intensity\",\n)\n\niv2 = IV(\n    name=\"S2\",\n    allowed_values=np.linspace(0, 5, 5),\n    units=\"intensity\",\n    variable_label=\"Stimulus 2 Intensity\",\n)\n\n# The experimentalist pipeline doesn't actually use DVs, they are just specified here for\n# example.\ndv1 = DV(\n    name=\"difference_detected\",\n    value_range=(0, 1),\n    units=\"probability\",\n    variable_label=\"P(difference detected)\",\n    type=ValueType.PROBABILITY,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv1, iv2],\n    dependent_variables=[dv1],\n)\n</pre> # Specifying  Dependent and Independent Variables # Specify independent variables iv1 = IV(     name=\"S1\",     allowed_values=np.linspace(0, 5, 5),     units=\"intensity\",     variable_label=\"Stimulus 1 Intensity\", )  iv2 = IV(     name=\"S2\",     allowed_values=np.linspace(0, 5, 5),     units=\"intensity\",     variable_label=\"Stimulus 2 Intensity\", )  # The experimentalist pipeline doesn't actually use DVs, they are just specified here for # example. dv1 = DV(     name=\"difference_detected\",     value_range=(0, 1),     units=\"probability\",     variable_label=\"P(difference detected)\",     type=ValueType.PROBABILITY, )  # Variable collection with ivs and dvs metadata = VariableCollection(     independent_variables=[iv1, iv2],     dependent_variables=[dv1], ) <p>Next we set up the <code>Pipeline</code> with three functions:</p> <ol> <li><code>grid_pool</code> - Generates an exhaustive pool of condition combinations using the Cartesian product of discrete IV values.<ul> <li>The discrete IV values are specified with the <code>allowed_values</code> attribute when defining the IVs.</li> </ul> </li> <li><code>weber_filer</code> - Filter that selects the experimental design constraint where IV1 &lt;= IV2.</li> <li><code>random_sample</code> - Samples the pool of conditions</li> </ol> <p>Functions that require keyword inputs are initialized using the <code>partial</code> function before passing into <code>PoolPipeline</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## Set up pipeline functions with the partial function\n# Random Sampler\n\n# Initialize the pipeline\npipeline_random_samp = Pipeline([\n    (\"grid_pool\", grid_pool),\n    (\"weber_filer\", weber_filter), # Filter that selects conditions with IV1 &lt;= IV2\n    (\"random_sample\", random_sample)\n],\n    {\"grid_pool\": {\"ivs\": metadata.independent_variables}, \"random_sample\": {\"n\": 10}}\n)\npipeline_random_samp\n</pre> ## Set up pipeline functions with the partial function # Random Sampler  # Initialize the pipeline pipeline_random_samp = Pipeline([     (\"grid_pool\", grid_pool),     (\"weber_filer\", weber_filter), # Filter that selects conditions with IV1 &lt;= IV2     (\"random_sample\", random_sample) ],     {\"grid_pool\": {\"ivs\": metadata.independent_variables}, \"random_sample\": {\"n\": 10}} ) pipeline_random_samp Out[\u00a0]: <pre>Pipeline(steps=[('grid_pool', &lt;function grid_pool at 0x1077bdf70&gt;), ('weber_filer', &lt;function weber_filter at 0x1077c8550&gt;), ('random_sampler', &lt;function random_sampler at 0x1077c8160&gt;)], params={'grid_pool': {'ivs': [IV(name='S1', value_range=None, allowed_values=array([0.  , 1.25, 2.5 , 3.75, 5.  ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Stimulus 1 Intensity', rescale=1, is_covariate=False), IV(name='S2', value_range=None, allowed_values=array([0.  , 1.25, 2.5 , 3.75, 5.  ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Stimulus 2 Intensity', rescale=1, is_covariate=False)]}, 'random_sampler': {'n': 10}})</pre> <p>The pipleine can be run by calling the <code>run</code> method.</p> <p>The pipeline is run twice below to illustrate that random sampling is performed. Rerunning the cell will produce different results.</p> In\u00a0[\u00a0]: Copied! <pre># Run the Pipeline\nresults1 = pipeline_random_samp.run()\nresults2 = pipeline_random_samp.run()\nprint('Sampled Conditions:')\nprint(f' Run 1: {results1}\\n',\n      f'Run 2: {results2}')\n</pre> # Run the Pipeline results1 = pipeline_random_samp.run() results2 = pipeline_random_samp.run() print('Sampled Conditions:') print(f' Run 1: {results1}\\n',       f'Run 2: {results2}') <pre>Sampled Conditions:\n Run 1: [(3.75, 3.75), (1.25, 1.25), (1.25, 2.5), (3.75, 5.0), (2.5, 2.5), (1.25, 5.0), (2.5, 3.75), (0.0, 2.5), (0.0, 1.25), (5.0, 5.0)]\n Run 2: [(0.0, 5.0), (2.5, 5.0), (5.0, 5.0), (1.25, 1.25), (2.5, 3.75), (0.0, 1.25), (1.25, 3.75), (3.75, 3.75), (0.0, 0.0), (1.25, 2.5)]\n</pre> <p>An alternative method of passing an instantiated pool iterator is demonstrated below. Note the difference where <code>grid_pool</code> is not initialized using the <code>partial</code> function but instantiated before initializing the <code>Pipeline</code>. <code>grid_pool</code> returns an iterator of the exhaustive pool. This will result in unexpected behavior when the Pipeline is run multiple times.</p> In\u00a0[\u00a0]: Copied! <pre>## Set up pipeline functions with the partial function\n# Pool Function\npooler_iterator = grid_pool(metadata.independent_variables)\n\n# Initialize the pipeline\npipeline_random_samp2 = Pipeline(\n    [\n        (\"pool (iterator)\", pooler_iterator),\n        (\"filter\",weber_filter), # Filter that selects conditions with IV1 &lt;= IV2\n        (\"sample\", random_sample) # Sampler defined in the first implementation example\n    ],\n    {\"sample\": {\"n\": 10}}\n)\n# Run the Pipeline\nresults1 = pipeline_random_samp2.run()\nresults2 = pipeline_random_samp2.run()\nprint('Sampled Conditions:')\nprint(f' Run 1: {results1}\\n',\n      f'Run 2: {results2}')\n</pre> ## Set up pipeline functions with the partial function # Pool Function pooler_iterator = grid_pool(metadata.independent_variables)  # Initialize the pipeline pipeline_random_samp2 = Pipeline(     [         (\"pool (iterator)\", pooler_iterator),         (\"filter\",weber_filter), # Filter that selects conditions with IV1 &lt;= IV2         (\"sample\", random_sample) # Sampler defined in the first implementation example     ],     {\"sample\": {\"n\": 10}} ) # Run the Pipeline results1 = pipeline_random_samp2.run() results2 = pipeline_random_samp2.run() print('Sampled Conditions:') print(f' Run 1: {results1}\\n',       f'Run 2: {results2}') <pre>Sampled Conditions:\n Run 1: [(1.25, 1.25), (0.0, 5.0), (1.25, 5.0), (2.5, 3.75), (1.25, 2.5), (5.0, 5.0), (2.5, 5.0), (1.25, 3.75), (0.0, 1.25), (2.5, 2.5)]\n Run 2: []\n</pre> <p>Running the pipeline multiple times results in an empty list. This is because the iterator is exhausted after first run and no longer yields results. If the pipeline needs to be run multiple times, initializing the functions as a callable using the <code>partial</code> function is recommended because the iterator will be initialized at the start of each run.</p> <p>You could also use the scikit-learn \"__\" syntax to pass parameter sets into the pipeline:</p> In\u00a0[\u00a0]: Copied! <pre>pipeline_random_samp = Pipeline([\n    (\"grid_pool\", grid_pool),\n    (\"weber_filer\", weber_filter), # Filter that selects conditions with IV1 &lt;= IV2\n    (\"random_sample\", random_sample)\n],\n    {\"grid_pool__ivs\": metadata.independent_variables, \"random_sample__n\": 10}\n)\npipeline_random_samp\n</pre> pipeline_random_samp = Pipeline([     (\"grid_pool\", grid_pool),     (\"weber_filer\", weber_filter), # Filter that selects conditions with IV1 &lt;= IV2     (\"random_sample\", random_sample) ],     {\"grid_pool__ivs\": metadata.independent_variables, \"random_sample__n\": 10} ) pipeline_random_samp  Out[\u00a0]: <pre>Pipeline(steps=[('grid_pool', &lt;function grid_pool at 0x1077bdf70&gt;), ('weber_filer', &lt;function weber_filter at 0x1077c8550&gt;), ('random_sampler', &lt;function random_sampler at 0x1077c8160&gt;)], params={'grid_pool__ivs': [IV(name='S1', value_range=None, allowed_values=array([0.  , 1.25, 2.5 , 3.75, 5.  ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Stimulus 1 Intensity', rescale=1, is_covariate=False), IV(name='S2', value_range=None, allowed_values=array([0.  , 1.25, 2.5 , 3.75, 5.  ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Stimulus 2 Intensity', rescale=1, is_covariate=False)], 'random_sampler__n': 10})</pre>"},{"location":"core/docs/pipeline/Experimentalist%20Pipeline%20Examples/#experimentalist-pipeline-examples","title":"Experimentalist Pipeline Examples\u00b6","text":"<p>This notebook demonstrates the use of the <code>Pipeline</code> class to create Experimentalists. Experimentalists consist of two main components:</p> <ol> <li>Condition Generation - Creating combinations of independent variables to test</li> <li>Experimental Design - Ensuring conditions meet design constraints.</li> </ol> <p>The <code>Pipeline</code> class allows us to define a series of functions to generate and process a pool of conditions that conform to an experimental design.</p>"},{"location":"core/docs/pipeline/Experimentalist%20Pipeline%20Examples/#implementation","title":"Implementation\u00b6","text":"<p>The <code>Pipeline</code> class consists of a series of steps:</p> <ol> <li>One or no \"pool\" steps which generate experimental conditions,</li> <li>An arbitrary number of steps to apply to the pool. Examples of steps may be:<ul> <li>samplers</li> <li>conditional filters</li> <li>sequencers</li> </ul> </li> </ol>"},{"location":"core/docs/pipeline/Experimentalist%20Pipeline%20Examples/#example-1-exhaustive-pool-with-random-sampler","title":"Example 1: Exhaustive Pool With Random Sampler\u00b6","text":"<p>The examples in this notebook will create a Weber line-lengths experiment. The Weber experiment tests human detection of differences between the lengths of two lines. The first example will sample a pool with simple random sampling. We will first define the independent and dependent variables (IVs and DVs, respectively).</p>"},{"location":"examples/","title":"Use Case Tutorials","text":"<p>This section provides a set of concrete examples that showcase how to use AutoRA for different purposes. These examples and use cases are designed to give you a head start on building your own AutoRA workflow for research projects.</p> <p>Each example walks through a complete workflow, from setting up the experiment to analyzing results, allowing you to see AutoRA in action. Whether you are setting up a basic procedure for automated piloting of experimental data, or a closed-loop study involving iterations between model discovery and experimental design, these examples will guide you through the key steps and provide code snippets to help you along the way.</p>"},{"location":"examples/#how-to-use-the-examples","title":"How to Use the Examples","text":"<ul> <li>Follow along: Each example is self-contained and includes the necessary code and explanations. Simply follow along, and adapt the code to your specific use case.</li> <li>Modify for your needs: These examples are designed as starting points. You can easily modify parameters, data inputs, or algorithms to fit your research domain.</li> <li>Integrate with existing projects: Use these examples as a template to integrate AutoRA with your ongoing research projects.</li> </ul> <p>If you are new to AutoRA, we recommend that you first look into its basic components and workflow. For an overview of these features, please refer to the Basic Tutorials section.</p> <p>Let's dive into the first example to see how you can get started with AutoRA in your experiments. If you are interested in using AutoRA for running closed-loop behavioral research studies, we recommend that you start with the Closed-Loop Psychophysics Study.</p>"},{"location":"examples/closed-loop-bandit-task/","title":"Basic Closed-Loop Two-Armed Bandit Study","text":"<p>In this example, we will guide you through setting up a closed-loop computational discovery study for a human reinforcement learning task. In this behavioral study, participants will interact with a two-armed bandit task, where they must choose between two options to maximize their reward. Using AutoRA, you will build dynamic research workflow that iterates between computational model discovery, experimental design, and behavioral data collection. The ultimate goal is to make AutoRA iteratively uncover a learning rule that characterizes human participants' behavior in a two-armed bandit task.</p> <p>The code builds in a method for automating the discovery of reinforcement learning rules:</p> <p>Weinhardt, W. Eckstein, M., &amp; Musslick, S. (2024). Computational discovery of human reinforcement learning dynamics from choice behavior. NeurIPS 2024 Workshop on Behavioral ML.</p> <p>This example provides a hands-on approach to understanding closed-loop computational discovery of human behavior using the AutoRA framework. </p> <p>It may also serve as a starting point for developing your own computational discovery project.</p>"},{"location":"examples/closed-loop-bandit-task/#what-youll-learn","title":"What You\u2019ll Learn:","text":"<ul> <li>Set up a closed-loop AutoRA workflow: Learn how to create an automated discovery process, iterating between computational model discovery, experimental design, and data collection.</li> <li>Automate experimental design with SweetPea: Use SweetPea to generate experimental designs that adapt as the study progresses.</li> <li>Interfacing with web experiments: Use AutoRA to update the parameterization of an existing web-based experiment written in jsPsych.</li> <li>Host experiments using Google Firebase: Set up a server for hosting your behavioral experiments, making them accessible to participants.</li> <li>Store experimental data with Google Firestore: Efficiently manage and store participant data collected from your experiment.</li> <li>Automate the discovery of reinforcement learning rules: Use AutoRA to discover reinforcement learning rules that explain participant behavior.</li> <li>Collect data from real participants with Prolific: Recruit and manage participants through Prolific, ensuring high-quality behavioral data.</li> </ul> <p>Hint</p> <p>This example set up most of your workflow automatically, so it will not cover how to write the code for the two-armed bandit task, or how to implement a method for discovering reinforcement learning rules from behavior. Instead, we will leverage existing templates and packages. </p>"},{"location":"examples/closed-loop-bandit-task/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Basic Python knowledge: While most of the workflow is Python-based, only a basic level of understanding is needed to follow along.</li> <li>Minimal JavaScript knowledge: Since the behavioral experiments are implemented in JavaScript (via jsPsych), a minimal understanding of JavaScript is required.</li> <li>A Google account: You will need a Google account to use Google Firebase and Firestore.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/#study-overview","title":"Study Overview","text":"<p>The goal of this closed-loop study is to iteratively discover learning rules that explain participant's choice behavior in a simple two-armed bandit task.</p>"},{"location":"examples/closed-loop-bandit-task/#experiment","title":"Experiment","text":"<p>In this experiment, participants are tasked to maximize the reward they obtain by selecting between differently colored boxes. On each trial, the participant is presented with a red box and a blue box:</p> <p></p> <p>Selecting one of the boxes will either result in a reward (<code>+1</code> point) or not (<code>0</code> points). The reward probability of each box changes over time, and participants must learn to select the box that provides the most reward.</p>"},{"location":"examples/closed-loop-bandit-task/#computational-model-discovery","title":"Computational Model Discovery","text":"<p>Our goal is to automatically discover a learning rule that best explains the choice behavior of all participants. We will accomplish this using a reinforcement learning model discovery method described in </p> <p>Weinhardt, W. Eckstein, M., &amp; Musslick, S. (2024). Computational discovery of human reinforcement learning dynamics from choice behavior. NeurIPS 2024 Workshop on Behavioral ML.</p> <p></p> <p>This method, available in the <code>autora[theorist-rnn-sindy-rl]</code> package, fits the choice behavior of participants with a recurrent neural network, which computes values for both of the two options (the red box and the blue box), and then chooses between the options based on their relative value. Once the neural network is fit, the method applies an equation discovery technique called \"Sparse Identification of Non-Linear Systems\" (SINDy) to extract the learning rule that the network has discovered. </p>"},{"location":"examples/closed-loop-bandit-task/#experimental-sampling","title":"Experimental Sampling","text":"<p>The AutoRA workflow will generate new experiment conditions based on best two learning rules that were discovered. Specifically, the workflow will generate reward trajectories that best distinguish between the two learning rules using the <code>autora[experimentalist-model-disagreement]</code> package.</p> <p>These reward trajectories will be uploaded to the Firebase project, where the next batch of participants can interact with the web-based experiment. Data collected from the next batch will be used to update the model and discover new learning rules.</p>"},{"location":"examples/closed-loop-bandit-task/#system-overview","title":"System Overview","text":"<p>Our closed-loop system consists of a bunch of interacting components. Here is a high-level overview of the system: </p> <p>Our closed-loop system will have two projects talking to each other. The Firebase project will host and run the web experiment that participants interact with. Our local AutoRA project will host the code that runs the AutoRA workflow, which will generate new experiment conditions, collect data from the web experiment, and update the model based on the collected data. </p>"},{"location":"examples/closed-loop-bandit-task/#firebase-project","title":"Firebase Project","text":"<p>To run an online experiment, we need to host it as a web app. We will leverage Google Firebase to host our web app. Participants from Prolific can then interact with the web app to complete the experiment.  </p> <p>Our experiment is configured by experiment conditions, which are stored in a Google Firestore database. In addition, we will use this database to store collected behavioral data from the web experiment. </p>"},{"location":"examples/closed-loop-bandit-task/#our-local-autora-project","title":"Our Local AutoRA Project","text":"<p>The local project will consist of two folders. The <code>testing_zone</code> folder will contain the web app that participants interact with. </p> <p>The <code>researcher_hub</code> folder will contain the AutoRA workflow. </p>"},{"location":"examples/closed-loop-bandit-task/#next-steps","title":"Next Steps","text":"<p>Each step in the example will lead guide you to set up each component of the closed-loop system. </p> <p>By the end of this example, you\u2019ll be able to run a closed-loop computational discovery study.</p> <p>Next: Set up the project.</p>"},{"location":"examples/closed-loop-bandit-task/customization/","title":"Customization","text":"<p>The code you used in this example may serve as a template for future projects. The most important files are:</p> <ul> <li>Autora Workflow <code>researcher_hub/autora_workflow.py</code>: Here, you can customize the AutoRA workflow.</li> </ul> <p>Adjusting the workflow may involve: - Using other experimentalists to change the selection of experiment-conditions.  - Adjusting experiment runners for alternative ways of collecting data. In this example, we use jsPsych and javascript for the experiments, but depending on the experiment, you can interface with other devices or use other means to collect data. - Using other theorists. The theorist that is used in the example is custom build for recovering reinforcement learning rules from behavior measured in 2-armed bandit tasks. Depending on your experiment, you will use other theorists.</p> <ul> <li>Behavioral experiment <code>testng_zone/src/design/main.js</code>: Here, you can customzie the web-based experiment that is shown to the participant. You can find great tutorials on how to build jsPsych experiments on their website. Keep in mind, that you should build your website in a way that the conditions are used to customize the experiment. A good way to do so is by creating trial-sequences in <code>autora_workflow.py</code> and using them as <code>timeline_variables</code>.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/firebase/","title":"Set Up The Project On The Firebase Website","text":"<p>Next, we want to set up Firebase for our project. </p> <p></p> <p>Firebase is a platform developed by Google for creating mobile and web applications. Here, we will leverage firebase as a platform for hosting our web-based behavioral experiment, and firestore for hosting associated experimental data.</p> <p>To serve a website via Firebase and use the Firestore Database, it is necessary to set up a Firebase project. Follow the steps below to get started:</p>"},{"location":"examples/closed-loop-bandit-task/firebase/#google-account","title":"Google Account","text":"<p>You'll need a Google account to use Firebase.</p>"},{"location":"examples/closed-loop-bandit-task/firebase/#firebase-project","title":"Firebase Project","text":"<ul> <li>While logged in into your Google account, head over to the Firebase website. Then, create a new project:</li> <li>Click on <code>Get started</code>.</li> <li>Click on the plus sign with <code>Create a project</code>.</li> <li>Name your project (e.g., <code>closed-loop-bandit</code>) and click on <code>Continue</code>.</li> <li>As we don't need Google Analytics, we can leave it disabled (you can leave it enabled if you want to use it in the future).</li> <li>Click <code>Create project</code>.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/firebase/#adding-a-web-app-to-your-project","title":"Adding a Web App to Your Project","text":"<ul> <li> <p>Now, we add a web app to the project, which will correspond to our web experiment. Navigate to the project and follow these steps:</p> </li> <li> <p>Click on <code>&lt;\\&gt;</code>.   </p> </li> <li>Name the app (can be the same as your project) and check the box <code>Also set up Firebase Hosting</code>. Click on <code>Register app</code>.</li> <li>Select <code>Use npm</code>. We will use the configuration details later, but for now, click on <code>Next</code>.</li> <li>We will install firebase tools later, for now, click on <code>Next</code>.</li> <li>We will log in and deploy our website later, for now, click on <code>Continue to console</code>.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/firebase/#adding-firestore-to-your-project","title":"Adding Firestore To Your Project","text":"<p>For our closed-loop study, we will use a Firestore Database to communicate between the AutoRA workflow and the website conducting the experiment. We will upload experiment conditions to the database and also store experiment data in the database.  - To build a Firestore Database, follow these steps:   1. In the left-hand menu of your project console, click on <code>Build</code> and select <code>Firestore Database</code>.      2. Click on <code>Create database</code>.   3. Select a location for the server hosting the database. Click on <code>Next</code>. Note that your institution may have restrictions on the location of the server. Click <code>Next</code>   4. Select <code>Start in production mode</code> selected and click <code>Create</code>.</p> <p>You have now configured Firebase for your project. Next, we will connect your local project to Firebase and deploy your web-based experiment.</p>"},{"location":"examples/closed-loop-bandit-task/firebase/#next-steps","title":"Next Steps","text":"<p>Next: Connect your local project with Firebase.</p>"},{"location":"examples/closed-loop-bandit-task/init-autora/","title":"Connect AutoRA WorkFlow to Firebase","text":"<p>After setting up a mechanism to deploy your experiments online, you can now connect the AutoRA workflow to Firestore database. This will allow us to update experiment conditions and to download observations collected from the experiment.</p> <p></p> <p>The workflow will manage the entire research process, from generating novel experiments to collecting data and modeling the results. </p> <p>Note that the workflow will talk to the Firebase project by uploading and downloading data to the Firestore database. We will upload new experiment conditions to the database and download the results of the experiment from it. </p> <p>The AutoRA workflow can be found in the <code>researcher_hub</code> folder, which contains a template for an AutoRA workflow for a two-armed bandit task using a model discovery process (AutoRA-theorist) described here: rnn-sindy-rl.</p> <ul> <li>Move into the <code>researcher_hub</code> directory, where the template for the workflow is stored.</li> </ul> <pre><code>cd researcher_hub\n</code></pre> <ul> <li>Then install the Python packages required for the workflow using <code>pip</code>. This may take some time:</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/init-autora/#add-firebase-credentials","title":"Add Firebase Credentials","text":"<p>The workflow will make use of the following three different experiment runners:    - <code>autora-synthetic-runner</code>: This runner is used to run the AutoRA workflow with simulated data from a reinforcement learning model.    - <code>autora-firebase-runner</code>: This runner is used to run the AutoRA workflow with Firebase. It will upload new experiment conditions to the Firestore database and download the results of the experiment from it. This can be used to test the online experiment.    - <code>autora-prolific-runner</code>: This runner is used to run the AutoRA workflow with Prolific. It will recruit participants via Prolific and upload the results of the experiment to Prolific.</p> <p>The AutoRA workflow (specifically the <code>autora-firebase-runner</code> and <code>autora-prolific-runner</code>) will need access to your firebase project. Therefore, we need the corresponding credentials. </p> <ul> <li>To obtain the credentials, go to the Firebase console.</li> <li>Navigate to the project.</li> <li>Click on the little gear on the left and then select <code>Project settings</code>. </li> <li>Click on <code>Service accounts</code>. </li> <li>Having <code>Node.js</code> selected, click <code>Generate a new private key</code>. This should generate a json file that you can download.</li> <li>Open the file <code>autora_workflow.py</code> in the <code>researcher_hub</code>-folder and navigate to the part of the code that contains a placeholder for the credentials. It should look like this <pre><code>firebase_credentials = {\n    \"type\": \"type\",\n    \"project_id\": \"project_id\",\n    \"private_key_id\": \"private_key_id\",\n    \"private_key\": \"private_key\",\n    \"client_email\": \"client_email\",\n    \"client_id\": \"client_id\",\n    \"auth_uri\": \"auth_uri\",\n    \"token_uri\": \"token_uri\",\n    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n    \"client_x509_cert_url\": \"client_x509_cert_url\"\n}\n</code></pre></li> <li>Replace the placeholders with the credentials from the json file you downloaded.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/init-autora/#try-out-the-workflow","title":"Try out the Workflow","text":"<p>In this section, we will simply check whether the code executes. The next section will explain in more detail what is happening in the workflow. </p>"},{"location":"examples/closed-loop-bandit-task/init-autora/#synthetic-experiment-runner","title":"Synthetic Experiment Runner","text":"<p>First try the synthetic runner to see if everything works correctly.</p> <ul> <li>Within your environment, you can now run <code>python autora_workflow.py</code>.</li> </ul> <p>The code should now execute using a synthetic loop. In the next section, we explain in more detail what is happening.  For now, you may just want to check if the code runs without errors.</p>"},{"location":"examples/closed-loop-bandit-task/init-autora/#firebase-experiment-runner","title":"Firebase Experiment Runner","text":"<ul> <li>To run the AutoRA workflow with the actual web experiment on Firebase, you need to change the experiment runner. Replace <code>RUNNER_TYPE = 'synthetic'</code> in the beginning of the <code>autora_workflow.py</code> file with</li> </ul> <pre><code>RUNNER_TYPE = 'firebase'\n</code></pre> <ul> <li> <p>Now run the workflow again with <code>python autora_workflow.py</code>.</p> </li> <li> <p>Head over to your website to test participate in the online experiment. You can find the link in the Firebase console. Navigate to your project and select <code>Hosting</code> in the left navigation menu. The domain of your experiment is listed on top under <code>Domains</code>.</p> </li> </ul> <p></p> <ul> <li>This will be the experiment as participants will see it. Your task in the experiment is to select the colored box that provides a reward, i.e., a point. Which box is rewarded is stochastic and will change over time.</li> </ul> <p></p> <ul> <li> <p>Complete the experiment. </p> </li> <li> <p>You can now check if the results of your experiment are properly stored in the Firebase database. To check the database, go to the Firebase console and select your project. On the left menu, navigate to <code>Firestore Database</code>. If everything worked, you should see database fields called <code>autora_in</code> and <code>autora_out</code>. The former contains the experiment conditions which are used to configure the experiment. The latter will contain the results of the experiment. If you click on <code>autora_out</code> after you completed the experiment, you should see a field such as <code>0</code> with the results of the experiment. </p> </li> </ul> <p></p> <p>Next: Workflow Explanation.</p>"},{"location":"examples/closed-loop-bandit-task/prolific/","title":"Connect Project With Prolific","text":"<p>Once you have your closed-loop workflow set up, it is fairly easy to connect it to Prolific,  a recruiting platform for web-based experiments. By connecting your project with Prolific via the <code>firebase-prolific-runner</code>, you can automatically recruit participants for your study and collect data from them. </p> <p></p> <p>!!! hint: The <code>firebase-prolific-runner</code> will automatically set up a study on Prolific and recruit participants. It is highly recommended to test the experiment before recruiting participants, to have approval from an ethics committee, and to adhere to the ethical guidelines.</p>"},{"location":"examples/closed-loop-bandit-task/prolific/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a Prolific account.</li> <li>Your behavioral study is approved by an ethics committee or institutional review board (IRB).</li> <li>You have a corresponding consent form for your study.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/prolific/#add-consent-form-to-experiment","title":"Add Consent Form to Experiment","text":"<p>Before you can connect your project with Prolific, you will likely need to add a consent form to your experiment. The consent form should be displayed to participants before they start the experiment.</p> <p>This will require you to modify the jsPsych code of your experiment which can be found in <code>testing_zone/src/design.main.js</code>. </p>"},{"location":"examples/closed-loop-bandit-task/prolific/#update-autora-workflow-to-use-prolific","title":"Update AutoRA Workflow to Use Prolific","text":"<ul> <li> <p>Navigate to the <code>autora_workflow.py</code> file in the <code>researcher_hub</code> folder</p> </li> <li> <p>All we need to do is to change the experiment runner. Replace <code>RUNNER_TYPE = 'firebase'</code> in the beginning of the <code>autora_workflow.py</code> file with</p> </li> </ul> <pre><code>RUNNER_TYPE = 'prolific'\n</code></pre> <ul> <li>Next, you need to fill in the relevant data from Prolific and Firebase in the respective part ofh the <code>autora_workflow.py</code> file.</li> </ul> <pre><code># time between checks\nsleep_time = 30\n\n# Study name: This will be the name that will appear on prolific, participants that have participated in a study with the same name will be\n# excluded automatically\nstudy_name = 'my autora experiment'\n\n# Study description: This will appear as study description on prolific\nstudy_description= 'Two bandit experiment'\n\n# Study Url: The url of your study (you can find this in the Firebase Console)\nstudy_url = 'www.my-autora-experiment.com'\n\n# Study completion time (minutes): The estimated time a participant will take to finish your study. We use the compensation suggested by Prolific to calculate how much a participant will earn based on the completion time.\nstudy_completion_time = 5\n\n# Prolific Token: You can generate a token on your Prolific account\nprolific_token = 'my prolific token'\n\n# Completion code: The code a participant gets to prove they participated. If you are using the standard set up (with cookiecutter), please make sure this is the same code that you have provided in the .env file of the testing zone.\ncompletion_code = 'my completion code'\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/prolific/#update-env-in-testing_zone-optional","title":"Update .env in testing_zone (Optional)","text":"<p>The <code>firebase_prolific_runner</code> optimally allocates slots for the experiments you submit to Prolific. If you are done with testing, and are ready for data collection you may want to update the <code>.env</code> file in the <code>testing_zone</code> folder.</p> <ul> <li>Navigate to the <code>testing_zone</code> folder.</li> <li>Open the <code>.env</code> file.</li> <li>Set the <code>REACT_APP_useProlificId</code> variable to <code>True</code>. <pre><code>REACT_APP_useProlificId=\"True\"\n</code></pre></li> </ul>"},{"location":"examples/closed-loop-bandit-task/prolific/#summary","title":"Summary","text":"<ul> <li>This is it! Running the <code>autora_workflow.py</code> in the <code>researcher_hub</code> should now result in closed-loop reinforcement learning study that recruits human participants from Prolific to participate in your web-based experiment hosted on Firebase.</li> </ul> <p>Next: Customize your experiment.</p>"},{"location":"examples/closed-loop-bandit-task/setup/","title":"Set Up Project","text":"<p>First, we need to set up our local AutoRA project.</p> <p></p>"},{"location":"examples/closed-loop-bandit-task/setup/#create-repository","title":"Create Repository","text":"<p>To ease the setup process for this example, we provide a template repository that contains all the necessary files and configurations. </p> <p>Simply visit the following repository on GitHub: https://github.com/AutoResearch/autora-closed-loop-firebase-prolific-bandit-task</p> <p>Next, click on the green \"Use this template\" button to create a new repository in your account. </p> <p>You may then enter the name of the repository (e.g., \"closed-loop-study\") and click on the \"Create repository\" button.</p>"},{"location":"examples/closed-loop-bandit-task/setup/#clone-repository-or-open-it-in-github-codespace","title":"Clone Repository or Open it in GitHub Codespace","text":"<p>Once you created your own repository from the template, you can clone it to your local machine using <code>git clone</code>. However, we recommend using GitHub Codespaces for this example, as it provides a more streamlined development environment.</p> <p>To open the repository in GitHub Codespaces, click on the <code>Code</code> button and select <code>Create codespace on main</code>. </p>"},{"location":"examples/closed-loop-bandit-task/setup/#set-up-environment","title":"Set Up Environment","text":"<p>Once you cloned your repository or opened it in Codespaces, it is time to set up your environment. Here, we will use a Python virtual environment to manage dependencies.</p> <p>Success</p> <p>We recommend setting up your development environment using a manager like <code>venv</code>, which creates isolated python  environments. Other environment managers, like  virtualenv, pipenv, virtualenvwrapper,  hatch,  poetry,  are available and will likely work, but will have different syntax to the syntax shown here. Our packages are set up using <code>virtualenv</code> with <code>pip</code>.</p> <ul> <li>In the <code>&lt;project directory&gt;</code>, run the following command to create a new virtual environment in the <code>.venv</code> directory</li> </ul> <p><pre><code>python3 -m \"venv\" \".venv\" \n</code></pre> </p> <p>Hint</p> <p>If you have multiple Python versions installed on your system, it may be necessary to specify the Python version when creating a virtual environment. For example, run the following command to specify Python 3.8 for the virtual environment.  <pre><code>python3.8 -m \"venv\" \".venv\" \n</code></pre></p> <ul> <li>Activate it by running <pre><code>source \".venv/bin/activate\"\n</code></pre></li> </ul>"},{"location":"examples/closed-loop-bandit-task/setup/#install-dependencies","title":"Install Dependencies","text":"<ul> <li>First, install the cookiecutter package using pip via</li> </ul> <pre><code>pip install cookiecutter\n</code></pre> <p>We will use this package to automatically configure our project folder structure. </p> <ul> <li> <p>Then we install some python dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>We will also install firebase-tools: <pre><code>npm install -g firebase-tools\n</code></pre></p> </li> </ul> <p>We will use this package to deploy our firebase functions for storing data.</p> <p>Hint</p> <p>If you encounter any issues with the installation, make sure that you have the necessary permissions to install packages on your system. You may need to run the commands with <code>sudo</code> or as an administrator.</p>"},{"location":"examples/closed-loop-bandit-task/setup/#set-up-project-folder-with-coockiecutter","title":"Set Up Project Folder with Coockiecutter","text":"<p>To establish an online closed-loop for AutoRA, there are two key components that need to be configured:</p> <ol> <li> <p>AutoRA Workflow</p> <ul> <li>This workflow can be executed locally, on a server, or using <code>Cylc</code>. It must have the ability to communicate with a website, allowing for the writing of new conditions and reading of observation data.</li> <li>The AutoRA workflow can be customized by adding or removing AutoRA functions, such as AutoRA experimentalists or AutoRA theorists. It relies on an AutoRA Prolific Firebase runner to collect data from an online experiment hosted via Firebase and recruit participants via prolific.</li> </ul> </li> <li> <p>Website To Conduct Experiment:</p> <ul> <li>The website serves as a platform for conducting experiments and needs to be compatible with the AutoRA workflow.</li> <li>In this setup, we use <code>Firebase</code> to host on website.</li> </ul> </li> </ol> <p>To simplify the setup process, we provide a <code>cookiecutter</code> template that generates a project folder containing the following two directories:</p> <ol> <li> <p><code>researcher_hub</code>:</p> <ul> <li>This directory includes a basic example of an AutoRA workflow.</li> </ul> </li> <li> <p><code>testing_zone</code>:</p> <ul> <li>This directory provides a basic example of a website served with Firebase, ensuring compatibility with the AutoRA workflow.</li> </ul> </li> </ol> <p>Once you installed the packages above, you can create the project by running the following command in the root directory of your project:</p> <pre><code>cookiecutter https://github.com/AutoResearch/autora-user-cookiecutter\n</code></pre> <p>If cookiecutter is not recognized, you may need to run the following command:</p> <pre><code>python -m cookiecutter https://github.com/AutoResearch/autora-user-cookiecutter\n</code></pre> <ul> <li>You will be prompted to enter some information about your project. You can select single options by pressing SPACE and confirm your selection by pressing ENTER.</li> <li>You may first enter a project name, e.g., \"closed-loop-study\".</li> <li>Select <code>yes</code> to use advanced features.</li> <li>We are going to use a theorist from a different package, so we don't need to select any theorist here.</li> <li>Choose at least <code>autora[experimentalist-bandit-random]</code> and <code>autora[experimentalist-model-disagreement]</code>.</li> <li>Make sure to select the <code>autora[experiment-runner-firebase-prolific]</code> option</li> <li>Select <code>yes</code> to set up a firebase experiment. When asked to install further packages (create-react-app@5.0.1), select yes (y). This may take some time.</li> <li>Finally, select <code>JsPsych - Bandit</code> as project type.</li> </ul> <p>You should now have the following project structure:</p> <p></p> <p>Hint</p> <p>If you encounter any issues with the cookiecutter setup, make sure that you have the necessary permissions to install packages on your system. You may need to run the commands with <code>sudo</code> or as an administrator.</p> <p>Next: Set up Firebase to host our experiment.</p>"},{"location":"examples/closed-loop-bandit-task/testing/","title":"Testing the Workflow","text":"<p>Now that we have a better understanding of the workflow, let's test it out for real.</p> <ul> <li>Navigate to the <code>researcher_hub</code> folder and open the <code>autora_workflow.py</code> file.</li> <li> <p>Change the number of participants per cycle to <code>1</code>: <pre><code>PARTICIPANTS_PER_CYCLE = 1\n</code></pre> In this way, you only have to perform the experiment once to advance the workflow cycle.</p> </li> <li> <p>Also, change the number of cycles to <code>2</code>: <pre><code>CYCLES = 2\n</code></pre></p> </li> <li> <p>Finally, we will adjust the number of epochs we train the neural network for. We will set it to <code>100</code>: <pre><code>EPOCHS = 100\n</code></pre></p> </li> </ul> <p>Hint</p> <p>If you want to speed up the testing process, you can also set the number of experiment trials to a lower number, e.g., <code>TRIALS_PER_PARTICIPANTS = 30</code>. However, this will likely yield a poorer model fit. </p> <ul> <li>Run the workflow with <code>python autora_workflow.py</code>.</li> <li>Once it is running, head over to your website to test participate in the online experiment. You can find the link in the Firebase console. Navigate to your project and select <code>Hosting</code> in the left navigation menu. The domain of your experiment is listed on top under <code>Domains</code>.</li> </ul> <p></p> <ul> <li>Complete the experiment.</li> <li>Check the console output of the <code>autora_workflow.py</code> file to see if the data was collected correctly. If everything worked, you should see the discovered reinforcement learning equations in the console output: </li> </ul> <p></p> <p>The output displays a latex formatting of the equations. <code>Q_{chosen,}</code> refers to the value of the chosen box and <code>Q_{not chosen,}</code> refers to the value of the non-chosen box. The right hand side of the equation specifies how each of these values are updated as a function of the current value <code>Q_{chosen,}_{t}</code> or the reward received <code>Reward</code>. </p> <ul> <li>Once the model output is displayed, the workflow will upload a new experiment. You should now be able to complete the experiment one more time. This time, the workflow selected an experiment that it thinks best differentiates between the two identified models. </li> <li>After you complete the experiment one more time, the workflow will fit the two models again to the collected data. </li> <li>Once the two cycles are completed, and the workflow script finished, you should see the final models listed in the console.</li> </ul> <p></p> <ul> <li>Congratulations, you just set up and ran a closed-loop reinforcement learning study!</li> </ul> <p>Next: Connect your closed-loop experiment with Prolific in order to recruit real participants.</p>"},{"location":"examples/closed-loop-bandit-task/testingzone/","title":"Connect Project With Firebase","text":"<p>Next, we need to connect your project with Firebase, so we can deploy web experiments.</p> <p></p> <p>Move to the <code>testing_zone</code> folder in your project (e.g., locally or on GitHub Codespaces). The <code>testing_zone</code> contains a basic template for a website that is compatible with the AutoRA Experimentation Manager for Firebase and the AutoRA Recruitment Manager for Prolific.</p>"},{"location":"examples/closed-loop-bandit-task/testingzone/#copy-web-app-credentials-from-firebase","title":"Copy Web App Credentials From Firebase","text":"<ul> <li>Navigate to the Firebase console and select the project you created.</li> <li>On the gear-symbol next to <code>Project Overview</code>, you can find <code>Project settings</code>. </li> <li>You will find credentials in the tab <code>general</code> (you might have to scroll down). They should contain something like this <pre><code>// Your web app's Firebase configuration\nconst firebaseConfig = {\n  apiKey: \"AIzaadskKasjjasjKqsIt-UPXSDsdkdDhjBDU\",\n  authDomain: \"closed-loop-study.firebaseapp.com\",\n  projectId: \"closed-loop-study\",\n  storageBucket: \"closed-loop-study.appspot.com\",\n  messagingSenderId: \"338700008594\",\n  appId: \"1:3328439208594:web:136f203fe48f63ea4b\"\n};\n</code></pre></li> <li>Copy the credentials to the corresponding variables in the <code>.env</code> file in the <code>testing_zone</code> folder that was created on your system using create-react-app or cookiecutter. After you copied the credentials, it should look something like this: <pre><code>REACT_APP_apiKey=\"AIzaadskKasjjasjKqsIt-UPXSDsdkdDhjBDU\"\nREACT_APP_authDomain=\"closed-loop-study.firebaseapp.com\"\nREACT_APP_projectId=\"closed-loop-study\"\nREACT_APP_storageBucket=\"closed-loop-study.appspot.com\"\nREACT_APP_messagingSenderId=\"338700008594\"\nREACT_APP_appId=\"1:3328439208594:web:136f203fe48f63ea4b\"\nREACT_APP_devNoDb=\"True\"\nREACT_APP_useProlificId=\"False\"\nREACT_APP_completionCode=\"complete\"\n</code></pre></li> </ul> <p>For now, we will leave <code>REACT_APP_useProlificId=\"False\"</code>. We will change these later when we set up the Prolific integration.</p>"},{"location":"examples/closed-loop-bandit-task/testingzone/#configure-your-project-for-firebase","title":"Configure Your Project For Firebase","text":"<ul> <li> <p>Make sure to <code>cd</code> into the <code>testing_zone</code> folder in your terminal.</p> </li> <li> <p>In the <code>testing_zone</code> folder, enter the following commands in your terminal: First log in to your Firebase account using</p> </li> </ul> <p><pre><code>firebase login\n</code></pre> or (if you run this in Codespaces) <pre><code>firebase login --no-localhost\n</code></pre></p> <ul> <li>If asked to collect CLI and Emulator Suite usage information, you can choose to answer yes or no.</li> </ul> <p>Hint</p> <p>You may be asked to authenticate. Follow the instructions in your terminal to authenticate your account. This will require following a link that requires you to sign in with your Google account. Once you selected a Google account. Follow the setup and allow Firebase CLI access. Click <code>Yes, I just ran this command.</code> Assuming this is your session ID, click <code>Yes, this is my session ID</code>. Next, you are taken to a long string that you can copy into the terminal. It should look something like this: <code>2/0AVGKJADKHAXKg4ML_Usub28sdhjdskjsdktwWeAJASKFCjrKMck-SBEfgsddDmKQlfrDSQ</code>. </p> <ul> <li>Then initialize the Firebase project in the <code>testing_zone</code> folder by running: <pre><code>firebase init\n</code></pre></li> <li> <p>An interactive initialization process will now run in your command line. You can select options with SPACE and confirm your selection with ENTER.</p> </li> <li> <p>For the first question, select the following options. Once you selected both options, press ENTER.</p> <ul> <li><code>Firestore: Configure security rules and indexes files for Firestore</code></li> <li><code>Hosting: Configure files for Firebase Hosting and (optionally) set up GitHub Action deploys</code></li> </ul> </li> <li>Select <code>use an existing project</code>. Press ENTER.</li> <li>Select the project you created earlier. You should recognize it by the project name you entered when setting up Firebase. Press ENTER.</li> <li>Confirm the default option <code>(firestore.rules)</code> with ENTER. </li> <li>Confirm the default option <code>((firestore.indexes.json))</code> with ENTER.</li> <li>!!! IMPORTANT !!! Use the build directory instead of the public directory here. Enter <code>build</code> and press ENTER.</li> <li>Configure as a single-page app. Enter <code>y</code> and press ENTER.</li> <li>Don't set up automatic builds and deploys with GitHub. Enter <code>N</code> and press ENTER. </li> <li>Don't overwrite the index.html file if the question pops up.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/testingzone/#install-jspsych","title":"Install jsPsych","text":"<ul> <li>Next, we install jsPsych (still within the <code>testing_zone</code> folder): <pre><code>npm install jspsych@7.3.1\n</code></pre></li> <li>and a couple packages that we'll use for the experiment:</li> </ul> <pre><code>npm install @jspsych/plugin-html-keyboard-response\n</code></pre> <pre><code>npm install @jspsych-contrib/plugin-rdk\n</code></pre> <pre><code>npm install @jspsych-contrib/plugin-html-choice\n</code></pre> <pre><code>npm install @jspsych/plugin-fullscreen\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/testingzone/#build-and-deploy-to-firebase","title":"Build And Deploy To Firebase","text":"<p>The testing zone folder already contains a default web-based experiment. To serve this web-experiment on the internet, you must build and deploy it to Firebase. We will repeat this step later once we deploy our actual experiment.</p> <ul> <li>To build the experiment, run <pre><code>npm run build\n</code></pre></li> <li>To deploy the experiment to Firebase, run <pre><code>firebase deploy\n</code></pre></li> </ul> <p>If everything worked, you should see something like this: </p> <p>This includes a link to your web-based experiment. You can now open this link in your browser. However, at this moment, we haven't built an experiment yet, so you will see nothing interesting. Next, we will set up the AutoRA workflow and generate the experiment.</p>"},{"location":"examples/closed-loop-bandit-task/testingzone/#next-steps","title":"Next Steps","text":"<p>Next: Initialize your AutoRA workflow in the researcher hub.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/","title":"Workflow Explanation","text":"<p>In this section, we will take a deeper dive into the workflow of the closed-loop bandit task. You can find an overview of the study here. We will explain the different components of the workflow and how they interact with each other.</p> <ul> <li>Navigate to the workflow <code>autora_workflow.py</code> in the <code>researcher_hub</code> folder.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/workflow/#workflow-variables","title":"Workflow Variables","text":"<p>The workflow has a few variables that you can adjust to customize the workflow to your needs. Here are the most important ones:</p> <pre><code># *** CONSTANTS *** #\n\nRUNNER_TYPE = 'firebase'  # Options: synthetic, firebase, prolific\n\nTRIALS_PER_PARTICIPANTS = 100\nSAMPLES_PER_CYCLE = 1\nPARTICIPANTS_PER_CYCLE = 40\nCYCLES = 4\nINITIAL_REWARD_PROBABILITY_RANGE = [.2, .8]\nSIGMA_RANGE = [.2, .2]\n\nEPOCHS = 10 # 100\n</code></pre> <ul> <li><code>RUNNER_TYPE</code>: This variable determines which runner to use. You can choose between <code>synthetic</code>, <code>firebase</code>, and <code>prolific</code>. The <code>synthetic</code> runner is used to test the workflow with simulated data. The <code>firebase</code> runner is used to run the workflow with the actual web experiment on Firebase. The <code>prolific</code> runner is used to run the workflow on Firebase and recruit participants via Prolific.</li> <li><code>TRIALS_PER_PARTICIPANTS</code>: The number of trials each participant will have to complete in the two-armed bandit experiment.</li> <li><code>SAMPLES_PER_CYCLE</code>: The number of new experiment conditions (i.e., reward sequences) to generate in each cycle. Here, we generate one new experiment condition per cycle and let all participants perform the experiment with the same reward sequence.</li> <li><code>PARTICIPANTS_PER_CYCLE</code>: The number of participants to recruit in each cycle. </li> <li><code>CYCLES</code>: The number of cycles to run the workflow. In each cycle, we generate new experiment conditions, recruit participants, and discover a learning rule.</li> <li><code>INITIAL_REWARD_PROBABILITY_RANGE</code>: The range of initial reward probabilities for the two boxes. The reward probabilities are randomly sampled from this range for each new experiment condition.</li> <li><code>SIGMA_RANGE</code>: The range of the standard deviation of the Gaussian noise added to the reward probabilities. This noise models the stochasticity of the rewards.</li> <li><code>EPOCHS</code>: The number of epochs to train the recurrent neural network (RNN) on the behavioral data. The RNN is used to model the choice behavior of participants.</li> </ul>"},{"location":"examples/closed-loop-bandit-task/workflow/#seeding-the-workflow","title":"Seeding the Workflow","text":"<p>The workflow starts by seeding the random number generator to ensure reproducibility:</p> <pre><code>seed = 11\n\n# for reproducible results:\nif seed is not None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/workflow/#experimental-variables","title":"Experimental Variables","text":"<p>This study includes one independent and one dependent variable. <pre><code>variables = VariableCollection(\n    independent_variables=[Variable(name=\"reward-trajectory\")],\n    dependent_variables=[Variable(name=\"choice-trajectory\")]\n)\n</code></pre></p>"},{"location":"examples/closed-loop-bandit-task/workflow/#independent-variable","title":"Independent Variable","text":"<p>The independent variable corresponds to the actual reward trajectory for the two (red and blue) boxes. It is a 2 x <code>n_trials</code> vector with entries between 0 and 1. Each row corresponds to the reward probability of one box. </p>"},{"location":"examples/closed-loop-bandit-task/workflow/#dependent-variable","title":"Dependent Variable","text":"<p>The dependent variable corresponds to the choice trajectory of the participant. It is a 2 x <code>n_trials</code> vector with entries 0 or 1, indicating the choice of the participant for each of the two boxes. Thus, every column is 1 one-hot encoding of the choice, indicating whether the participant chose the red or blue box.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#autora-state","title":"AutoRA State","text":"<p>In this example, we choose a non-standard state for the AutoRA workflow. We use a <code>RnnState</code> that extends the <code>StandardState</code> with additional models.  <pre><code>@dataclass(frozen=True)\nclass RnnState(StandardState):\n    models_additional:  List[BaseEstimator] = field(\n        default_factory=list,\n        metadata={\"delta\": \"extend\"},\n    )\n\n# initialize the state:\nstate = RnnState(variables=variables)\n</code></pre></p> <p>The <code>RnnState</code> is a dataclass that is used to store the state of the AutoRA workflow. You can learn more about non-standard states in the User Guide.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#experimentalists","title":"Experimentalists","text":"<p>In this study, we will leverage two experimentalist, one for initializing the first experiment conditions and one for generating new experiment conditions in each cycle. </p>"},{"location":"examples/closed-loop-bandit-task/workflow/#random-sampler","title":"Random Sampler","text":"<p>The <code>RandomPooler</code> is used to initialize the first experiment conditions. It randomly samples reward trajectories from the initial reward probability range and sigma range.</p> <pre><code>@on_state()\ndef pool_on_state(num_samples, n_trials=TRIALS_PER_PARTICIPANTS):\n    \"\"\"\n    This is creates `num_samples` randomized reward-trajectories of length `n_trials`\n    \"\"\"\n    sigma = np.random.uniform(SIGMA_RANGE[0], SIGMA_RANGE[1])\n    trajectory_array = bandit_random_pool(\n        num_rewards=2,\n        sequence_length=n_trials,\n        initial_probabilities=[INITIAL_REWARD_PROBABILITY_RANGE, INITIAL_REWARD_PROBABILITY_RANGE],\n        sigmas=[sigma, sigma],\n        num_samples=num_samples\n    )\n    trajectory_df = pd.DataFrame({'reward-trajectory': trajectory_array})\n    return Delta(conditions=trajectory_df)\n</code></pre> <p>Note that it is important to use the <code>@on_state()</code> decorator to ensure that the pooler is only called when the state is updated. This wrapper ensures that the new reward trajectory <code>trajectory_df</code> is stored int the <code>conditions</code> field of the AutoRA <code>state</code>.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#model-disagreement-sampler","title":"Model Disagreement Sampler","text":"<p>The <code>ModelDisagreementPooler</code> is used to generate new experiment conditions in each cycle. It uses the best two models discovered so far to generate new reward trajectories that best distinguish between the two models.</p> <p>Since the predictions of a model has a non-standard format (it isn't a single number but a 2 x <code>n_trials</code> array), we need to create a custom distance function. </p> <p>The prediction for a model is a list of two-dimensional vectors, e.g., <code>array([[0.5, 0.5], [0.68..., 0.31...], ...])</code>, indicating the reward probability for the boxes on each trial. The following code specifies how to compute a distance between two proposed reward trajectories specified in this format:</p> <pre><code>def custom_distance(prob_array_a, prob_array_b):\n    return np.mean([(prob_array_a[0] - prob_array_b[0])**2 + (prob_array_a[1] - prob_array_b[1])**2])\n</code></pre> <p>This distance function is then used in model disagreement sampler, which is also wrapped in an <code>@on_state()</code> decorator:</p> <pre><code>@on_state()\ndef model_disagreement_on_state(\n        conditions, models, models_additional, num_samples):\n    conditions = model_disagreement_sampler_custom_distance(\n        conditions=conditions['reward-trajectory'],\n        models=[models[-1], models_additional[-1]],\n        distance_fct=custom_distance,\n        num_samples=num_samples,\n    )\n    return Delta(conditions=conditions)\n</code></pre> <p>Note that this sampler takes two models as input, which are the best two models discovered so far. It then generates new reward trajectories that best distinguish between the two models.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#experiment-runners","title":"Experiment Runners","text":"<p>The experiment runners are responsible for generating data. Here, we provide three different experiment runners. You will be able to switch between these runners by changing the <code>RUNNER_TYPE</code> variable at the beginning of the workflow.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#synthetic-runner","title":"Synthetic Runner","text":"<p>The <code>SyntheticRunner</code> is used to generate synthetic data for testing the workflow. It generates synthetic data by simulating a known reinforcement learning rule (Q-learning) on the reward trajectories. The runner is part of the <code>autora-synthetic</code> package.</p> <pre><code>runner = q_learning()\n</code></pre> <p>The synthetic runner is also wrapped in an <code>@on_state()</code> decorator, allowing it to add to the <code>experiment_data</code> field in the AutoRA <code>state</code>. </p> <pre><code>@on_state()\ndef runner_on_state_synthetic(conditions):\n    choices, choice_probabilities = runner.run(conditions, return_choice_probabilities=True)\n    experiment_data = pd.DataFrame({\n        'reward-trajectory': conditions['reward-trajectory'].tolist(),\n        'choice-trajectory': choices,\n        'choice-probability-trajectory': choice_probabilities\n    })\n    return Delta(experiment_data=experiment_data)\n</code></pre> <p>The wrapper takes as input the reward trajectories <code>conditions</code>, and generates the corresponding choices. Because this is a synthetic model, we can also extract the choice probability for each option (which are unknown if the choices are generated from a human participant).</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#firebase-runner","title":"Firebase Runner","text":"<p>The <code>FirebaseRunner</code> is used to run the actual web experiment on Firebase. It uploads new experiment conditions to the Firestore database and downloads the results of the experiment. </p>"},{"location":"examples/closed-loop-bandit-task/workflow/#credentials","title":"Credentials","text":"<p>The firebase credentials are used to configure the runner (yours should look different compared to the ones below).</p> <pre><code># firebase credentials\nfirebase_credentials = {\n  \"type\": \"service_account\",\n  \"project_id\": \"closed-loop-bandit\",\n  \"private_key_id\": \"1eYOURCREDENTIALS7a43c\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvYOURCREDENTIALSu/VNaT\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"firebase-adminsdk-jkvmg@closed-loop-bandit.iam.gserviceaccount.com\",\n  \"client_id\": \"106074818984686391054\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-jkvmg%40closed-loop-bandit.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/workflow/#runner-specification","title":"Runner Specification","text":"<p>The firebase runner is defined by its credentials, which are used to authenticate with the Firebase project. In addition, we can specify the <code>time_out</code> (in minutes) and <code>sleep_time</code> (in seconds) for the runner. The former specifies the minutes a participant has available to complete the experiment, and the latter specifies the seconds between checks for new data. </p> <pre><code>sleep_time = 30\n\nstudy_completion_time = 5\n\nexperiment_runner_firebase = firebase_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=study_completion_time,\n    sleep_time=sleep_time)\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/workflow/#the-actual-web-experiment","title":"The Actual Web Experiment","text":"<p>The actual experiment is written with the <code>jsPsych</code> in <code>JavaScript</code> and located in <code>testing_zone/src/design/main.js</code>. The experiment is a simple two-armed bandit task where participants have to choose between a red and a blue box. The reward probabilities of the boxes change over time, and participants must learn to select the box that provides the most reward.</p> <p>Here, we won't explain how to program jsPsych experiments. Please refer to the jsPsych documentation for more information.</p> <p>Note that the <code>main</code> function of the <code>main.js</code> file starts like this</p> <pre><code>const main = async (id, condition) =&gt; {\n\n    const timeline_variables = JSON.parse(condition)\n    ...\n</code></pre> <p>The function takes an <code>id</code> and a <code>condition</code> as input. The <code>id</code> is the participant id, and the <code>condition</code> contains the reward trajectory that the participant should interact with. However, the <code>condition</code> also contains other things. It is a JSON string that is parsed into a JavaScript object, containing the entire timeline of the experiment. This timeline specifies which boxes are presented where and which one is rewarded.</p> <p>Our workflow uses a helper function to ensure that the <code>condition</code> is correctly formatted and uploaded to the Firebase database.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#helper-function-for-uploading-experiment-conditions-to-web-experiment","title":"Helper Function for Uploading Experiment Conditions to Web Experiment","text":"<p>The following helper function transforms a condition (the output of an experimentalist) to a trial sequence, that is readable by the jsPsych experiment. jsPsych expects a trial sequence in the following format: <code>trial_sequence = {'feature_a': [1, 2, 3, ...], 'feature_b': ['red', 'green', ...], ...}</code>.</p> <pre><code>def _condition_to_trial_sequence(conditions):\n    \"\"\"\n    Transforms conditions created by the experimentalist in a list of trial sequences\n    \"\"\"\n    trial_sequences = []\n    for c in conditions['reward-trajectory'].tolist():\n        sequence = []\n        if len(c) % 2:\n            print('WARNING: trajectory has an odd number of entries. ')\n\n        # create a counterbalanced position list:\n        _n = len(c) // 2\n        pos_list = [['left', 'right']] * _n + [['right', 'left']] * _n\n        random.shuffle(pos_list)\n\n        # a condition c is a list of values (c = [[0, 1], [1, 1], [0, 0], ...])\n        for idx, trial in enumerate(c):\n            sequence.append({'values': trial.tolist(), 'pos': pos_list[idx]})\n        trial_sequences.append(sequence)\n    return trial_sequences\n</code></pre> <p>Note that this function also counterbalances the position of the boxes (left or right) to avoid any biases due to the position of the boxes.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#helper-function-for-processing-experimental-data","title":"Helper Function for Processing Experimental Data","text":"<p>We also have to transform the data returned by the <code>jsPsych</code> experiment to our <code>experiment_data</code>. This is achieved by the following function:</p> <pre><code>def _jsPsych_to_experiment_data(data):\n    result = []\n    # For the output format of the jsPsych script, see the return value in testing_zone/src/design/main.js\n    for item in data:\n        parsed = json.loads(item)\n        condition = json.loads(parsed['condition'])\n        observation = parsed['observation']\n        c_subj = {'reward-trajectory': [], 'choice-trajectory': []}\n        for c, o in zip(condition, observation['values']):\n            t = c['values']\n            c_subj['reward-trajectory'].append(t)\n            if o == 0:\n                c_subj['choice-trajectory'].append([1, 0])\n            else:\n                c_subj['choice-trajectory'].append([0, 1])\n\n        c_subj['reward-trajectory'] = np.array(c_subj['reward-trajectory'])\n        c_subj['choice-trajectory'] = np.array(c_subj['choice-trajectory'])\n        result.append(c_subj)\n    return result\n</code></pre> <p>Here, we extract both the choices and the reward sequence from the data returned by the experiment. The choices are stored in the <code>chocie-trajectory</code> field, and the reward sequence is stored in the <code>reward-trajectory</code> field. Critically, both fields are designed as lists of one-hot vectors, with each vector specifying the choice and reward for the two options in each trial, respectively.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#wrapper-for-firebase-runner","title":"Wrapper for Firebase Runner","text":"<p>Ultimately, we will wrap the Firebase runner in an <code>@on_state()</code> decorator. This wrapper allows the runner to add to the <code>experiment_data</code> field in the AutoRA <code>state</code>. It makes use of the helper functions defined above.</p> <pre><code>@on_state()\ndef runner_on_state_firebase(conditions):\n    trial_sequences = _condition_to_trial_sequence(conditions)\n    data = experiment_runner_firebase(trial_sequences)\n    experiment_data = _jsPsych_to_experiment_data(data)\n    return Delta(experiment_data=experiment_data)\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/workflow/#prolific-runner","title":"Prolific Runner","text":"<p>The <code>firebase_prolific_runner</code> is used to recruit participants via Prolific. It uploads the experiment to Firebase and uses Prolific to recruit participants for the experiment.</p> <p>We begin with specifying some variables for the runner...</p> <pre><code># time in seconds between checks\nsleep_time = 30\n\n# Study name: This will be the name that will appear on prolific, participants that have participated in a study with the same name will be\n# excluded automatically\nstudy_name = 'my autora experiment'\n\n# Study description: This will appear as study description on prolific\nstudy_description= 'Two bandit experiment'\n\n# Study Url: The url of your study (you can find this in the Firebase Console)\nstudy_url = 'www.my-autora-experiment.com'\n\n# Study completion time (minutes): The estimated time a participant will take to finish your study. We use the compensation suggested by Prolific to calculate how much a participant will earn based on the completion time.\nstudy_completion_time = 5\n\n# Prolific Token: You can generate a token on your Prolific account\nprolific_token = 'my prolific token'\n\n# Completion code: The code a participant gets to prove they participated. If you are using the standard set up (with cookiecutter), please make sure this is the same code that you have provided in the .env file of the testing zone.\ncompletion_code = 'my completion code'\n</code></pre> <p>...and then define the runner based on these variables. </p> <pre><code>experiment_runner_prolific = firebase_prolific_runner(\n            firebase_credentials=firebase_credentials,\n            sleep_time=sleep_time,\n            study_name=study_name,\n            study_description=study_description,\n            study_url=study_url,\n            study_completion_time=study_completion_time,\n            prolific_token=prolific_token,\n            completion_code=completion_code,\n        )\n</code></pre> <p>Similar to the firebase runner, we can wrap it in an <code>@on_state()</code> decorator to add to the <code>experiment_data</code> field in the AutoRA <code>state</code>.</p> <pre><code>@on_state()\ndef runner_on_state_prolific(conditions):\n    trial_sequences = _condition_to_trial_sequence(conditions)\n    data = experiment_runner_prolific(trial_sequences)\n    experiment_data = _jsPsych_to_experiment_data(data)\n    return Delta(experiment_data=experiment_data)\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/workflow/#theorists","title":"Theorists","text":"<p>Our goal is to automatically discover a learning rule that best explains the choice behavior of all participants. We will accomplish this using a method that combines recurrent neural network modeling with equation discovery.</p> <p></p> <p>This method, available in the <code>autora[theorist-rnn-sindy-rl]</code> package, fits the choice behavior of participants with a recurrent neural network, which computes values for both of the two options (the red box and the blue box), and then chooses between the options based on their relative value. Once the neural network is fit, the method applies an equation discovery technique called \"Sparse Identification of Non-Linear Systems\" (SINDy) to extract the learning rule that the network has discovered.</p> <p>Here, we define two theorist, each with a different parameterization of the method above:</p> <pre><code>theorist = RNNSindy(2, epochs=EPOCHS, polynomial_degree=2)\ntheorist_additional = RNNSindy(2, epochs=EPOCHS, polynomial_degree=1)\n</code></pre> <p>The firs theorist uses a polynomial degree of 2, while the second uses a polynomial degree of 1. The polynomial degree specifies the complexity of the learning rule that the method can discover. A higher polynomial degree allows for more complex learning rules but also increases the risk of overfitting.</p> <p>We wrapp both of these theorists in an <code>@on_state()</code> decorator to add to the <code>models</code> and <code>models_additional</code> fields in the AutoRA <code>state</code>.</p> <pre><code>@on_state()\ndef theorist_on_state(experiment_data):\n    x = experiment_data['reward-trajectory']\n    y = experiment_data['choice-trajectory']\n    return Delta(models=[theorist.fit(x, y)])\n\n\n@on_state()\ndef theorist_additional_on_state(experiment_data):\n    x = experiment_data['reward-trajectory']\n    y = experiment_data['choice-trajectory']\n    return Delta(models_additional=[theorist_additional.fit(x, y)])\n</code></pre> <p>The <code>RNNSindy</code> theorists take as input the reward trajectories <code>x</code> and the choice trajectories <code>y</code> and return a model that best explains the choice behavior of the participants.</p>"},{"location":"examples/closed-loop-bandit-task/workflow/#autora-workflow","title":"AutoRA Workflow","text":"<p>Finally, we specify the AutoRA workflow based on the components defined above.</p> <pre><code>for c in range(1, CYCLES + 1):\n\n    if len(state.models) &gt; 0:\n        state = pool_on_state(state, num_samples=20)\n        state = model_disagreement_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n    else:\n        state = pool_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n\n    if RUNNER_TYPE == 'synthetic':\n        state = runner_on_state_synthetic(state)\n    elif RUNNER_TYPE == 'firebase':\n        state = runner_on_state_firebase(state)\n    elif RUNNER_TYPE == 'prolific':\n        state = runner_on_state_prolific(state)\n\n    state = theorist_on_state(state)\n    state = theorist_additional_on_state(state)\n</code></pre> <p>Here, we repeat the following steps for each cycle: 1. If we don't have any models yet, we will sample the initial experiment conditions using the <code>pool_on_state</code> function. Otherwise, we will use the <code>model_disagreement_on_state</code> function to generate new experiment conditions that best distinguish between the two best models discovered so far. 2. We will then run the experiment using the specified runner.  3. Finally, we will fit the choice behavior of the participants with the recurrent neural network and extract the learning rule using the SINDy method, for each of the two theorists. </p> <p>The final lines of the code simply print the discovered learning rules, stored in the <code>models</code> and <code>models_additional</code> fields of the AutoRA <code>state</code>.:</p> <pre><code>    model = state.models[-1]\n    model_additional = state.models_additional[-1]\n\n    equations_model = parse_equation(model)\n    equation_model_additional = parse_equation(model_additional)\n\n    print('# MODEL DEGREE = 2#')\n    print(f'chosen: {equations_model[0]}')\n    print(f'non chosen: {equations_model[1]}')\n\n    print('# MODEL DEGREE = 1#')\n    print(f'chosen: {equation_model_additional[0]}')\n    print(f'non chosen: {equation_model_additional[1]}')\n</code></pre>"},{"location":"examples/closed-loop-bandit-task/workflow/#next-steps","title":"Next Steps","text":"<p>Next: Test your workflow on Firebase.</p>"},{"location":"examples/closed-loop-basic/","title":"Basic Closed-Loop Psychophysics Study","text":"<p>In this example, we will guide you through setting up a closed-loop behavioral study for a psychophysics experiment. By leveraging AutoRA, you\u2019ll build a dynamic research workflow that iterates between model discovery, experimental design, and behavioral data collection, all within the context of a psychophysics experiment. The ultimate goal is to make AutoRA iteratively uncover an equation that characterizes human participants' ability to distinguish between various visual stimuli.</p> <p>This example provides a hands-on approach to understanding closed-loop behavioral research in the context of the AutoRA framework. </p>"},{"location":"examples/closed-loop-basic/#what-youll-learn","title":"What You\u2019ll Learn:","text":"<ul> <li>Set up a closed-loop AutoRA workflow: Learn how to create an automated discovery process, iterating between hypothesis generation and data collection.</li> <li>Automate experimental design with SweetPea: Use SweetPea to generate experimental designs that adapt as the study progresses.</li> <li>Generate behavioral experiments with SweetBean: Automate the creation of simple behavioral experiments, minimizing the need for manual coding.</li> <li>Host experiments using Google Firebase: Set up a server for hosting your behavioral experiments, making them accessible to participants.</li> <li>Store experimental data with Google Firestore: Efficiently manage and store participant data collected from your experiment.</li> <li>Collect data from real participants with Prolific: Recruit and manage participants through Prolific, ensuring high-quality behavioral data.</li> </ul>"},{"location":"examples/closed-loop-basic/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Basic Python knowledge: While most of the workflow is Python-based, only a basic level of understanding is needed to follow along.</li> <li>Minimal JavaScript knowledge: Since the behavioral experiments are implemented in JavaScript (via jsPsych), SweetBean will handle much of the complexity for you. The code is generated in Python and converted into JavaScript, so only a minimal understanding of JavaScript is required.</li> <li>A Google account: You will need a Google account to use Google Firebase and Firestore.</li> </ul>"},{"location":"examples/closed-loop-basic/#study-overview","title":"Study Overview","text":"<p>In this example study, we are interested in quantifying participant's ability to differentiate between two visual stimuli. Specifically, we will ask participants to indicate whether the number of dots in a left stimulus is the same as the number of dots in a right stimulus.</p> <p></p> <p>Our goal is to predict the participant's response based on the number of dots in the left and right stimuli. We will use two methods of predicting the response: - a simple logistic regression model - an equation discovery algorithm (Bayesian Machine Scientist)</p> <p>After each data collection phase, we will fit the logistic regression model and the Bayesian Machine Scientist from the  <code>autora[theorist-bms]</code> package to the collected data. We will then use both models to determine the next set of experimental conditions worth testing. Specifically, we will identify experimental conditions for which the models disagree the most, using the <code>autora[experimentalist-model-disagreement]</code> package.</p> <p>Critically, we will leverage AutoRA to embed the entire research process into a closed-loop system. This system will automatically generate new experimental conditions, collect data from the web experiment, and update the models based on the collected data.</p>"},{"location":"examples/closed-loop-basic/#system-overview","title":"System Overview","text":"<p>Our closed-loop system consists of a bunch of interacting components. Here is a high-level overview of the system: </p> <p>Our closed-loop system will have two projects talking to each other. The Firebase project will host and run the web experiment that participants interact with. Our local AutoRA project will host the code that runs the AutoRA workflow, which will generate new experiment conditions, collect data from the web experiment, and update the model based on the collected data. </p>"},{"location":"examples/closed-loop-basic/#firebase-project","title":"Firebase Project","text":"<p>To run an online experiment, we need to host it as a web app. We will leverage Google Firebase to host our web app. Participants from Prolific can then interact with the web app to complete the experiment.  </p> <p>Our experiment is configured by experiment conditions, which are stored in a Google Firestore database. In addition, we will use this database to store collected behavioral data from the web experiment. </p>"},{"location":"examples/closed-loop-basic/#our-local-autora-project","title":"Our Local AutoRA Project","text":"<p>The local project will consist of two folders. The ``testing_zone```` folder will contain the web app that participants interact with. </p> <p>The <code>researcher_hub</code> folder will contain the AutoRA workflow. </p>"},{"location":"examples/closed-loop-basic/#next-steps","title":"Next Steps","text":"<p>Each step in the example will lead guide you to set up each component of the closed-loop system. </p> <p>By the end of this example, you\u2019ll be able to create a fully automated behavioral closed-loop study that adapts based on the collected participant data.</p> <p>Next: Set up the local project.</p>"},{"location":"examples/closed-loop-basic/experiment/","title":"Constructing the Experiment","text":"<p>In this study, we will want to design a simple psychophysics experiment in which participants are tasked to decide which of two sets of dots has more dots. </p> <p>In this part of the example, we will code up two functions, one function <code>trial_sequence</code> for generating a sequence of experiment conditions, and another function <code>stimulus_sequence</code> for compiling the corresponding experiment. We will need both functions to run the AutoRA workflow. Accordingly, we will integrate these functions into the Researcher Hub.</p> <p>Hint</p> <p>Below, we will leverage other packages, such as SweetPea and SweetBean for generating experiment sequences and stimulus sequences, respectively. That said, you are not required to use those packages; you can also write your own functions for generating experiment sequences and stimulus sequences.</p>"},{"location":"examples/closed-loop-basic/experiment/#experiment-overview","title":"Experiment Overview","text":""},{"location":"examples/closed-loop-basic/experiment/#independent-and-dependent-variables","title":"Independent and Dependent Variables","text":"<p>The experiment has two independent variables: The number of dots in the first set and the number of dots in the second set. The dependent variable is the participant's response, i.e., whether they correctly identified which set has more dots.</p>"},{"location":"examples/closed-loop-basic/experiment/#procedure","title":"Procedure","text":"<p>The experiment will consist of a series of 20 trials. Each trial begins with the display of a fixation cross. After a short delay, two sets of dots will be displayed, one at the left and one at the right. Participants will be asked to perform a same-different task, i.e., to select whether the number of dots in the two sets is the same by pressing <code>y</code> or not by pressing <code>n</code>.</p>"},{"location":"examples/closed-loop-basic/experiment/#stimuli","title":"Stimuli","text":"<p>The stimuli consist of two sets of dots. The number of dots in each set can range from 1 to 100 and is determined by the AutoRA Workflow. Here, for simplicity, we constrain each experiment such that there are only two possible values for the number of dots in each set.</p>"},{"location":"examples/closed-loop-basic/experiment/#generating-trial-sequences-with-sweetpea","title":"Generating Trial Sequences With SweetPea","text":"<p>Next, we want to write a function that generates a counterbalanced sequence of trials, i.e., a list of trials that ensures that each condition appears equally often in each position of the sequence. </p> <p>Here, we will generate such a sequence of trials using SweetPea. SweetPea is a declarative language implemented in Python that allows you to define experimental designs in a concise and readable way, and to automatically generate counterbalanced sequences of trials.</p> <p>Hint</p> <p>If you want to tinker with the SweetPea code just for this example, you can open the corresponding notebook in Google Colab by clicking the following badge: </p> <p>The following function generates an experimental sequence of at least <code>min_trials</code> trials. Each trial consists of two conditions: the number of dots in the first set (<code>num_dots_1</code>) and the number of dots in the second set (<code>num_dots_2</code>). The function <code>trial_sequence</code> returns a list of dictionaries, where each dictionary represents a trial of a counterbalanced experiment sequence</p> <pre><code>from sweetpea import Factor, MinimumTrials, CrossBlock, synthesize_trials, CMSGen, experiments_to_dicts\n\ndef trial_sequence(num_dots_1, num_dots_2, min_trials):\n\n  # define regular factors\n  num_dots_left = Factor('dots left', [num_dots_1, num_dots_2])\n  num_dots_right = Factor('dots right', [num_dots_1, num_dots_2])\n\n  # define experimental block\n  design = [num_dots_left, num_dots_right]\n  crossing = [num_dots_left, num_dots_right]\n  constraints = [MinimumTrials(min_trials)]\n\n  block = CrossBlock(design, crossing, constraints)\n\n  # synthesize trial sequence\n  experiment = synthesize_trials(block, 1, CMSGen)\n\n  # export as dictionary\n  return experiments_to_dicts(block, experiment)[0]\n</code></pre>"},{"location":"examples/closed-loop-basic/experiment/#integrating-the-code-into-the-researcher-hub","title":"Integrating the Code Into the Researcher Hub","text":"<ul> <li>Create a new python script in the <code>researcher_hub</code> folder and name it <code>trial_sequence.py</code>. Copy the code above into the script.</li> <li>To execute the script you will also need to <code>pip install sweetpea</code> in your virtual environment.</li> <li>Make sure to add <code>sweetpea</code> as a dependency in the <code>requirements.txt</code> file within the <code>researcher_hub</code> folder.</li> </ul> <p>Below, we elaborate a bit more on the code. However, if you are already familiar with SweetPea, you may skip the \"Explanation\" section. Alternatively, you can gain an intuition for the code in the corresponding notebook:  . </p>"},{"location":"examples/closed-loop-basic/experiment/#code-explanation","title":"Code Explanation","text":"<p>The first two lines define the experimental factors.  <pre><code>num_dots_left = Factor('dots left', [num_dots_1, num_dots_2])\nnum_dots_right = Factor('dots right', [num_dots_1, num_dots_2])\n</code></pre></p> <p>The first experimental factor indicates the number of dots in the left stimulus. It has two levels, i.e., two possible values for the number of dots, which is given by <code>num_dots_1</code> and <code>num_dots_2</code>. The same goes for the second experimental factor, which indicates the number of dots in the right stimulus.</p> <p>The next lines defines the experimental design, which includes all experimental factors, irrespective of whether they are counterbalanced or not. <pre><code>design = [num_dots_left, num_dots_right]\n</code></pre></p> <p>The next line defines the crossing of the experimental factors, i.e., the conditions that need to be counterbalanced. <pre><code>crossing = [num_dots_left, num_dots_right]\n</code></pre></p> <p>Then, we define the constraints of the experimental design. Here, we only have one constraint, which is to have a minimum number of trials, which is given by <code>min_trials</code>. <pre><code>constraints = [MinimumTrials(min_trials)]\n</code></pre></p> <p>Finally, we define the experimental block, which includes the design, crossing, and constraints. <pre><code>block = CrossBlock(design, crossing, constraints)\n</code></pre></p> <p>The next line synthesizes the experimental sequence. The function <code>synthesize_trials</code> generates a counterbalanced sequence of trials based on the experimental block. <pre><code>experiment = synthesize_trials(block, 1, CMSGen)\n</code></pre></p> <p>The last line exports the experimental sequence as a list of dictionaries. Here, we only care about a single experiment, so we select the first one in the list. <pre><code>return experiments_to_dicts(block, experiment)[0]\n</code></pre></p>"},{"location":"examples/closed-loop-basic/experiment/#generating-the-stimulus-sequences-with-sweetbean","title":"Generating the Stimulus Sequences With SweetBean","text":"<p>Next, we need to write a function that automates the generation of our web-based experiment, which is then sent to Firebase.</p> <p>Here, we will generate such a web experiment using SweetBean. SweetBean is a declarative language implemented in Python that allows you to define describe a sequence of events for an experiment in Python, and then generate a corresponding web-based experiment in JavaScript.</p> <ul> <li>Within your current environment, make sure to install the latest version of SweetBean directly from the GitHub repository: <pre><code>pip install sweetbean \n</code></pre></li> </ul> <p>The following function generates JavaScript code for a web-based experiment. The experiment corresponds to a sequence of stimuli, thereby we call it <code>stimulus_sequence</code>. It returns a string that contains the JavaScript code for the experiment.</p> <p>Note that the function receives as input a <code>timeline</code> specifying a sequence of timeline variables. This timeline corresponds to the trial sequence generated by the SweetPea program above.</p> <p>Hint</p> <p>If you want to tinker with the SweetBean code just for this example, you can open the corresponding notebook in Google Colab by clicking the following badge: </p> <pre><code>from sweetbean.stimulus import Text, Fixation, RandomDotPatterns\nfrom sweetbean import Block, Experiment\nfrom sweetbean.variable import TimelineVariable\n\ndef stimulus_sequence(timeline):\n\n  # INSTRUCTION BLOCK\n\n  # generate several text stimuli that serve as instructions\n  introduction_welcome = Text(text='Welcome to our perception experiment.&lt;br&gt;&lt;br&gt; \\\n                                          Press the SPACE key to continue.', \n                                    choices=[' '])\n\n  introduction_pictures = Text(text='Each picture contains two sets of dots, one left and one right.&lt;br&gt;&lt;br&gt;\\\n                                       Press the SPACE key to continue.', \n                                    choices=[' '])\n\n  introduction_responses = Text(text='You have to indicate whether the two sets contain an equal number of dots.&lt;br&gt;&lt;br&gt;\\\n                                       Press the y-key for yes (equal number) and&lt;br&gt; the n-key for no (unequal number).&lt;br&gt;&lt;br&gt;\\\n                                       Press the SPACE key to continue.', \n                                    choices=[' '])\n\n  introduction_note = Text(text='Note: For each picture, you have only 2 seconds to respond, so respond quickly.&lt;br&gt;&lt;br&gt;\\\n                                       You can only respond with the y and n keys while the dots are shown.&lt;br&gt;&lt;br&gt; \\\n                                       Press the SPACE key to BEGIN the experiment.', \n                                    choices=[' '])\n\n\n  # create a list of instruction stimuli for the instruction block\n  introduction_list = [introduction_welcome, \n                       introduction_pictures, \n                       introduction_responses, \n                       introduction_note]\n\n  # create the instruction block\n  instruction_block = Block(introduction_list)\n\n  # EXIT BLOCK\n\n  # create a text stimulus shown at the end of the experiment\n  instruction_exit = Text(duration=3000, \n                                  text='Thank you for participating in the experiment.', \n                                  )\n\n  # create a list of instruction stimuli for the exit block\n  exit_list = [instruction_exit]\n\n  # create the exit block\n  exit_block = Block(exit_list)\n\n  # TASK BLOCK\n\n  # define fixation cross\n  fixation = Fixation(1500)\n\n  # define the stimuli features as timeline variables\n  dot_stimulus_left = TimelineVariable('dots left')\n  dot_stimulus_right = TimelineVariable('dots right')\n\n  # We can define a stimulus as a function of those stimulus features\n  rdp = RandomDotPatterns(\n      duration=2000,\n      number_of_oobs=[dot_stimulus_left, dot_stimulus_right],\n      number_of_apertures=2,\n      choices=[\"y\", \"n\"],\n      background_color=\"black\",\n  )\n\n  # define the sequence of events within a trial\n  event_sequence = [fixation, rdp]\n\n  # group trials into blocks\n  task_block = Block(event_sequence, timeline)\n\n  # EXPERIMENT\n\n  # define the entire experiment\n  experiment = Experiment([instruction_block, task_block, exit_block])\n\n  # return a js string to transfer to autora\n  return experiment.to_js_string(as_function=True, is_async=True)\n</code></pre>"},{"location":"examples/closed-loop-basic/experiment/#integrating-the-code-into-the-researcher-hub_1","title":"Integrating the Code Into the Researcher Hub","text":"<ul> <li>Create a new python script in the <code>researcher_hub</code> folder and name it <code>stimulus_sequence.py</code>. Copy the code above into the script.</li> <li>If sweetbean isn't already installed in your virtual environment, you will also need to install it with<code>pip install sweetbean</code>.</li> <li>If it isn't already added, make sure to add <code>sweetbean</code> as a dependency in the <code>requirements.txt</code> file within the <code>researcher_hub</code> folder.</li> </ul> <p>Once both functions are integrated both functions, your researcher hub should contain the following files: - <code>autora_workflow.py</code> - <code>trial_sequence.py</code> - <code>stimulus_sequence.py</code></p> <p></p> <p>Below, we elaborate a bit more on the code. However, if you are already familiar with SweetBean, you may skip the \"Explanation\" section. Alternatively, you can gain an intuition for the code in the corresponding notebook:  . </p>"},{"location":"examples/closed-loop-basic/experiment/#code-explanation_1","title":"Code Explanation","text":"<p>SweetBean organizes events into event sequences, and event sequences into blocks. Event sequences may correspond to trials, and blocks correspond to a sequence of experiment trials. </p> <p>First, we define the instruction block, which consists of a series of text stimuli that provide instructions to the participants. Each text stimulus is defined as a <code>Text</code> object. The <code>Text</code> object takes as input the text to be displayed and the choices that the participant can make. The choices are defined as a list of keys that the participant can press to continue to the next instruction. In this case, the participant can continue by pressing the <code>SPACE</code> key.</p> <pre><code># generate several text stimuli that serve as instructions\n  introduction_welcome = Text(text='Welcome to our perception experiment.&lt;br&gt;&lt;br&gt; \\\n                                          Press the SPACE key to continue.', \n                                    choices=[' '])\n\n  introduction_pictures = Text(text='Each picture contains two sets of dots, one left and one right.&lt;br&gt;&lt;br&gt;\\\n                                       Press the SPACE key to continue.', \n                                    choices=[' '])\n\n  introduction_responses = Text(text='You have to indicate whether the two sets contain an equal number of dots.&lt;br&gt;&lt;br&gt;\\\n                                       Press the y-key for yes (equal number) and&lt;br&gt; the n-key for no (unequal number).&lt;br&gt;&lt;br&gt;\\\n                                       Press the SPACE key to continue.', \n                                    choices=[' '])\n\n  introduction_note = Text(text='Note: For each picture, you have only 2 seconds to respond, so respond quickly.&lt;br&gt;&lt;br&gt;\\\n                                       You can only respond with the y and n keys while the dots are shown.&lt;br&gt;&lt;br&gt; \\\n                                       Press the SPACE key to BEGIN the experiment.', \n                                    choices=[' '])\n</code></pre> <p>The following code turns these stimuli into a list, which is then compiled into an instruction block.</p> <pre><code># create a list of instruction stimuli for the instruction block\n  introduction_list = [introduction_welcome, \n                       introduction_pictures, \n                       introduction_responses, \n                       introduction_note]\n\n  # create the instruction block\n  instruction_block = Block(introduction_list)\n</code></pre> <p>Similarly, we can define instructions for the end of the experiment. In this case, we present just a single sentence for the duration of 3000 milliseconds.</p> <pre><code># EXIT BLOCK\n\n  # create a text stimulus shown at the end of the experiment\n  instruction_exit = Text(duration=3000, \n                                  text='Thank you for participating in the experiment.', \n                                  )\n\n  # create a list of instruction stimuli for the exit block\n  exit_list = [instruction_exit]\n\n  # create the exit block\n  exit_block = Block(exit_list)\n</code></pre> <p>Next, we define the task block. This block consists of the experimental trials, which are defined as a sequence of events. Here, we define two events, a fixation cross and a random dot pattern stimulus. Let's begin with the fixation cross, which is shown for 1500ms:</p> <pre><code># TASK BLOCK\n\n  # define fixation cross\n  fixation = Fixation(1500)\n</code></pre> <p>The random dot pattern stimulus is parameterized by two stimulus features, <code>dot_stimulus_left</code> and <code>dot_stimulus_right</code>, which correspond to the number of dots in the left and right stimulus, respectively. These stimulus features are defined as <code>TimelineVariable</code> objects. These objects are updated based on the timeline, i.e., the trial sequence, provided to the function. The <code>TimelineVariable</code> object takes as input the name of the variable.</p> <pre><code># define the stimuli features as timeline variables\n  dot_stimulus_left = TimelineVariable('dots left')\n  dot_stimulus_right = TimelineVariable('dots right')\n</code></pre> <p>Next, we define the random dot pattern stimulus, as a <code>RandomDotPatterns</code> object, which is shown for 2000ms (<code>duration=2000</code>). It consists of two set of dots (<code>number_of_apertures=2</code>), which are parameterized by the two timeline variables <code>number_of_oobs=[dot_stimulus_left, dot_stimulus_right]</code>. Finally, we allow participants to record a response on each stimulus, indicating whether the dots match or not by pressing the respective keys for <code>y</code> and <code>n</code> (<code>choices=[\"y\", \"n\"]</code>)</p> <pre><code># We can define a stimulus as a function of those stimulus features\n  rdp = RandomDotPatterns(\n      duration=2000,\n      number_of_oobs=[dot_stimulus_left, dot_stimulus_right],\n      number_of_apertures=2,\n      choices=[\"y\", \"n\"],\n      background_color=\"black\",\n  )\n</code></pre> <p>Lastly we group the events into a block, which we call the task block.</p> <pre><code># define the sequence of events within a trial\n  event_sequence = [fixation, rdp]\n\n  # group trials into blocks\n  task_block = Block(event_sequence, timeline)\n</code></pre> <p>Finally, we define the entire experiment, which consists of the instruction block, the task block, and the exit block.</p> <pre><code># EXPERIMENT\n\n  # define the entire experiment\n  experiment = Experiment([instruction_block, task_block, exit_block])\n</code></pre> <p>The last line returns the experiment as a JavaScript string, which can be sent to Firebase.</p> <pre><code># return a js string to transfer to autora\n  return experiment.to_js_string(as_function=True, is_async=True)\n</code></pre>"},{"location":"examples/closed-loop-basic/experiment/#updating-the-testing-zone","title":"Updating the Testing Zone","text":"<p>Now that we have code for generating the jsPsych experiment, we want to make sure that our testing_zone can deploy it to firebase.</p> <p>Head over to the <code>testing_zone</code> folder:</p> <pre><code>cd testing_zone\n</code></pre> <p>Make sure to include the following lines in the <code>main.js</code> file in <code>testing_zone/src/design</code>: <pre><code>import jsPsychRok from '@jspsych-contrib/plugin-rok'\nglobal.jsPsychRok = jsPsychRok\n</code></pre></p> <p>In this example, we will generate the entire js Psych experiment in the workflow, we will be sending the full JavaScript code to the testing zone. Replace the <code>main</code> function in the <code>main.js</code> file with the following code:</p> <pre><code>const main = async (id, condition) =&gt; {\n    const observation = await eval(condition['experiment_code'] + \"\\nrunExperiment();\");\n    return JSON.stringify(observation)\n}\n</code></pre> <p>Your <code>main.js</code> file should now look like this:</p> <pre><code>import { initJsPsych } from 'jspsych';\nimport 'jspsych/css/jspsych.css'\nimport 'sweetbean/dist/style/main.css';\nimport 'sweetbean/dist/style/bandit.css';\nimport * as SweetBeanRuntime from 'sweetbean/dist/runtime';\n\nimport htmlKeyboardResponse from '@jspsych/plugin-html-keyboard-response';\nimport jsPsychRok from '@jspsych-contrib/plugin-rok'\nglobal.jsPsychRok = jsPsychRok\n\nglobal.initJsPsych = initJsPsych;\nglobal.jsPsychHtmlKeyboardResponse = htmlKeyboardResponse\n\n\nObject.entries(SweetBeanRuntime).forEach(([key, value]) =&gt; {\n    global[key] = value;\n});\n\n/**\n * This is the main function where you program your experiment. For example, you can install jsPsych via node and\n * use functions from there\n * @param id this is a number between 0 and number of participants. You can use it for example to counterbalance between subjects\n * @param condition this is a condition (for example uploaded to the database with the experiment runner in autora)\n * @returns {Promise&lt;*&gt;} after running the experiment for the subject return the observation in this function, it will be uploaded to autora\n */\nconst main = async (id, condition) =&gt; {\n    const observation = await eval(condition['experiment_code'] + \"\\nrunExperiment();\");\n    return JSON.stringify(observation)\n}\n\n\nexport default main\n</code></pre> <p>Once the <code>main.js</code> file is updated, you can rebuild and deploy the website:</p> <pre><code>npm run build\nfirebase deploy\n</code></pre>"},{"location":"examples/closed-loop-basic/experiment/#next-steps","title":"Next Steps","text":"<p>Next: Add a preprocessing script for the data generated by the experiment.</p>"},{"location":"examples/closed-loop-basic/firebase/","title":"Set Up The Project On The Firebase Website","text":"<p>Next, we want to set up Firebase for our project. </p> <p></p> <p>Firebase is a platform developed by Google for creating mobile and web applications. Here, we will leverage firebase as a platform for hosting our web-based behavioral experiment, and firestore for hosting associated experimental data.</p> <p>To serve a website via Firebase and use the Firestore Database, it is necessary to set up a Firebase project. Follow the steps below to get started:</p>"},{"location":"examples/closed-loop-basic/firebase/#google-account","title":"Google Account","text":"<p>You'll need a Google account to use Firebase.</p>"},{"location":"examples/closed-loop-basic/firebase/#firebase-project","title":"Firebase Project","text":"<ul> <li> <p>While logged in into your Google account, head over to the Firebase website. Then, create a new project:</p> </li> <li> <p>Click on <code>Get started</code>.</p> </li> <li>Click on the plus sign with <code>Create a project</code>.</li> <li>Name your project (e.g., <code>closed-loop-study</code>) and click on <code>Continue</code>.</li> <li>As we don't need Google Analytics, we can leave it disabled (you can leave it enabled if you want to use it in the future).</li> <li>Click <code>Create project</code>.</li> </ul>"},{"location":"examples/closed-loop-basic/firebase/#adding-a-web-app-to-your-project","title":"Adding a Web App to Your Project","text":"<ul> <li>Now, we add a web app to the project, which will correspond to our web experiment. Navigate to the project and follow these steps:</li> <li>Click on <code>&lt;\\&gt;</code>.   </li> <li>Name the app (can be the same as your project) and check the box <code>Also set up Firebase Hosting</code>. Click on <code>Register app</code>.</li> <li>Select <code>Use npm</code>. We will use the configuration details later, but for now, click on <code>Next</code>.</li> <li>We will install firebase tools later, for now, click on <code>Next</code>.</li> <li>We will log in and deploy our website later, for now, click on <code>Continue to console</code>.</li> </ul>"},{"location":"examples/closed-loop-basic/firebase/#adding-firestore-to-your-project","title":"Adding Firestore To Your Project","text":"<p>For our closed-loop study, we will use a Firestore Database to communicate between the AutoRA workflow and the website conducting the experiment. We will upload experiment conditions to the database and also store experiment data in the database.  - To build a Firestore Database, follow these steps:   1. In the left-hand menu of your project console, click on <code>Build</code> and select <code>Firestore Database</code>.      2. Click on <code>Create database</code>.   3. Select a location for the server hosting the database. Click on <code>Next</code>. Note that your institution may have restrictions on the location of the server. Click <code>Next</code>   4. Select <code>Start in production mode</code> selected and click <code>Create</code>.</p> <p>You have now configured Firebase for your project. Next, we will connect your local project to Firebase and deploy your web-based experiment.</p>"},{"location":"examples/closed-loop-basic/firebase/#next-steps","title":"Next Steps","text":"<p>Next: Connect your local project with Firebase.</p>"},{"location":"examples/closed-loop-basic/init-autora/","title":"Connect AutoRA WorkFlow to Firebase","text":"<p>After setting up a mechanism to deploy your experiments online, you can now connect the AutoRA workflow to Firestore database. This will allow us to update experiment conditions and to download observations collected from the experiment.</p> <p></p> <p>The workflow will manage the entire research process, from generating novel experiments to collecting data and modeling the results. </p> <p>Note that the workflow will talk to the Firebase project by uploading and downloading data to the Firestore database. We will upload new experiment conditions to the database and download the results of the experiment from it. </p> <p>The AutoRA workflow can be found in the <code>researcher_hub</code> folder, which contains a basic template for an AutoRA workflow integrated with <code>SweetBean</code>.</p> <ul> <li>Move into the <code>researcher_hub</code> directory, where the template for the workflow is stored.</li> </ul> <pre><code>cd researcher_hub\n</code></pre> <ul> <li>Then install the Python packages required for the workflow using <code>pip</code>:</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"examples/closed-loop-basic/init-autora/#add-firebase-credentials","title":"Add Firebase Credentials","text":"<p>The AutoRA workflow (specifically the <code>autora-firebase-runner</code>) will need access to your firebase project. Therefore, we need the corresponding credentials. </p> <ul> <li>To obtain the credentials, go to the Firebase console.</li> <li>Navigate to the project.</li> <li>Click on the little gear on the left and then select <code>Project settings</code>. </li> <li>Click on <code>Service accounts</code>. </li> <li>Having <code>Node.js</code> selected, click <code>Generate a new private key</code>. This should generate a json file that you can download.</li> <li>Open the file <code>autora_workflow.py</code> in the <code>researcher_hub</code>-folder and navigate to the part of the code that contains a placeholder for the credentials. It should look like this <pre><code>firebase_credentials = {\n    \"type\": \"type\",\n    \"project_id\": \"project_id\",\n    \"private_key_id\": \"private_key_id\",\n    \"private_key\": \"private_key\",\n    \"client_email\": \"client_email\",\n    \"client_id\": \"client_id\",\n    \"auth_uri\": \"auth_uri\",\n    \"token_uri\": \"token_uri\",\n    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n    \"client_x509_cert_url\": \"client_x509_cert_url\"\n}\n</code></pre></li> <li>Replace the placeholders with the credentials from the json file you downloaded.</li> </ul>"},{"location":"examples/closed-loop-basic/init-autora/#try-out-the-workflow","title":"Try out the Workflow","text":"<ul> <li>Within your environment, you can now run <code>python autora_workflow.py</code></li> <li>Head over to your website to test your first online experiment. You can find the link in the Firebase console. Navigate to your project and select <code>Hosting</code> in the left navigation menu. The domain of your experiment is listed on top under <code>Domains</code>.</li> </ul> <p>There shouldn't be much happening with the web experiment. However, you can check whether the workflow is running correctly by looking at Firestore database. The workflow uploads experiment conditions to this database which should be visible.</p> <ul> <li>To check the database, go to the Firebase console and select your project. On the left menu, navigate to <code>Firestore Database</code>. If everything worked, you should see database fields called <code>autora_in</code> and <code>autora_out</code>. The former contains the experiment conditions which are used to configure the experiment. The latter will contain the results of the experiment.</li> </ul> <p></p>"},{"location":"examples/closed-loop-basic/init-autora/#next-steps","title":"Next Steps","text":"<p>Next: Add functions to automatically design and build a simple psychophysics experiment.</p>"},{"location":"examples/closed-loop-basic/preprocessing/","title":"Adding Preprocessing Script","text":"<p>After retrieving the data from our experiment, we need to preprocess it for further analysis. In this example, we will simply extract the key presses from each participant's data to compute whether their responses were correct or not. </p> <p>In principle, we can implement all kinds of fancy preprocessing analyses, such as averaging responses across trial blocks or participants, filtering outliers, etc. Here, for simplicity, we will then simply collect every single response in a single data frame. </p> <ul> <li>Navigate to the <code>researcher_hub</code> directory.</li> <li>Add the following code to the file <code>preprocessing.py</code>:</li> </ul> <pre><code>import pandas as pd\n\ndef trial_list_to_experiment_data(trial_sequence):\n    \"\"\"\n    Parse a trial sequence (from jsPsych) into dependent and independent variables\n\n    independent variables: dots_left, dots_right\n    dependent: accuracy\n    \"\"\"\n\n    # define dictionary to store the results\n    results_dict = {\n        'dots_left': [],\n        'dots_right': [],\n        'accuracy': []\n    }\n    for trial in trial_sequence:\n        # Filter experiment events that are not displaying the dots\n        if trial['trial_type'] != 'rok':\n            continue\n\n        # Filter trials without reaction time\n        if 'rt' not in trial or trial['rt'] is None: # key_response\n            continue\n\n        # the number of dots is equivalent to the number of oobs (oriented objects) as set in the SweetBean script\n        dots_left = trial['number_of_oobs'][0] # oriented objects\n        dots_right = trial['number_of_oobs'][1]\n        choice = trial['key_press']\n\n        # compute accuracy\n        if dots_left == dots_right and choice == 'y' or dots_left != dots_right and choice == 'n':\n            accuracy = 1\n        else:\n            accuracy = 0\n\n        # add results to dictionary\n        results_dict['dots_left'].append(int(dots_left))\n        results_dict['dots_right'].append(int(dots_right))\n        results_dict['accuracy'].append(float(accuracy))\n\n    # convert dictionary to pandas dataframe\n    experiment_data = pd.DataFrame(results_dict)\n\n    return experiment_data\n</code></pre> <p>Below, we explain relevant parts of the code:</p>"},{"location":"examples/closed-loop-basic/preprocessing/#explanation","title":"Explanation","text":"<p>First, note that we are looping through the trial sequence and filtering out events that are not relevant to our analysis. We are only interested in trials where the participant is asked to compare the number of dots on the left and right sides of the screen, which are represented by the 'rok' trial type. Furthermore, we are interested in events that have a reaction time (rt) and a corresponding key press (key_response).</p> <pre><code>    for trial in trial_sequence:\n        # Filter experiment events that are not displaying the dots\n        if trial['trial_type'] != 'rok':\n            continue\n\n        # Filter trials without reaction time\n        if 'rt' not in trial or trial['rt'] is None: # key_response\n            continue\n</code></pre> <p>Next, we extract the number of dots on the left and right sides of the screen, as well as the participant's response. </p> <pre><code>        # the number of dots is equivalent to the number of oobs (oriented objects) as set in the SweetBean script\n        dots_left = trial['number_of_oobs'][0] # oriented objects\n        dots_right = trial['number_of_oobs'][1]\n        choice = trial['key_press']\n</code></pre> <p>We then calculate the accuracy of the participant's response based on the number of dots on each side of the screen. If the participant correctly identified whether the number of dots was equal or not, we assign a value of 1; otherwise, we assign a value of 0.</p> <p><pre><code>        # compute accuracy\n        if dots_left == dots_right and choice == 'y' or dots_left != dots_right and choice == 'n':\n            accuracy = 1\n        else:\n            accuracy = 0\n</code></pre> Finally, we store the results in a dictionary and convert it to a pandas DataFrame.</p> <pre><code>        # add results to dictionary\n        results_dict['dots_left'].append(int(dots_left))\n        results_dict['dots_right'].append(int(dots_right))\n        results_dict['accuracy'].append(float(accuracy))\n\n    # convert dictionary to pandas dataframe\n    experiment_data = pd.DataFrame(results_dict)\n</code></pre> <p>Note that this is a simple example of preprocessing. Depending on the complexity of your experiment and the analyses you wish to perform, you may need to implement more sophisticated preprocessing steps, such as averaging the accuracy across trials:</p> <pre><code># Calculate the mean rt for each S1/S2 combination\n    experiment_data = experiment_data.groupby(['dots_left', 'dots_right']).mean().reset_index()\n</code></pre>"},{"location":"examples/closed-loop-basic/preprocessing/#next-steps","title":"Next Steps","text":"<p>Next: Update the AutoRA workflow to use the experiment and preprocessing functions.</p>"},{"location":"examples/closed-loop-basic/prolific/","title":"Connect Project With Prolific","text":"<p>Once you have your closed-loop workflow set up, it is fairly easy to connect it to Prolific,  a recruiting platform for web-based experiments. By connecting your project with Prolific via the <code>firebase-prolific-runner</code>, you can automatically recruit participants for your study and collect data from them. </p> <p></p> <p>!!! hint: The <code>firebase-prolific-runner</code> will automatically set up a study on Prolific and recruit participants. It is highly recommended to test the experiment before recruiting participants, to have approval from an ethics committee, and to adhere to the ethical guidelines.</p>"},{"location":"examples/closed-loop-basic/prolific/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a Prolific account.</li> <li>Your behavioral study is approved by an ethics committee or institutional review board (IRB).</li> <li>You have a corresponding consent form for your study.</li> </ul>"},{"location":"examples/closed-loop-basic/prolific/#add-consent-form-to-experiment","title":"Add Consent Form to Experiment","text":"<p>Before you can connect your project with Prolific, you will likely need to add a consent form to your experiment. The consent form should be displayed to participants before they start the experiment.</p> <p>One way of adding a consent form is to modify your web experiment. In this example we automatically generated a web experiment using <code>SweetBean</code> within the <code>stimulus_sequence.py</code> script in the <code>researcher_hub</code> folder. </p> <p>One way of adding the consent form would be to add a consent forme event:</p> <pre><code>  consent_form = TextStimulus(text='Content of your consent form.&lt;br&gt;&lt;br&gt; \\\n                                          Press the SPACE key to accept and continue.', \n                                    choices=[' '])\n</code></pre> <p>and add it to the instruction block:</p> <pre><code>  introduction_list = [consent_form,\n                       introduction_welcome, \n                       introduction_pictures, \n                       introduction_responses, \n                       introduction_note]\n</code></pre>"},{"location":"examples/closed-loop-basic/prolific/#update-autora-workflow-to-use-prolific","title":"Update AutoRA Workflow to Use Prolific","text":"<ul> <li> <p>Navigate to the <code>autora_workflow.py</code> file in the <code>researcher_hub</code> folder</p> </li> <li> <p>First, we want to add this import statement for the <code>firebase_prolific_runner</code>:</p> </li> </ul> <pre><code>from autora.experiment_runner.firebase_prolific import firebase_prolific_runner\n</code></pre> <ul> <li>Finally, all you need to do is to replace the following <code>experiment_runner</code>  in the <code>autora_workflow.py</code> script:</li> </ul> <pre><code>experiment_runner = firebase_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=100,\n    sleep_time=5)\n</code></pre> <p>with the following <code>firebase_prolific_runner</code>:</p> <pre><code># Sleep time (seconds): The time between checks to the firebase database and updates of the prolific experiment\nsleep_time = 30\n\n# Study name: This will be the name that will appear on prolific.\n# Participants that have participated in a study with the same name will be excluded.\nstudy_name = 'my autora experiment'\n\n# Study description: This will appear as study description on prolific\nstudy_description= 'Psychophysics Study'\n\n# Study Url: The url of your study (you can find this in the Firebase Console\nstudy_url = 'https://closed-loop-study.web.app/'\n\n# Study completion time (minutes): The estimated time a participant will take to finish your study. We use the compensation suggested by Prolific to calculate how much a participant will earn based on the completion time.\nstudy_completion_time = 5\n\n# Prolific Token: You can generate a token on your Prolific account\nprolific_token = 'my prolific token'\n\n# Completion code: The code a participant gets to prove they participated. If you are using the standard project set up (with cookiecutter), please make sure this is the same code that you have provided in the .env file of the testing zone. The code can be anything you want.\ncompletion_code = 'my completion code'\n\nexperiment_runner = firebase_prolific_runner(\n            firebase_credentials=firebase_credentials,\n            sleep_time=sleep_time,\n            study_name=study_name,\n            study_description=study_description,\n            study_url=study_url,\n            study_completion_time=study_completion_time,\n            prolific_token=prolific_token,\n            completion_code=completion_code,\n        )\n</code></pre> <ul> <li>Make sure to update the input arguments above according to your configuration on Prolific and Firebase.</li> </ul>"},{"location":"examples/closed-loop-basic/prolific/#update-env-in-testing_zone-optional","title":"Update .env in testing_zone (Optional)","text":"<p>The <code>firebase_prolific_runner</code> optimally allocates slots for the experiments you submit to Prolific. If you are done with testing, and are ready for data collection you may want to update the <code>.env</code> file in the <code>testing_zone</code> folder.</p> <ul> <li>Navigate to the <code>testing_zone</code> folder.</li> <li>Open the <code>.env</code> file.</li> <li>Set the <code>REACT_APP_useProlificId</code> variable to <code>True</code>. <pre><code>REACT_APP_useProlificId=\"True\"\n</code></pre></li> </ul>"},{"location":"examples/closed-loop-basic/prolific/#summary","title":"Summary","text":"<ul> <li>This is it! Running the <code>autora_workflow.py</code> in the <code>researcher_hub</code> should now result in closed-loop psychophysics study that recruits human participants from Prolific to participate in your web-based experiment hosted on Firebase.</li> </ul>"},{"location":"examples/closed-loop-basic/setup/","title":"Set Up Project","text":"<p>First, we need to set up our local AutoRA project.</p> <p></p>"},{"location":"examples/closed-loop-basic/setup/#create-repository","title":"Create Repository","text":"<p>To ease the setup process for this example, we provide a template repository that contains all the necessary files and configurations. </p> <ul> <li> <p>Simply visit the following repository on GitHub: https://github.com/AutoResearch/autora-closed-loop-firebase-prolific</p> </li> <li> <p>Next, click on the green \"Use this template\" button to create a new repository in your account. </p> </li> <li> <p>You may then enter the name of the repository (e.g., \"closed-loop-study\") and click on the \"Create repository\" button.</p> </li> </ul>"},{"location":"examples/closed-loop-basic/setup/#clone-repository-or-open-it-in-github-codespace","title":"Clone Repository or Open it in GitHub Codespace","text":"<ul> <li> <p>Once you created your own repository from the template, you can clone it to your local machine using <code>git clone</code>. However, we recommend using GitHub Codespaces for this example, as it provides a more streamlined development environment.</p> </li> <li> <p>To open the repository in GitHub Codespaces, click on the <code>Code</code> button and select <code>Create codespace on main</code>. </p> </li> </ul>"},{"location":"examples/closed-loop-basic/setup/#set-up-environment","title":"Set Up Environment","text":"<p>Once you cloned your repository or opened it in Codespaces, it is time to set up your environment. Here, we will use a Python virtual environment to manage dependencies.</p> <p>Success</p> <p>We recommend setting up your development environment using a manager like <code>venv</code>, which creates isolated python  environments. Other environment managers, like  virtualenv, pipenv, virtualenvwrapper,  hatch,  poetry,  are available and will likely work, but will have different syntax to the syntax shown here. </p> <p>Our packages are set up using <code>virtualenv</code> with <code>pip</code> </p> <ul> <li>In the <code>&lt;project directory&gt;</code>, run the following command to create a new virtual environment in the <code>.venv</code> directory</li> </ul> <p><pre><code>python3 -m \"venv\" \".venv\" \n</code></pre> </p> <p>Hint</p> <p>If you have multiple Python versions installed on your system, it may be necessary to specify the Python version when creating a virtual environment. For example, run the following command to specify Python 3.8 for the virtual environment.  <pre><code>python3.8 -m \"venv\" \".venv\" \n</code></pre></p> <ul> <li>Activate it by running <pre><code>source \".venv/bin/activate\"\n</code></pre></li> </ul>"},{"location":"examples/closed-loop-basic/setup/#install-dependencies","title":"Install Dependencies","text":"<ul> <li>First, install the cookiecutter package using pip via</li> </ul> <pre><code>pip install cookiecutter\n</code></pre> <p>We will use this package to automatically configure our project folder structure. </p> <ul> <li> <p>Then we install some python dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>We  will also need to install firebase-tools: <pre><code>npm install -g firebase-tools\n</code></pre></p> </li> </ul> <p>We will use this package to deploy our firebase functions for storing data.</p> <p>Hint</p> <p>If you encounter any issues with the installation, make sure that you have the necessary permissions to install packages on your system. You may need to run the commands with <code>sudo</code> or as an administrator.</p>"},{"location":"examples/closed-loop-basic/setup/#set-up-project-folder-with-coockiecutter","title":"Set Up Project Folder with Coockiecutter","text":"<p>To establish an online closed-loop for AutoRA, there are two key components that need to be configured:</p> <ol> <li> <p>AutoRA Workflow</p> <ul> <li>This workflow can be executed locally, on a server, or using <code>Cylc</code>. It must have the ability to communicate with a website, allowing for the writing of new conditions and reading of observation data.</li> <li>The AutoRA workflow can be customized by adding or removing AutoRA functions, such as AutoRA experimentalists or AutoRA theorists. It relies on an AutoRA Prolific Firebase runner to collect data from an online experiment hosted via Firebase and recruit participants via prolific.</li> </ul> </li> <li> <p>Website To Conduct Experiment:</p> <ul> <li>The website serves as a platform for conducting experiments and needs to be compatible with the AutoRA workflow.</li> <li>In this setup, we use <code>Firebase</code> to host on website.</li> </ul> </li> </ol> <p>To simplify the setup process, we provide a <code>cookiecutter</code> template that generates a project folder containing the following two directories:</p> <ol> <li> <p><code>researcher_hub</code>:</p> <ul> <li>This directory includes a basic example of an AutoRA workflow.</li> </ul> </li> <li> <p><code>testing_zone</code>:</p> <ul> <li>This directory provides a basic example of a website served with Firebase, ensuring compatibility with the AutoRA workflow.</li> </ul> </li> <li> <p>Once you installed the packages above, you can create the project by running the following command in the root directory of your project:</p> </li> </ol> <pre><code>cookiecutter https://github.com/AutoResearch/autora-user-cookiecutter\n</code></pre> <p>If cookiecutter is not recognized, you may need to run the following command:</p> <pre><code>python -m cookiecutter https://github.com/AutoResearch/autora-user-cookiecutter\n</code></pre> <ul> <li>You will be prompted to enter some information about your project. You can select single options by pressing SPACE and confirm your selection by pressing ENTER.</li> <li>You may first enter a project name, e.g., \"closed-loop-study\".</li> <li>Select <code>yes</code> to use advanced features.</li> <li>You may select any theorist for automated model discovery. Here, we will select the <code>autora[theorist-bms]</code> option.</li> <li>You may select any experimentalist for determining novel experiment conditions. Here, we will select the <code>autora[experimentalist-model-disagreement]</code> option.</li> <li>Make sure to select the <code>autora[experiment-runner-firebase-prolific]</code> option</li> <li>Select <code>yes</code> to set up a firebase experiment. When asked to install further packages (create-react-app@5.0.1), select yes (y). This may take some time.</li> <li>Select <code>Sweetbean</code> as project type.</li> <li> <p>If you are asked to enter a Firebase project ID, you can just press ENTER. It will fail to setup Firebase but we will set it up later manually.</p> </li> <li> <p>Confirm that you have the following project folders:</p> </li> </ul> <p></p> <p>Hint</p> <p>If you encounter any issues with the cookiecutter setup, make sure that you have the necessary permissions to install packages on your system. You may need to run the commands with <code>sudo</code> or as an administrator.</p>"},{"location":"examples/closed-loop-basic/setup/#next-steps","title":"Next Steps","text":"<p>Next: Set up Firebase to host our experiment.</p>"},{"location":"examples/closed-loop-basic/testingzone/","title":"Connect Project With Firebase","text":"<p>Next, we need to connect your project with Firebase, so we can deploy web experiments.</p> <p></p> <ul> <li>Move to the <code>testing_zone</code> folder in your project (e.g., locally or on GitHub Codespaces). </li> </ul> <p>The <code>testing_zone</code> contains a basic template for a website that is compatible with the AutoRA Experimentation Manager for Firebase and the AutoRA Recruitment Manager for Prolific.</p>"},{"location":"examples/closed-loop-basic/testingzone/#copy-web-app-credentials-from-firebase","title":"Copy Web App Credentials From Firebase","text":"<ul> <li>Navigate to the Firebase console and select the project you created.</li> <li>On the gear-symbol next to <code>Project Overview</code>, you can find <code>project settings</code>. </li> <li>You will find credentials in the tab <code>general</code> (you might have to scroll down). They should contain something like this <pre><code>// Your web app's Firebase configuration\nconst firebaseConfig = {\n  apiKey: \"AIzaadskKasjjasjKqsIt-UPXSDsdkdDhjBDU\",\n  authDomain: \"closed-loop-study.firebaseapp.com\",\n  projectId: \"closed-loop-study\",\n  storageBucket: \"closed-loop-study.appspot.com\",\n  messagingSenderId: \"338700008594\",\n  appId: \"1:3328439208594:web:136f203fe48f63ea4b\"\n};\n</code></pre></li> <li> <p>Copy the credentials to the corresponding variables in the <code>.env</code> file in the <code>testing_zone</code> folder that was created on your system using create-react-app or cookiecutter. After you copied the credentials, it should look something like this: <pre><code>REACT_APP_apiKey=\"AIzaadskKasjjasjKqsIt-UPXSDsdkdDhjBDU\"\nREACT_APP_authDomain=\"closed-loop-study.firebaseapp.com\"\nREACT_APP_projectId=\"closed-loop-study\"\nREACT_APP_storageBucket=\"closed-loop-study.appspot.com\"\nREACT_APP_messagingSenderId=\"338700008594\"\nREACT_APP_appId=\"1:3328439208594:web:136f203fe48f63ea4b\"\nREACT_APP_devNoDb=\"True\"\nREACT_APP_useProlificId=\"False\"\nREACT_APP_completionCode=\"complete\"\n</code></pre></p> </li> <li> <p>For now, we will leave <code>REACT_APP_useProlificId=\"False\"</code>. We will change these later when we set up the Prolific integration.</p> </li> </ul>"},{"location":"examples/closed-loop-basic/testingzone/#configure-your-project-for-firebase","title":"Configure Your Project For Firebase","text":"<ul> <li> <p>!!! IMPORTANT !!! Make sure to <code>cd</code> into the <code>testing_zone</code> folder in your terminal.</p> </li> <li> <p>In the <code>testing_zone</code> folder, enter the following commands in your terminal: First log in to your Firebase account using</p> </li> </ul> <p><pre><code>firebase login\n</code></pre> or (if you run this in codespace) <pre><code>firebase login --no-localhost\n</code></pre></p> <ul> <li>If asked to collect CLI and Emulator Suite usage information, you can choose to answer yes or no.</li> </ul> <p>Hint</p> <p>You may be asked to authenticate. Follow the instructions in your terminal to authenticate your account. This will require following a link that requires you to sign in with your Google account. Once you selected a Google account. Follow the setup and allow Firebase CLI access. Click <code>Yes, I just ran this command.</code> Assuming this is your session ID, click <code>Yes, this is my session ID</code>. Next, you are taken to a long string that you can copy into the terminal. It should look something like this: <code>2/0AVGKJADKHAXKg4ML_Usub28sdhjdskjsdktwWeAJASKFCjrKMck-SBEfgsddDmKQlfrDSQ</code>. </p> <ul> <li>Initialize the Firebase project in the <code>testing_zone</code> folder by running: <pre><code>firebase init\n</code></pre></li> <li> <p>An interactive initialization process will now run in your command line. You can select options with SPACE and confirm your selection with ENTER.</p> </li> <li> <p>For the first question, select the following options. Once you selected both options, press ENTER.</p> </li> <li> <p><code>Firestore: Configure security rules and indexes files for Firestore</code></p> </li> <li> <p><code>Hosting: Configure files for Firebase Hosting and (optionally) set up GitHub Action deploys</code></p> </li> <li> <p>Select <code>use an existing project</code>. Press ENTER.</p> </li> <li>Select the project you created earlier. You should recognize it by the project name you entered when setting up Firebase. Press ENTER.</li> <li>Confirm the default option <code>(firestore.rules)</code> with ENTER. </li> <li>Confirm the default option <code>((firestore.indexes.json))</code> with ENTER.</li> <li>!!! IMPORTANT !!! Use the build directory instead of the public directory here. Enter <code>build</code> and press ENTER.</li> <li>Configure as a single-page app. Enter <code>y</code> and press ENTER.</li> <li>Don't set up automatic builds and deploys with GitHub. Enter <code>N</code> and press ENTER. </li> <li>Don't overwrite the index.html file if the question pops up.</li> </ul>"},{"location":"examples/closed-loop-basic/testingzone/#install-sweetbean","title":"Install sweetbean","text":"<ul> <li>Next, we install sweetbean (still within the <code>testing_zone</code> folder): <pre><code>npm install sweetbean\n</code></pre></li> </ul>"},{"location":"examples/closed-loop-basic/testingzone/#build-and-deploy-to-firebase","title":"Build And Deploy To Firebase","text":"<p>The testing zone folder already contains a default web-based experiment. To serve this web-experiment on the internet, you must build and deploy it to Firebase. We will repeat this step later once we deploy our actual experiment.</p> <ul> <li>To build the experiment, run <pre><code>npm run build\n</code></pre></li> <li>To deploy the experiment to Firebase, run <pre><code>firebase deploy\n</code></pre> If everything worked, you should see something like this: </li> </ul> <p>This includes a link to your web-based experiment. You can now open this link in your browser. However, at this moment, we haven't built an experiment yet, so you will see nothing interesting. Next, we will set up the AutoRA workflow and generate the experiment.</p>"},{"location":"examples/closed-loop-basic/testingzone/#next-steps","title":"Next Steps","text":"<p>Next: Initialize your AutoRA workflow in the researcher hub.</p>"},{"location":"examples/closed-loop-basic/workflow/","title":"Updating the AutoRA Workflow","text":"<p>Now that we configured all the pieces, let's integrate them into a single workflow. The AutoRA workflow will generate the experiment, upload the experiment conditions to the Firestore database, and download the results of the experiment. </p>"},{"location":"examples/closed-loop-basic/workflow/#workflow-overview","title":"Workflow Overview","text":"<p>You can learn more about AutoRA workflows in the AutoRA Tutorial. The figure below illustrates the workflow for our closed-loop example.</p> <p></p> <p>The workflow involves the following steps: 1. Seed the experiment with two initial conditions. 2. Repeat the following steps three times:    1. Collect data from an experiment runner that communicates with our web-based experiment. Here, we will use the <code>experiment-runnner-firebase-prolific</code>.    2. Identify models that characterize the data using an AutoRA theorist. Here, we will construct two models, a simple Logistic Regression from the <code>sklearn</code> package and the Bayesian Machine Scientist (BMS) from the <code>autora-theorist-bms</code> package.    3. We will compare the two models to identify two new experiments that differentiate between these models. For this part, we will use the <code>autora-experimentalist-model-disagreement</code> package.</p> <p>AutoRA passes around a <code>state</code> object that contains all relevant data for the workflow. This object is updated at each step of the workflow (see the Figure above). </p>"},{"location":"examples/closed-loop-basic/workflow/#updating-the-workflow","title":"Updating the Workflow","text":"<ul> <li>Replace the code in the <code>autora-workflow.py</code> file with the following code:</li> </ul> <pre><code>\"\"\"\nBasic Workflow\n    Two Independent Variables, One Dependent Variable\n    Theorist: Logistic Regression, Bayesian Machine Scientist\n    Experimentalist: Random Sampling, Model Disagreement\n    Runner: Firebase Runner (no prolific recruitment)\n\"\"\"\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom autora.variable import VariableCollection, Variable\nfrom autora.theorist.bms import BMSRegressor\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\nfrom autora.experimentalist.model_disagreement import model_disagreement_sample\nfrom autora.experiment_runner.firebase_prolific import firebase_runner\nfrom autora.state import StandardState, on_state, Delta\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\nfrom trial_sequence import trial_sequence\nfrom stimulus_sequence import stimulus_sequence\nfrom preprocessing import trial_list_to_experiment_data\n\n# Some study parameters\nnum_cycles = 3\nnum_trials = 20\nnum_conditions_per_cycle = 2\n\n# *** Set up variables *** #\n# The independent variables correspond to dot numbers for left and right stimulus.\n# The dependent variable is accuracy ranging from 0 (0 percent) to 1 (100 percent)\nvariables = VariableCollection(\n    independent_variables=[\n        Variable(name=\"dots_left\", allowed_values=np.linspace(1, 100, 100)),\n        Variable(name=\"dots_right\", allowed_values=np.linspace(1, 100, 100)),\n        ],\n    dependent_variables=[Variable(name=\"accuracy\", value_range=(0, 1))])\n\n# Here, we enlist the entire design space, by combining all possibles values \n# of each of the two independent variables\nallowed_conditions = grid_pool(variables)\n# We remove the conditions where the number of dots is the same on both sides\nallowed_conditions = allowed_conditions[allowed_conditions['dots_left'] != allowed_conditions['dots_right']]\n\n# *** State *** #\n# With the variables, we can set up a state. The state object represents the state of our\n# closed-loop experiment.\nstate = StandardState(\n    variables=variables,\n)\n\n# *** Components/Agents *** #\n# Components are functions that run on the state. The main components are:\n# - theorist\n# - experiment-runner\n# - experimentalist\n# Learn more about components here: \n# https://autoresearch.github.io/autora/tutorials/basic/Tutorial%20I%20Components/\n\n\n# ** Theorist ** #\n# Here we use two different theorists:\n# - Logistic Regression for discovering a linear regression model\n# - Bayesian Machine Scientist for discovering a (non-linear) model\n# Later, we will compare the models generated by both theorists \n# For more information about AutoRA theorists, see \n# https://autoresearch.github.io/autora/theorist/\n\n# First, we define a custom LogisticRegressor class that inherits from BaseEstimator and ClassifierMixin\n# We override the predict method to return the probability of the '1' (accurate) class,\n# to align its output with that of the BMSRegressor\nclass LogisticRegressor(BaseEstimator, ClassifierMixin):\n    def __init__(self, *args, **kwargs):\n        self.model = LogisticRegression(*args, **kwargs)  # Initialize the LogisticRegression model with any passed arguments\n\n    def fit(self, X, y):\n        \"\"\"Fit the LogisticRegression model.\"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Override the predict method to return the probability of the '1' (accurate) class.\"\"\"\n        return self.model.predict_proba(X)[:, 1].reshape(-1, 1) \n\n# Initialize the BMSRegressor and LogisticRegressor\nbms_theorist = BMSRegressor(epochs=500)\nlr_theorist = LogisticRegressor()\n\n# To use the theorists on the state object, we wrap it with the on_state functionality and return a\n# Delta object, indicating how we want to modify the state.\n# Note: The if the input arguments of the theorist_on_state function are state-fields like\n# experiment_data, variables, ..., then calling this function on a state object will automatically\n# update those state fields. You can learn more about states and the on_state function here:\n# https://autoresearch.github.io/autora/tutorials/basic/Tutorial%20IV%20Customization/\n\n@on_state()\ndef theorist_on_state(experiment_data, variables):\n    ivs = [iv.name for iv in variables.independent_variables]\n    dvs = [dv.name for dv in variables.dependent_variables]\n    x = experiment_data[ivs]\n    y = experiment_data[dvs]\n    return Delta(models=[bms_theorist.fit(x, y), lr_theorist.fit(x, y)])\n\n\n# ** Experimentalists ** #\n# We will seed our study with randomly sampled experiment conditions, \n# using a random pool experimentalist. Here, we use a random pool and use the wrapper\n# to that operates on the state object.\n@on_state()\ndef initialize_state(allowed_conditions, num_samples):\n    return Delta(conditions=random_sample(allowed_conditions, num_samples))\n\n# After collecting our first data, we will use a model disagreement experimentalist\n# to determine the next experiment conditions.\n@on_state()\ndef experimentalist_on_state(allowed_conditions, models_to_compare, num_samples):\n    return Delta(conditions=model_disagreement_sample(allowed_conditions, models_to_compare, num_samples))\n\n\n# ** Experiment Runner ** #\n# We will run our experiment on firebase and need credentials. You will find them here:\n# (https://console.firebase.google.com/)\n#   -&gt; project -&gt; project settings -&gt; service accounts -&gt; generate new private key\n\nfirebase_credentials = {\n  \"type\": \"service_account\",\n  \"project_id\": \"closed-loop-study\",\n  \"private_key_id\": \"YOURKEYID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nYOURCREDENTIALS\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"firebase-adminsdk-y7hnh@closed-loop-study.iam.gserviceaccount.com\",\n  \"client_id\": \"YOURLIENTID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-y7hnh%40closed-loop-study.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n\n# Simple experiment runner that runs the experiment on firebase\n# The runner defines a timeout of 100 seconds, which means that a participant\n# has 5 *minutes* to complete an experiment. Afterward, it will be freed for another participant.\n# The sleep time is set to 3 *seconds*, which means that the runner will check every 5 seconds for data.\nexperiment_runner = firebase_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=5,\n    sleep_time=3)\n\n# Again, we need to wrap the experiment runner to use it on the state.\n# Specifically, the runner compiles the identified conditions (i.e., number of tested dots)\n# into an experiment sequence, which is then used to generate the full JavaScript experiment\n# to be sent to Firebase. Once the data are collected, the runner will then pre-process it \n# and store it in the experimental_data object\n@on_state()\ndef runner_on_state(conditions):\n    res = []\n    for idx, c in conditions.iterrows():\n        iv_1 = c['dots_left']\n        iv_2 = c['dots_right']\n        # get a timeline via sweetPea\n        timeline = trial_sequence(iv_1, iv_2, num_trials)\n        print(\"Generated counterbalanced trial sequence.\")\n        # get js code via sweetBeaan\n        js_code = stimulus_sequence(timeline)\n        print(\"Compiled experiment.\")\n        res.append(js_code)\n\n    # prepare conditions to send\n    conditions_to_send = conditions.copy()\n    conditions_to_send['experiment_code'] = res\n    # upload and run the experiment:\n    print(\"Uploading the experiment...\")\n    data_raw = experiment_runner(conditions_to_send)\n    print(\"Collected experimental data.\")\n\n    # process the experiment data\n    experiment_data = pd.DataFrame()\n    for item in data_raw:\n        _lst = json.loads(item)['trials']\n        _df = trial_list_to_experiment_data(_lst)\n        experiment_data = pd.concat([experiment_data, _df], axis=0)\n    print(\"Preprocessed experimental data.\")\n    return Delta(experiment_data=experiment_data)\n\n\n# *** AutoRA Workflow *** #\n# Next, we specify our actual workflow using the components defined above.\n# We begin with sampling two random initial conditions for the dots, resulting in two experiments.\n# We then iterate three times through the following steps:\n# 1) Collect data.\n# 2) Fit models.\n# 3) Identify novel experiment conditions.\n\nstate = initialize_state(state, allowed_conditions=allowed_conditions, num_samples=num_conditions_per_cycle)\n\n# Now, we can run our components in a loop\nfor _ in range(num_cycles):\n    state = runner_on_state(state)\n    print(\"Finished data collection and preprocessing.\")\n    state = theorist_on_state(state)\n    print(\"Fitted models.\")\n    models_to_compare = [state.models[-1], state.models[-2]]\n    state = experimentalist_on_state(state, \n                                     allowed_conditions=allowed_conditions, \n                                     models_to_compare=models_to_compare, \n                                     num_samples=num_conditions_per_cycle)  \n    print(\"Determined experiment conditions.\")\n\n\n# *** Plot *** #\n# This plot visualizes the fit of the final logistic regression model and the model identified by the\n# Bayesian Machine Scientist. \nivs = [iv.name for iv in variables.independent_variables]\ndvs = [dv.name for dv in variables.dependent_variables]\nX = state.experiment_data[ivs]\ny = state.experiment_data[dvs]\n\n# Create a meshgrid for plotting the logistic regression decision boundary\niv1_range = variables.independent_variables[0].allowed_values\niv2_range = variables.independent_variables[1].allowed_values\niv1_grid, iv2_grid = np.meshgrid(iv1_range, iv2_range)\niv_grid = np.c_[iv1_grid.ravel(), iv2_grid.ravel()]\n\n# Predict probabilities for the meshgrid (for the surface plot)\ndv_pred_lr = state.models[-1].predict(iv_grid).reshape(iv1_grid.shape)  # Probability of the positive class\ndv_pred_bms = state.models[-2].predict(iv_grid).reshape(iv1_grid.shape)  # Probability of the positive class\n\n# Create the 3D plot\nfig = plt.figure(figsize=(14, 6))\n\nax1 = fig.add_subplot(121, projection='3d')\n# Plot the actual data points\nax1.scatter(X['dots_left'], X['dots_right'], y, color='red', label='Data Points')\n# Plot the logistic regression surface\nax1.plot_surface(iv1_grid, iv2_grid, dv_pred_lr, cmap='viridis', alpha=0.6)\n# Label the axes\nax1.set_xlabel('Number of Dots (left)')\nax1.set_ylabel('Number of Dots (right)')\nax1.set_zlabel('Accuracy')\nax1.set_zlim(0, 1)\nax1.set_title(\"Logistic Regression\")\n\nax2 = fig.add_subplot(122, projection='3d')\n# Plot the actual data points\nax2.scatter(X['dots_left'], X['dots_right'], y, color='red', label='Data Points')\n# Plot the logistic regression surface\nax2.plot_surface(iv1_grid, iv2_grid, dv_pred_bms, cmap='viridis', alpha=0.6)\n# Label the axes\nax2.set_xlabel('Number of Dots (left)')\nax2.set_ylabel('Number of Dots (right)')\nax2.set_zlabel('Accuracy')\nax2.set_zlim(0, 1)\nax2.set_title(\"BMS Equation: \" + state.models[-2].repr())\n\n# Show the plot\nplt.savefig('model_comparison.png')\nplt.show()\n</code></pre> <ul> <li>Next, you should be able to execute the workflow in the <code>researcher_hub</code>:</li> </ul> <pre><code>python autora_workflow.py\n</code></pre> <ul> <li>Once the workflow uploads the experiment, you should be able to participate in the experiment, helping AutoRA to collect data.</li> </ul> <p>Hint</p> <p>If you forgot the website address for your experiment, you can find it in the Firebase Console. One the console, select your project and then click on <code>Hosting</code> in the left navigation menu.  You should then see the link to your experiment in the section <code>Domains''. It's the upper one, in this case</code>https://closed-loop-study.web.app``. </p> <ul> <li>Try to run the workflow for three cycles. Once completed, you should see the plot (also stored in the file <code>model_comparison.png</code>) that compares the logistic regression model with the Bayesian Machine Scientist model.</li> </ul> <p>Hint</p> <p>Note that you need to wait until the experiment is finished until you see a page with white background. If you end the experiment before, the <code>firebase_runner</code> will wait the minutes specified in <code>time_out</code> before it will be available for the next participant, i.e., run. If no more slots are currently available, you should see something like \"We are sorry, there has been an unexpected technical issue. Thank you for your understanding.\" </p> <p>Hint</p> <p>You can check which experiments were successfully completed by looking into the Firestore database. In your project on the Firebase Console, simply navigate to <code>FireStore Database</code>. The fields in <code>autora</code> &gt; <code>autora_out</code> &gt; <code>observations</code> list all the conditions. \"null\" means that no data has been collected for that condition yet.</p> <ul> <li>Congratulations, you just set up and ran a closed-loop behavioral study!</li> </ul> <p>Below, we provide some more detailed explanations for the code above.</p>"},{"location":"examples/closed-loop-basic/workflow/#explanation","title":"Explanation","text":""},{"location":"examples/closed-loop-basic/workflow/#study-parameters","title":"Study Parameters","text":"<pre><code>num_cycles = 3\nnum_trials = 20\nnum_conditions_per_cycle = 2\n</code></pre> <p>In the first part of the code, we set on some simulation parameters. Here, the workflow is set to run for three cycles, with 20 trials per experiment. In each cycle, we will generate two new experimental conditions, which, in this case also results in two experiments.</p>"},{"location":"examples/closed-loop-basic/workflow/#variables","title":"Variables","text":"<pre><code>variables = VariableCollection(\n    independent_variables=[\n        Variable(name=\"dots_left\", allowed_values=np.linspace(1, 100, 100)),\n        Variable(name=\"dots_right\", allowed_values=np.linspace(1, 100, 100)),\n        ],\n    dependent_variables=[Variable(name=\"accuracy\", value_range=(0, 1))])\n</code></pre> <p>Here, we define the variables for our study. We have two independent variables, the number of dots on the left and right side, and one dependent variable, the accuracy of the participant's response. The independent variables range from 1 to 100, and the dependent variable ranges from 0 to 1.</p>"},{"location":"examples/closed-loop-basic/workflow/#allowed-conditions","title":"Allowed Conditions","text":"<pre><code>allowed_conditions = grid_pool(variables)\nallowed_conditions = allowed_conditions[allowed_conditions['dots_left'] != allowed_conditions['dots_right']]\n</code></pre> <p>We generate all possible conditions for the study by combining all possible values of the independent variables. We then remove the conditions where the number of dots is the same on both sides. The latter is needed because our experimental design script will consider conditions one of the two values is set for both the left and right side of the dots.</p>"},{"location":"examples/closed-loop-basic/workflow/#state","title":"State","text":"<pre><code>state = StandardState(\n    variables=variables,\n)\n</code></pre> <p>The state object represents the state of our closed-loop experiment. It contains all relevant data for the workflow (e.g., <code>experiment_data</code> or generated <code>models</code>) and is updated at each step of the workflow.</p>"},{"location":"examples/closed-loop-basic/workflow/#theorist","title":"Theorist","text":"<pre><code>class LogisticRegressor(BaseEstimator, ClassifierMixin):\n    def __init__(self, *args, **kwargs):\n        self.model = LogisticRegression(*args, **kwargs)  # Initialize the LogisticRegression model with any passed arguments\n\n    def fit(self, X, y):\n        \"\"\"Fit the LogisticRegression model.\"\"\"\n        self.model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        \"\"\"Override the predict method to return the probability of the '1' (accurate) class.\"\"\"\n        return self.model.predict_proba(X)[:, 1].reshape(-1, 1) \n</code></pre> <p>Here, we define one of the two theorists, the Logistic Regression. Specifically, we define a custom class <code>LogisticRegressor</code> that inherits from <code>BaseEstimator</code> and <code>ClassifierMixin</code>. We override the <code>predict</code> method to return the probability of the '1' (accurate) class, aligning its output with that of the BMSRegressor. This is necessary because the predictions of both theorists are compared later on, so they are required to have the same output format.</p> <pre><code>bms_theorist = BMSRegressor(epochs=500)\nlr_theorist = LogisticRegressor()\n</code></pre> <p>We then initialize the Bayesian Machine Scientist (BMS) and the Logistic Regression (LR) theorist. The BMSRegressor is a custom class from the AutoRA package, while the LogisticRegressor is the custom class we defined above. The BMSRegressor is initialized to run for 500 epochs.</p> <pre><code>@on_state()\ndef theorist_on_state(experiment_data, variables):\n    ivs = [iv.name for iv in variables.independent_variables]\n    dvs = [dv.name for dv in variables.dependent_variables]\n    x = experiment_data[ivs]\n    y = experiment_data[dvs]\n    return Delta(models=[bms_theorist.fit(x, y), lr_theorist.fit(x, y)])\n</code></pre> <p>We then define a function <code>theorist_on_state</code> that fits the models to the data. The function is wrapped with the <code>on_state</code> decorator, which allows the function to update the state object of our workflow. The function takes the experiment data and variables as input and returns a <code>Delta</code> object that updates the state with the two identified models.</p>"},{"location":"examples/closed-loop-basic/workflow/#experimentalists","title":"Experimentalists","text":"<pre><code>@on_state()\ndef initialize_state(allowed_conditions, num_samples):\n    return Delta(conditions=random_sample(allowed_conditions, num_samples))\n</code></pre> <p>Here, we define the function <code>initialize_state</code> that seeds our study with randomly sampled experiment conditions. The function is wrapped with the <code>on_state</code> decorator, allowing it to update the state object. The function takes the allowed conditions and the number of samples as input and returns a <code>Delta</code> object that updates the state with the randomly sampled conditions using the random experimentalist.</p> <pre><code>@on_state()\ndef experimentalist_on_state(allowed_conditions, models_to_compare, num_samples):\n    return Delta(conditions=model_disagreement_sample(allowed_conditions, models_to_compare, num_samples))\n</code></pre> <p>We then define the function <code>experimentalist_on_state</code> that determines the next experiment conditions for every cycle. The function is wrapped with the <code>on_state</code> decorator, allowing it to update the state object. The function takes the allowed conditions, the models to compare, and the number of samples as input and returns a <code>Delta</code> object that updates the state with the conditions determined by the model disagreement experimentalist.</p>"},{"location":"examples/closed-loop-basic/workflow/#firebase-credentials","title":"Firebase Credentials","text":"<pre><code>firebase_credentials = {\n  \"type\": \"service_account\",\n  \"project_id\": \"closed-loop-study\",\n  \"private_key_id\": \"YOURKEYID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nYOURCREDENTIALS\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"firebase-adminsdk-y7hnh@closed-loop-study.iam.gserviceaccount.com\",\n  \"client_id\": \"YOURLIENTID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-y7hnh%40closed-loop-study.iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\n</code></pre> <p>Here, we define the Firebase credentials required to run the experiment on Firebase. You can find these credentials in the Firebase console under <code>project -&gt; project settings -&gt; service accounts -&gt; generate new private key</code>.</p>"},{"location":"examples/closed-loop-basic/workflow/#experiment-runner","title":"Experiment Runner","text":"<pre><code>experiment_runner = firebase_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=5,\n    sleep_time=3)\n</code></pre> <p>We then define the experiment runner that runs the experiment on Firebase. The runner is wrapped with the <code>on_state</code> decorator, allowing it to update the state object. The runner takes the Firebase credentials, the timeout, and the sleep time as input and returns a <code>Delta</code> object that updates the state with the experiment data. The <code>time_out</code> determines the amount of time in minutes a participant has available to complete the experiment, while the <code>sleep_time</code> determines how many seconds the runner waits run another check for experimental data.</p> <pre><code>@on_state()\ndef runner_on_state(conditions):\n    res = []\n</code></pre> <p>We then define the function <code>runner_on_state</code> that uploads the experiment to Firebase and collects the experimental data. The function is wrapped with the <code>on_state</code> decorator, allowing it to update the state object. The function takes the conditions as input and returns a <code>Delta</code> object that updates the state with the preprocessed experiment data.</p> <pre><code>    for idx, c in conditions.iterrows():\n        iv_1 = c['dots_left']\n        iv_2 = c['dots_right']\n        # get a timeline via sweetPea\n        timeline = trial_sequence(iv_1, iv_2, num_trials)\n        print(\"Generated counterbalanced trial sequence.\")\n        # get js code via sweetBeaan\n        js_code = stimulus_sequence(timeline)\n        print(\"Compiled experiment.\")\n        res.append(js_code)\n</code></pre> <p>First, we iterate through all the sampled conditions (in this case, there are two). Each conditions consists of a pair of values for the independent variables <code>dots_left</code> and <code>dots_right</code>. For each condition, we generate a counterbalanced trial sequence using the <code>trial_sequence</code> function that we programmed in a prior step. We then compile the experiment using the <code>stimulus_sequence</code> function. The compiled experiment is then appended to the list <code>res</code>.</p> <pre><code>    conditions_to_send = conditions.copy()\n    conditions_to_send['experiment_code'] = res\n    # upload and run the experiment:\n    print(\"Uploading the experiment...\")\n    data_raw = experiment_runner(conditions_to_send)\n    print(\"Collected experimental data.\")\n</code></pre> <p>We then update the conditions with the compiled experiment code and send the conditions to the experiment runner. The runner uploads the experiment to Firebase and collects the experimental data. The raw data is then stored in the variable <code>data_raw</code>.</p> <pre><code>    # process the experiment data\n    experiment_data = pd.DataFrame()\n    for item in data_raw:\n        _lst = json.loads(item)['trials']\n        _df = trial_list_to_experiment_data(_lst)\n        experiment_data = pd.concat([experiment_data, _df], axis=0)\n    print(\"Preprocessed experimental data.\")\n    return Delta(experiment_data=experiment_data)\n</code></pre> <p>Finally, we preprocess the experimental data by converting the raw data into a pandas DataFrame. Here, we iterate through data from each conducted experiment. The preprocessed data is then stored in the variable <code>experiment_data</code>.</p>"},{"location":"examples/closed-loop-basic/workflow/#autora-workflow","title":"AutoRA Workflow","text":"<pre><code>state = initialize_state(state, allowed_conditions=allowed_conditions, num_samples=num_conditions_per_cycle)\n</code></pre> <p>We first seed the study with randomly sampled conditions using the <code>initialize_state</code> function.</p> <pre><code>for _ in range(num_cycles):\n    state = runner_on_state(state)\n    print(\"Finished data collection and preprocessing.\")\n    state = theorist_on_state(state)\n    print(\"Fitted models.\")\n    models_to_compare = [state.models[-1], state.models[-2]]\n    state = experimentalist_on_state(state, \n                                     allowed_conditions=allowed_conditions, \n                                     models_to_compare=models_to_compare, \n                                     num_samples=num_conditions_per_cycle)  \n    print(\"Determined experiment conditions.\")\n</code></pre> <p>We then run the workflow for the specified number of cycles. In each cycle, we collect data, fit the two models, and identify novel experiment conditions using the wrapper functions defined above. </p> <p>Note that we can retrieve the two models using <code>state.models[-1]</code> and <code>state.models[-2]</code>. As determined in the <code>theorist_on_state</code> function, the BMS model is stored at the second-to-last index of the models list, while the Logistic Regression model is stored at thelast index.</p>"},{"location":"examples/closed-loop-basic/workflow/#plot","title":"Plot","text":"<p>The remainder of the script simply plots the variables. Note that we can extract the relevant data from the state:  <pre><code>ivs = [iv.name for iv in variables.independent_variables]\ndvs = [dv.name for dv in variables.dependent_variables]\nX = state.experiment_data[ivs]\ny = state.experiment_data[dvs]\n</code></pre></p> <p>This includes the set of allowed values which is convenient for plotting the entire space of possible conditions. </p> <pre><code># Create a meshgrid for plotting the logistic regression decision boundary\niv1_range = variables.independent_variables[0].allowed_values\niv2_range = variables.independent_variables[1].allowed_values\niv1_grid, iv2_grid = np.meshgrid(iv1_range, iv2_range)\niv_grid = np.c_[iv1_grid.ravel(), iv2_grid.ravel()]\n</code></pre> <p>Predictions can also be obtained directly from the models stored in the state:</p> <pre><code># Predict probabilities for the meshgrid (for the surface plot)\ndv_pred_lr = state.models[-1].predict(iv_grid).reshape(iv1_grid.shape)  # Probability of the positive class\ndv_pred_bms = state.models[-2].predict(iv_grid).reshape(iv1_grid.shape)  # Probability of the positive class\n</code></pre> <p>The rest of the code generates the 3D plot comparing the predictions of the two models from the last cycle.</p> <pre><code># Create the 3D plot\nfig = plt.figure(figsize=(14, 6))\n\nax1 = fig.add_subplot(121, projection='3d')\n# Plot the actual data points\nax1.scatter(X['dots_left'], X['dots_right'], y, color='red', label='Data Points')\n# Plot the logistic regression surface\nax1.plot_surface(iv1_grid, iv2_grid, dv_pred_lr, cmap='viridis', alpha=0.6)\n# Label the axes\nax1.set_xlabel('Number of Dots (left)')\nax1.set_ylabel('Number of Dots (right)')\nax1.set_zlabel('Accuracy')\nax1.set_zlim(0, 1)\nax1.set_title(\"Logistic Regression\")\n\nax2 = fig.add_subplot(122, projection='3d')\n# Plot the actual data points\nax2.scatter(X['dots_left'], X['dots_right'], y, color='red', label='Data Points')\n# Plot the logistic regression surface\nax2.plot_surface(iv1_grid, iv2_grid, dv_pred_bms, cmap='viridis', alpha=0.6)\n# Label the axes\nax2.set_xlabel('Number of Dots (left)')\nax2.set_ylabel('Number of Dots (right)')\nax2.set_zlabel('Accuracy')\nax2.set_zlim(0, 1)\nax2.set_title(\"BMS Equation: \" + state.models[-2].repr())\n\n# Show the plot\nplt.savefig('model_comparison.png')\nplt.show()\n</code></pre>"},{"location":"examples/closed-loop-basic/workflow/#next-steps","title":"Next Steps","text":"<p>Next: Connect your closed-loop experiment with Prolific in order to recruit real participants.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/","title":"Sweetbean","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install sweetbean\n</pre> %%capture !pip install sweetbean <p>To illustrate the use of SweetBean, we first assume a fixed trial sequence:</p> In\u00a0[\u00a0]: Copied! <pre>timeline = [{'dots_left': 40, 'dots_right': 70},\n  {'dots_left': 70, 'dots_right': 70},\n  {'dots_left': 70, 'dots_right': 40},\n  {'dots_left': 70, 'dots_right': 70},\n  {'dots_left': 40, 'dots_right': 70},\n  {'dots_left': 70, 'dots_right': 40},\n  {'dots_left': 40, 'dots_right': 40},\n  {'dots_left': 40, 'dots_right': 40},\n  {'dots_left': 40, 'dots_right': 40},\n  {'dots_left': 70, 'dots_right': 40}]\n</pre> timeline = [{'dots_left': 40, 'dots_right': 70},   {'dots_left': 70, 'dots_right': 70},   {'dots_left': 70, 'dots_right': 40},   {'dots_left': 70, 'dots_right': 70},   {'dots_left': 40, 'dots_right': 70},   {'dots_left': 70, 'dots_right': 40},   {'dots_left': 40, 'dots_right': 40},   {'dots_left': 40, 'dots_right': 40},   {'dots_left': 40, 'dots_right': 40},   {'dots_left': 70, 'dots_right': 40}] <p>Let's begin with writing down our instructions in html code. We can specify the key required to move on to the next instruction.</p> In\u00a0[\u00a0]: Copied! <pre>from sweetbean.stimulus import Text\n\nintroduction_welcome = Text(text='Welcome to our perception experiment.&lt;br&gt;&lt;br&gt; \\\n                                          Press the SPACE key to continue.',\n                                    choices=[' '])\n\nintroduction_pictures = Text(text='Each picture contains two sets of dots, one left and one right.&lt;br&gt;&lt;br&gt;\\\n                                      Press the SPACE key to continue.',\n                                  choices=[' '])\n\nintroduction_responses = Text(text='You have to indicate whether the two sets contain an equal number of dots.&lt;br&gt;&lt;br&gt;\\\n                                      Press the y-key for yes (equal number) and&lt;br&gt; the n-key for no (unequal number).&lt;br&gt;&lt;br&gt;\\\n                                      Press the SPACE key to continue.',\n                                  choices=[' '])\n\nintroduction_note = Text(text='Note: For each picture, you have only 2 seconds to respond, so respond quickly.&lt;br&gt;&lt;br&gt;\\\n                                      You can only respond with the y and n keys while the dots are shown.&lt;br&gt;&lt;br&gt; \\\n                                      Press the SPACE key to BEGIN the experiment.',\n                                  choices=[' '])\n</pre> from sweetbean.stimulus import Text  introduction_welcome = Text(text='Welcome to our perception experiment. \\                                           Press the SPACE key to continue.',                                     choices=[' '])  introduction_pictures = Text(text='Each picture contains two sets of dots, one left and one right.\\                                       Press the SPACE key to continue.',                                   choices=[' '])  introduction_responses = Text(text='You have to indicate whether the two sets contain an equal number of dots.\\                                       Press the y-key for yes (equal number) and the n-key for no (unequal number).\\                                       Press the SPACE key to continue.',                                   choices=[' '])  introduction_note = Text(text='Note: For each picture, you have only 2 seconds to respond, so respond quickly.\\                                       You can only respond with the y and n keys while the dots are shown. \\                                       Press the SPACE key to BEGIN the experiment.',                                   choices=[' ']) <p>Next, will pack these stimuli into a list to form an instruction block.</p> In\u00a0[\u00a0]: Copied! <pre>from sweetbean import Block\n\n# create a list of instruction stimuli for the instruction block\nintroduction_list = [introduction_welcome,\n                      introduction_pictures,\n                      introduction_responses,\n                      introduction_note]\n\n# create the instruction block\ninstruction_block = Block(introduction_list)\n</pre> from sweetbean import Block  # create a list of instruction stimuli for the instruction block introduction_list = [introduction_welcome,                       introduction_pictures,                       introduction_responses,                       introduction_note]  # create the instruction block instruction_block = Block(introduction_list) In\u00a0[\u00a0]: Copied! <pre># create a text stimulus shown at the end of the experiment\ninstruction_exit = Text(duration=3000,\n                                text='Thank you for participating in the experiment.',\n                                )\n\n# create a list of instruction stimuli for the exit block\nexit_list = [instruction_exit]\n\n# create the exit block\nexit_block = Block(exit_list)\n</pre> # create a text stimulus shown at the end of the experiment instruction_exit = Text(duration=3000,                                 text='Thank you for participating in the experiment.',                                 )  # create a list of instruction stimuli for the exit block exit_list = [instruction_exit]  # create the exit block exit_block = Block(exit_list) In\u00a0[\u00a0]: Copied! <pre>from sweetbean.stimulus import Fixation\n\nduration = 1500 # the duration is given in ms\nfixation = Fixation(duration)\n</pre> from sweetbean.stimulus import Fixation  duration = 1500 # the duration is given in ms fixation = Fixation(duration) In\u00a0[\u00a0]: Copied! <pre>from sweetbean.variable import TimelineVariable\nfrom sweetbean.stimulus import RandomDotPatterns\n\n# define the stimuli features as timeline variables\ndot_stimulus_left = TimelineVariable('dots_left')\ndot_stimulus_right = TimelineVariable('dots_right')\n\n# We can use these variables in the stimuli declaration:\nrdp = RandomDotPatterns(\n    duration=2000,\n    number_of_oobs=[dot_stimulus_left, dot_stimulus_right],\n    number_of_apertures=2,\n    choices=[\"y\", \"n\"],\n    background_color=\"black\",\n)\n</pre> from sweetbean.variable import TimelineVariable from sweetbean.stimulus import RandomDotPatterns  # define the stimuli features as timeline variables dot_stimulus_left = TimelineVariable('dots_left') dot_stimulus_right = TimelineVariable('dots_right')  # We can use these variables in the stimuli declaration: rdp = RandomDotPatterns(     duration=2000,     number_of_oobs=[dot_stimulus_left, dot_stimulus_right],     number_of_apertures=2,     choices=[\"y\", \"n\"],     background_color=\"black\", ) <p>Note that the dot stimulus is shown for 2000ms (<code>duration=2000</code>). It consists of two set of dots (<code>number_of_apertures=2</code>), which are parameterized by the two timeline variables <code>number_of_oobs=[dot_stimulus_left, dot_stimulus_right]</code>. Finally, we allow participants to record a response on each stimulus, indicating whether the dots match or not by pressing the respective keys for <code>y</code> and <code>n</code> (<code>choices=[\"y\", \"n\"]</code>)</p> In\u00a0[\u00a0]: Copied! <pre>from sweetbean import Block, Experiment\n\n# define the sequence of events within a trial\nevent_sequence = [fixation, rdp]\n\n# group trials into blocks\ntask_block = Block(event_sequence, timeline)\n</pre> from sweetbean import Block, Experiment  # define the sequence of events within a trial event_sequence = [fixation, rdp]  # group trials into blocks task_block = Block(event_sequence, timeline)  In\u00a0[\u00a0]: Copied! <pre># define the entire experiment\nexperiment = Experiment([instruction_block, task_block, exit_block])\n\n# export experiment to html file\nexperiment.to_html(\"psychophysics_experiment.html\")\n</pre> # define the entire experiment experiment = Experiment([instruction_block, task_block, exit_block])  # export experiment to html file experiment.to_html(\"psychophysics_experiment.html\") <p>The code above should have generated a local html file  <code>rok_weber_fechner.html</code> which can be opened and run.</p> In\u00a0[\u00a0]: Copied! <pre>from sweetbean.stimulus import Text, Fixation, RandomDotPatterns\nfrom sweetbean import Block, Experiment\nfrom sweetbean.variable import TimelineVariable\n\ndef stimulus_sequence(timeline):\n\n  # INSTRUCTION BLOCK\n\n  # generate several text stimuli that serve as instructions\n  introduction_welcome = Text(text='Welcome to our perception experiment.&lt;br&gt;&lt;br&gt; \\\n                                          Press the SPACE key to continue.',\n                                    choices=[' '])\n\n  introduction_pictures = Text(text='Each picture contains two sets of dots, one left and one right.&lt;br&gt;&lt;br&gt;\\\n                                       Press the SPACE key to continue.',\n                                    choices=[' '])\n\n  introduction_responses = Text(text='You have to indicate whether the two sets contain an equal number of dots.&lt;br&gt;&lt;br&gt;\\\n                                       Press the y-key for yes (equal number) and&lt;br&gt; the n-key for no (unequal number).&lt;br&gt;&lt;br&gt;\\\n                                       Press the SPACE key to continue.',\n                                    choices=[' '])\n\n  introduction_note = Text(text='Note: For each picture, you have only 2 seconds to respond, so respond quickly.&lt;br&gt;&lt;br&gt;\\\n                                       You can only respond with the y and n keys while the dots are shown.&lt;br&gt;&lt;br&gt; \\\n                                       Press the SPACE key to BEGIN the experiment.',\n                                    choices=[' '])\n\n\n  # create a list of instruction stimuli for the instruction block\n  introduction_list = [introduction_welcome,\n                       introduction_pictures,\n                       introduction_responses,\n                       introduction_note]\n\n  # create the instruction block\n  instruction_block = Block(introduction_list)\n\n  # EXIT BLOCK\n\n  # create a text stimulus shown at the end of the experiment\n  instruction_exit = Text(duration=3000,\n                                  text='Thank you for participating in the experiment.',\n                                  )\n\n  # create a list of instruction stimuli for the exit block\n  exit_list = [instruction_exit]\n\n  # create the exit block\n  exit_block = Block(exit_list)\n\n  # TASK BLOCK\n\n  # define fixation cross\n  fixation = Fixation(1500)\n\n  # define the stimuli features as timeline variables\n  dot_stimulus_left = TimelineVariable('dots_left')\n  dot_stimulus_right = TimelineVariable('dots_right')\n\n  # We can define a stimulus as a function of those stimulus features\n  rdp = RandomDotPatterns(\n      duration=2000,\n      number_of_oobs=[dot_stimulus_left, dot_stimulus_right],\n      number_of_apertures=2,\n      choices=[\"y\", \"n\"],\n      background_color=\"black\",\n  )\n\n  # define the sequence of events within a trial\n  event_sequence = [fixation, rdp]\n\n  # group trials into blocks\n  task_block = Block(event_sequence, timeline)\n\n  # EXPERIMENT\n\n  # define the entire experiment\n  experiment = Experiment([instruction_block, task_block, exit_block])\n\n  # return a js string to transfer to autora\n  return experiment.to_js_string(as_function=True, is_async=True)\n</pre> from sweetbean.stimulus import Text, Fixation, RandomDotPatterns from sweetbean import Block, Experiment from sweetbean.variable import TimelineVariable  def stimulus_sequence(timeline):    # INSTRUCTION BLOCK    # generate several text stimuli that serve as instructions   introduction_welcome = Text(text='Welcome to our perception experiment. \\                                           Press the SPACE key to continue.',                                     choices=[' '])    introduction_pictures = Text(text='Each picture contains two sets of dots, one left and one right.\\                                        Press the SPACE key to continue.',                                     choices=[' '])    introduction_responses = Text(text='You have to indicate whether the two sets contain an equal number of dots.\\                                        Press the y-key for yes (equal number) and the n-key for no (unequal number).\\                                        Press the SPACE key to continue.',                                     choices=[' '])    introduction_note = Text(text='Note: For each picture, you have only 2 seconds to respond, so respond quickly.\\                                        You can only respond with the y and n keys while the dots are shown. \\                                        Press the SPACE key to BEGIN the experiment.',                                     choices=[' '])     # create a list of instruction stimuli for the instruction block   introduction_list = [introduction_welcome,                        introduction_pictures,                        introduction_responses,                        introduction_note]    # create the instruction block   instruction_block = Block(introduction_list)    # EXIT BLOCK    # create a text stimulus shown at the end of the experiment   instruction_exit = Text(duration=3000,                                   text='Thank you for participating in the experiment.',                                   )    # create a list of instruction stimuli for the exit block   exit_list = [instruction_exit]    # create the exit block   exit_block = Block(exit_list)    # TASK BLOCK    # define fixation cross   fixation = Fixation(1500)    # define the stimuli features as timeline variables   dot_stimulus_left = TimelineVariable('dots_left')   dot_stimulus_right = TimelineVariable('dots_right')    # We can define a stimulus as a function of those stimulus features   rdp = RandomDotPatterns(       duration=2000,       number_of_oobs=[dot_stimulus_left, dot_stimulus_right],       number_of_apertures=2,       choices=[\"y\", \"n\"],       background_color=\"black\",   )    # define the sequence of events within a trial   event_sequence = [fixation, rdp]    # group trials into blocks   task_block = Block(event_sequence, timeline)    # EXPERIMENT    # define the entire experiment   experiment = Experiment([instruction_block, task_block, exit_block])    # return a js string to transfer to autora   return experiment.to_js_string(as_function=True, is_async=True)"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#generating-stimulus-sequences-for-a-closed-loop-psychophysics-study","title":"Generating Stimulus Sequences for a Closed-Loop Psychophysics Study\u00b6","text":"<p>In this example, we use SweetBean to generate an experimental sequence for a same-different Psychophysics experiment. In this experiment, each trials has the following sequence of events:</p> <ul> <li>a fixation cross is displayed for 1500 ms</li> <li>a set of two dot stimuli is displayed, one on the left and one on the right for 200ms. Each set contains a certain number of dots and the participant has to indicate whether the numbers of dots in the two sets are the same or different by pressing <code>y</code> or <code>n</code>.</li> </ul> <p>You can find more in-depth tutorials on automated web experiment generation at the SweetBean Website.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#installation","title":"Installation\u00b6","text":"<p>First, we will install SweetBean and Setup.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#instruction-block","title":"Instruction Block\u00b6","text":"<p>Many experiments require instructions that tell the participants what to do.</p> <p>Creating instructions in SweetBean is quite simple. First, we define a number of text stimuli that the participant sees. Then, we specify the order of the text stimuli within a block of instructions.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#exit-block","title":"Exit Block\u00b6","text":"<p>Similarly, we can specify a final instruction displayed at the end of the experiment.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#task-block","title":"Task Block\u00b6","text":""},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#fixation-cross","title":"Fixation Cross\u00b6","text":"<p>First, we define the fixation cross. SweetBean provides a convenient method:</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#dots-stimulus","title":"Dots Stimulus\u00b6","text":"<p>Next, we declare our sets of dots as features in the stimulus, using the timeline variables:</p> <p>First, declare the stimulus features <code>dot_stimulus_left</code> and <code>dot_stimulus_right</code> as timeline variables (since they come from the timeline). For the parser, we also need to provide all the possible levels of the stimulus. For now, we assume that each dot display can contain either 40 or 70 dots.</p> <p>Then, we define the entire stimulus which is composed of the two features. SweetPea provides a convenient way of generating a stimulus with two sets of dots via <code>RandomDotPatternsStimulus</code>.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#task-event-sequence","title":"Task Event Sequence\u00b6","text":"<p>Now, we define the event sequence which determines the order of events within a trial. SweetBean groups event into event sequences, and event sequences into blocks. Here, an event sequence corresponds to a trial and a block to series of trials.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#experiment-block-sequence","title":"Experiment Block Sequence\u00b6","text":"<p>Now that we have specified all of our experiment blocks, we put them together into an experiment. The function below compiles the experiment and converts it into an html file.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetbean/#writing-a-function-to-automate-the-generation-of-stimulus-sequences","title":"Writing a Function to Automate the Generation of Stimulus Sequences\u00b6","text":"<p>The function below compiles the code above into a single function, and returns a web-based (JavaScript) experiment, written in <code>jsPsych</code>.</p> <p>The function takes a timeline, containing a sequence of trials, as input.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetpea/","title":"Sweetpea","text":"In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install sweetpea\n</pre> %%capture !pip install sweetpea In\u00a0[\u00a0]: Copied! <pre>from sweetpea import Factor\n\n# the first experimental factor indicates the number of dots in the\n# left stimulus. It has two levels, i.e., two possible values for the\n# number of dots, either 40 and or 70 dots.\nnum_dots_left = Factor('dots_left', [40, 70])\n\n# the second experimental factor indicates the number of dots in the\n# right stimulus. It also has two levels: 40 and 70 dots.\nnum_dots_right = Factor('dots_right', [70, 40])\n</pre> from sweetpea import Factor  # the first experimental factor indicates the number of dots in the # left stimulus. It has two levels, i.e., two possible values for the # number of dots, either 40 and or 70 dots. num_dots_left = Factor('dots_left', [40, 70])  # the second experimental factor indicates the number of dots in the # right stimulus. It also has two levels: 40 and 70 dots. num_dots_right = Factor('dots_right', [70, 40]) <p>The code defines the two variables in terms of two experimental factors, respectively: the number of dots in the left stimulus and the number of dots in the right stimulus. Here, we assume that the number of dots can either be 40 or 70.</p> In\u00a0[\u00a0]: Copied! <pre>from sweetpea import MinimumTrials, CrossBlock, synthesize_trials, CMSGen, experiments_to_dicts\n\n# the experimental design includes all relevant factors\n# (whether counterbalanced or not)\ndesign = [num_dots_left, num_dots_right]\n\n# the crossing specifies which factors are fully counterbalanced\ncrossing = [num_dots_left, num_dots_right]\n\n# we also add a constraint to include at least 20 trials\nconstraints = [MinimumTrials(20)]\n\n# next, we define an experimental block based on the design, crossing,\n# and constraints\nblock = CrossBlock(design, crossing, constraints)\n\n# we then use a SAT-Solver to find one suitable experimental sequence\nexperiment = synthesize_trials(block, 1, CMSGen)\n</pre> from sweetpea import MinimumTrials, CrossBlock, synthesize_trials, CMSGen, experiments_to_dicts  # the experimental design includes all relevant factors # (whether counterbalanced or not) design = [num_dots_left, num_dots_right]  # the crossing specifies which factors are fully counterbalanced crossing = [num_dots_left, num_dots_right]  # we also add a constraint to include at least 20 trials constraints = [MinimumTrials(20)]  # next, we define an experimental block based on the design, crossing, # and constraints block = CrossBlock(design, crossing, constraints)  # we then use a SAT-Solver to find one suitable experimental sequence experiment = synthesize_trials(block, 1, CMSGen) <pre>Sampling 1 trial sequences using CMSGen.\nEncoding experiment constraints...\nRunning CMSGen...\n</pre> <p>We can print the resulting experiment:</p> In\u00a0[\u00a0]: Copied! <pre>from sweetpea import print_experiments\n\nprint_experiments(block, experiment)\n</pre> from sweetpea import print_experiments  print_experiments(block, experiment) <pre>\n1 trial sequences found.\n\nExperiment 0:\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 40\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 40 | Number of Dots for Right Stimulus 1 70\nNumber of Dots for Left Stimulus 70 | Number of Dots for Right Stimulus 1 70\n\n</pre> <p>And we can store the resulting experimental sequence in a dictionary:</p> In\u00a0[\u00a0]: Copied! <pre># we can export the experimental sequence as a dictionary\nsequence = experiments_to_dicts(block, experiment)\nprint(sequence)\n</pre> # we can export the experimental sequence as a dictionary sequence = experiments_to_dicts(block, experiment) print(sequence) <pre>[[{'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 40}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 40, 'Number of Dots for Right Stimulus 1': 70}, {'Number of Dots for Left Stimulus': 70, 'Number of Dots for Right Stimulus 1': 70}]]\n</pre> <p>Next, we wrap the code above into a function that will generate an experimental sequence for an arbitrary set of two stimulus intensities, e.g., number of dots. We will accept the number of trials as an argument.</p> In\u00a0[\u00a0]: Copied! <pre>from sweetpea import Factor, MinimumTrials, CrossBlock, synthesize_trials, CMSGen, experiments_to_dicts\n\ndef trial_sequence(num_dots_1, num_dots_2, min_trials):\n\n  # define regular factors\n  num_dots_left = Factor('dots_left', [num_dots_1, num_dots_2])\n  num_dots_right = Factor('dots_right', [num_dots_1, num_dots_2])\n\n  # define experimental block\n  design = [num_dots_left, num_dots_right]\n  crossing = [num_dots_left, num_dots_right]\n  constraints = [MinimumTrials(min_trials)]\n\n  block = CrossBlock(design, crossing, constraints)\n\n  # synthesize trial sequence\n  experiment = synthesize_trials(block, 1, CMSGen)\n\n  # export as dictionary\n  return experiments_to_dicts(block, experiment)[0]\n</pre> from sweetpea import Factor, MinimumTrials, CrossBlock, synthesize_trials, CMSGen, experiments_to_dicts  def trial_sequence(num_dots_1, num_dots_2, min_trials):    # define regular factors   num_dots_left = Factor('dots_left', [num_dots_1, num_dots_2])   num_dots_right = Factor('dots_right', [num_dots_1, num_dots_2])    # define experimental block   design = [num_dots_left, num_dots_right]   crossing = [num_dots_left, num_dots_right]   constraints = [MinimumTrials(min_trials)]    block = CrossBlock(design, crossing, constraints)    # synthesize trial sequence   experiment = synthesize_trials(block, 1, CMSGen)    # export as dictionary   return experiments_to_dicts(block, experiment)[0]"},{"location":"examples/closed-loop-basic/notebooks/sweetpea/#generating-trial-sequences-for-a-closed-loop-psychophysics-study","title":"Generating Trial Sequences for a Closed-Loop Psychophysics Study\u00b6","text":"<p>In this example, we use SweetPea to generate an experimental sequence for a same-different Psychophysics experiment.</p> <p>You can find more in-depth tutorials on automated experimental design at the SweetPea Website.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetpea/#installation","title":"Installation\u00b6","text":"<p>First, we will install SweetPea.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetpea/#regular-factors","title":"Regular Factors\u00b6","text":"<p>Next, we define the experimental factors of our experiment. In this experiment, select whether the number of dots in the two sets is the same or not.</p> <p>The experiment has two independent variables: The number of dots in the first set and the number of dots in the second set. Here, we want to counterbalance the number of dots for stimulus 1 and stimulus, respectively.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetpea/#synthesize-trial-sequences","title":"Synthesize Trial Sequences\u00b6","text":"<p>Next, we generate an experiment trial sequence. In this sequence, we want to counterbalance the levels of both factors. We also want to specify a constraint to include at least 20 trials.</p>"},{"location":"examples/closed-loop-basic/notebooks/sweetpea/#printing-and-extracting-experimental-sequence","title":"Printing and Extracting Experimental Sequence\u00b6","text":""},{"location":"examples/closed-loop-basic/notebooks/sweetpea/#writing-a-function-to-automate-the-generation-of-trial-sequences","title":"Writing a Function to Automate the Generation of Trial Sequences\u00b6","text":""},{"location":"experiment-runner/","title":"Experiment Runner Overview","text":"<p><code>autora</code> includes tools for running synthetic and real-world experiments.</p> <p></p> <p>For synthetic experiments, these tools consist of ground-truth models from various disciplines that can be used to generate synthetic data. For real-world experiments, these tools consist of (i) experimentation managers, (ii) recruitment managers, and (iii) experiment runners, which combine experimentation managers with recruitment managers. Experimentation managers facilitate communication of conditions and observations between <code>autora</code> and environments in which experiments are hosted. Recruitment managers facilitate recruitment and coordination of study participants.</p> <p>Experiment runner tools may take in information about new experimental conditions or entire experiments. The following table includes the various experiment runner tools currently implemented in <code>autora</code>.</p> Name Links Description Synthetic Package, Docs A compendium of ground-truth models across psychology, psychophysics, behavioral economics, and other disciplines. Firebase (experimentation manager) Package, Docs An experimentation manager that provides functionality to manage communication of conditions and observations between <code>autora</code> and an experiment on Firebase. Prolific (recruitment manager) Package, Docs A recruitment manager that provides functionality to recruit participants via Prolific to conduct an experiment using <code>autora</code>. Firebase-Prolific (experiment runner) Package, Docs An experiment runner that combines the Firebase experimentation manager and the Prolific recruitment manager to automatically run human behavioral experiments."},{"location":"experiment-runner/behavioral/","title":"Online Experiments To Collect AutoRA Observations","text":"<p>AutoRA provides functionality for running closed-loop behavioral studies with online experiments. This functionality allows researchers to set up websites that integrate with AutoRA workflows. These websites allow observations gathered from participants online.</p> <p></p> <p>To enable this functionality, AutoRA can interface with databases, such as Firebase for hosting the website and storing data. In addition, AutoRA can recruit participants through platforms like Prolific.</p>"},{"location":"experiment-runner/behavioral/example/","title":"Example: Closed-Loop Behavioral Study","text":"<p>For a detailed example of setting up a closed-loop behavioral study with an online experiment, see the Basic Closed-Loop Psychophysics Study.</p>"},{"location":"experiment-runner/behavioral/firebase/","title":"Firebase Integration","text":"<p>On this page, you can find information on how to set up a Firebase website to collect observations for an AutoRA workflow. You can find information on how to connect such a website to AutoRA and how to automatically recruit participants via Prolific at the following pages, respectively: AutoRA Firebase Experimentation manager, AutoRA Prolific Recruitment Manager.</p> <p>For setting up the online experiment, we recommend using either the user cookiecutter template or the create-react-app template.</p> <p>Hint</p> <p>The cookiecutter template also provides a template for the AutoRA workflow used in online experiments.</p>"},{"location":"experiment-runner/behavioral/firebase/#installation","title":"Installation","text":"<p>To make sure that <code>node</code> is installed on your system, run the following command.  <pre><code>node -v\n</code></pre> If you don't see a version number or the version number is below 16.0, install or update <code>node</code> following the instruction on the node.js website.</p> <p>When <code>node</code> is available on your system, you can use create-react-app by running the following command. <pre><code>npx create-react-app path/to/react/pp --template autora-firebase\n</code></pre> If you want to use cookiecutter, run the following command and follow the instructions. <pre><code>cookiecutter https://github.com/AutoResearch/autora-user-cookiecutter\n</code></pre> This creates your project folder. Before writing code for your website, you also need to set up a Firebase project.</p>"},{"location":"experiment-runner/behavioral/firebase/#firebase-project-setup","title":"Firebase Project Setup","text":""},{"location":"experiment-runner/behavioral/firebase/#initialize-firebase-account-and-project","title":"Initialize Firebase Account And Project","text":"<ul> <li>Create and log in to a Firebase account on the Firebase website.</li> <li>Create a Firebase project by clicking add project and enter a project name.</li> <li>You can choose to disable Google Analytics in the next page if you are not planning on using it for your project.</li> </ul>"},{"location":"experiment-runner/behavioral/firebase/#copy-web-app-credentials","title":"Copy Web App Credentials","text":"<ul> <li>Navigate to the Firebase console and select the project</li> <li>To create a new web app, click on <code>Add App</code> or the <code>&lt;&gt;</code> symbol and follow the prompts</li> <li>Enter a name for the Firebase app (could be the same as the project name)</li> <li>Check <code>Also set up Firebase Hosting for this app</code></li> <li>Click <code>Register App</code>. This auto-generates a script with several values that you need to copy for the next step.</li> <li>Copy the auto-generated values from the Firebase console to the corresponding variables in the <code>.env</code> file in the project folder that was created on your system using create-react-app or cookiecutter <pre><code>REACT_APP_apiKey=\nREACT_APP_authDomain=\nREACT_APP_projectId=\nREACT_APP_storageBucket=\nREACT_APP_messagingSenderId=\nREACT_APP_appId=\nREACT_APP_devNoDb=\"True\"\nREACT_APP_useProlificId=\"False\"\n</code></pre></li> <li>Click on <code>Next</code></li> <li>You will not need to run the command that is displayed after first clicking <code>Next</code>, so click <code>Next</code> again</li> <li>Click <code>Continue to console</code></li> </ul>"},{"location":"experiment-runner/behavioral/firebase/#firestore-setup","title":"Firestore Setup","text":"<p>AutoRA includes cloud storage for task data using Firestore. Follow these steps to initialize Firestore:</p> <ul> <li>Navigate to the current project in the developer console and select <code>Firestore Database</code> from the sidebar under <code>Build</code>.</li> <li>Click <code>Create Database</code></li> <li>Select production mode and click <code>Next</code></li> <li>Choose the current location and click <code>Enable</code></li> </ul>"},{"location":"experiment-runner/behavioral/firebase/#configure-your-project-for-firebase","title":"Configure Your Project For Firebase","text":"<p>In the project folder, enter the following commands in your terminal: First log in to your Firebase account using <pre><code>firebase login\n</code></pre> Then initialize the Firebase project in this folder by running: <pre><code>firebase init\n</code></pre> An interactive initialization process will now run in your command line. For the first question, select these options:</p> <ul> <li>Firestore: Configure security rules and indexes files for Firestore</li> <li>Hosting: Configure files for Firebase Hosting and (optionally) set up GitHub Action deploys</li> <li>For a Firebase project, use the one you created earlier</li> <li>Use the default options for the Firestore rules and the Firestore indexes.</li> <li>!!! IMPORTANT !!! Use the build directory instead of the public directory here.</li> <li>When asked for the directory, write <code>build</code> and press <code>Enter</code>.</li> <li>Configure as a single-page app; don't set up automatic builds and deploys with GitHub. </li> <li>Don't overwrite the index.html file if the question pops up.</li> </ul>"},{"location":"experiment-runner/behavioral/firebase/#write-code-for-your-experiment","title":"Write Code For Your Experiment","text":"<p>To write code for your experiment, use the <code>main.js</code> file in the <code>src/design</code> folder. For example, you can use jsPsych and install packages using <code>npm</code>. The main function should return an observation (the data created by a participant).</p> <p>You can test the experiment locally using <pre><code>npm start\n</code></pre> During development, the Firestore database will not be used. If you want to load conditions from the database, you need to upload them first (for example using the AutoRA Firebase Experimentation Manager) and set <code>REACT_APP_devNoDb=\"False\"</code> in the <code>.env</code> file.</p>"},{"location":"experiment-runner/behavioral/firebase/#using-prolific-ids","title":"Using Prolific Id's","text":"<p>If you want to recruit participants via Prolific (for example using the AutoRA Prolific Recruitment Manager), we highly recommend setting <code>REACT_APP_useProlificId=\"True\"</code>. This will speed up the recruitment of participants.</p>"},{"location":"experiment-runner/behavioral/firebase/#build-and-deploy-to-firebase","title":"Build And Deploy To Firebase","text":"<p>To serve the website on the internet, you must build and deploy it to Firebase. To build the project, run <pre><code>npm run build\n</code></pre> To deploy to Firebase, run <pre><code>firebase deploy\n</code></pre> This will make the website available on the web. You can find the URL of the website in the command line or on the Firebase console of your project under <code>Hosting</code>.</p>"},{"location":"experimentalist/","title":"Experimentalist Overview","text":"<p>The primary goal of an experimentalist is to identify experiments that yield  scientific merit. <code>autora</code> implements techniques for automating the identification  of novel experiments.</p> <p>An experiment consists of a series of conditions \\(\\vec{x} \\in X\\). The variables manipulated in each condition  are defined as independent variables. As an example, consider a visual discrimination task in which participants are presented with two lines of different lengths, and are asked to indicate which line is longer. There are two independent variables in this experiment: the length of the first line and the length of the second line, which each have values (e.g., 2.0 cm for the first line and 2.1 cm for the second line). Thus, a condition is a vector of values that corresponds to a specific combination of values of the independent variables \\(x_i\\).</p> <p></p> <p>Experimentalists may use information about candidate models \\(M\\) obtained from a theorist,  conditions that have already been probed \\(\\vec{x}' \\in X'\\), or  respective observations \\(\\vec{y}' \\in Y'\\). The following table includes the experimentalists currently implemented  in <code>autora</code>.</p> Name Links Description Arguments Random Package, Docs An experimentalist with pooling and sampling functionality. The pooler creates combinations of conditions from lists of discrete values using random selection; the sampler draws from a pool of conditions without replacement using uniform random sampling. Novelty Package, Docs Identifies conditions \\(\\vec{x}' \\in X'\\) with respect to a pairwise distance metric applied to existing conditions \\(\\vec{x} \\in X\\). \\(X'\\) Uncertainty Package, Docs Identifies conditions \\(\\vec{x}' \\in X'\\) with respect to model uncertainty, which can be calculated in three different ways. \\(M\\) Model Disagreement Package, Docs Identifies conditions \\(\\vec{x}' \\in X'\\) with respect to a pairwise distance metric between theorist models, \\(P_{M_{i}}(\\hat{y}, \\vec{x}')\\). \\(M\\) Falsification Package, Docs An experimentalist with pooling and sampling functionality that generates and samples from novel conditions under which the loss \\(\\hat{\\mathcal{L}}(M,X,Y,X')\\) of the best candidate model is predicted to be the highest. \\(M, X', Y'\\) Mixture Package, Docs Uses a mixture of specified sampling strategies to identify novel conditions. Conditions are selected based on a weighted sum of scores obtained from the specified strategies. \\(M, X', Y'\\) Nearest Value Package, Docs Returns the nearest values between the input samples and the allowed values, without replacement. \\(X'\\) Leverage Package, Docs Identifies conditions using the statistical concept of leverage to refit candidate models iteratively with the leave-one-out method. \\(M, X', Y'\\) Inequality Package, Docs Uses a pairwise distance metric to compare and select new conditions. This metric along with a difference threshold are used to calculate inequality scores for candidate conditions, and conditions with the highest scores are chosen. \\(X'\\)"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>autora<ul> <li>experiment_runner<ul> <li>synthetic<ul> <li>abstract<ul> <li>equation<ul> <li>__init__</li> </ul> </li> <li>lmm</li> <li>template_experiment</li> </ul> </li> <li>economics<ul> <li>expected_value_theory</li> <li>prospect_theory</li> </ul> </li> <li>neuroscience<ul> <li>task_switching</li> </ul> </li> <li>psychology<ul> <li>exp_learning</li> <li>luce_choice_ratio</li> <li>q_learning</li> </ul> </li> <li>psychophysics<ul> <li>stevens_power_law</li> <li>weber_fechner_law</li> </ul> </li> <li>utilities</li> </ul> </li> <li>firebase_prolific<ul> <li>__init__</li> </ul> </li> </ul> </li> <li>experimentalist<ul> <li>bandit_random<ul> <li>__init__</li> </ul> </li> <li>grid</li> <li>pipeline</li> <li>random</li> <li>utils</li> <li>model_disagreement<ul> <li>__init__</li> </ul> </li> <li>falsification<ul> <li>__init__</li> <li>popper_net</li> <li>utils</li> </ul> </li> <li>inequality<ul> <li>__init__</li> </ul> </li> <li>leverage<ul> <li>__init__</li> </ul> </li> <li>mixture<ul> <li>__init__</li> </ul> </li> <li>nearest_value<ul> <li>__init__</li> </ul> </li> <li>novelty<ul> <li>__init__</li> </ul> </li> <li>prediction_filter<ul> <li>__init__</li> </ul> </li> <li>uncertainty<ul> <li>__init__</li> </ul> </li> </ul> </li> <li>theorist<ul> <li>bms<ul> <li>__init__</li> <li>fit_prior</li> <li>mcmc</li> <li>parallel</li> <li>prior</li> <li>regressor</li> <li>utils</li> </ul> </li> <li>bsr<ul> <li>__init__</li> <li>funcs</li> <li>misc</li> <li>node</li> <li>operation</li> <li>prior</li> <li>regressor</li> </ul> </li> <li>darts<ul> <li>__init__</li> <li>architect</li> <li>dataset</li> <li>fan_out</li> <li>model_search</li> <li>operations</li> <li>regressor</li> <li>utils</li> <li>visualize</li> </ul> </li> </ul> </li> <li>serializer<ul> <li>__init__</li> <li>yaml_</li> </ul> </li> <li>state</li> <li>utils<ul> <li>deprecation</li> <li>dictionary</li> </ul> </li> <li>variable<ul> <li>__init__</li> <li>time</li> </ul> </li> <li>workflow<ul> <li>__main__</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/autora/state/","title":"autora.state","text":"<p>Classes to represent cycle state \\(S\\) as \\(S_n = S_{0} + \\sum_{i=1}^n \\Delta S_{i}\\).</p>"},{"location":"reference/autora/state/#autora.state.Result","title":"<code>Result = Delta</code>  <code>module-attribute</code>","text":"<p><code>Result</code> is an alias for <code>Delta</code>.</p>"},{"location":"reference/autora/state/#autora.state.Delta","title":"<code>Delta</code>","text":"<p>               Bases: <code>UserDict</code>, <code>Generic[S]</code></p> <p>Represents a delta where the base object determines the extension behavior.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass\n</code></pre> <p>First we define the dataclass to act as the basis:</p> <pre><code>&gt;&gt;&gt; from typing import Optional, List\n&gt;&gt;&gt; @dataclass(frozen=True)\n... class ListState:\n...     l: Optional[List] = None\n...     m: Optional[List] = None\n...\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>class Delta(UserDict, Generic[S]):\n    \"\"\"\n    Represents a delta where the base object determines the extension behavior.\n\n    Examples:\n        &gt;&gt;&gt; from dataclasses import dataclass\n\n        First we define the dataclass to act as the basis:\n        &gt;&gt;&gt; from typing import Optional, List\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class ListState:\n        ...     l: Optional[List] = None\n        ...     m: Optional[List] = None\n        ...\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/autora/state/#autora.state.DeltaAddable","title":"<code>DeltaAddable</code>","text":"<p>               Bases: <code>Protocol[C]</code></p> <p>A class which a Delta or other Mapping can be added to, returning the same class</p> Source code in <code>autora/state.py</code> <pre><code>class DeltaAddable(Protocol[C]):\n    \"\"\"A class which a Delta or other Mapping can be added to, returning the same class\"\"\"\n\n    def __add__(self: C, other: Union[Delta, Mapping]) -&gt; C:\n        ...\n</code></pre>"},{"location":"reference/autora/state/#autora.state.StandardState","title":"<code>StandardState</code>  <code>dataclass</code>","text":"<p>               Bases: <code>State</code></p> <p>Examples:</p> <p>The state can be initialized emtpy</p> <pre><code>&gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n&gt;&gt;&gt; s = StandardState()\n&gt;&gt;&gt; s\nStandardState(variables=None, conditions=None, experiment_data=None, models=[])\n</code></pre> <p>The <code>variables</code> can be updated using a <code>Delta</code>:</p> <pre><code>&gt;&gt;&gt; dv1 = Delta(variables=VariableCollection(independent_variables=[Variable(\"1\")]))\n&gt;&gt;&gt; s + dv1\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='1',...)\n</code></pre> <p>... and are replaced by each <code>Delta</code>:</p> <pre><code>&gt;&gt;&gt; dv2 = Delta(variables=VariableCollection(independent_variables=[Variable(\"2\")]))\n&gt;&gt;&gt; s + dv1 + dv2\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='2',...)\n</code></pre> <p>The <code>conditions</code> can be updated using a <code>Delta</code>:</p> <pre><code>&gt;&gt;&gt; dc1 = Delta(conditions=pd.DataFrame({\"x\": [1, 2, 3]}))\n&gt;&gt;&gt; (s + dc1).conditions\n   x\n0  1\n1  2\n2  3\n</code></pre> <p>... and are replaced by each <code>Delta</code>:</p> <pre><code>&gt;&gt;&gt; dc2 = Delta(conditions=pd.DataFrame({\"x\": [4, 5]}))\n&gt;&gt;&gt; (s + dc1 + dc2).conditions\n   x\n0  4\n1  5\n</code></pre> <p>Datatypes other than <code>pd.DataFrame</code> will be coerced into a <code>DataFrame</code> if possible.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dc3 = Delta(conditions=np.core.records.fromrecords([(8, \"h\"), (9, \"i\")], names=\"n,c\"))\n&gt;&gt;&gt; (s + dc3).conditions\n   n  c\n0  8  h\n1  9  i\n</code></pre> <p>If they are passed without column names, no column names are inferred. This is to ensure that accidental mislabeling of columns cannot occur. Column names should usually be provided.</p> <pre><code>&gt;&gt;&gt; dc4 = Delta(conditions=[(6,), (7,)])\n&gt;&gt;&gt; (s + dc4).conditions\n   0\n0  6\n1  7\n</code></pre> <p>Datatypes which are incompatible with a pd.DataFrame will throw an error:</p> <pre><code>&gt;&gt;&gt; s + Delta(conditions=\"not compatible with pd.DataFrame\")\nTraceback (most recent call last):\n...\nValueError: ...\n</code></pre> <p>Experiment data can be updated using a Delta:</p> <pre><code>&gt;&gt;&gt; ded1 = Delta(experiment_data=pd.DataFrame({\"x\": [1,2,3], \"y\": [\"a\", \"b\", \"c\"]}))\n&gt;&gt;&gt; (s + ded1).experiment_data\n   x  y\n0  1  a\n1  2  b\n2  3  c\n</code></pre> <p>... and are extended with each Delta:</p> <pre><code>&gt;&gt;&gt; ded2 = Delta(experiment_data=pd.DataFrame({\"x\": [4, 5, 6], \"y\": [\"d\", \"e\", \"f\"]}))\n&gt;&gt;&gt; (s + ded1 + ded2).experiment_data\n   x  y\n0  1  a\n1  2  b\n2  3  c\n3  4  d\n4  5  e\n5  6  f\n</code></pre> <p>If they are passed without column names, no column names are inferred. This is to ensure that accidental mislabeling of columns cannot occur.</p> <pre><code>&gt;&gt;&gt; ded3 = Delta(experiment_data=pd.DataFrame([(7, \"g\"), (8, \"h\")]))\n&gt;&gt;&gt; (s + ded3).experiment_data\n   0  1\n0  7  g\n1  8  h\n</code></pre> <p>If there are already data present, the column names must match.</p> <pre><code>&gt;&gt;&gt; (s + ded2 + ded3).experiment_data\n     x    y    0    1\n0  4.0    d  NaN  NaN\n1  5.0    e  NaN  NaN\n2  6.0    f  NaN  NaN\n3  NaN  NaN  7.0    g\n4  NaN  NaN  8.0    h\n</code></pre> <p><code>experiment_data</code> other than <code>pd.DataFrame</code> will be coerced into a <code>DataFrame</code> if possible.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ded4 = Delta(\n...     experiment_data=np.core.records.fromrecords([(1, \"a\"), (2, \"b\")], names=[\"x\", \"y\"]))\n&gt;&gt;&gt; (s + ded4).experiment_data\n   x  y\n0  1  a\n1  2  b\n</code></pre> <p><code>experiment_data</code> which are incompatible with a pd.DataFrame will throw an error:</p> <pre><code>&gt;&gt;&gt; s + Delta(experiment_data=\"not compatible with pd.DataFrame\")\nTraceback (most recent call last):\n...\nValueError: ...\n</code></pre> <p><code>models</code> can be updated using a Delta:</p> <pre><code>&gt;&gt;&gt; from sklearn.dummy import DummyClassifier\n&gt;&gt;&gt; dm1 = Delta(models=[DummyClassifier(constant=1)])\n&gt;&gt;&gt; dm2 = Delta(models=[DummyClassifier(constant=2), DummyClassifier(constant=3)])\n&gt;&gt;&gt; (s + dm1).models\n[DummyClassifier(constant=1)]\n</code></pre> <pre><code>&gt;&gt;&gt; (s + dm1 + dm2).models\n[DummyClassifier(constant=1), DummyClassifier(constant=2), DummyClassifier(constant=3)]\n</code></pre> <p>The last model is available under the <code>model</code> property:</p> <pre><code>&gt;&gt;&gt; (s + dm1 + dm2).model\nDummyClassifier(constant=3)\n</code></pre> <p>If there is no model, <code>None</code> is returned:</p> <pre><code>&gt;&gt;&gt; print(s.model)\nNone\n</code></pre> <p><code>models</code> can also be updated using a Delta with a single <code>model</code>:</p> <pre><code>&gt;&gt;&gt; dm3 = Delta(model=DummyClassifier(constant=4))\n&gt;&gt;&gt; (s + dm1 + dm3).model\nDummyClassifier(constant=4)\n</code></pre> <p>We can use properties X, y, iv_names and dv_names as 'getters' ...</p> <pre><code>&gt;&gt;&gt; x_v = Variable('x')\n&gt;&gt;&gt; y_v = Variable('y')\n&gt;&gt;&gt; variables = VariableCollection(independent_variables=[x_v], dependent_variables=[y_v])\n&gt;&gt;&gt; e_data = pd.DataFrame({'x': [1, 2, 3], 'y': [2, 4, 6]})\n&gt;&gt;&gt; s = StandardState(variables=variables, experiment_data=e_data)\n&gt;&gt;&gt; @inputs_from_state\n... def show_X(X):\n...     return X\n&gt;&gt;&gt; show_X(s)\n   x\n0  1\n1  2\n2  3\n</code></pre> <p>... but nothing happens if we use them as <code>setters</code>:</p> <pre><code>&gt;&gt;&gt; @on_state\n... def add_to_X(X):\n...     res = X.copy()\n...     res['x'] += 1\n...     return Delta(X=res)\n&gt;&gt;&gt; s = add_to_X(s)\n&gt;&gt;&gt; s.X\n   x\n0  1\n1  2\n2  3\n</code></pre> <p>However, if the property has a deticated setter, we can still use them as getter:</p> <pre><code>&gt;&gt;&gt; s.model is None\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; @on_state\n... def add_model(_model):\n...     return Delta(model=_model)\n&gt;&gt;&gt; s = add_model(s, _model=LinearRegression())\n&gt;&gt;&gt; s.models\n[LinearRegression()]\n</code></pre> <pre><code>&gt;&gt;&gt; s.model\nLinearRegression()\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>@dataclass(frozen=True)\nclass StandardState(State):\n    \"\"\"\n    Examples:\n        The state can be initialized emtpy\n        &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n        &gt;&gt;&gt; s = StandardState()\n        &gt;&gt;&gt; s\n        StandardState(variables=None, conditions=None, experiment_data=None, models=[])\n\n        The `variables` can be updated using a `Delta`:\n        &gt;&gt;&gt; dv1 = Delta(variables=VariableCollection(independent_variables=[Variable(\"1\")]))\n        &gt;&gt;&gt; s + dv1 # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        StandardState(variables=VariableCollection(independent_variables=[Variable(name='1',...)\n\n        ... and are replaced by each `Delta`:\n        &gt;&gt;&gt; dv2 = Delta(variables=VariableCollection(independent_variables=[Variable(\"2\")]))\n        &gt;&gt;&gt; s + dv1 + dv2 # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        StandardState(variables=VariableCollection(independent_variables=[Variable(name='2',...)\n\n        The `conditions` can be updated using a `Delta`:\n        &gt;&gt;&gt; dc1 = Delta(conditions=pd.DataFrame({\"x\": [1, 2, 3]}))\n        &gt;&gt;&gt; (s + dc1).conditions\n           x\n        0  1\n        1  2\n        2  3\n\n        ... and are replaced by each `Delta`:\n        &gt;&gt;&gt; dc2 = Delta(conditions=pd.DataFrame({\"x\": [4, 5]}))\n        &gt;&gt;&gt; (s + dc1 + dc2).conditions\n           x\n        0  4\n        1  5\n\n        Datatypes other than `pd.DataFrame` will be coerced into a `DataFrame` if possible.\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; dc3 = Delta(conditions=np.core.records.fromrecords([(8, \"h\"), (9, \"i\")], names=\"n,c\"))\n        &gt;&gt;&gt; (s + dc3).conditions\n           n  c\n        0  8  h\n        1  9  i\n\n        If they are passed without column names, no column names are inferred.\n        This is to ensure that accidental mislabeling of columns cannot occur.\n        Column names should usually be provided.\n        &gt;&gt;&gt; dc4 = Delta(conditions=[(6,), (7,)])\n        &gt;&gt;&gt; (s + dc4).conditions\n           0\n        0  6\n        1  7\n\n        Datatypes which are incompatible with a pd.DataFrame will throw an error:\n        &gt;&gt;&gt; s + Delta(conditions=\"not compatible with pd.DataFrame\") \\\n# doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        ValueError: ...\n\n        Experiment data can be updated using a Delta:\n        &gt;&gt;&gt; ded1 = Delta(experiment_data=pd.DataFrame({\"x\": [1,2,3], \"y\": [\"a\", \"b\", \"c\"]}))\n        &gt;&gt;&gt; (s + ded1).experiment_data\n           x  y\n        0  1  a\n        1  2  b\n        2  3  c\n\n        ... and are extended with each Delta:\n        &gt;&gt;&gt; ded2 = Delta(experiment_data=pd.DataFrame({\"x\": [4, 5, 6], \"y\": [\"d\", \"e\", \"f\"]}))\n        &gt;&gt;&gt; (s + ded1 + ded2).experiment_data\n           x  y\n        0  1  a\n        1  2  b\n        2  3  c\n        3  4  d\n        4  5  e\n        5  6  f\n\n        If they are passed without column names, no column names are inferred.\n        This is to ensure that accidental mislabeling of columns cannot occur.\n        &gt;&gt;&gt; ded3 = Delta(experiment_data=pd.DataFrame([(7, \"g\"), (8, \"h\")]))\n        &gt;&gt;&gt; (s + ded3).experiment_data\n           0  1\n        0  7  g\n        1  8  h\n\n        If there are already data present, the column names must match.\n        &gt;&gt;&gt; (s + ded2 + ded3).experiment_data\n             x    y    0    1\n        0  4.0    d  NaN  NaN\n        1  5.0    e  NaN  NaN\n        2  6.0    f  NaN  NaN\n        3  NaN  NaN  7.0    g\n        4  NaN  NaN  8.0    h\n\n        `experiment_data` other than `pd.DataFrame` will be coerced into a `DataFrame` if possible.\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ded4 = Delta(\n        ...     experiment_data=np.core.records.fromrecords([(1, \"a\"), (2, \"b\")], names=[\"x\", \"y\"]))\n        &gt;&gt;&gt; (s + ded4).experiment_data\n           x  y\n        0  1  a\n        1  2  b\n\n        `experiment_data` which are incompatible with a pd.DataFrame will throw an error:\n        &gt;&gt;&gt; s + Delta(experiment_data=\"not compatible with pd.DataFrame\") \\\n# doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        ValueError: ...\n\n        `models` can be updated using a Delta:\n        &gt;&gt;&gt; from sklearn.dummy import DummyClassifier\n        &gt;&gt;&gt; dm1 = Delta(models=[DummyClassifier(constant=1)])\n        &gt;&gt;&gt; dm2 = Delta(models=[DummyClassifier(constant=2), DummyClassifier(constant=3)])\n        &gt;&gt;&gt; (s + dm1).models\n        [DummyClassifier(constant=1)]\n\n        &gt;&gt;&gt; (s + dm1 + dm2).models\n        [DummyClassifier(constant=1), DummyClassifier(constant=2), DummyClassifier(constant=3)]\n\n        The last model is available under the `model` property:\n        &gt;&gt;&gt; (s + dm1 + dm2).model\n        DummyClassifier(constant=3)\n\n        If there is no model, `None` is returned:\n        &gt;&gt;&gt; print(s.model)\n        None\n\n        `models` can also be updated using a Delta with a single `model`:\n        &gt;&gt;&gt; dm3 = Delta(model=DummyClassifier(constant=4))\n        &gt;&gt;&gt; (s + dm1 + dm3).model\n        DummyClassifier(constant=4)\n\n\n        We can use properties X, y, iv_names and dv_names as 'getters' ...\n        &gt;&gt;&gt; x_v = Variable('x')\n        &gt;&gt;&gt; y_v = Variable('y')\n        &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x_v], dependent_variables=[y_v])\n        &gt;&gt;&gt; e_data = pd.DataFrame({'x': [1, 2, 3], 'y': [2, 4, 6]})\n        &gt;&gt;&gt; s = StandardState(variables=variables, experiment_data=e_data)\n        &gt;&gt;&gt; @inputs_from_state\n        ... def show_X(X):\n        ...     return X\n        &gt;&gt;&gt; show_X(s)\n           x\n        0  1\n        1  2\n        2  3\n\n        ... but nothing happens if we use them as `setters`:\n        &gt;&gt;&gt; @on_state\n        ... def add_to_X(X):\n        ...     res = X.copy()\n        ...     res['x'] += 1\n        ...     return Delta(X=res)\n        &gt;&gt;&gt; s = add_to_X(s)\n        &gt;&gt;&gt; s.X\n           x\n        0  1\n        1  2\n        2  3\n\n        However, if the property has a deticated setter, we can still use them as getter:\n        &gt;&gt;&gt; s.model is None\n        True\n\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; @on_state\n        ... def add_model(_model):\n        ...     return Delta(model=_model)\n        &gt;&gt;&gt; s = add_model(s, _model=LinearRegression())\n        &gt;&gt;&gt; s.models\n        [LinearRegression()]\n\n        &gt;&gt;&gt; s.model\n        LinearRegression()\n\n\n\n\n\n\n    \"\"\"\n\n    variables: Optional[VariableCollection] = field(\n        default=None, metadata={\"delta\": \"replace\"}\n    )\n    conditions: Optional[pd.DataFrame] = field(\n        default=None, metadata={\"delta\": \"replace\", \"converter\": pd.DataFrame}\n    )\n    experiment_data: Optional[pd.DataFrame] = field(\n        default=None, metadata={\"delta\": \"extend\", \"converter\": pd.DataFrame}\n    )\n    models: List[BaseEstimator] = field(\n        default_factory=list,\n        metadata={\"delta\": \"extend\", \"aliases\": {\"model\": lambda model: [model]}},\n    )\n\n    @property\n    def iv_names(self) -&gt; List[str]:\n        \"\"\"\n        Returns the names of the independent variables\n        Examples:\n            &gt;&gt;&gt; s_empty = StandardState()\n            &gt;&gt;&gt; s_empty.iv_names\n            []\n            &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n            &gt;&gt;&gt; iv_1 = Variable('variable_1')\n            &gt;&gt;&gt; iv_2 = Variable('variable_2')\n            &gt;&gt;&gt; variables = VariableCollection(independent_variables=[iv_1, iv_2])\n            &gt;&gt;&gt; s_variables = StandardState(variables=variables)\n            &gt;&gt;&gt; s_variables.iv_names\n            ['variable_1', 'variable_2']\n        \"\"\"\n        if not self.variables or not self.variables.independent_variables:\n            return []\n        return [iv.name for iv in self.variables.independent_variables]\n\n    @property\n    def dv_names(self) -&gt; List[str]:\n        \"\"\"\n        Returns the names of the independent variables\n        Examples:\n            &gt;&gt;&gt; s_empty = StandardState()\n            &gt;&gt;&gt; s_empty.dv_names\n            []\n            &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n            &gt;&gt;&gt; x = Variable('x')\n            &gt;&gt;&gt; y = Variable('y')\n            &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x], dependent_variables=[y])\n            &gt;&gt;&gt; s_variables = StandardState(variables=variables)\n            &gt;&gt;&gt; s_variables.dv_names\n            ['y']\n        \"\"\"\n        if not self.variables or not self.variables.dependent_variables:\n            return []\n        return [dv.name for dv in self.variables.dependent_variables]\n\n    @property\n    def X(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the already observed conditions as a pd.DataFrame\n        Examples:\n            &gt;&gt;&gt; s_empty = StandardState()\n            &gt;&gt;&gt; s_empty.X\n            Empty DataFrame\n            Columns: []\n            Index: []\n            &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n            &gt;&gt;&gt; x = Variable('x')\n            &gt;&gt;&gt; y = Variable('y')\n            &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x], dependent_variables=[y])\n            &gt;&gt;&gt; experiment_data = pd.DataFrame({'x': [0, 1, 2, 3], 'y': [0, 2, 4, 6]})\n            &gt;&gt;&gt; s = StandardState(variables=variables, experiment_data=experiment_data)\n            &gt;&gt;&gt; s.X\n               x\n            0  0\n            1  1\n            2  2\n            3  3\n        \"\"\"\n        if self.experiment_data is None:\n            return pd.DataFrame()\n        return self.experiment_data[self.iv_names]\n\n    @property\n    def y(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the observations as a pd.DataFrame\n        Examples:\n            &gt;&gt;&gt; s_empty = StandardState()\n            &gt;&gt;&gt; s_empty.y\n            Empty DataFrame\n            Columns: []\n            Index: []\n            &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n            &gt;&gt;&gt; x = Variable('x')\n            &gt;&gt;&gt; y = Variable('y')\n            &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x], dependent_variables=[y])\n            &gt;&gt;&gt; experiment_data = pd.DataFrame({'x': [0, 1, 2, 3], 'y': [0, 2, 4, 6]})\n            &gt;&gt;&gt; s = StandardState(variables=variables, experiment_data=experiment_data)\n            &gt;&gt;&gt; s.y\n               y\n            0  0\n            1  2\n            2  4\n            3  6\n        \"\"\"\n        if self.experiment_data is None:\n            return pd.DataFrame()\n        return self.experiment_data[self.dv_names]\n\n    @property\n    def model(self):\n        if len(self.models) == 0:\n            return None\n        # The property to access the backing field\n        return self.models[-1]\n\n    @model.setter\n    def model(self, value):\n        # Control the setting behavior\n        self.models.append(value)\n</code></pre>"},{"location":"reference/autora/state/#autora.state.StandardState.X","title":"<code>X</code>  <code>property</code>","text":"<p>Returns the already observed conditions as a pd.DataFrame Examples:     &gt;&gt;&gt; s_empty = StandardState()     &gt;&gt;&gt; s_empty.X     Empty DataFrame     Columns: []     Index: []     &gt;&gt;&gt; from autora.variable import VariableCollection, Variable     &gt;&gt;&gt; x = Variable('x')     &gt;&gt;&gt; y = Variable('y')     &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x], dependent_variables=[y])     &gt;&gt;&gt; experiment_data = pd.DataFrame({'x': [0, 1, 2, 3], 'y': [0, 2, 4, 6]})     &gt;&gt;&gt; s = StandardState(variables=variables, experiment_data=experiment_data)     &gt;&gt;&gt; s.X        x     0  0     1  1     2  2     3  3</p>"},{"location":"reference/autora/state/#autora.state.StandardState.dv_names","title":"<code>dv_names</code>  <code>property</code>","text":"<p>Returns the names of the independent variables Examples:     &gt;&gt;&gt; s_empty = StandardState()     &gt;&gt;&gt; s_empty.dv_names     []     &gt;&gt;&gt; from autora.variable import VariableCollection, Variable     &gt;&gt;&gt; x = Variable('x')     &gt;&gt;&gt; y = Variable('y')     &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x], dependent_variables=[y])     &gt;&gt;&gt; s_variables = StandardState(variables=variables)     &gt;&gt;&gt; s_variables.dv_names     ['y']</p>"},{"location":"reference/autora/state/#autora.state.StandardState.iv_names","title":"<code>iv_names</code>  <code>property</code>","text":"<p>Returns the names of the independent variables Examples:     &gt;&gt;&gt; s_empty = StandardState()     &gt;&gt;&gt; s_empty.iv_names     []     &gt;&gt;&gt; from autora.variable import VariableCollection, Variable     &gt;&gt;&gt; iv_1 = Variable('variable_1')     &gt;&gt;&gt; iv_2 = Variable('variable_2')     &gt;&gt;&gt; variables = VariableCollection(independent_variables=[iv_1, iv_2])     &gt;&gt;&gt; s_variables = StandardState(variables=variables)     &gt;&gt;&gt; s_variables.iv_names     ['variable_1', 'variable_2']</p>"},{"location":"reference/autora/state/#autora.state.StandardState.y","title":"<code>y</code>  <code>property</code>","text":"<p>Returns the observations as a pd.DataFrame Examples:     &gt;&gt;&gt; s_empty = StandardState()     &gt;&gt;&gt; s_empty.y     Empty DataFrame     Columns: []     Index: []     &gt;&gt;&gt; from autora.variable import VariableCollection, Variable     &gt;&gt;&gt; x = Variable('x')     &gt;&gt;&gt; y = Variable('y')     &gt;&gt;&gt; variables = VariableCollection(independent_variables=[x], dependent_variables=[y])     &gt;&gt;&gt; experiment_data = pd.DataFrame({'x': [0, 1, 2, 3], 'y': [0, 2, 4, 6]})     &gt;&gt;&gt; s = StandardState(variables=variables, experiment_data=experiment_data)     &gt;&gt;&gt; s.y        y     0  0     1  2     2  4     3  6</p>"},{"location":"reference/autora/state/#autora.state.State","title":"<code>State</code>  <code>dataclass</code>","text":"<p>Base object for dataclasses which use the Delta mechanism.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt; from typing import List, Optional\n</code></pre> <p>We define a dataclass where each field (which is going to be delta-ed) has additional metadata \"delta\" which describes its delta behaviour.</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class ListState(State):\n...    l: List = field(default_factory=list, metadata={\"delta\": \"extend\"})\n...    m: List = field(default_factory=list, metadata={\"delta\": \"replace\"})\n</code></pre> <p>Now we instantiate the dataclass...</p> <pre><code>&gt;&gt;&gt; l = ListState(l=list(\"abc\"), m=list(\"xyz\"))\n&gt;&gt;&gt; l\nListState(l=['a', 'b', 'c'], m=['x', 'y', 'z'])\n</code></pre> <p>... and can add deltas to it. <code>l</code> will be extended:</p> <pre><code>&gt;&gt;&gt; l + Delta(l=list(\"def\"))\nListState(l=['a', 'b', 'c', 'd', 'e', 'f'], m=['x', 'y', 'z'])\n</code></pre> <p>... wheras <code>m</code> will be replaced:</p> <pre><code>&gt;&gt;&gt; l + Delta(m=list(\"uvw\"))\nListState(l=['a', 'b', 'c'], m=['u', 'v', 'w'])\n</code></pre> <p>... they can be chained:</p> <pre><code>&gt;&gt;&gt; l + Delta(l=list(\"def\")) + Delta(m=list(\"uvw\"))\nListState(l=['a', 'b', 'c', 'd', 'e', 'f'], m=['u', 'v', 'w'])\n</code></pre> <p>... and we update multiple fields with one Delta:</p> <pre><code>&gt;&gt;&gt; l + Delta(l=list(\"ghi\"), m=list(\"rst\"))\nListState(l=['a', 'b', 'c', 'g', 'h', 'i'], m=['r', 's', 't'])\n</code></pre> <p>A non-existent field will be ignored:</p> <pre><code>&gt;&gt;&gt; l + Delta(o=\"not a field\")\nListState(l=['a', 'b', 'c'], m=['x', 'y', 'z'])\n</code></pre> <p>... but will trigger a warning:</p> <pre><code>&gt;&gt;&gt; with warnings.catch_warnings(record=True) as w:\n...     _ = l + Delta(o=\"not a field\")\n...     print(w[0].message)\nThese fields: ['o'] could not be used to update ListState,\nwhich has these fields &amp; aliases: ['l', 'm']\n</code></pre> <p>We can also use the <code>.update</code> method to do the same thing:</p> <pre><code>&gt;&gt;&gt; l.update(l=list(\"ghi\"), m=list(\"rst\"))\nListState(l=['a', 'b', 'c', 'g', 'h', 'i'], m=['r', 's', 't'])\n</code></pre> <p>We can also define fields which <code>append</code> the last result:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class AppendState(State):\n...    n: List = field(default_factory=list, metadata={\"delta\": \"append\"})\n</code></pre> <pre><code>&gt;&gt;&gt; m = AppendState(n=list(\"\u0251\u03b2\u0263\"))\n&gt;&gt;&gt; m\nAppendState(n=['\u0251', '\u03b2', '\u0263'])\n</code></pre> <p><code>n</code> will be appended:</p> <pre><code>&gt;&gt;&gt; m + Delta(n=\"\u2202\")\nAppendState(n=['\u0251', '\u03b2', '\u0263', '\u2202'])\n</code></pre> <p>The metadata key \"converter\" is used to coerce types (inspired by PEP 712):</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class CoerceStateList(State):\n...    o: Optional[List] = field(default=None, metadata={\"delta\": \"replace\"})\n...    p: List = field(default_factory=list, metadata={\"delta\": \"replace\",\n...                                                    \"converter\": list})\n</code></pre> <pre><code>&gt;&gt;&gt; r = CoerceStateList()\n</code></pre> <p>If there is no <code>metadata[\"converter\"]</code> set for a field, no coercion occurs</p> <pre><code>&gt;&gt;&gt; r + Delta(o=\"not a list\")\nCoerceStateList(o='not a list', p=[])\n</code></pre> <p>If there is a <code>metadata[\"converter\"]</code> set for a field, the data are coerced:</p> <pre><code>&gt;&gt;&gt; r + Delta(p=\"not a list\")\nCoerceStateList(o=None, p=['n', 'o', 't', ' ', 'a', ' ', 'l', 'i', 's', 't'])\n</code></pre> <p>If the input data are of the correct type, they are returned unaltered:</p> <pre><code>&gt;&gt;&gt; r + Delta(p=[\"a\", \"list\"])\nCoerceStateList(o=None, p=['a', 'list'])\n</code></pre> <p>With a converter, inputs are converted to the type output by the converter:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class CoerceStateDataFrame(State):\n...    q: pd.DataFrame = field(default_factory=pd.DataFrame,\n...                            metadata={\"delta\": \"replace\",\n...                                      \"converter\": pd.DataFrame})\n</code></pre> <p>If the type is already correct, the object is passed to the converter, but should be returned unchanged:</p> <pre><code>&gt;&gt;&gt; s = CoerceStateDataFrame()\n&gt;&gt;&gt; (s + Delta(q=pd.DataFrame([(\"a\",1,\"alpha\"), (\"b\",2,\"beta\")], columns=list(\"xyz\")))).q\n   x  y      z\n0  a  1  alpha\n1  b  2   beta\n</code></pre> <p>If the type is not correct, the object is converted if possible. For a dataframe, we can convert records:</p> <pre><code>&gt;&gt;&gt; (s + Delta(q=[(\"a\",1,\"alpha\"), (\"b\",2,\"beta\")])).q\n   0  1      2\n0  a  1  alpha\n1  b  2   beta\n</code></pre> <p>... or an array:</p> <pre><code>&gt;&gt;&gt; (s + Delta(q=np.linspace([1, 2], [10, 15], 3))).q\n      0     1\n0   1.0   2.0\n1   5.5   8.5\n2  10.0  15.0\n</code></pre> <p>... or a dictionary:</p> <pre><code>&gt;&gt;&gt; (s + Delta(q={\"a\": [1,2,3], \"b\": [4,5,6]})).q\n   a  b\n0  1  4\n1  2  5\n2  3  6\n</code></pre> <p>... or a list:</p> <pre><code>&gt;&gt;&gt; (s + Delta(q=[11, 12, 13])).q\n    0\n0  11\n1  12\n2  13\n</code></pre> <p>... but not, for instance, a string:</p> <pre><code>&gt;&gt;&gt; (s + Delta(q=\"not compatible with pd.DataFrame\")).q\nTraceback (most recent call last):\n...\nValueError: DataFrame constructor not properly called!\n</code></pre> <p>Without a converter:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class CoerceStateDataFrameNoConverter(State):\n...    r: pd.DataFrame = field(default_factory=pd.DataFrame, metadata={\"delta\": \"replace\"})\n</code></pre> <p>... there is no coercion \u2013 the object is passed unchanged</p> <pre><code>&gt;&gt;&gt; t = CoerceStateDataFrameNoConverter()\n&gt;&gt;&gt; (t + Delta(r=np.linspace([1, 2], [10, 15], 3))).r\narray([[ 1. ,  2. ],\n       [ 5.5,  8.5],\n       [10. , 15. ]])\n</code></pre> <p>A converter can cast from a DataFrame to a np.ndarray (with a single datatype), for instance:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class CoerceStateArray(State):\n...    r: Optional[np.ndarray] = field(default=None,\n...                            metadata={\"delta\": \"replace\",\n...                                      \"converter\": np.asarray})\n</code></pre> <p>Here we pass a dataframe, but expect a numpy array:</p> <pre><code>&gt;&gt;&gt; (CoerceStateArray() + Delta(r=pd.DataFrame([(\"a\",1), (\"b\",2)], columns=list(\"xy\")))).r\narray([['a', 1],\n       ['b', 2]], dtype=object)\n</code></pre> <p>We can define aliases which can transform between different potential field names.</p> Source code in <code>autora/state.py</code> <pre><code>@dataclass(frozen=True)\nclass State:\n    \"\"\"\n    Base object for dataclasses which use the Delta mechanism.\n\n    Examples:\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt; from typing import List, Optional\n\n        We define a dataclass where each field (which is going to be delta-ed) has additional\n        metadata \"delta\" which describes its delta behaviour.\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class ListState(State):\n        ...    l: List = field(default_factory=list, metadata={\"delta\": \"extend\"})\n        ...    m: List = field(default_factory=list, metadata={\"delta\": \"replace\"})\n\n        Now we instantiate the dataclass...\n        &gt;&gt;&gt; l = ListState(l=list(\"abc\"), m=list(\"xyz\"))\n        &gt;&gt;&gt; l\n        ListState(l=['a', 'b', 'c'], m=['x', 'y', 'z'])\n\n        ... and can add deltas to it. `l` will be extended:\n        &gt;&gt;&gt; l + Delta(l=list(\"def\"))\n        ListState(l=['a', 'b', 'c', 'd', 'e', 'f'], m=['x', 'y', 'z'])\n\n        ... wheras `m` will be replaced:\n        &gt;&gt;&gt; l + Delta(m=list(\"uvw\"))\n        ListState(l=['a', 'b', 'c'], m=['u', 'v', 'w'])\n\n        ... they can be chained:\n        &gt;&gt;&gt; l + Delta(l=list(\"def\")) + Delta(m=list(\"uvw\"))\n        ListState(l=['a', 'b', 'c', 'd', 'e', 'f'], m=['u', 'v', 'w'])\n\n        ... and we update multiple fields with one Delta:\n        &gt;&gt;&gt; l + Delta(l=list(\"ghi\"), m=list(\"rst\"))\n        ListState(l=['a', 'b', 'c', 'g', 'h', 'i'], m=['r', 's', 't'])\n\n        A non-existent field will be ignored:\n        &gt;&gt;&gt; l + Delta(o=\"not a field\")\n        ListState(l=['a', 'b', 'c'], m=['x', 'y', 'z'])\n\n        ... but will trigger a warning:\n        &gt;&gt;&gt; with warnings.catch_warnings(record=True) as w:\n        ...     _ = l + Delta(o=\"not a field\")\n        ...     print(w[0].message) # doctest: +NORMALIZE_WHITESPACE\n        These fields: ['o'] could not be used to update ListState,\n        which has these fields &amp; aliases: ['l', 'm']\n\n        We can also use the `.update` method to do the same thing:\n        &gt;&gt;&gt; l.update(l=list(\"ghi\"), m=list(\"rst\"))\n        ListState(l=['a', 'b', 'c', 'g', 'h', 'i'], m=['r', 's', 't'])\n\n        We can also define fields which `append` the last result:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class AppendState(State):\n        ...    n: List = field(default_factory=list, metadata={\"delta\": \"append\"})\n\n        &gt;&gt;&gt; m = AppendState(n=list(\"\u0251\u03b2\u0263\"))\n        &gt;&gt;&gt; m\n        AppendState(n=['\u0251', '\u03b2', '\u0263'])\n\n        `n` will be appended:\n        &gt;&gt;&gt; m + Delta(n=\"\u2202\")\n        AppendState(n=['\u0251', '\u03b2', '\u0263', '\u2202'])\n\n        The metadata key \"converter\" is used to coerce types (inspired by\n        [PEP 712](https://peps.python.org/pep-0712/)):\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class CoerceStateList(State):\n        ...    o: Optional[List] = field(default=None, metadata={\"delta\": \"replace\"})\n        ...    p: List = field(default_factory=list, metadata={\"delta\": \"replace\",\n        ...                                                    \"converter\": list})\n\n        &gt;&gt;&gt; r = CoerceStateList()\n\n        If there is no `metadata[\"converter\"]` set for a field, no coercion occurs\n        &gt;&gt;&gt; r + Delta(o=\"not a list\")\n        CoerceStateList(o='not a list', p=[])\n\n        If there is a `metadata[\"converter\"]` set for a field, the data are coerced:\n        &gt;&gt;&gt; r + Delta(p=\"not a list\")\n        CoerceStateList(o=None, p=['n', 'o', 't', ' ', 'a', ' ', 'l', 'i', 's', 't'])\n\n        If the input data are of the correct type, they are returned unaltered:\n        &gt;&gt;&gt; r + Delta(p=[\"a\", \"list\"])\n        CoerceStateList(o=None, p=['a', 'list'])\n\n        With a converter, inputs are converted to the type output by the converter:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class CoerceStateDataFrame(State):\n        ...    q: pd.DataFrame = field(default_factory=pd.DataFrame,\n        ...                            metadata={\"delta\": \"replace\",\n        ...                                      \"converter\": pd.DataFrame})\n\n        If the type is already correct, the object is passed to the converter,\n        but should be returned unchanged:\n        &gt;&gt;&gt; s = CoerceStateDataFrame()\n        &gt;&gt;&gt; (s + Delta(q=pd.DataFrame([(\"a\",1,\"alpha\"), (\"b\",2,\"beta\")], columns=list(\"xyz\")))).q\n           x  y      z\n        0  a  1  alpha\n        1  b  2   beta\n\n        If the type is not correct, the object is converted if possible. For a dataframe,\n        we can convert records:\n        &gt;&gt;&gt; (s + Delta(q=[(\"a\",1,\"alpha\"), (\"b\",2,\"beta\")])).q\n           0  1      2\n        0  a  1  alpha\n        1  b  2   beta\n\n        ... or an array:\n        &gt;&gt;&gt; (s + Delta(q=np.linspace([1, 2], [10, 15], 3))).q\n              0     1\n        0   1.0   2.0\n        1   5.5   8.5\n        2  10.0  15.0\n\n        ... or a dictionary:\n        &gt;&gt;&gt; (s + Delta(q={\"a\": [1,2,3], \"b\": [4,5,6]})).q\n           a  b\n        0  1  4\n        1  2  5\n        2  3  6\n\n        ... or a list:\n        &gt;&gt;&gt; (s + Delta(q=[11, 12, 13])).q\n            0\n        0  11\n        1  12\n        2  13\n\n        ... but not, for instance, a string:\n        &gt;&gt;&gt; (s + Delta(q=\"not compatible with pd.DataFrame\")).q\n        Traceback (most recent call last):\n        ...\n        ValueError: DataFrame constructor not properly called!\n\n        Without a converter:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class CoerceStateDataFrameNoConverter(State):\n        ...    r: pd.DataFrame = field(default_factory=pd.DataFrame, metadata={\"delta\": \"replace\"})\n\n        ... there is no coercion \u2013 the object is passed unchanged\n        &gt;&gt;&gt; t = CoerceStateDataFrameNoConverter()\n        &gt;&gt;&gt; (t + Delta(r=np.linspace([1, 2], [10, 15], 3))).r\n        array([[ 1. ,  2. ],\n               [ 5.5,  8.5],\n               [10. , 15. ]])\n\n\n        A converter can cast from a DataFrame to a np.ndarray (with a single datatype),\n        for instance:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class CoerceStateArray(State):\n        ...    r: Optional[np.ndarray] = field(default=None,\n        ...                            metadata={\"delta\": \"replace\",\n        ...                                      \"converter\": np.asarray})\n\n        Here we pass a dataframe, but expect a numpy array:\n        &gt;&gt;&gt; (CoerceStateArray() + Delta(r=pd.DataFrame([(\"a\",1), (\"b\",2)], columns=list(\"xy\")))).r\n        array([['a', 1],\n               ['b', 2]], dtype=object)\n\n        We can define aliases which can transform between different potential field\n        names.\n\n    \"\"\"\n\n    def __add__(self, other: Union[Delta, Mapping]):\n        updates = dict()\n        other_fields_unused = list(other.keys())\n        for self_field in fields(self):\n            other_value, key = _get_value(self_field, other)\n            if other_value is None:\n                continue\n            other_fields_unused.remove(key)\n\n            self_field_key = self_field.name\n            self_value = getattr(self, self_field_key)\n            delta_behavior = self_field.metadata[\"delta\"]\n\n            if (constructor := self_field.metadata.get(\"converter\", None)) is not None:\n                coerced_other_value = constructor(other_value)\n            else:\n                coerced_other_value = other_value\n\n            if delta_behavior == \"extend\":\n                extended_value = _extend(self_value, coerced_other_value)\n                updates[self_field_key] = extended_value\n            elif delta_behavior == \"append\":\n                appended_value = _append(self_value, coerced_other_value)\n                updates[self_field_key] = appended_value\n            elif delta_behavior == \"replace\":\n                updates[self_field_key] = coerced_other_value\n            else:\n                raise NotImplementedError(\n                    \"delta_behaviour=`%s` not implemented\" % delta_behavior\n                )\n\n        if len(other_fields_unused) &gt; 0:\n            warnings.warn(\n                \"These fields: %s could not be used to update %s, \"\n                \"which has these fields &amp; aliases: %s\"\n                % (\n                    other_fields_unused,\n                    type(self).__name__,\n                    _get_field_names_and_aliases(self),\n                ),\n            )\n\n        new = replace(self, **updates)\n        return new\n\n    def update(self, **kwargs):\n        \"\"\"\n        Return a new version of the State with values updated.\n\n        This is identical to adding a `Delta`.\n\n        If you need to replace values, ignoring the State value aggregation rules,\n        use `dataclasses.replace` instead.\n        \"\"\"\n        return self + Delta(**kwargs)\n\n    def copy(self):\n        \"\"\"\n        Return a deepcopy of the State\n        Examples:\n            &gt;&gt;&gt; @dataclass(frozen=True)\n            ... class DfState(State):\n            ...    q: pd.DataFrame = field(default_factory=pd.DataFrame,\n            ...                            metadata={\"delta\": \"replace\",\n            ...                                      \"converter\": pd.DataFrame})\n            &gt;&gt;&gt; data = pd.DataFrame({'x': [1, 2, 3]})\n            &gt;&gt;&gt; s_1 = DfState(q=data)\n            &gt;&gt;&gt; s_replace = replace(s_1)\n            &gt;&gt;&gt; s_copy = s_1.copy()\n\n            The build in replace method doesn't create a deepcopy:\n            &gt;&gt;&gt; s_1.q is s_replace.q\n            True\n            &gt;&gt;&gt; s_1.q['y'] = [1,2,3]\n            &gt;&gt;&gt; s_replace.q\n               x  y\n            0  1  1\n            1  2  2\n            2  3  3\n\n            But this copy method does:\n            &gt;&gt;&gt; s_1.q is s_copy.q\n            False\n            &gt;&gt;&gt; s_copy.q\n               x\n            0  1\n            1  2\n            2  3\n\n\n        \"\"\"\n        # Create a dictionary to hold the field copies\n        field_copies = {}\n\n        # Iterate over all fields of the class\n        for _field in fields(self):\n            value = getattr(self, _field.name)\n            # Use deepcopy to ensure that mutable fields are also copied\n            field_copies[_field.name] = copy.deepcopy(value)\n\n        # Use replace with **field_copies to create a new instance of the same class\n        return replace(self, **field_copies)\n</code></pre>"},{"location":"reference/autora/state/#autora.state.State.copy","title":"<code>copy()</code>","text":"<p>Return a deepcopy of the State Examples:     &gt;&gt;&gt; @dataclass(frozen=True)     ... class DfState(State):     ...    q: pd.DataFrame = field(default_factory=pd.DataFrame,     ...                            metadata={\"delta\": \"replace\",     ...                                      \"converter\": pd.DataFrame})     &gt;&gt;&gt; data = pd.DataFrame({'x': [1, 2, 3]})     &gt;&gt;&gt; s_1 = DfState(q=data)     &gt;&gt;&gt; s_replace = replace(s_1)     &gt;&gt;&gt; s_copy = s_1.copy()</p> <pre><code>The build in replace method doesn't create a deepcopy:\n&gt;&gt;&gt; s_1.q is s_replace.q\nTrue\n&gt;&gt;&gt; s_1.q['y'] = [1,2,3]\n&gt;&gt;&gt; s_replace.q\n   x  y\n0  1  1\n1  2  2\n2  3  3\n\nBut this copy method does:\n&gt;&gt;&gt; s_1.q is s_copy.q\nFalse\n&gt;&gt;&gt; s_copy.q\n   x\n0  1\n1  2\n2  3\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Return a deepcopy of the State\n    Examples:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class DfState(State):\n        ...    q: pd.DataFrame = field(default_factory=pd.DataFrame,\n        ...                            metadata={\"delta\": \"replace\",\n        ...                                      \"converter\": pd.DataFrame})\n        &gt;&gt;&gt; data = pd.DataFrame({'x': [1, 2, 3]})\n        &gt;&gt;&gt; s_1 = DfState(q=data)\n        &gt;&gt;&gt; s_replace = replace(s_1)\n        &gt;&gt;&gt; s_copy = s_1.copy()\n\n        The build in replace method doesn't create a deepcopy:\n        &gt;&gt;&gt; s_1.q is s_replace.q\n        True\n        &gt;&gt;&gt; s_1.q['y'] = [1,2,3]\n        &gt;&gt;&gt; s_replace.q\n           x  y\n        0  1  1\n        1  2  2\n        2  3  3\n\n        But this copy method does:\n        &gt;&gt;&gt; s_1.q is s_copy.q\n        False\n        &gt;&gt;&gt; s_copy.q\n           x\n        0  1\n        1  2\n        2  3\n\n\n    \"\"\"\n    # Create a dictionary to hold the field copies\n    field_copies = {}\n\n    # Iterate over all fields of the class\n    for _field in fields(self):\n        value = getattr(self, _field.name)\n        # Use deepcopy to ensure that mutable fields are also copied\n        field_copies[_field.name] = copy.deepcopy(value)\n\n    # Use replace with **field_copies to create a new instance of the same class\n    return replace(self, **field_copies)\n</code></pre>"},{"location":"reference/autora/state/#autora.state.State.update","title":"<code>update(**kwargs)</code>","text":"<p>Return a new version of the State with values updated.</p> <p>This is identical to adding a <code>Delta</code>.</p> <p>If you need to replace values, ignoring the State value aggregation rules, use <code>dataclasses.replace</code> instead.</p> Source code in <code>autora/state.py</code> <pre><code>def update(self, **kwargs):\n    \"\"\"\n    Return a new version of the State with values updated.\n\n    This is identical to adding a `Delta`.\n\n    If you need to replace values, ignoring the State value aggregation rules,\n    use `dataclasses.replace` instead.\n    \"\"\"\n    return self + Delta(**kwargs)\n</code></pre>"},{"location":"reference/autora/state/#autora.state.combined_functions_on_state","title":"<code>combined_functions_on_state(functions, output=None)</code>","text":"<p>Decorator (factory) to make target list of <code>functions</code> into a function on a <code>State</code>. The resulting function uses a state field as input and combines the outputs of the <code>functions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>functions</code> <code>List[Tuple[str, Callable]]</code> <p>the list of functions to be wrapped</p> required <code>output</code> <code>Optional[Sequence[str]]</code> <p>list specifying State field names for the return values of <code>function</code></p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class U(State):\n...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n&gt;&gt;&gt; identity = lambda conditions : conditions\n&gt;&gt;&gt; double_conditions = combined_functions_on_state(\n...     [('id_1', identity), ('id_2', identity)], output=['conditions'])\n&gt;&gt;&gt; s = U([1, 2])\n&gt;&gt;&gt; s_double = double_conditions(s)\n&gt;&gt;&gt; s\nU(conditions=[1, 2])\n&gt;&gt;&gt; s_double\nU(conditions=[1, 2, 1, 2])\n</code></pre> <p>We can also pass parameters to the functions:</p> <pre><code>&gt;&gt;&gt; def multiply(conditions, multiplier):\n...     return [el * multiplier for el in conditions]\n&gt;&gt;&gt; double_and_triple = combined_functions_on_state(\n...     [('doubler', multiply), ('tripler', multiply)], output=['conditions']\n... )\n&gt;&gt;&gt; s = U([1, 2])\n&gt;&gt;&gt; s_double_triple = double_and_triple(\n...     s, params={'doubler': {'multiplier': 2}, 'tripler': {'multiplier': 3}}\n... )\n&gt;&gt;&gt; s_double_triple\nU(conditions=[2, 4, 3, 6])\n</code></pre> <p>If the functions return a Delta object, we don't need to provide an output argument:</p> <pre><code>&gt;&gt;&gt; def decrement(conditions, dec):\n...     return Delta(conditions=[el-dec for el in conditions])\n&gt;&gt;&gt; def increment(conditions, inc):\n...     return Delta(conditions=[el+inc for el in conditions])\n&gt;&gt;&gt; dec_and_inc = combined_functions_on_state(\n...     [('decrement', decrement), ('increment', increment)])\n&gt;&gt;&gt; s_dec_and_inc = dec_and_inc(\n...     s, params={'decrement': {'dec': 10}, 'increment': {'inc': 2}})\n&gt;&gt;&gt; s_dec_and_inc\nU(conditions=[-9, -8, 3, 4])\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def combined_functions_on_state(\n    functions: List[Tuple[str, Callable]], output: Optional[Sequence[str]] = None\n):\n    \"\"\"\n    Decorator (factory) to make target list of `functions` into a function on a `State`.\n    The resulting function uses a state field as input and combines the outputs of the\n    `functions`.\n\n    Args:\n        functions: the list of functions to be wrapped\n        output: list specifying State field names for the return values of `function`\n\n    Examples:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class U(State):\n        ...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n        &gt;&gt;&gt; identity = lambda conditions : conditions\n        &gt;&gt;&gt; double_conditions = combined_functions_on_state(\n        ...     [('id_1', identity), ('id_2', identity)], output=['conditions'])\n        &gt;&gt;&gt; s = U([1, 2])\n        &gt;&gt;&gt; s_double = double_conditions(s)\n        &gt;&gt;&gt; s\n        U(conditions=[1, 2])\n        &gt;&gt;&gt; s_double\n        U(conditions=[1, 2, 1, 2])\n\n        We can also pass parameters to the functions:\n        &gt;&gt;&gt; def multiply(conditions, multiplier):\n        ...     return [el * multiplier for el in conditions]\n        &gt;&gt;&gt; double_and_triple = combined_functions_on_state(\n        ...     [('doubler', multiply), ('tripler', multiply)], output=['conditions']\n        ... )\n        &gt;&gt;&gt; s = U([1, 2])\n        &gt;&gt;&gt; s_double_triple = double_and_triple(\n        ...     s, params={'doubler': {'multiplier': 2}, 'tripler': {'multiplier': 3}}\n        ... )\n        &gt;&gt;&gt; s_double_triple\n        U(conditions=[2, 4, 3, 6])\n\n        If the functions return a Delta object, we don't need to provide an output argument:\n        &gt;&gt;&gt; def decrement(conditions, dec):\n        ...     return Delta(conditions=[el-dec for el in conditions])\n        &gt;&gt;&gt; def increment(conditions, inc):\n        ...     return Delta(conditions=[el+inc for el in conditions])\n        &gt;&gt;&gt; dec_and_inc = combined_functions_on_state(\n        ...     [('decrement', decrement), ('increment', increment)])\n        &gt;&gt;&gt; s_dec_and_inc = dec_and_inc(\n        ...     s, params={'decrement': {'dec': 10}, 'increment': {'inc': 2}})\n        &gt;&gt;&gt; s_dec_and_inc\n        U(conditions=[-9, -8, 3, 4])\n\n    \"\"\"\n\n    def f_(_state: State, params: Optional[Dict] = None):\n        result_delta = None\n        for name, function in functions:\n            _f_input_from_state = inputs_from_state(function)\n            if params is None:\n                _params = {}\n            else:\n                _params = params\n            if name in _params.keys():\n                _delta = _f_input_from_state(_state, **_params[name])\n            else:\n                _delta = _f_input_from_state(_state)\n            result_delta = _extend(result_delta, _delta)\n        return result_delta\n\n    if output:\n        f_ = outputs_to_delta(*output)(f_)\n    f_ = delta_to_state(f_)\n    return f_\n</code></pre>"},{"location":"reference/autora/state/#autora.state.delta_to_state","title":"<code>delta_to_state(f)</code>","text":"<p>Decorator to make <code>f</code> which takes a <code>State</code> and returns a <code>Delta</code> return an updated <code>State</code>.</p> <p>This wrapper handles adding a returned Delta to an input State object.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <p>the function which returns a <code>Delta</code> object</p> required <p>Returns: the function modified to return a State object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from typing import List, Optional\n</code></pre> <p>The <code>State</code> it operates on needs to have the metadata described in the state module:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class U(State):\n...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n</code></pre> <p>We indicate the inputs required by the parameter names. The output must be (compatible with) a <code>Delta</code> object.</p> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def experimentalist(conditions):\n...     new_conditions = [c + 10 for c in conditions]\n...     return Delta(conditions=new_conditions)\n</code></pre> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\nU(conditions=[11, 12, 13, 14])\n</code></pre> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[101,102,103,104]))\nU(conditions=[111, 112, 113, 114])\n</code></pre> <p>If the output of the function is not a <code>Delta</code> object (or something compatible with its interface), then an error is thrown.</p> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def returns_bare_conditions(conditions):\n...     new_conditions = [c + 10 for c in conditions]\n...     return new_conditions\n</code></pre> <pre><code>&gt;&gt;&gt; returns_bare_conditions(U(conditions=[1]))\nTraceback (most recent call last):\n...\nAssertionError: Output of &lt;function returns_bare_conditions at 0x...&gt; must be a `Delta`,\n`UserDict`, or `dict`.\n</code></pre> <p>A dictionary can be returned and used:</p> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def returns_a_dictionary(conditions):\n...     new_conditions = [c + 10 for c in conditions]\n...     return {\"conditions\": new_conditions}\n&gt;&gt;&gt; returns_a_dictionary(U(conditions=[2]))\nU(conditions=[12])\n</code></pre> <p>... as can an object which subclasses UserDict (like <code>Delta</code>)</p> <pre><code>&gt;&gt;&gt; class MyDelta(UserDict):\n...     pass\n&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def returns_a_userdict(conditions):\n...     new_conditions = [c + 10 for c in conditions]\n...     return MyDelta(conditions=new_conditions)\n&gt;&gt;&gt; returns_a_userdict(U(conditions=[3]))\nU(conditions=[13])\n</code></pre> <p>We recommend using the <code>Delta</code> object rather than a <code>UserDict</code> or <code>dict</code> as its functionality may be expanded in future.</p> <pre><code>&gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n&gt;&gt;&gt; from sklearn.base import BaseEstimator\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n...     ivs = [vi.name for vi in variables.independent_variables]\n...     dvs = [vi.name for vi in variables.dependent_variables]\n...     X, y = experiment_data[ivs], experiment_data[dvs]\n...     new_model = LinearRegression(fit_intercept=True).set_params(**kwargs).fit(X, y)\n...     return Delta(model=new_model)\n</code></pre> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class V(State):\n...     variables: VariableCollection  # field(metadata={\"delta\":... }) omitted \u2234 immutable\n...     experiment_data: pd.DataFrame = field(metadata={\"delta\": \"extend\"})\n...     model: Optional[BaseEstimator] = field(metadata={\"delta\": \"replace\"}, default=None)\n</code></pre> <pre><code>&gt;&gt;&gt; v = V(\n...     variables=VariableCollection(independent_variables=[Variable(\"x\")],\n...                                  dependent_variables=[Variable(\"y\")]),\n...     experiment_data=pd.DataFrame({\"x\": [0,1,2,3,4], \"y\": [2,3,4,5,6]})\n... )\n&gt;&gt;&gt; v_prime = theorist(v)\n&gt;&gt;&gt; v_prime.model.coef_, v_prime.model.intercept_\n(array([[1.]]), array([2.]))\n</code></pre> <p>Arguments from the state can be overridden by passing them in as keyword arguments (kwargs):</p> <pre><code>&gt;&gt;&gt; theorist(v, experiment_data=pd.DataFrame({\"x\": [0,1,2,3], \"y\": [12,13,14,15]}))\\\n...     .model.intercept_\narray([12.])\n</code></pre> <p>... and other arguments supported by the inner function can also be passed (if and only if the inner function allows for and handles <code>**kwargs</code> arguments alongside the values from the state).</p> <pre><code>&gt;&gt;&gt; theorist(v, fit_intercept=False).model.intercept_\n0.0\n</code></pre> <p>Any parameters not provided by the state must be provided by default values or by the caller. If the default is specified:</p> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def experimentalist(conditions, offset=25):\n...     new_conditions = [c + offset for c in conditions]\n...     return Delta(conditions=new_conditions)\n</code></pre> <p>... then it need not be passed.</p> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\nU(conditions=[26, 27, 28, 29])\n</code></pre> <p>If a default isn't specified:</p> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def experimentalist(conditions, offset):\n...     new_conditions = [c + offset for c in conditions]\n...     return Delta(conditions=new_conditions)\n</code></pre> <p>... then calling the experimentalist without it will throw an error:</p> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\nTraceback (most recent call last):\n...\nTypeError: experimentalist() missing 1 required positional argument: 'offset'\n</code></pre> <p>... which can be fixed by passing the argument as a keyword to the wrapped function.</p> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]), offset=2)\nU(conditions=[3, 4, 5, 6])\n</code></pre> <p>The state itself is passed through if the inner function requests the <code>state</code>:</p> <pre><code>&gt;&gt;&gt; @delta_to_state\n... @inputs_from_state\n... def function_which_needs_whole_state(state, conditions):\n...     print(\"Doing something on: \", state)\n...     new_conditions = [c + 2 for c in conditions]\n...     return Delta(conditions=new_conditions)\n&gt;&gt;&gt; function_which_needs_whole_state(U(conditions=[1,2,3,4]))\nDoing something on:  U(conditions=[1, 2, 3, 4])\nU(conditions=[3, 4, 5, 6])\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def delta_to_state(f):\n    \"\"\"Decorator to make `f` which takes a `State` and returns a `Delta` return an updated `State`.\n\n    This wrapper handles adding a returned Delta to an input State object.\n\n    Args:\n        f: the function which returns a `Delta` object\n\n    Returns: the function modified to return a State object\n\n    Examples:\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from typing import List, Optional\n\n        The `State` it operates on needs to have the metadata described in the state module:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class U(State):\n        ...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n\n        We indicate the inputs required by the parameter names.\n        The output must be (compatible with) a `Delta` object.\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def experimentalist(conditions):\n        ...     new_conditions = [c + 10 for c in conditions]\n        ...     return Delta(conditions=new_conditions)\n\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n        U(conditions=[11, 12, 13, 14])\n\n        &gt;&gt;&gt; experimentalist(U(conditions=[101,102,103,104]))\n        U(conditions=[111, 112, 113, 114])\n\n        If the output of the function is not a `Delta` object (or something compatible with its\n        interface), then an error is thrown.\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def returns_bare_conditions(conditions):\n        ...     new_conditions = [c + 10 for c in conditions]\n        ...     return new_conditions\n\n        &gt;&gt;&gt; returns_bare_conditions(U(conditions=[1])) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Traceback (most recent call last):\n        ...\n        AssertionError: Output of &lt;function returns_bare_conditions at 0x...&gt; must be a `Delta`,\n        `UserDict`, or `dict`.\n\n        A dictionary can be returned and used:\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def returns_a_dictionary(conditions):\n        ...     new_conditions = [c + 10 for c in conditions]\n        ...     return {\"conditions\": new_conditions}\n        &gt;&gt;&gt; returns_a_dictionary(U(conditions=[2]))\n        U(conditions=[12])\n\n        ... as can an object which subclasses UserDict (like `Delta`)\n        &gt;&gt;&gt; class MyDelta(UserDict):\n        ...     pass\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def returns_a_userdict(conditions):\n        ...     new_conditions = [c + 10 for c in conditions]\n        ...     return MyDelta(conditions=new_conditions)\n        &gt;&gt;&gt; returns_a_userdict(U(conditions=[3]))\n        U(conditions=[13])\n\n        We recommend using the `Delta` object rather than a `UserDict` or `dict` as its\n        functionality may be expanded in future.\n\n        &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n        &gt;&gt;&gt; from sklearn.base import BaseEstimator\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n        ...     ivs = [vi.name for vi in variables.independent_variables]\n        ...     dvs = [vi.name for vi in variables.dependent_variables]\n        ...     X, y = experiment_data[ivs], experiment_data[dvs]\n        ...     new_model = LinearRegression(fit_intercept=True).set_params(**kwargs).fit(X, y)\n        ...     return Delta(model=new_model)\n\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class V(State):\n        ...     variables: VariableCollection  # field(metadata={\"delta\":... }) omitted \u2234 immutable\n        ...     experiment_data: pd.DataFrame = field(metadata={\"delta\": \"extend\"})\n        ...     model: Optional[BaseEstimator] = field(metadata={\"delta\": \"replace\"}, default=None)\n\n        &gt;&gt;&gt; v = V(\n        ...     variables=VariableCollection(independent_variables=[Variable(\"x\")],\n        ...                                  dependent_variables=[Variable(\"y\")]),\n        ...     experiment_data=pd.DataFrame({\"x\": [0,1,2,3,4], \"y\": [2,3,4,5,6]})\n        ... )\n        &gt;&gt;&gt; v_prime = theorist(v)\n        &gt;&gt;&gt; v_prime.model.coef_, v_prime.model.intercept_\n        (array([[1.]]), array([2.]))\n\n        Arguments from the state can be overridden by passing them in as keyword arguments (kwargs):\n        &gt;&gt;&gt; theorist(v, experiment_data=pd.DataFrame({\"x\": [0,1,2,3], \"y\": [12,13,14,15]}))\\\\\n        ...     .model.intercept_\n        array([12.])\n\n        ... and other arguments supported by the inner function can also be passed\n        (if and only if the inner function allows for and handles `**kwargs` arguments alongside\n        the values from the state).\n        &gt;&gt;&gt; theorist(v, fit_intercept=False).model.intercept_\n        0.0\n\n        Any parameters not provided by the state must be provided by default values or by the\n        caller. If the default is specified:\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def experimentalist(conditions, offset=25):\n        ...     new_conditions = [c + offset for c in conditions]\n        ...     return Delta(conditions=new_conditions)\n\n        ... then it need not be passed.\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n        U(conditions=[26, 27, 28, 29])\n\n        If a default isn't specified:\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def experimentalist(conditions, offset):\n        ...     new_conditions = [c + offset for c in conditions]\n        ...     return Delta(conditions=new_conditions)\n\n        ... then calling the experimentalist without it will throw an error:\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n        Traceback (most recent call last):\n        ...\n        TypeError: experimentalist() missing 1 required positional argument: 'offset'\n\n        ... which can be fixed by passing the argument as a keyword to the wrapped function.\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]), offset=2)\n        U(conditions=[3, 4, 5, 6])\n\n        The state itself is passed through if the inner function requests the `state`:\n        &gt;&gt;&gt; @delta_to_state\n        ... @inputs_from_state\n        ... def function_which_needs_whole_state(state, conditions):\n        ...     print(\"Doing something on: \", state)\n        ...     new_conditions = [c + 2 for c in conditions]\n        ...     return Delta(conditions=new_conditions)\n        &gt;&gt;&gt; function_which_needs_whole_state(U(conditions=[1,2,3,4]))\n        Doing something on:  U(conditions=[1, 2, 3, 4])\n        U(conditions=[3, 4, 5, 6])\n\n    \"\"\"\n\n    @wraps(f)\n    def _f(state_: S, **kwargs) -&gt; S:\n        delta = f(state_, **kwargs)\n        assert isinstance(delta, Mapping), (\n            \"Output of %s must be a `Delta`, `UserDict`, \" \"or `dict`.\" % f\n        )\n        new_state = state_ + delta\n        return new_state\n\n    return _f\n</code></pre>"},{"location":"reference/autora/state/#autora.state.estimator_on_state","title":"<code>estimator_on_state(estimator)</code>","text":"<p>Convert a scikit-learn compatible estimator into a function on a <code>State</code> object.</p> <p>Supports passing additional <code>**kwargs</code> which are used to update the estimator's params before fitting.</p> <p>Examples:</p> <p>Initialize a function which operates on the state, <code>state_fn</code> and runs a LinearRegression.</p> <pre><code>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; state_fn = estimator_on_state(LinearRegression())\n</code></pre> <p>Define the state on which to operate (here an instance of the <code>StandardState</code>):</p> <pre><code>&gt;&gt;&gt; from autora.state import StandardState\n&gt;&gt;&gt; from autora.variable import Variable, VariableCollection\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; s = StandardState(\n...     variables=VariableCollection(\n...         independent_variables=[Variable(\"x\")],\n...         dependent_variables=[Variable(\"y\")]),\n...     experiment_data=pd.DataFrame({\"x\": [1,2,3], \"y\":[3,6,9]})\n... )\n</code></pre> <p>Run the function, which fits the model and adds the result to the <code>StandardState</code> as the last entry in the .models list.</p> <pre><code>&gt;&gt;&gt; state_fn(s).models[-1].coef_\narray([[3.]])\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def estimator_on_state(estimator: BaseEstimator) -&gt; StateFunction:\n    \"\"\"\n    Convert a scikit-learn compatible estimator into a function on a `State` object.\n\n    Supports passing additional `**kwargs` which are used to update the estimator's params\n    before fitting.\n\n    Examples:\n        Initialize a function which operates on the state, `state_fn` and runs a LinearRegression.\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n        &gt;&gt;&gt; state_fn = estimator_on_state(LinearRegression())\n\n        Define the state on which to operate (here an instance of the `StandardState`):\n        &gt;&gt;&gt; from autora.state import StandardState\n        &gt;&gt;&gt; from autora.variable import Variable, VariableCollection\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; s = StandardState(\n        ...     variables=VariableCollection(\n        ...         independent_variables=[Variable(\"x\")],\n        ...         dependent_variables=[Variable(\"y\")]),\n        ...     experiment_data=pd.DataFrame({\"x\": [1,2,3], \"y\":[3,6,9]})\n        ... )\n\n        Run the function, which fits the model and adds the result to the `StandardState` as the\n        last entry in the .models list.\n        &gt;&gt;&gt; state_fn(s).models[-1].coef_\n        array([[3.]])\n\n    \"\"\"\n\n    @on_state()\n    def theorist(\n        experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs\n    ):\n        ivs = [v.name for v in variables.independent_variables]\n        dvs = [v.name for v in variables.dependent_variables]\n        X, y = experiment_data[ivs], experiment_data[dvs]\n        new_model = estimator.set_params(**kwargs).fit(X, y)\n        return Delta(models=[new_model])\n\n    return theorist\n</code></pre>"},{"location":"reference/autora/state/#autora.state.experiment_runner_on_state","title":"<code>experiment_runner_on_state(f)</code>","text":"<p>Wrapper for experiment_runner of the form \\(f(x)  arrow (x,y)\\), where <code>f</code> returns both \\(x\\) and \\(y\\) values in a complete dataframe.</p> <p>Examples:</p> <p>The conditions are some x-values in a StandardState object:</p> <pre><code>&gt;&gt;&gt; from autora.state import StandardState\n&gt;&gt;&gt; s = StandardState(conditions=pd.DataFrame({\"x\": [1, 2, 3]}))\n</code></pre> <p>The function can be defined on a DataFrame, allowing the explicit inclusion of metadata like column names.</p> <pre><code>&gt;&gt;&gt; def x_to_xy_fn(c: pd.DataFrame) -&gt; pd.Series:\n...     result = c.assign(y=lambda df: 2 * df.x + 1)\n...     return result\n</code></pre> <p>We apply the wrapped function to <code>s</code> and look at the returned experiment_data:</p> <pre><code>&gt;&gt;&gt; experiment_runner_on_state(x_to_xy_fn)(s).experiment_data\n   x  y\n0  1  3\n1  2  5\n2  3  7\n</code></pre> <p>We can also define functions of several variables:</p> <pre><code>&gt;&gt;&gt; def xs_to_xy_fn(c: pd.DataFrame) -&gt; pd.Series:\n...     result = c.assign(y=c.x0 + c.x1)\n...     return result\n</code></pre> <p>With the relevant variables as conditions:</p> <pre><code>&gt;&gt;&gt; t = StandardState(conditions=pd.DataFrame({\"x0\": [1, 2, 3], \"x1\": [10, 20, 30]}))\n&gt;&gt;&gt; experiment_runner_on_state(xs_to_xy_fn)(t).experiment_data\n   x0  x1   y\n0   1  10  11\n1   2  20  22\n2   3  30  33\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def experiment_runner_on_state(f: Callable[[X], XY]) -&gt; StateFunction:\n    \"\"\"Wrapper for experiment_runner of the form $f(x) \\rarrow (x,y)$, where `f`\n    returns both $x$ and $y$ values in a complete dataframe.\n\n    Examples:\n        The conditions are some x-values in a StandardState object:\n        &gt;&gt;&gt; from autora.state import StandardState\n        &gt;&gt;&gt; s = StandardState(conditions=pd.DataFrame({\"x\": [1, 2, 3]}))\n\n        The function can be defined on a DataFrame, allowing the explicit inclusion of\n        metadata like column names.\n        &gt;&gt;&gt; def x_to_xy_fn(c: pd.DataFrame) -&gt; pd.Series:\n        ...     result = c.assign(y=lambda df: 2 * df.x + 1)\n        ...     return result\n\n        We apply the wrapped function to `s` and look at the returned experiment_data:\n        &gt;&gt;&gt; experiment_runner_on_state(x_to_xy_fn)(s).experiment_data\n           x  y\n        0  1  3\n        1  2  5\n        2  3  7\n\n        We can also define functions of several variables:\n        &gt;&gt;&gt; def xs_to_xy_fn(c: pd.DataFrame) -&gt; pd.Series:\n        ...     result = c.assign(y=c.x0 + c.x1)\n        ...     return result\n\n        With the relevant variables as conditions:\n        &gt;&gt;&gt; t = StandardState(conditions=pd.DataFrame({\"x0\": [1, 2, 3], \"x1\": [10, 20, 30]}))\n        &gt;&gt;&gt; experiment_runner_on_state(xs_to_xy_fn)(t).experiment_data\n           x0  x1   y\n        0   1  10  11\n        1   2  20  22\n        2   3  30  33\n\n    \"\"\"\n\n    @on_state()\n    def experiment_runner(conditions: pd.DataFrame, **kwargs):\n        x = conditions\n        experiment_data = f(x, **kwargs)\n        return Delta(experiment_data=experiment_data)\n\n    return experiment_runner\n</code></pre>"},{"location":"reference/autora/state/#autora.state.inputs_from_state","title":"<code>inputs_from_state(f, input_mapping={})</code>","text":"<p>Decorator to make target <code>f</code> into a function on a <code>State</code> and <code>**kwargs</code>.</p> <p>This wrapper makes it easier to pass arguments to a function from a State.</p> <p>It was inspired by the pytest \"fixtures\" mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <p>a function with arguments that could be fields on a <code>State</code> and that returns a <code>Delta</code>.</p> required <code>input_mapping</code> <code>Dict</code> <p>a dict that maps the input arguments of the function to the state fields</p> <code>{}</code> <p>Returns: a version of <code>f</code> which takes and returns <code>State</code> objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from typing import List, Optional\n</code></pre> <p>The <code>State</code> it operates on needs to have the metadata described in the state module:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class U(State):\n...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n</code></pre> <p>We indicate the inputs required by the parameter names. The output must be (compatible with) a <code>Delta</code> object.</p> <pre><code>&gt;&gt;&gt; @inputs_from_state\n... def experimentalist(conditions):\n...     new_conditions = [c + 10 for c in conditions]\n...     return new_conditions\n</code></pre> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n[11, 12, 13, 14]\n</code></pre> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[101,102,103,104]))\n[111, 112, 113, 114]\n</code></pre> <p>If our function uses a different keyword argument than the state field, we can use the input mapping:</p> <pre><code>&gt;&gt;&gt; def experimentalist_(X):\n...     new_conditions = [x + 10 for x in X]\n...     return new_conditions\n&gt;&gt;&gt; experimentalist_on_state = inputs_from_state(experimentalist_, {'X': 'conditions'})\n&gt;&gt;&gt; experimentalist_on_state(U(conditions=[1,2,3,4]))\n[11, 12, 13, 14]\n</code></pre> <p>Both also work with the <code>State</code> as UserDict. Here, we use the StandardState</p> <pre><code>&gt;&gt;&gt; experimentalist(StandardState(conditions=[1, 2, 3, 4]))\n[11, 12, 13, 14]\n</code></pre> <pre><code>&gt;&gt;&gt; experimentalist_on_state(StandardState(conditions=[1, 2, 3, 4]))\n[11, 12, 13, 14]\n</code></pre> <p>A dictionary can be returned and used:</p> <pre><code>&gt;&gt;&gt; @inputs_from_state\n... def returns_a_dictionary(conditions):\n...     new_conditions = [c + 10 for c in conditions]\n...     return {\"conditions\": new_conditions}\n&gt;&gt;&gt; returns_a_dictionary(U(conditions=[2]))\n{'conditions': [12]}\n</code></pre> <pre><code>&gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n&gt;&gt;&gt; from sklearn.base import BaseEstimator\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code>&gt;&gt;&gt; @inputs_from_state\n... def theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n...     ivs = [vi.name for vi in variables.independent_variables]\n...     dvs = [vi.name for vi in variables.dependent_variables]\n...     X, y = experiment_data[ivs], experiment_data[dvs]\n...     model = LinearRegression(fit_intercept=True).set_params(**kwargs).fit(X, y)\n...     return model\n</code></pre> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class V(State):\n...     variables: VariableCollection  # field(metadata={\"delta\":... }) omitted \u2234 immutable\n...     experiment_data: pd.DataFrame = field(metadata={\"delta\": \"extend\"})\n...     model: Optional[BaseEstimator] = field(metadata={\"delta\": \"replace\"}, default=None)\n</code></pre> <pre><code>&gt;&gt;&gt; v = V(\n...     variables=VariableCollection(independent_variables=[Variable(\"x\")],\n...                                  dependent_variables=[Variable(\"y\")]),\n...     experiment_data=pd.DataFrame({\"x\": [0,1,2,3,4], \"y\": [2,3,4,5,6]})\n... )\n&gt;&gt;&gt; model = theorist(v)\n&gt;&gt;&gt; model.coef_, model.intercept_\n(array([[1.]]), array([2.]))\n</code></pre> <p>Arguments from the state can be overridden by passing them in as keyword arguments (kwargs):</p> <pre><code>&gt;&gt;&gt; theorist(v, experiment_data=pd.DataFrame({\"x\": [0,1,2,3], \"y\": [12,13,14,15]}))\\\n...     .intercept_\narray([12.])\n</code></pre> <p>... and other arguments supported by the inner function can also be passed (if and only if the inner function allows for and handles <code>**kwargs</code> arguments alongside the values from the state).</p> <pre><code>&gt;&gt;&gt; theorist(v, fit_intercept=False).intercept_\n0.0\n</code></pre> <p>Any parameters not provided by the state must be provided by default values or by the caller. If the default is specified:</p> <pre><code>&gt;&gt;&gt; @inputs_from_state\n... def experimentalist(conditions, offset=25):\n...     new_conditions = [c + offset for c in conditions]\n...     return new_conditions\n</code></pre> <p>... then it need not be passed.</p> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n[26, 27, 28, 29]\n</code></pre> <p>If a default isn't specified:</p> <pre><code>&gt;&gt;&gt; @inputs_from_state\n... def experimentalist(conditions, offset):\n...     new_conditions = [c + offset for c in conditions]\n...     return new_conditions\n</code></pre> <p>... then calling the experimentalist without it will throw an error:</p> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\nTraceback (most recent call last):\n...\nTypeError: experimentalist() missing 1 required positional argument: 'offset'\n</code></pre> <p>... which can be fixed by passing the argument as a keyword to the wrapped function.</p> <pre><code>&gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]), offset=2)\n[3, 4, 5, 6]\n</code></pre> <p>The same is true, if we don't provide a mapping for arguments:</p> <pre><code>&gt;&gt;&gt; def experimentalist_(X, offset):\n...     new_conditions = [x + offset for x in X]\n...     return new_conditions\n&gt;&gt;&gt; experimentalist_on_state = inputs_from_state(experimentalist_, {'X': 'conditions'})\n&gt;&gt;&gt; experimentalist_on_state(StandardState(conditions=[1,2,3,4]), offset=2)\n[3, 4, 5, 6]\n</code></pre> <p>The state itself is passed through if the inner function requests the <code>state</code>:</p> <pre><code>&gt;&gt;&gt; @inputs_from_state\n... def function_which_needs_whole_state(state, conditions):\n...     print(\"Doing something on: \", state)\n...     new_conditions = [c + 2 for c in conditions]\n...     return new_conditions\n&gt;&gt;&gt; function_which_needs_whole_state(U(conditions=[1,2,3,4]))\nDoing something on:  U(conditions=[1, 2, 3, 4])\n[3, 4, 5, 6]\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def inputs_from_state(f, input_mapping: Dict = {}):\n    \"\"\"Decorator to make target `f` into a function on a `State` and `**kwargs`.\n\n    This wrapper makes it easier to pass arguments to a function from a State.\n\n    It was inspired by the pytest \"fixtures\" mechanism.\n\n    Args:\n        f: a function with arguments that could be fields on a `State`\n            and that returns a `Delta`.\n        input_mapping: a dict that maps the input arguments of the function to the state fields\n\n    Returns: a version of `f` which takes and returns `State` objects.\n\n    Examples:\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from typing import List, Optional\n\n        The `State` it operates on needs to have the metadata described in the state module:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class U(State):\n        ...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n\n        We indicate the inputs required by the parameter names.\n        The output must be (compatible with) a `Delta` object.\n        &gt;&gt;&gt; @inputs_from_state\n        ... def experimentalist(conditions):\n        ...     new_conditions = [c + 10 for c in conditions]\n        ...     return new_conditions\n\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n        [11, 12, 13, 14]\n\n        &gt;&gt;&gt; experimentalist(U(conditions=[101,102,103,104]))\n        [111, 112, 113, 114]\n\n        If our function uses a different keyword argument than the state field, we can use\n        the input mapping:\n        &gt;&gt;&gt; def experimentalist_(X):\n        ...     new_conditions = [x + 10 for x in X]\n        ...     return new_conditions\n        &gt;&gt;&gt; experimentalist_on_state = inputs_from_state(experimentalist_, {'X': 'conditions'})\n        &gt;&gt;&gt; experimentalist_on_state(U(conditions=[1,2,3,4]))\n        [11, 12, 13, 14]\n\n        Both also work with the `State` as UserDict. Here, we use the StandardState\n        &gt;&gt;&gt; experimentalist(StandardState(conditions=[1, 2, 3, 4]))\n        [11, 12, 13, 14]\n\n        &gt;&gt;&gt; experimentalist_on_state(StandardState(conditions=[1, 2, 3, 4]))\n        [11, 12, 13, 14]\n\n        A dictionary can be returned and used:\n        &gt;&gt;&gt; @inputs_from_state\n        ... def returns_a_dictionary(conditions):\n        ...     new_conditions = [c + 10 for c in conditions]\n        ...     return {\"conditions\": new_conditions}\n        &gt;&gt;&gt; returns_a_dictionary(U(conditions=[2]))\n        {'conditions': [12]}\n\n        &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n        &gt;&gt;&gt; from sklearn.base import BaseEstimator\n        &gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n\n        &gt;&gt;&gt; @inputs_from_state\n        ... def theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n        ...     ivs = [vi.name for vi in variables.independent_variables]\n        ...     dvs = [vi.name for vi in variables.dependent_variables]\n        ...     X, y = experiment_data[ivs], experiment_data[dvs]\n        ...     model = LinearRegression(fit_intercept=True).set_params(**kwargs).fit(X, y)\n        ...     return model\n\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class V(State):\n        ...     variables: VariableCollection  # field(metadata={\"delta\":... }) omitted \u2234 immutable\n        ...     experiment_data: pd.DataFrame = field(metadata={\"delta\": \"extend\"})\n        ...     model: Optional[BaseEstimator] = field(metadata={\"delta\": \"replace\"}, default=None)\n\n        &gt;&gt;&gt; v = V(\n        ...     variables=VariableCollection(independent_variables=[Variable(\"x\")],\n        ...                                  dependent_variables=[Variable(\"y\")]),\n        ...     experiment_data=pd.DataFrame({\"x\": [0,1,2,3,4], \"y\": [2,3,4,5,6]})\n        ... )\n        &gt;&gt;&gt; model = theorist(v)\n        &gt;&gt;&gt; model.coef_, model.intercept_\n        (array([[1.]]), array([2.]))\n\n        Arguments from the state can be overridden by passing them in as keyword arguments (kwargs):\n        &gt;&gt;&gt; theorist(v, experiment_data=pd.DataFrame({\"x\": [0,1,2,3], \"y\": [12,13,14,15]}))\\\\\n        ...     .intercept_\n        array([12.])\n\n        ... and other arguments supported by the inner function can also be passed\n        (if and only if the inner function allows for and handles `**kwargs` arguments alongside\n        the values from the state).\n        &gt;&gt;&gt; theorist(v, fit_intercept=False).intercept_\n        0.0\n\n        Any parameters not provided by the state must be provided by default values or by the\n        caller. If the default is specified:\n        &gt;&gt;&gt; @inputs_from_state\n        ... def experimentalist(conditions, offset=25):\n        ...     new_conditions = [c + offset for c in conditions]\n        ...     return new_conditions\n\n        ... then it need not be passed.\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n        [26, 27, 28, 29]\n\n        If a default isn't specified:\n        &gt;&gt;&gt; @inputs_from_state\n        ... def experimentalist(conditions, offset):\n        ...     new_conditions = [c + offset for c in conditions]\n        ...     return new_conditions\n\n        ... then calling the experimentalist without it will throw an error:\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]))\n        Traceback (most recent call last):\n        ...\n        TypeError: experimentalist() missing 1 required positional argument: 'offset'\n\n        ... which can be fixed by passing the argument as a keyword to the wrapped function.\n        &gt;&gt;&gt; experimentalist(U(conditions=[1,2,3,4]), offset=2)\n        [3, 4, 5, 6]\n\n        The same is true, if we don't provide a mapping for arguments:\n        &gt;&gt;&gt; def experimentalist_(X, offset):\n        ...     new_conditions = [x + offset for x in X]\n        ...     return new_conditions\n        &gt;&gt;&gt; experimentalist_on_state = inputs_from_state(experimentalist_, {'X': 'conditions'})\n        &gt;&gt;&gt; experimentalist_on_state(StandardState(conditions=[1,2,3,4]), offset=2)\n        [3, 4, 5, 6]\n\n        The state itself is passed through if the inner function requests the `state`:\n        &gt;&gt;&gt; @inputs_from_state\n        ... def function_which_needs_whole_state(state, conditions):\n        ...     print(\"Doing something on: \", state)\n        ...     new_conditions = [c + 2 for c in conditions]\n        ...     return new_conditions\n        &gt;&gt;&gt; function_which_needs_whole_state(U(conditions=[1,2,3,4]))\n        Doing something on:  U(conditions=[1, 2, 3, 4])\n        [3, 4, 5, 6]\n\n    \"\"\"\n    # Get the set of parameter names from function f's signature\n\n    reversed_mapping = {v: k for k, v in input_mapping.items()}\n\n    parameters_ = set(inspect.signature(f).parameters.keys())\n    missing_func_params = set(input_mapping.keys()).difference(parameters_)\n    if missing_func_params:\n        raise ValueError(\n            f\"The following keys in input_state_mapping are not parameters of the function: \"\n            f\"{missing_func_params}\"\n        )\n\n    @wraps(f)\n    def _f(state_: State, /, **kwargs) -&gt; State:\n        # Get the parameters needed which are available from the state_.\n        # All others must be provided as kwargs or default values on f.\n        assert is_dataclass(state_) or isinstance(state_, UserDict)\n        if is_dataclass(state_):\n            from_state = parameters_.intersection(\n                _get_field_names_and_properties(state_)\n            )\n            arguments_from_state = {k: getattr(state_, k) for k in from_state}\n            from_state_input_mapping = {\n                reversed_mapping.get(field_name, field_name): getattr(\n                    state_, field_name\n                )\n                for field_name in _get_field_names_and_properties(state_)\n                if reversed_mapping.get(field_name, field_name) in parameters_\n            }\n            arguments_from_state.update(from_state_input_mapping)\n        elif isinstance(state_, UserDict):\n            from_state = parameters_.intersection(set(state_.keys()))\n            arguments_from_state = {k: state_[k] for k in from_state}\n            from_state_input_mapping = {\n                reversed_mapping.get(key, key): state_[key]\n                for key in state_.keys()\n                if reversed_mapping.get(key, key) in parameters_\n            }\n            arguments_from_state.update(from_state_input_mapping)\n        if \"state\" in parameters_:\n            arguments_from_state[\"state\"] = state_\n        arguments = dict(arguments_from_state, **kwargs)\n        result = f(**arguments)\n        return result\n\n    return _f\n</code></pre>"},{"location":"reference/autora/state/#autora.state.on_state","title":"<code>on_state(function=None, input_mapping={}, output=None)</code>","text":"<p>Decorator (factory) to make target <code>function</code> into a function on a <code>State</code> and <code>**kwargs</code>.</p> <p>This combines the functionality of <code>outputs_to_delta</code> and <code>inputs_from_state</code></p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Optional[Callable]</code> <p>the function to be wrapped</p> <code>None</code> <code>output</code> <code>Optional[Sequence[str]]</code> <p>list specifying State field names for the return values of <code>function</code></p> <code>None</code> <code>input_mapping</code> <code>Dict</code> <p>a dict that maps the keywords of the functions to the state fields</p> <code>{}</code> <p>Returns:</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from typing import List, Optional\n</code></pre> <p>The <code>State</code> it operates on needs to have the metadata described in the state module:</p> <pre><code>&gt;&gt;&gt; @dataclass(frozen=True)\n... class W(State):\n...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n</code></pre> <p>We indicate the inputs required by the parameter names.</p> <pre><code>&gt;&gt;&gt; def add_ten(conditions):\n...     return [c + 10 for c in conditions]\n&gt;&gt;&gt; experimentalist = on_state(function=add_ten, output=[\"conditions\"])\n</code></pre> <pre><code>&gt;&gt;&gt; experimentalist(W(conditions=[1,2,3,4]))\nW(conditions=[11, 12, 13, 14])\n</code></pre> <p>You can wrap functions which return a Delta object natively, by omitting the <code>output</code> argument:</p> <pre><code>&gt;&gt;&gt; @on_state()\n... def add_five(conditions):\n...     return Delta(conditions=[c + 5 for c in conditions])\n</code></pre> <pre><code>&gt;&gt;&gt; add_five(W(conditions=[1, 2, 3, 4]))\nW(conditions=[6, 7, 8, 9])\n</code></pre> <p>If you fail to declare outputs for a function which doesn't return a Delta:</p> <pre><code>&gt;&gt;&gt; @on_state()\n... def missing_output_param(conditions):\n...     return [c + 5 for c in conditions]\n</code></pre> <p>... an exception is raised:</p> <pre><code>&gt;&gt;&gt; missing_output_param(W(conditions=[1]))\nTraceback (most recent call last):\n...\nAssertionError: Output of &lt;function missing_output_param at 0x...&gt; must be a `Delta`,\n`UserDict`, or `dict`.\n</code></pre> <p>You can use the @on_state(output=[...]) as a decorator:</p> <pre><code>&gt;&gt;&gt; @on_state(output=[\"conditions\"])\n... def add_six(conditions):\n...     return [c + 6 for c in conditions]\n</code></pre> <pre><code>&gt;&gt;&gt; add_six(W(conditions=[1, 2, 3, 4]))\nW(conditions=[7, 8, 9, 10])\n</code></pre> <p>You can also declare an input-to-output mapping if the keyword arguments of the functions don't match the state fields:</p> <pre><code>&gt;&gt;&gt; @on_state(input_mapping={'X': 'conditions'}, output=[\"conditions\"])\n... def add_six(X):\n...     return [x + 6 for x in X]\n</code></pre> <pre><code>&gt;&gt;&gt; add_six(W(conditions=[1, 2, 3, 4]))\nW(conditions=[7, 8, 9, 10])\n</code></pre> <p>This also works on the StandardState or other States that are defined as UserDicts:</p> <pre><code>&gt;&gt;&gt; add_six(StandardState(conditions=[1, 2, 3,4])).conditions\n    0\n0   7\n1   8\n2   9\n3  10\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def on_state(\n    function: Optional[Callable] = None,\n    input_mapping: Dict = {},\n    output: Optional[Sequence[str]] = None,\n):\n    \"\"\"Decorator (factory) to make target `function` into a function on a `State` and `**kwargs`.\n\n    This combines the functionality of `outputs_to_delta` and `inputs_from_state`\n\n    Args:\n        function: the function to be wrapped\n        output: list specifying State field names for the return values of `function`\n        input_mapping: a dict that maps the keywords of the functions to the state fields\n\n    Returns:\n\n    Examples:\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from typing import List, Optional\n\n        The `State` it operates on needs to have the metadata described in the state module:\n        &gt;&gt;&gt; @dataclass(frozen=True)\n        ... class W(State):\n        ...     conditions: List[int] = field(metadata={\"delta\": \"replace\"})\n\n        We indicate the inputs required by the parameter names.\n        &gt;&gt;&gt; def add_ten(conditions):\n        ...     return [c + 10 for c in conditions]\n        &gt;&gt;&gt; experimentalist = on_state(function=add_ten, output=[\"conditions\"])\n\n        &gt;&gt;&gt; experimentalist(W(conditions=[1,2,3,4]))\n        W(conditions=[11, 12, 13, 14])\n\n        You can wrap functions which return a Delta object natively, by omitting the `output`\n        argument:\n        &gt;&gt;&gt; @on_state()\n        ... def add_five(conditions):\n        ...     return Delta(conditions=[c + 5 for c in conditions])\n\n        &gt;&gt;&gt; add_five(W(conditions=[1, 2, 3, 4]))\n        W(conditions=[6, 7, 8, 9])\n\n        If you fail to declare outputs for a function which doesn't return a Delta:\n        &gt;&gt;&gt; @on_state()\n        ... def missing_output_param(conditions):\n        ...     return [c + 5 for c in conditions]\n\n        ... an exception is raised:\n        &gt;&gt;&gt; missing_output_param(W(conditions=[1])) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Traceback (most recent call last):\n        ...\n        AssertionError: Output of &lt;function missing_output_param at 0x...&gt; must be a `Delta`,\n        `UserDict`, or `dict`.\n\n        You can use the @on_state(output=[...]) as a decorator:\n        &gt;&gt;&gt; @on_state(output=[\"conditions\"])\n        ... def add_six(conditions):\n        ...     return [c + 6 for c in conditions]\n\n        &gt;&gt;&gt; add_six(W(conditions=[1, 2, 3, 4]))\n        W(conditions=[7, 8, 9, 10])\n\n        You can also declare an input-to-output mapping if the keyword arguments of the functions\n        don't match the state fields:\n        &gt;&gt;&gt; @on_state(input_mapping={'X': 'conditions'}, output=[\"conditions\"])\n        ... def add_six(X):\n        ...     return [x + 6 for x in X]\n\n        &gt;&gt;&gt; add_six(W(conditions=[1, 2, 3, 4]))\n        W(conditions=[7, 8, 9, 10])\n\n        This also works on the StandardState or other States that are defined as UserDicts:\n        &gt;&gt;&gt; add_six(StandardState(conditions=[1, 2, 3,4])).conditions\n            0\n        0   7\n        1   8\n        2   9\n        3  10\n    \"\"\"\n\n    def decorator(f):\n        f_ = f\n        if output is not None:\n            f_ = outputs_to_delta(*output)(f_)\n        f_ = inputs_from_state(f_, input_mapping)\n        f_ = delta_to_state(f_)\n        return f_\n\n    if function is None:\n        return decorator\n    else:\n        return decorator(function)\n</code></pre>"},{"location":"reference/autora/state/#autora.state.outputs_to_delta","title":"<code>outputs_to_delta(*output)</code>","text":"<p>Decorator factory to wrap outputs from a function as Deltas.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @outputs_to_delta(\"conditions\")\n... def add_five(x):\n...     return [xi + 5 for xi in x]\n</code></pre> <pre><code>&gt;&gt;&gt; add_five([1, 2, 3])\n{'conditions': [6, 7, 8]}\n</code></pre> <pre><code>&gt;&gt;&gt; @outputs_to_delta(\"c\")\n... def add_six(conditions):\n...     return [c + 5 for c in conditions]\n</code></pre> <pre><code>&gt;&gt;&gt; add_six([1, 2, 3])\n{'c': [6, 7, 8]}\n</code></pre> <pre><code>&gt;&gt;&gt; @outputs_to_delta(\"+1\", \"-1\")\n... def plus_minus_1(x):\n...     a = [xi + 1 for xi in x]\n...     b = [xi - 1 for xi in x]\n...     return a, b\n</code></pre> <pre><code>&gt;&gt;&gt; plus_minus_1([1, 2, 3])\n{'+1': [2, 3, 4], '-1': [0, 1, 2]}\n</code></pre> <p>If the wrong number of values are specified for the return, then there might be errors. If multiple outputs are expected, but only a single output is returned, we get a warning:</p> <pre><code>&gt;&gt;&gt; @outputs_to_delta(\"1\", \"2\")\n... def returns_single_result_when_more_expected():\n...     return \"a\"\n&gt;&gt;&gt; returns_single_result_when_more_expected()\nTraceback (most recent call last):\n...\nAssertionError: function `&lt;function returns_single_result_when_more_expected at 0x...&gt;`\nhas to return multiple values to match `('1', '2')`. Got `a` instead.\n</code></pre> <p>If multiple outputs are expected, but the wrong number are returned, we get a warning:</p> <pre><code>&gt;&gt;&gt; @outputs_to_delta(\"1\", \"2\", \"3\")\n... def returns_wrong_number_of_results():\n...     return \"a\", \"b\"\n&gt;&gt;&gt; returns_wrong_number_of_results()\nTraceback (most recent call last):\n...\nAssertionError: function `&lt;function returns_wrong_number_of_results at 0x...&gt;`\nhas to return exactly `3` values to match `('1', '2', '3')`. Got `('a', 'b')` instead.\n</code></pre> <p>However, if a single output is expected, and multiple are returned, these are treated as a single object and no error occurs:</p> <pre><code>&gt;&gt;&gt; @outputs_to_delta(\"foo\")\n... def returns_a_tuple():\n...     return \"a\", \"b\", \"c\"\n&gt;&gt;&gt; returns_a_tuple()\n{'foo': ('a', 'b', 'c')}\n</code></pre> <p>If we fail to specify output names, an error is returned immediately.</p> <pre><code>&gt;&gt;&gt; @outputs_to_delta()\n... def decorator_missing_arguments():\n...     return \"a\", \"b\", \"c\"\nTraceback (most recent call last):\n...\nValueError: `output` names must be specified.\n</code></pre> Source code in <code>autora/state.py</code> <pre><code>def outputs_to_delta(*output: str):\n    \"\"\"\n    Decorator factory to wrap outputs from a function as Deltas.\n\n    Examples:\n        &gt;&gt;&gt; @outputs_to_delta(\"conditions\")\n        ... def add_five(x):\n        ...     return [xi + 5 for xi in x]\n\n        &gt;&gt;&gt; add_five([1, 2, 3])\n        {'conditions': [6, 7, 8]}\n\n        &gt;&gt;&gt; @outputs_to_delta(\"c\")\n        ... def add_six(conditions):\n        ...     return [c + 5 for c in conditions]\n\n        &gt;&gt;&gt; add_six([1, 2, 3])\n        {'c': [6, 7, 8]}\n\n        &gt;&gt;&gt; @outputs_to_delta(\"+1\", \"-1\")\n        ... def plus_minus_1(x):\n        ...     a = [xi + 1 for xi in x]\n        ...     b = [xi - 1 for xi in x]\n        ...     return a, b\n\n        &gt;&gt;&gt; plus_minus_1([1, 2, 3])\n        {'+1': [2, 3, 4], '-1': [0, 1, 2]}\n\n\n        If the wrong number of values are specified for the return, then there might be errors.\n        If multiple outputs are expected, but only a single output is returned, we get a warning:\n        &gt;&gt;&gt; @outputs_to_delta(\"1\", \"2\")\n        ... def returns_single_result_when_more_expected():\n        ...     return \"a\"\n        &gt;&gt;&gt; returns_single_result_when_more_expected()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        AssertionError: function `&lt;function returns_single_result_when_more_expected at 0x...&gt;`\n        has to return multiple values to match `('1', '2')`. Got `a` instead.\n\n        If multiple outputs are expected, but the wrong number are returned, we get a warning:\n        &gt;&gt;&gt; @outputs_to_delta(\"1\", \"2\", \"3\")\n        ... def returns_wrong_number_of_results():\n        ...     return \"a\", \"b\"\n        &gt;&gt;&gt; returns_wrong_number_of_results()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        AssertionError: function `&lt;function returns_wrong_number_of_results at 0x...&gt;`\n        has to return exactly `3` values to match `('1', '2', '3')`. Got `('a', 'b')` instead.\n\n        However, if a single output is expected, and multiple are returned, these are treated as\n        a single object and no error occurs:\n        &gt;&gt;&gt; @outputs_to_delta(\"foo\")\n        ... def returns_a_tuple():\n        ...     return \"a\", \"b\", \"c\"\n        &gt;&gt;&gt; returns_a_tuple()\n        {'foo': ('a', 'b', 'c')}\n\n        If we fail to specify output names, an error is returned immediately.\n        &gt;&gt;&gt; @outputs_to_delta()\n        ... def decorator_missing_arguments():\n        ...     return \"a\", \"b\", \"c\"\n        Traceback (most recent call last):\n        ...\n        ValueError: `output` names must be specified.\n\n    \"\"\"\n\n    def decorator(f):\n        if len(output) == 0:\n            raise ValueError(\"`output` names must be specified.\")\n\n        elif len(output) == 1:\n\n            @wraps(f)\n            def inner(*args, **kwargs):\n                result = f(*args, **kwargs)\n                delta = Delta(**{output[0]: result})\n                return delta\n\n        else:\n\n            @wraps(f)\n            def inner(*args, **kwargs):\n                result = f(*args, **kwargs)\n                assert isinstance(result, tuple), (\n                    \"function `%s` has to return multiple values \"\n                    \"to match `%s`. Got `%s` instead.\" % (f, output, result)\n                )\n                assert len(output) == len(result), (\n                    \"function `%s` has to return \"\n                    \"exactly `%s` values \"\n                    \"to match `%s`. \"\n                    \"Got `%s` instead.\"\n                    \"\" % (f, len(output), output, result)\n                )\n                delta = Delta(**dict(zip(output, result)))\n                return delta\n\n        return inner\n\n    return decorator\n</code></pre>"},{"location":"reference/autora/experiment_runner/firebase_prolific/","title":"autora.experiment_runner.firebase_prolific","text":""},{"location":"reference/autora/experiment_runner/firebase_prolific/#autora.experiment_runner.firebase_prolific.firebase_prolific_runner","title":"<code>firebase_prolific_runner(**kwargs)</code>","text":"<p>A runner that uses firebase to store the condition and the dependent variable and prolific to recruit participants. Args:     **kwargs: the configuration of the experiment.         firebase_credentials: a dict with firebase service account credentials         sleep_time: the time between checks to the firebase database and updates of the prolific experiment         study_name: a name for the study showing up in prolific         study_description: a description for the study showing up in prolific         study_url: the url to your experiment         study_completion_time: the average completion time for a participant to complete the study         prolific_token: api token from prolific Returns:     the runner</p> Source code in <code>temp_dir/experiment-runner-firebase-prolific/src/autora/experiment_runner/firebase_prolific/__init__.py</code> <pre><code>def firebase_prolific_runner(**kwargs):\n    \"\"\"\n    A runner that uses firebase to store the condition and the dependent variable and prolific to\n    recruit participants.\n    Args:\n        **kwargs: the configuration of the experiment.\n            firebase_credentials: a dict with firebase service account credentials\n            sleep_time: the time between checks to the firebase database and updates of the prolific experiment\n            study_name: a name for the study showing up in prolific\n            study_description: a description for the study showing up in prolific\n            study_url: the url to your experiment\n            study_completion_time: the average completion time for a participant to complete the study\n            prolific_token: api token from prolific\n    Returns:\n        the runner\n    \"\"\"\n\n    def runner(x):\n        return _firebase_prolific_run(x, **kwargs)\n\n    return runner\n</code></pre>"},{"location":"reference/autora/experiment_runner/firebase_prolific/#autora.experiment_runner.firebase_prolific.firebase_runner","title":"<code>firebase_runner(**kwargs)</code>","text":"<p>A runner that uses firebase to store the condition and the dependent variable. Args:     **kwargs: the configuration of the experiment         firebase_credentials: a dict with firebase service account credentials         time_out: time out to reset a condition that was started but not finished         sleep_time: the time between checks and updates of the firebase database</p> <p>Returns:</p> Type Description <p>the runner</p> Source code in <code>temp_dir/experiment-runner-firebase-prolific/src/autora/experiment_runner/firebase_prolific/__init__.py</code> <pre><code>def firebase_runner(**kwargs):\n    \"\"\"\n    A runner that uses firebase to store the condition and the dependent variable.\n    Args:\n        **kwargs: the configuration of the experiment\n            firebase_credentials: a dict with firebase service account credentials\n            time_out: time out to reset a condition that was started but not finished\n            sleep_time: the time between checks and updates of the firebase database\n\n    Returns:\n        the runner\n    \"\"\"\n\n    def runner(x):\n        return _firebase_run(x, **kwargs)\n\n    return runner\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/utilities/","title":"autora.experiment_runner.synthetic.utilities","text":"<p>Module for registering and retrieving synthetic models from an inventory.</p> <p>Examples:</p> <p>To add and recover a new model from the inventory, we need to define it using a factory function. We start by importing the modules we'll need:</p> <pre><code>&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from autora.experiment_runner.synthetic.utilities import (register, retrieve, describe,\n...     SyntheticExperimentCollection)\n&gt;&gt;&gt; from autora.variable import IV, DV, VariableCollection\n</code></pre> <p>Then we can define the function. We define all the arguments we want and add them to a dictionary. The factory_function \u2013 in this case <code>sinusoid_experiment</code> \u2013 is the scope for all the parameters we need.</p> <pre><code>&gt;&gt;&gt; def sinusoid_experiment(omega=np.pi/3, delta=np.pi/2., m=0.3, resolution=1000,\n...                         rng=np.random.default_rng()):\n...     \"\"\"Shifted sinusoid experiment, combining a sinusoid and a gradient drift.\n...     Ground truth: y = sin((x - delta) * omega) + (x * m)\n...     Parameters:\n...         omega: angular speed in radians\n...         delta: offset in radians\n...         m: drift gradient in [radians ^ -1]\n...         resolution: number of x values\n...     \"\"\"\n...\n...     name = \"Shifted Sinusoid\"\n...\n...     params = dict(omega=omega, delta=delta, resolution=resolution, m=m, rng=rng)\n...\n...     x = IV(name=\"x\", value_range=(-6 * np.pi, 6 * np.pi))\n...     y = DV(name=\"y\", value_range=(-1, 1))\n...     variables = VariableCollection(independent_variables=[x], dependent_variables=[y])\n...\n...     def domain():\n...         return np.linspace(*x.value_range, resolution).reshape(-1, 1)\n...\n...     def run(X, std=0.1):\n...         return np.sin((X - delta) * omega) + (X * m) + rng.normal(0, std, X.shape)\n...\n...     def ground_truth(X):\n...         return run(X, std=0.)\n...\n...     def plotter(model=None):\n...         plt.plot(domain(), ground_truth(domain()), label=\"Ground Truth\")\n...         if model is not None:\n...             plt.plot(domain(), model.predict(domain()), label=\"Model\")\n...         plt.title(name)\n...\n...     collection = SyntheticExperimentCollection(\n...         name=name,\n...         description=sinusoid_experiment.__doc__,\n...         params=params,\n...         variables=variables,\n...         domain=domain,\n...         run=run,\n...         ground_truth=ground_truth,\n...         plotter=plotter,\n...         factory_function=sinusoid_experiment,\n...     )\n...\n...     return collection\n</code></pre> <p>Then we can register the experiment. We register the function, rather than evaluating it.</p> <pre><code>&gt;&gt;&gt; register(\"sinusoid_experiment\", sinusoid_experiment)\n</code></pre> <p>When we want to retrieve the experiment, we can just use the default values if we like:</p> <pre><code>&gt;&gt;&gt; s = retrieve(\"sinusoid_experiment\")\n</code></pre> <p>We can retrieve the docstring of the model using the <code>describe</code> function</p> <pre><code>&gt;&gt;&gt; print(describe(s))\nShifted sinusoid experiment, combining a sinusoid and a gradient drift.\n    Ground truth: y = sin((x - delta) * omega) + (x * m)\n    ...\n</code></pre> <p>... or using its id:</p> <pre><code>&gt;&gt;&gt; print(describe(\"sinusoid_experiment\"))\nShifted sinusoid experiment, combining a sinusoid and a gradient drift.\n    Ground truth: y = sin((x - delta) * omega) + (x * m)\n    ...\n</code></pre> <p>... or we can look at the factory function directly:</p> <pre><code>&gt;&gt;&gt; print(describe(sinusoid_experiment))\nShifted sinusoid experiment, combining a sinusoid and a gradient drift.\n    Ground truth: y = sin((x - delta) * omega) + (x * m)\n    ...\n</code></pre> <p>The object returned includes all the used parameters as a dictionary</p> <pre><code>&gt;&gt;&gt; s.params\n{'omega': 1.0..., 'delta': 1.5..., 'resolution': 1000, 'm': 0.3, ...}\n</code></pre> <p>If we need to modify the parameter values, we can pass them as arguments to the retrieve function:</p> <pre><code>&gt;&gt;&gt; t = retrieve(\"sinusoid_experiment\",delta=0.2)\n&gt;&gt;&gt; t.params\n{..., 'delta': 0.2, ...}\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/utilities/#autora.experiment_runner.synthetic.utilities.Inventory","title":"<code>Inventory = dict()</code>  <code>module-attribute</code>","text":"<p>The dictionary of <code>SyntheticExperimentCollection</code>.</p>"},{"location":"reference/autora/experiment_runner/synthetic/utilities/#autora.experiment_runner.synthetic.utilities.SyntheticExperimentCollection","title":"<code>SyntheticExperimentCollection</code>  <code>dataclass</code>","text":"<p>Represents a synthetic experiment.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <p>the name of the theory</p> <code>params</code> <code>Optional[Dict]</code> <p>a dictionary with the settable parameters of the model and their respective values</p> <code>variables</code> <code>Optional[VariableCollection]</code> <p>a VariableCollection describing the variables of the model</p> <code>domain</code> <code>Optional[Callable]</code> <p>a function which returns all the available X values for the model</p> <code>run</code> <code>Optional[Callable]</code> <p>a function which takes X values and returns simulated y values with statistical noise</p> <code>ground_truth</code> <code>Optional[Callable]</code> <p>a function which takes X values and returns simulated y values without any statistical noise</p> <code>plotter</code> <code>Optional[Callable[[Optional[_SupportsPredict]], None]]</code> <p>a function which plots the ground truth and, optionally, a model with a <code>predict</code> method (e.g. scikit-learn estimators)</p> Source code in <code>autora/experiment_runner/synthetic/utilities.py</code> <pre><code>@dataclass(frozen=True)\nclass SyntheticExperimentCollection:\n    \"\"\"\n    Represents a synthetic experiment.\n\n    Attributes:\n        name: the name of the theory\n        params: a dictionary with the settable parameters of the model and their respective values\n        variables: a VariableCollection describing the variables of the model\n        domain: a function which returns all the available X values for the model\n        run: a function which takes X values and returns simulated y values **with\n            statistical noise**\n        ground_truth: a function which takes X values and returns simulated y values **without any\n            statistical noise**\n        plotter: a function which plots the ground truth and, optionally, a model with a\n            `predict` method (e.g. scikit-learn estimators)\n    \"\"\"\n\n    name: Optional[str] = None\n    description: Optional[str] = None\n    params: Optional[Dict] = None\n    variables: Optional[VariableCollection] = None\n    domain: Optional[Callable] = None\n    run: Optional[Callable] = None\n    ground_truth: Optional[Callable] = None\n    plotter: Optional[Callable[[Optional[_SupportsPredict]], None]] = None\n    factory_function: Optional[_SyntheticExperimentFactory] = None\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/utilities/#autora.experiment_runner.synthetic.utilities.describe","title":"<code>describe(arg)</code>","text":"<p>Print the docstring for a synthetic experiment.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <p>the experiment's ID, an object returned from the <code>retrieve</code> function, or a factory_function which creates a new experiment.</p> required Source code in <code>autora/experiment_runner/synthetic/utilities.py</code> <pre><code>@singledispatch\ndef describe(arg):\n    \"\"\"\n    Print the docstring for a synthetic experiment.\n\n    Args:\n        arg: the experiment's ID, an object returned from the `retrieve` function,\n            or a factory_function which creates a new experiment.\n    \"\"\"\n    raise NotImplementedError(f\"{arg=} not yet supported\")\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/utilities/#autora.experiment_runner.synthetic.utilities.register","title":"<code>register(id_, factory_function)</code>","text":"<p>Add a new synthetic experiment to the Inventory.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>str</code> <p>the unique id for the model.</p> required <code>factory_function</code> <code>_SyntheticExperimentFactory</code> <p>a function which returns a SyntheticExperimentCollection</p> required Source code in <code>autora/experiment_runner/synthetic/utilities.py</code> <pre><code>def register(id_: str, factory_function: _SyntheticExperimentFactory) -&gt; None:\n    \"\"\"\n    Add a new synthetic experiment to the Inventory.\n\n    Parameters:\n         id_: the unique id for the model.\n         factory_function: a function which returns a SyntheticExperimentCollection\n\n    \"\"\"\n    Inventory[id_] = factory_function\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/utilities/#autora.experiment_runner.synthetic.utilities.retrieve","title":"<code>retrieve(id_, **kwargs)</code>","text":"<p>Retrieve a synthetic experiment from the Inventory.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>str</code> <p>the unique id for the model</p> required <code>**kwargs</code> <p>keyword arguments for the synthetic experiment (variables, coefficients etc.)</p> <code>{}</code> <p>Returns:     the synthetic experiment</p> Source code in <code>autora/experiment_runner/synthetic/utilities.py</code> <pre><code>def retrieve(id_: str, **kwargs) -&gt; SyntheticExperimentCollection:\n    \"\"\"\n    Retrieve a synthetic experiment from the Inventory.\n\n    Parameters:\n        id_: the unique id for the model\n        **kwargs: keyword arguments for the synthetic experiment (variables, coefficients etc.)\n    Returns:\n        the synthetic experiment\n    \"\"\"\n    result = Inventory[id_](**kwargs)\n    return result\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/abstract/lmm/","title":"autora.experiment_runner.synthetic.abstract.lmm","text":"<p>A synthetic experiment that runs a linear mixed model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from autora.experiment_runner.synthetic.abstract.lmm import (\n...     lmm_experiment\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; formula = 'rt ~ 1'\n&gt;&gt;&gt; fixed_effects = {'Intercept': 1.5}\n&gt;&gt;&gt; experiment = lmm_experiment(formula=formula,fixed_effects=fixed_effects)\n&gt;&gt;&gt; conditions = pd.DataFrame({\n...     'x1':np.linspace(0, 1, 5)\n... })\n&gt;&gt;&gt; experiment.ground_truth(conditions=conditions)\n     x1   rt\n0  0.00  1.5\n1  0.25  1.5\n2  0.50  1.5\n3  0.75  1.5\n4  1.00  1.5\n</code></pre> <pre><code>&gt;&gt;&gt; formula = 'rt ~ 1 + x1'\n&gt;&gt;&gt; fixed_effects = {'Intercept': 1., 'x1': 2.}\n&gt;&gt;&gt; experiment = lmm_experiment(formula=formula,fixed_effects=fixed_effects)\n&gt;&gt;&gt; experiment.ground_truth(conditions=conditions)\n     x1   rt\n0  0.00  1.0\n1  0.25  1.5\n2  0.50  2.0\n3  0.75  2.5\n4  1.00  3.0\n</code></pre> <pre><code>&gt;&gt;&gt; formula_1 = 'rt ~ 1 + x1'\n&gt;&gt;&gt; fixed_effects_1 = {'Intercept': 0., 'x1': 2.}\n&gt;&gt;&gt; experiment_1 = lmm_experiment(formula=formula_1,fixed_effects=fixed_effects_1)\n&gt;&gt;&gt; formula_2 = 'rt ~ x1'\n&gt;&gt;&gt; fixed_effects_2 = {'x1': 2.}\n&gt;&gt;&gt; experiment_2 = lmm_experiment(formula=formula_2,fixed_effects=fixed_effects_2)\n&gt;&gt;&gt; experiment_1.ground_truth(conditions=conditions) ==experiment_2.ground_truth(conditions=conditions)\n     x1    rt\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n</code></pre> <pre><code>&gt;&gt;&gt; formula = 'rt ~ 1 + (1|subject) + x1'\n&gt;&gt;&gt; fixed_effects = {'Intercept': 1, 'x1': 2}\n&gt;&gt;&gt; random_effects = {'subject': {'Intercept': .1}}\n&gt;&gt;&gt; experiment = lmm_experiment(formula=formula,\n...                             fixed_effects=fixed_effects,\n...                             random_effects=random_effects)\n&gt;&gt;&gt; conditions_1 = pd.DataFrame({\n...     'x1':np.linspace(0, 1, 3),\n...     'subject': np.repeat(1, 3)\n... })\n&gt;&gt;&gt; conditions_2 = pd.DataFrame({\n...     'x1':np.linspace(0, 1, 3),\n...     'subject': np.repeat(2, 3)\n... })\n&gt;&gt;&gt; conditions = pd.concat([conditions_1, conditions_2])\n&gt;&gt;&gt; conditions\n    x1  subject\n0  0.0        1\n1  0.5        1\n2  1.0        1\n0  0.0        2\n1  0.5        2\n2  1.0        2\n&gt;&gt;&gt; experiment.ground_truth(conditions=conditions,random_state=42)\n    x1  subject        rt\n0  0.0        1  1.030472\n1  0.5        1  2.030472\n2  1.0        1  3.030472\n0  0.0        2  0.896002\n1  0.5        2  1.896002\n2  1.0        2  2.896002\n</code></pre> <pre><code>&gt;&gt;&gt; formula = 'rt ~ (x1|subject)'\n&gt;&gt;&gt; random_effects = {'subject': {'x1': .1}}\n&gt;&gt;&gt; experiment = lmm_experiment(formula=formula,random_effects=random_effects)\n&gt;&gt;&gt; experiment.ground_truth(conditions=conditions,random_state=42)\n    x1  subject        rt\n0  0.0        1  0.000000\n1  0.5        1  0.015236\n2  1.0        1  0.030472\n0  0.0        2  0.000000\n1  0.5        2 -0.051999\n2  1.0        2 -0.103998\n</code></pre> <pre><code>&gt;&gt;&gt; formula = 'rt ~ (x1|subject) + x1'\n&gt;&gt;&gt; fixed_effects = {'x1': 1.}\n&gt;&gt;&gt; random_effects = {'subject': {'x1': .01}}\n&gt;&gt;&gt; experiment = lmm_experiment(formula=formula,\n...                             fixed_effects=fixed_effects,\n...                             random_effects=random_effects)\n&gt;&gt;&gt; experiment.ground_truth(conditions=conditions,random_state=42)\n    x1  subject        rt\n0  0.0        1  0.000000\n1  0.5        1  0.501524\n2  1.0        1  1.003047\n0  0.0        2  0.000000\n1  0.5        2  0.494800\n2  1.0        2  0.989600\n</code></pre> <pre><code>&gt;&gt;&gt; formula = 'y ~ x1 + x2 + (1 + x1|subject) + (x2|group)'\n&gt;&gt;&gt; fixed_effects = {'Intercept': 1.5, 'x1': 2.0, 'x2': -1.2}\n&gt;&gt;&gt; random_effects = {\n...        'subject': {'1': 0.5, 'x1': 0.3},\n...        'group': {'x2': 0.4}\n...    }\n&gt;&gt;&gt; experiment = lmm_experiment(formula=formula,\n...                             fixed_effects=fixed_effects,\n...                             random_effects=random_effects)\n&gt;&gt;&gt; n_samples = 10\n&gt;&gt;&gt; rng = np.random.default_rng(0)\n&gt;&gt;&gt; conditions = pd.DataFrame({\n...        'x1': rng.normal(0, 1, n_samples),\n...        'x2': rng.normal(0, 1, n_samples),\n...        'subject': rng.choice(['A', 'B', 'C', 'D'], n_samples),\n...        'group': rng.choice(['E', 'F', 'G', 'H'], n_samples)\n...    })\n&gt;&gt;&gt; experiment.ground_truth(conditions=conditions, random_state=42)\n         x1        x2 subject group         y\n0  0.125730 -0.623274       B     H  2.502995\n1 -0.132105  0.041326       A     F  1.258294\n2  0.640423 -2.325031       A     F  5.490146\n3  0.104900 -0.218792       A     H  1.899763\n4 -0.535669 -1.245911       A     H  2.173576\n5  0.361595 -0.732267       C     H  2.923207\n6  1.304000 -0.544259       C     F  4.320545\n7  0.947081 -0.316300       C     G  3.405867\n8 -0.703735  0.411631       B     H -0.578950\n9 -1.265421  1.042513       C     G -1.794523\n</code></pre> <pre><code>&gt;&gt;&gt; experiment.run(conditions=conditions, added_noise=.1, random_state=42)\n         x1        x2 subject group         y\n0  0.125730 -0.623274       B     H  2.417691\n1 -0.132105  0.041326       A     F  1.346234\n2  0.640423 -2.325031       A     F  5.567925\n3  0.104900 -0.218792       A     H  1.906366\n4 -0.535669 -1.245911       A     H  2.286300\n5  0.361595 -0.732267       C     H  2.969958\n6  1.304000 -0.544259       C     F  4.234616\n7  0.947081 -0.316300       C     G  3.442742\n8 -0.703735  0.411631       B     H -0.674839\n9 -1.265421  1.042513       C     G -1.706678\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/abstract/lmm/#autora.experiment_runner.synthetic.abstract.lmm.lmm_experiment","title":"<code>lmm_experiment(formula, fixed_effects=None, random_effects=None, X=None, random_state=None, name='Linear Mixed Model Experiment')</code>","text":"<p>A linear mixed model synthetic experiments.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the experiment</p> <code>'Linear Mixed Model Experiment'</code> <code>formula</code> <code>str</code> <p>formula of the linear mixed model (similar to lmer package in R)</p> required <code>fixed_effects</code> <code>Optional[dict]</code> <p>dictionary describing the fixed effects (Intercept and slopes)</p> <code>None</code> <code>random_effects</code> <code>Optional[dict]</code> <p>nested dictionary describing the random effects of slopes and intercept. These are standard deviasions in a normal distribution with a mean of zero.</p> <code>None</code> <code>X</code> <code>Optional[Sequence[IV]]</code> <p>Independent variable descriptions. Used to add allowed values</p> <code>None</code> Source code in <code>autora/experiment_runner/synthetic/abstract/lmm.py</code> <pre><code>def lmm_experiment(\n    # Add any configurable parameters with their defaults here:\n    formula: str,\n    fixed_effects: Optional[dict] = None,\n    random_effects: Optional[dict] = None,\n    X: Optional[Sequence[IV]] = None,\n    random_state: Optional[int] = None,\n    name: str = \"Linear Mixed Model Experiment\",\n):\n    \"\"\"\n    A linear mixed model synthetic experiments.\n\n    Parameters:\n        name: name of the experiment\n        formula: formula of the linear mixed model (similar to lmer package in R)\n        fixed_effects: dictionary describing the fixed effects (Intercept and slopes)\n        random_effects: nested dictionary describing the random effects of slopes and intercept.\n            These are standard deviasions in a normal distribution with a mean of zero.\n        X: Independent variable descriptions. Used to add allowed values\n    \"\"\"\n\n    if not fixed_effects:\n        fixed_effects = {}\n    if not random_effects:\n        random_effects = {}\n\n    params = dict(\n        # Include all parameters here:\n        name=name,\n        formula=formula,\n        fixed_effects=fixed_effects,\n        random_effects=random_effects,\n    )\n\n    dependent, fixed_variables, random_variables = _extract_variable_names(formula)\n\n    dependent = DV(name=dependent)\n    if not X:\n        independent = [IV(name=iv) for iv in fixed_variables + random_variables]\n    else:\n        if set([x.name for x in X]) != set(fixed_variables + random_variables):\n            raise Exception(\n                \"Variable names in formula don't match given variable names\"\n            )\n        independent = X\n\n    variables = VariableCollection(\n        independent_variables=independent,\n        dependent_variables=[dependent],\n    )\n\n    rng = np.random.default_rng(random_state)\n\n    # Define experiment runner\n    def run(\n        conditions: pd.DataFrame,\n        added_noise=0.01,\n        random_state=None,\n    ):\n        \"\"\"A function which simulates noisy observations.\"\"\"\n        if random_state is not None:\n            rng_ = np.random.default_rng(random_state)\n        else:\n            rng_ = rng  # use the RNG from the outer scope\n\n        dependent_var, rhs = formula.split(\"~\")\n        dependent_var = dependent_var.strip()\n        fixed_vars = fixed_variables\n\n        # Check for the presence of an intercept in the formula\n        has_intercept = (\n            True if \"1\" in fixed_effects or re.search(r\"\\b0\\b\", rhs) is None else False\n        )\n\n        if not isinstance(conditions, pd.DataFrame):\n            _conditions = np.array(conditions)\n            _conditions = pd.DataFrame(_conditions)\n            _conditions.columns = [iv.name for iv in variables.independent_variables]\n        else:\n            _conditions = conditions\n        experiment_data = _conditions.copy()\n\n        # Initialize the dependent variable\n        experiment_data[dependent_var] = (\n            fixed_effects.get(\"Intercept\", 0) if has_intercept else 0\n        )\n\n        # Add fixed effects\n        for var in fixed_vars:\n            if var in experiment_data.columns:\n                experiment_data[dependent_var] += (\n                    fixed_effects.get(var, 0) * experiment_data[var]\n                )\n\n        # Process each random effect term\n        random_effect_terms = re.findall(r\"\\((.+?)\\|(.+?)\\)\", formula)\n        for term in random_effect_terms:\n            random_effects_, group_var = term\n            group_var = group_var.strip()\n\n            # Ensure the group_var is in the data\n            if group_var not in experiment_data.columns:\n                raise ValueError(f\"Group variable '{group_var}' not found in the data\")\n\n            # Process each part of the random effect (intercept and slopes)\n            for part in random_effects_.split(\"+\"):\n                part = \"Intercept\" if part == \"1\" else part\n                part = part.strip()\n                std_dev = random_effects[group_var].get(part, 0.5)\n                random_effect_values = {\n                    group: rng_.normal(0, std_dev)\n                    for group in experiment_data[group_var].unique()\n                }\n                if part == \"Intercept\":  # Random intercept\n                    if has_intercept:\n                        experiment_data[dependent_var] += experiment_data[\n                            group_var\n                        ].map(random_effect_values)\n                else:  # Random slopes\n                    if part in experiment_data.columns:\n                        experiment_data[dependent_var] += (\n                            experiment_data[group_var].map(random_effect_values)\n                            * experiment_data[part]\n                        )\n\n        # Add noise\n        experiment_data[dependent_var] += rng_.normal(\n            0, added_noise, len(experiment_data)\n        )\n\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n    \"\"\"A function which simulates perfect observations.\n    This still uses random values for random effects.\"\"\"\n\n    def domain():\n        \"\"\"A function which returns all possible independent variable values as a 2D array.\"\"\"\n        x = variables.independent_variables[0].allowed_values.reshape(-1, 1)\n        return x\n\n    def plotter(model=None):\n        \"\"\"A function which plots the ground truth and (optionally) a fitted model.\"\"\"\n        import matplotlib.pyplot as plt\n\n        plt.figure()\n        dom = domain()\n        data = ground_truth(dom)\n\n        y = data[dependent.name]\n        x = data.drop(dependent.name, axis=1)\n\n        if x.shape[1] &gt; 2:\n            Exception(\n                \"No standard way to plot more then 2 independent variables implemented\"\n            )\n\n        if x.shape[1] == 1:\n            plt.plot(x, y, label=\"Ground Truth\")\n            if model is not None:\n                plt.plot(x, model.predict(x), label=\"Fitted Model\")\n        else:\n            fig = plt.figure()\n            ax = fig.add_subplot(projection=\"3d\")\n            x_ = x.iloc[:, 0]\n\n            y_ = x.iloc[:, 1]\n            z_ = y\n\n            ax.scatter(x_, y_, z_, s=1, alpha=0.3, label=\"Ground Truth\")\n            if model is not None:\n                z_m = model.predict(x)\n                ax.scatter(x_, y_, z_m, s=1, alpha=0.5, label=\"Fitted Model\")\n\n        plt.legend()\n        plt.title(name)\n        plt.show()\n\n    # The object which gets stored in the synthetic inventory\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=lmm_experiment.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=lmm_experiment,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/abstract/template_experiment/","title":"autora.experiment_runner.synthetic.abstract.template_experiment","text":"<p>A template synthetic experiment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from autora.experiment_runner.synthetic.abstract.template_experiment import (\n...     template_experiment\n... )\n</code></pre> <p>We can instantiate the experiment using the imported function</p> <pre><code>&gt;&gt;&gt; s = template_experiment()\n&gt;&gt;&gt; s\nSyntheticExperimentCollection(name='Template Experiment', description='...',\n    params={'name': ...}, ...)\n</code></pre> <pre><code>&gt;&gt;&gt; s.name\n'Template Experiment'\n</code></pre> <pre><code>&gt;&gt;&gt; s.variables\nVariableCollection(...)\n</code></pre> <pre><code>&gt;&gt;&gt; s.domain()\narray([[0],\n       [1],\n       [2],\n       [3]])\n</code></pre> <pre><code>&gt;&gt;&gt; s.ground_truth\nfunctools.partial(&lt;function template_experiment.&lt;locals&gt;.run at 0x...&gt;,\n                  added_noise=0.0)\n</code></pre> <pre><code>&gt;&gt;&gt; float(s.ground_truth(1.))\n2.0\n</code></pre> <pre><code>&gt;&gt;&gt; s.ground_truth(s.domain())\narray([[1.],\n       [2.],\n       [3.],\n       [4.]])\n</code></pre> <pre><code>&gt;&gt;&gt; s.run\n&lt;function template_experiment.&lt;locals&gt;.run at 0x...&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; float(s.run(1., random_state=42))\n2.003047170797544\n</code></pre> <pre><code>&gt;&gt;&gt; s.run(s.domain(), random_state=42)\narray([[1.00304717],\n       [1.98960016],\n       [3.00750451],\n       [4.00940565]])\n</code></pre> <pre><code>&gt;&gt;&gt; s.plotter()\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Generate a new version of the experiment with different parameters:</p> <pre><code>&gt;&gt;&gt; new_params = dict(s.params)\n&gt;&gt;&gt; s.factory_function(**new_params)\nSyntheticExperimentCollection(...)\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/abstract/template_experiment/#autora.experiment_runner.synthetic.abstract.template_experiment.template_experiment","title":"<code>template_experiment(name='Template Experiment')</code>","text":"<p>A template for synthetic experiments.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the experiment</p> <code>'Template Experiment'</code> Source code in <code>autora/experiment_runner/synthetic/abstract/template_experiment.py</code> <pre><code>def template_experiment(\n    # Add any configurable parameters with their defaults here:\n    name: str = \"Template Experiment\",\n):\n    \"\"\"\n    A template for synthetic experiments.\n\n    Parameters:\n        name: name of the experiment\n    \"\"\"\n\n    params = dict(\n        # Include all parameters here:\n        name=name,\n    )\n\n    # Define variables\n    x = IV(name=\"Intensity\", allowed_values=np.arange(4))\n    y = DV(name=\"Response\")\n    variables = VariableCollection(\n        independent_variables=[x],\n        dependent_variables=[y],\n    )\n\n    # Define experiment runner\n\n    def run(\n        conditions: ArrayLike,\n        added_noise: float = 0.01,\n        random_state: Optional[int] = None,\n    ):\n        \"\"\"A function which simulates noisy observations.\"\"\"\n        rng = np.random.default_rng(random_state)\n        x_ = np.array(conditions)\n        y = x_ + 1.0 + rng.normal(0, added_noise, size=x_.shape)\n        return y\n\n    ground_truth = partial(run, added_noise=0.0)\n    \"\"\"A function which simulates perfect observations\"\"\"\n\n    def domain():\n        \"\"\"A function which returns all possible independent variable values as a 2D array.\"\"\"\n        x = variables.independent_variables[0].allowed_values.reshape(-1, 1)\n        return x\n\n    def plotter(model=None):\n        \"\"\"A function which plots the ground truth and (optionally) a fitted model.\"\"\"\n        import matplotlib.pyplot as plt\n\n        plt.figure()\n        x = domain()\n        plt.plot(x, ground_truth(x), label=\"Ground Truth\")\n\n        if model is not None:\n            plt.plot(x, model.predict(x), label=\"Fitted Model\")\n\n        plt.xlabel(variables.independent_variables[0].name)\n        plt.ylabel(variables.dependent_variables[0].name)\n        plt.legend()\n        plt.title(name)\n\n    # The object which gets stored in the synthetic inventory\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=template_experiment.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=template_experiment,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/abstract/equation/","title":"autora.experiment_runner.synthetic.abstract.equation","text":""},{"location":"reference/autora/experiment_runner/synthetic/abstract/equation/#autora.experiment_runner.synthetic.abstract.equation.equation_experiment","title":"<code>equation_experiment(expression, X, y, name='Equation Experiment', rename_output_columns=True, random_state=None)</code>","text":"<p>A synthetic experiments that uses a sympy expression as ground truth.</p> <p>Sympy: https://www.sympy.org/en/index.html</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Expr</code> <p>A sympy expression. The expression is interpreted as definition for a function</p> required <code>X</code> <code>List[IV]</code> <p>The domain of independent variables</p> required <code>y</code> <code>DV</code> <p>The codomain of the dependent variables</p> required <code>name</code> <code>str</code> <p>Name of the experiment</p> <code>'Equation Experiment'</code> <code>random_state</code> <code>Optional[int]</code> <p>Seed for random number generator</p> <code>None</code> <code>rename_output_columns</code> <code>bool</code> <p>If true, rename the columns of the output DataFrame based on the variable names in the expression.</p> <code>True</code> <p>Examples:</p> <p>First we define an expression that will be interpreted as function. We need to define the symbols in sympy.</p> <pre><code>&gt;&gt;&gt; from sympy import symbols\n&gt;&gt;&gt; x, y = symbols(\"x y\")\n</code></pre> <p>We also have to define the independent and dependent variables:</p> <pre><code>&gt;&gt;&gt; iv_x = IV(name='x', allowed_values=np.linspace(-10,10) ,value_range=(-10,10))\n&gt;&gt;&gt; iv_y = IV(name='y', allowed_values=np.linspace(-10,10) ,value_range=(-10,10))\n&gt;&gt;&gt; dv_z = DV(name='z')\n</code></pre> <p>Now we can define an expression:</p> <pre><code>&gt;&gt;&gt; expr = x ** y\n</code></pre> <p>Then we use this expression in our experiment</p> <pre><code>&gt;&gt;&gt; experiment = equation_experiment(expr, [iv_x, iv_y], dv_z, random_state=42)\n</code></pre> <p>To run an experiment on some conditions, first we define those conditions as a pandas dataframe:</p> <pre><code>&gt;&gt;&gt; conditions = pd.DataFrame({'x':[1, 2, 3], 'y': [2, 3, 4]})\n&gt;&gt;&gt; conditions\n   x  y\n0  1  2\n1  2  3\n2  3  4\n</code></pre> <p>Then to run the experiment, we pass that dataframe to the <code>.run</code> function:</p> <pre><code>&gt;&gt;&gt; experiment.run(conditions)\n   x  y          z\n0  1  2   1.003047\n1  2  3   7.989600\n2  3  4  81.007505\n</code></pre> <p>If the names the expression requires are not part of the dataframe, we get an error message:</p> <pre><code>&gt;&gt;&gt; experiment.run(\n...     pd.DataFrame({'z':[1, 2, 2], 'x': [1, 2, 3]})\n... )\nTraceback (most recent call last):\n...\nException: Variables of expression x**y not found in columns of dataframe with columns\nIndex(['z', 'x'], dtype='object')\n</code></pre> <p>Each time an experiment is initialized with the same random_state, it should produce the same results:</p> <pre><code>&gt;&gt;&gt; experiment = equation_experiment(expr, [iv_x, iv_y], dv_z, random_state=42)\n&gt;&gt;&gt; results42 = experiment.run(conditions)\n&gt;&gt;&gt; results42\n   x  y          z\n0  1  2   1.003047\n1  2  3   7.989600\n2  3  4  81.007505\n</code></pre> <p>We can specify the random_state for a particular run to reproduce it:</p> <pre><code>&gt;&gt;&gt; results42_reproduced = experiment.run(conditions, random_state=42)\n&gt;&gt;&gt; pd.DataFrame.equals(results42, results42_reproduced)\nTrue\n</code></pre> <p>If we don't specify the random_state, it produces different values:</p> <pre><code>&gt;&gt;&gt; experiment.run(conditions)\n   x  y          z\n0  1  2   1.009406\n1  2  3   7.980490\n2  3  4  80.986978\n</code></pre> <p>An alternative input format for the experiment runner is a numpy array (not recommended):</p> <pre><code>&gt;&gt;&gt; experiment.run(np.array([[1, 1], [2, 2], [2, 3]]))\n   x  y         z\n0  1  1  1.001278\n1  2  2  3.996838\n2  2  3  7.999832\n</code></pre> <p>But we have to be careful with the order of the arguments in the runner. The arguments will be sorted alphabetically. In the following case the first entry of the numpy array is still x:</p> <pre><code>&gt;&gt;&gt; expr = y ** x\n&gt;&gt;&gt; experiment.run(np.array([[1, 1], [2, 2] , [2, 3]]), random_state=42)\n   x  y         z\n0  1  1  1.003047\n1  2  2  3.989600\n2  2  3  8.007505\n</code></pre> Source code in <code>temp_dir/abstract-equation/src/autora/experiment_runner/synthetic/abstract/equation/__init__.py</code> <pre><code>def equation_experiment(\n    expression: Expr,\n    X: List[IV],\n    y: DV,\n    name: str = \"Equation Experiment\",\n    rename_output_columns: bool = True,\n    random_state: Optional[int] = None,\n):\n    \"\"\"\n\n    A synthetic experiments that uses a sympy expression as ground truth.\n\n    Sympy: https://www.sympy.org/en/index.html\n\n    Args:\n        expression: A sympy expression. The expression is interpreted as definition for a function\n        X: The domain of independent variables\n        y: The codomain of the dependent variables\n        name: Name of the experiment\n        random_state: Seed for random number generator\n        rename_output_columns: If true, rename the columns of the output DataFrame based on the\n            variable names in the expression.\n\n\n    Examples:\n        First we define an expression that will be interpreted as function. We need to define the\n        symbols in sympy.\n        &gt;&gt;&gt; from sympy import symbols\n        &gt;&gt;&gt; x, y = symbols(\"x y\")\n\n        We also have to define the independent and dependent variables:\n        &gt;&gt;&gt; iv_x = IV(name='x', allowed_values=np.linspace(-10,10) ,value_range=(-10,10))\n        &gt;&gt;&gt; iv_y = IV(name='y', allowed_values=np.linspace(-10,10) ,value_range=(-10,10))\n        &gt;&gt;&gt; dv_z = DV(name='z')\n\n        Now we can define an expression:\n        &gt;&gt;&gt; expr = x ** y\n\n        Then we use this expression in our experiment\n        &gt;&gt;&gt; experiment = equation_experiment(expr, [iv_x, iv_y], dv_z, random_state=42)\n\n        To run an experiment on some conditions, first we define those conditions as a pandas\n        dataframe:\n        &gt;&gt;&gt; conditions = pd.DataFrame({'x':[1, 2, 3], 'y': [2, 3, 4]})\n        &gt;&gt;&gt; conditions\n           x  y\n        0  1  2\n        1  2  3\n        2  3  4\n\n        Then to run the experiment, we pass that dataframe to the `.run` function:\n        &gt;&gt;&gt; experiment.run(conditions)\n           x  y          z\n        0  1  2   1.003047\n        1  2  3   7.989600\n        2  3  4  81.007505\n\n        If the names the expression requires are not part of the dataframe, we get an error message:\n        &gt;&gt;&gt; experiment.run(\n        ...     pd.DataFrame({'z':[1, 2, 2], 'x': [1, 2, 3]})\n        ... )  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        Exception: Variables of expression x**y not found in columns of dataframe with columns\n        Index(['z', 'x'], dtype='object')\n\n\n        Each time an experiment is initialized with the same random_state, it should produce the\n        same results:\n        &gt;&gt;&gt; experiment = equation_experiment(expr, [iv_x, iv_y], dv_z, random_state=42)\n        &gt;&gt;&gt; results42 = experiment.run(conditions)\n        &gt;&gt;&gt; results42\n           x  y          z\n        0  1  2   1.003047\n        1  2  3   7.989600\n        2  3  4  81.007505\n\n        We can specify the random_state for a particular run to reproduce it:\n        &gt;&gt;&gt; results42_reproduced = experiment.run(conditions, random_state=42)\n        &gt;&gt;&gt; pd.DataFrame.equals(results42, results42_reproduced)\n        True\n\n        If we don't specify the random_state, it produces different values:\n        &gt;&gt;&gt; experiment.run(conditions)\n           x  y          z\n        0  1  2   1.009406\n        1  2  3   7.980490\n        2  3  4  80.986978\n\n        An alternative input format for the experiment runner is a numpy array (not recommended):\n        &gt;&gt;&gt; experiment.run(np.array([[1, 1], [2, 2], [2, 3]]))\n           x  y         z\n        0  1  1  1.001278\n        1  2  2  3.996838\n        2  2  3  7.999832\n\n        But we have to be careful with the order of the arguments in the runner. The arguments\n        will be sorted alphabetically.\n        In the following case the first entry of the numpy array is still x:\n        &gt;&gt;&gt; expr = y ** x\n        &gt;&gt;&gt; experiment.run(np.array([[1, 1], [2, 2] , [2, 3]]), random_state=42)\n           x  y         z\n        0  1  1  1.003047\n        1  2  2  3.989600\n        2  2  3  8.007505\n\n    \"\"\"\n\n    params = dict(\n        # Include all parameters here:\n        expression=expression,\n        name=name,\n        random_state=random_state,\n    )\n\n    args = list(expression.free_symbols)\n    args = sorted(args, key=lambda el: el.name)\n\n    f_numpy = lambdify(args, expression, \"numpy\")\n\n    # Define variables\n    variables = VariableCollection(\n        independent_variables=X,\n        dependent_variables=[y],\n    )\n    if not set([el.name for el in variables.independent_variables]).issubset(\n        set([str(a) for a in args])\n    ):\n        raise Exception(\n            f\"Independent variables {[iv.name for iv in X]} and symbols of the equation tree \"\n            f\"{args} do not match.\"\n        )\n\n    # Define experiment runner\n    rng = np.random.default_rng(random_state)\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise=0.01,\n        random_state=None,\n    ):\n        \"\"\"A function which simulates noisy observations.\"\"\"\n\n        if random_state is not None:\n            rng_ = np.random.default_rng(random_state)\n        else:\n            rng_ = rng  # use the RNG from the outer scope\n\n        x = conditions\n        if isinstance(x, pd.DataFrame):\n            x = x.copy()\n            if not set([el.name for el in args]).issubset(x.columns):\n                raise Exception(\n                    f\"Variables of expression {expression} \"\n                    f\"not found in columns of dataframe with columns {x.columns}\"\n                )\n            x_filtered = x[[el.name for el in args]]\n            x_sorted = x_filtered.sort_index(axis=1)\n            x_ = np.array(x_sorted)\n        else:\n            x_ = x\n            warnings.warn(\n                \"Unnamed data is used. Arguments will be sorted alphabetically. \"\n                \"Consider using a Pandas DataFrame with named columns for \"\n                \"better clarity and ease of use.\",\n                category=RuntimeWarning,\n            )\n\n        out = f_numpy(*x_.T)\n        out = out + rng_.normal(0, added_noise, size=out.shape)\n        if isinstance(x, pd.DataFrame):\n            _res = pd.DataFrame(x_, columns=x_sorted.columns)\n            res = x\n            for col in x_sorted.columns:\n                res[col] = list(_res[col])\n        else:\n            if rename_output_columns:\n                res = pd.DataFrame(x_, columns=[el.name for el in args])\n            else:\n                res = pd.DataFrame(x_.T)\n\n        res[y.name] = out\n        return res\n\n    ground_truth = partial(run, added_noise_=0.0)\n    \"\"\"A function which simulates perfect observations\"\"\"\n\n    def domain():\n        \"\"\"A function which returns all possible independent variable values as a grid.\"\"\"\n        iv_values = [iv.allowed_values for iv in variables.independent_variables[0]]\n        X_combinations = product(*iv_values)\n        X = np.array(list(X_combinations))\n        return X\n\n    def plotter(model=None):\n        \"\"\"A function which plots the ground truth and (optionally) a fitted model.\"\"\"\n        import matplotlib.pyplot as plt\n\n        plt.figure()\n        dom = domain()\n        data = ground_truth(dom)\n\n        y = data[\"observations\"]\n        x = data.drop(\"observations\", axis=1)\n\n        if x.shape[1] &gt; 2:\n            Exception(\n                \"No standard way to plot more then 2 independent variables implemented\"\n            )\n\n        if x.shape[1] == 1:\n            plt.plot(x, y, label=\"Ground Truth\")\n            if model is not None:\n                plt.plot(x, model.predict(x), label=\"Fitted Model\")\n        else:\n            fig = plt.figure()\n            ax = fig.add_subplot(projection=\"3d\")\n            x_ = x.iloc[:, 0]\n\n            y_ = x.iloc[:, 1]\n            z_ = y\n\n            ax.scatter(x_, y_, z_, s=1, alpha=0.3, label=\"Ground Truth\")\n            if model is not None:\n                z_m = model.predict(x)\n                ax.scatter(x_, y_, z_m, s=1, alpha=0.5, label=\"Fitted Model\")\n\n        plt.legend()\n        plt.title(name)\n        plt.show()\n\n    # The object which gets stored in the synthetic inventory\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=equation_experiment.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=equation_experiment,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/economics/expected_value_theory/","title":"autora.experiment_runner.synthetic.economics.expected_value_theory","text":""},{"location":"reference/autora/experiment_runner/synthetic/economics/expected_value_theory/#autora.experiment_runner.synthetic.economics.expected_value_theory.expected_value_theory","title":"<code>expected_value_theory(name='Expected Value Theory', choice_temperature=0.1, value_lambda=0.5, resolution=10, minimum_value=-1, maximum_value=1)</code>","text":"<p>Expected Value Theory</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>'Expected Value Theory'</code> <code>choice_temperature</code> <code>float</code> <code>0.1</code> <code>value_lambda</code> <code>float</code> <code>0.5</code> <code>resolution</code> <code>10</code> <code>minimum_value</code> <code>-1</code> <code>maximum_value</code> <code>1</code> <code>Examples</code> <p>s = expected_value_theory() s.run(np.array([[1,2,.1,.9]]), random_state=42)    V_A  P_A  V_B  P_B  choose_A 0  1.0  2.0  0.1  0.9  0.999938</p> required Source code in <code>autora/experiment_runner/synthetic/economics/expected_value_theory.py</code> <pre><code>def expected_value_theory(\n    name=\"Expected Value Theory\",\n    choice_temperature: float = 0.1,\n    value_lambda: float = 0.5,\n    resolution=10,\n    minimum_value=-1,\n    maximum_value=1,\n):\n    \"\"\"\n    Expected Value Theory\n\n    Parameters:\n        name:\n        choice_temperature:\n        value_lambda:\n        resolution:\n        minimum_value:\n        maximum_value:\n        Examples:\n            &gt;&gt;&gt; s = expected_value_theory()\n            &gt;&gt;&gt; s.run(np.array([[1,2,.1,.9]]), random_state=42)\n               V_A  P_A  V_B  P_B  choose_A\n            0  1.0  2.0  0.1  0.9  0.999938\n    \"\"\"\n\n    params = dict(\n        name=name,\n        minimum_value=minimum_value,\n        maximum_value=maximum_value,\n        resolution=resolution,\n        choice_temperature=choice_temperature,\n        value_lambda=value_lambda,\n    )\n\n    variables = get_variables(\n        minimum_value=minimum_value, maximum_value=maximum_value, resolution=resolution\n    )\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise: float = 0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n        Y = np.zeros((X.shape[0], 1))\n        for idx, x in enumerate(X):\n            value_A = value_lambda * x[0]\n            value_B = value_lambda * x[2]\n\n            probability_a = x[1]\n            probability_b = x[3]\n\n            expected_value_A = value_A * probability_a + rng.normal(0, added_noise)\n            expected_value_B = value_B * probability_b + rng.normal(0, added_noise)\n\n            # compute probability of choosing option A\n            p_choose_A = np.exp(expected_value_A / choice_temperature) / (\n                np.exp(expected_value_A / choice_temperature)\n                + np.exp(expected_value_B / choice_temperature)\n            )\n\n            Y[idx] = p_choose_A\n\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[variables.dependent_variables[0].name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        X = np.array(\n            np.meshgrid([x.allowed_values for x in variables.independent_variables])\n        ).T.reshape(-1, 4)\n        return X\n\n    def plotter(model=None):\n        import matplotlib.colors as mcolors\n        import matplotlib.pyplot as plt\n\n        v_a_list = [-1, 0.5, 1]\n        v_b = 0.5\n        p_b = 0.5\n        p_a = np.linspace(0, 1, 100)\n\n        for idx, v_a in enumerate(v_a_list):\n            X = np.zeros((len(p_a), 4))\n            X[:, 0] = v_a\n            X[:, 1] = p_a\n            X[:, 2] = v_b\n            X[:, 3] = p_b\n\n            y = ground_truth(X)[variables.dependent_variables[0].name]\n            colors = mcolors.TABLEAU_COLORS\n            col_keys = list(colors.keys())\n            plt.plot(\n                p_a, y, label=f\"$V(A) = {v_a}$ (Original)\", c=colors[col_keys[idx]]\n            )\n            if model is not None:\n                y = model.predict(X)\n                plt.plot(\n                    p_a,\n                    y,\n                    label=f\"$V(A) = {v_a}$ (Recovered)\",\n                    c=colors[col_keys[idx]],\n                    linestyle=\"--\",\n                )\n\n        x_limit = [0, variables.independent_variables[1].value_range[1]]\n        y_limit = [0, 1]\n        x_label = \"Probability of Choosing Option A\"\n        y_label = \"Probability of Obtaining V(A)\"\n\n        plt.xlim(x_limit)\n        plt.ylim(y_limit)\n        plt.xlabel(x_label, fontsize=\"large\")\n        plt.ylabel(y_label, fontsize=\"large\")\n        plt.legend(loc=2, fontsize=\"medium\")\n        plt.title(name, fontsize=\"x-large\")\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=expected_value_theory.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=expected_value_theory,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/economics/prospect_theory/","title":"autora.experiment_runner.synthetic.economics.prospect_theory","text":""},{"location":"reference/autora/experiment_runner/synthetic/economics/prospect_theory/#autora.experiment_runner.synthetic.economics.prospect_theory.prospect_theory","title":"<code>prospect_theory(name='Prospect Theory', choice_temperature=0.1, value_alpha=0.88, value_beta=0.88, value_lambda=2.25, probability_alpha=0.61, probability_beta=0.69, resolution=10, minimum_value=-1, maximum_value=1)</code>","text":"<p>Parameters from D. Kahneman, A. Tversky, Prospect theory: An analysis of decision under risk. Econometrica 47, 263\u2013292 (1979). doi:10.2307/1914185</p> Power value function according to <ul> <li> <p>A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of   uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574</p> </li> <li> <p>I. Gilboa, Expected utility with purely subjective non-additive probabilities.   J. Math. Econ. 16, 65\u201388 (1987). doi:10.1016/0304-4068(87)90022-X</p> </li> <li> <p>D. Schmeidler, Subjective probability and expected utility without additivity.   Econometrica 57, 571 (1989). doi:10.2307/1911053</p> </li> </ul> Probability function according to <p>A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574</p> <p>Examples:     &gt;&gt;&gt; s = prospect_theory()     &gt;&gt;&gt; s.run(np.array([[.9,.1,.1,.9]]), random_state=42)        V_A  P_A  V_B  P_B  choose_A     0  0.9  0.1  0.1  0.9  0.709777</p> Source code in <code>autora/experiment_runner/synthetic/economics/prospect_theory.py</code> <pre><code>def prospect_theory(\n    name=\"Prospect Theory\",\n    choice_temperature=0.1,\n    value_alpha=0.88,\n    value_beta=0.88,\n    value_lambda=2.25,\n    probability_alpha=0.61,\n    probability_beta=0.69,\n    resolution=10,\n    minimum_value=-1,\n    maximum_value=1,\n):\n    \"\"\"\n    Parameters from\n    D. Kahneman, A. Tversky, Prospect theory: An analysis of decision under risk.\n    Econometrica 47, 263\u2013292 (1979). doi:10.2307/1914185\n\n    Power value function according to:\n        - A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n          uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n\n        - I. Gilboa, Expected utility with purely subjective non-additive probabilities.\n          J. Math. Econ. 16, 65\u201388 (1987). doi:10.1016/0304-4068(87)90022-X\n\n        - D. Schmeidler, Subjective probability and expected utility without additivity.\n          Econometrica 57, 571 (1989). doi:10.2307/1911053\n\n    Probability function according to:\n        A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n        uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n    Examples:\n        &gt;&gt;&gt; s = prospect_theory()\n        &gt;&gt;&gt; s.run(np.array([[.9,.1,.1,.9]]), random_state=42)\n           V_A  P_A  V_B  P_B  choose_A\n        0  0.9  0.1  0.1  0.9  0.709777\n\n    \"\"\"\n\n    params = dict(\n        choice_temperature=choice_temperature,\n        value_alpha=value_alpha,\n        value_beta=value_beta,\n        value_lambda=value_lambda,\n        probability_alpha=probability_alpha,\n        probability_beta=probability_beta,\n        resolution=resolution,\n        minimum_value=minimum_value,\n        maximum_value=maximum_value,\n        name=name,\n    )\n\n    variables = get_variables(\n        minimum_value=minimum_value, maximum_value=maximum_value, resolution=resolution\n    )\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise=0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n        Y = np.zeros((X.shape[0], 1))\n        for idx, x in enumerate(X):\n            # power value function according to:\n\n            # A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n            # uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n\n            # I. Gilboa, Expected utility with purely subjective non-additive probabilities.\n            # J. Math. Econ. 16, 65\u201388 (1987). doi:10.1016/0304-4068(87)90022-X\n\n            # D. Schmeidler, Subjective probability and expected utility without additivity.\n            # Econometrica 57, 571 (1989). doi:10.2307/1911053\n\n            # compute value of option A\n            if x[0] &gt; 0:\n                value_A = x[0] ** value_alpha\n            else:\n                value_A = -value_lambda * (-x[0]) ** (value_beta)\n\n            # compute value of option B\n            if x[2] &gt; 0:\n                value_B = x[2] ** value_alpha\n            else:\n                value_B = -value_lambda * (-x[2]) ** (value_beta)\n\n            # probability function according to:\n\n            # A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n            # uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n\n            # compute probability of option A\n            if x[0] &gt;= 0:\n                coefficient = probability_alpha\n            else:\n                coefficient = probability_beta\n\n            probability_a = x[1] ** coefficient / (\n                x[1] ** coefficient + (1 - x[1]) ** coefficient\n            ) ** (1 / coefficient)\n\n            # compute probability of option B\n            if x[2] &gt;= 0:\n                coefficient = probability_alpha\n            else:\n                coefficient = probability_beta\n\n            probability_b = x[3] ** coefficient / (\n                x[3] ** coefficient + (1 - x[3]) ** coefficient\n            ) ** (1 / coefficient)\n\n            expected_value_A = value_A * probability_a + rng.normal(0, added_noise)\n            expected_value_B = value_B * probability_b + rng.normal(0, added_noise)\n\n            # compute probability of choosing option A\n            p_choose_A = np.exp(expected_value_A / choice_temperature) / (\n                np.exp(expected_value_A / choice_temperature)\n                + np.exp(expected_value_B / choice_temperature)\n            )\n\n            Y[idx] = p_choose_A\n\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[variables.dependent_variables[0].name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        v_a = variables.independent_variables[0].allowed_values\n        p_a = variables.independent_variables[1].allowed_values\n        v_b = variables.independent_variables[2].allowed_values\n        p_b = variables.independent_variables[3].allowed_values\n\n        X = np.array(np.meshgrid(v_a, p_a, v_b, p_b)).T.reshape(-1, 4)\n        return X\n\n    def plotter(model=None):\n        import matplotlib.colors as mcolors\n        import matplotlib.pyplot as plt\n\n        v_a_list = [-0.5, 0.5, 1]\n        p_a = np.linspace(0, 1, 100)\n\n        v_b = 0.5\n        p_b = 0.5\n\n        for idx, v_a in enumerate(v_a_list):\n            X = np.zeros((len(p_a), 4))\n            X[:, 0] = v_a\n            X[:, 1] = p_a\n            X[:, 2] = v_b\n            X[:, 3] = p_b\n\n            y = ground_truth(X)[variables.dependent_variables[0].name]\n            colors = mcolors.TABLEAU_COLORS\n            col_keys = list(colors.keys())\n            plt.plot(\n                p_a, y, label=f\"$V(A) = {v_a}$ (Original)\", c=colors[col_keys[idx]]\n            )\n            if model is not None:\n                y = model.predict(X)\n                plt.plot(\n                    p_a,\n                    y,\n                    label=f\"$V(A) = {v_a}$ (Recovered)\",\n                    c=colors[col_keys[idx]],\n                    linestyle=\"--\",\n                )\n\n        x_limit = [0, variables.independent_variables[1].value_range[1]]\n        y_limit = [0, 1]\n        x_label = \"Probability of Choosing Option A\"\n        y_label = \"Probability of Obtaining V(A)\"\n\n        plt.xlim(x_limit)\n        plt.ylim(y_limit)\n        plt.xlabel(x_label, fontsize=\"large\")\n        plt.ylabel(y_label, fontsize=\"large\")\n        plt.legend(loc=2, fontsize=\"medium\")\n        plt.title(name, fontsize=\"x-large\")\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=prospect_theory.__doc__,\n        params=params,\n        variables=variables,\n        domain=domain,\n        run=run,\n        ground_truth=ground_truth,\n        plotter=plotter,\n        factory_function=prospect_theory,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/neuroscience/task_switching/","title":"autora.experiment_runner.synthetic.neuroscience.task_switching","text":""},{"location":"reference/autora/experiment_runner/synthetic/neuroscience/task_switching/#autora.experiment_runner.synthetic.neuroscience.task_switching.task_switching","title":"<code>task_switching(name='Task Switching', resolution=50, priming_default=0.3, temperature=0.2, minimum_task_control=0.15, constant=1.5)</code>","text":"<p>Task Switching</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the experiment</p> <code>'Task Switching'</code> <code>resolution</code> <p>number of allowed values for stimulus</p> <code>50</code> <code>priming_default</code> <p>default for task priming</p> <code>0.3</code> <code>temperature</code> <p>temperature for softmax when computing performance of current task</p> <code>0.2</code> <code>constant</code> <p>constant for task activation</p> <code>1.5</code> <code>minimum_task_control</code> <p>minimum task control</p> <code>0.15</code> <p>Examples:     &gt;&gt;&gt; s = task_switching()     &gt;&gt;&gt; s.run(np.array([[.5,.7,0]]), random_state=42)        cur_task_strength  alt_task_strength  is_switch  cur_task_performance     0                0.5                0.7        0.0              0.685351</p> Source code in <code>autora/experiment_runner/synthetic/neuroscience/task_switching.py</code> <pre><code>def task_switching(\n    name=\"Task Switching\",\n    resolution=50,\n    priming_default=0.3,\n    temperature=0.2,\n    minimum_task_control=0.15,\n    constant=1.5,\n):\n    \"\"\"\n    Task Switching\n\n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus\n        priming_default: default for task priming\n        temperature: temperature for softmax when computing performance of current task\n        constant: constant for task activation\n        minimum_task_control: minimum task control\n    Examples:\n        &gt;&gt;&gt; s = task_switching()\n        &gt;&gt;&gt; s.run(np.array([[.5,.7,0]]), random_state=42)\n           cur_task_strength  alt_task_strength  is_switch  cur_task_performance\n        0                0.5                0.7        0.0              0.685351\n    \"\"\"\n\n    params = dict(\n        name=name,\n        resolution=resolution,\n        priming_default=priming_default,\n        temperature=temperature,\n        minimum_task_control=minimum_task_control,\n        constant=constant,\n    )\n\n    current_task_strength = IV(\n        name=\"cur_task_strength\",\n        allowed_values=np.linspace(1 / resolution, 1, resolution),  #\n        value_range=(0, 1),\n        units=\"intensity\",\n        variable_label=\"Strength of Current Task\",\n        type=ValueType.REAL,\n    )\n\n    alt_task_strength = IV(\n        name=\"alt_task_strength\",\n        allowed_values=np.linspace(1 / resolution, 1, resolution),\n        value_range=(0, 1),\n        units=\"intensity\",\n        variable_label=\"Strength of Alternative Task\",\n        type=ValueType.REAL,\n    )\n\n    is_switch = IV(\n        name=\"is_switch\",\n        allowed_values=[0, 1],\n        value_range=(0, 1),\n        units=\"indicator\",\n        variable_label=\"Is Switch\",\n        type=ValueType.PROBABILITY_SAMPLE,\n    )\n\n    cur_task_performance = DV(\n        name=\"cur_task_performance\",\n        value_range=(0, 1),\n        units=\"performance\",\n        variable_label=\"Accuracy of Current Task\",\n        type=ValueType.PROBABILITY,\n    )\n\n    variables = VariableCollection(\n        independent_variables=[current_task_strength, alt_task_strength, is_switch],\n        dependent_variables=[cur_task_performance],\n    )\n\n    def inverse(x, A, B):\n        y = 1 / (A * x + B)\n        return y\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise: float = 0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n        Y = np.zeros((X.shape[0], 1))\n        for idx, x in enumerate(X):\n            cur_task_strength = x[0]\n            alt_task_strength = x[1]\n            is_switch = x[2]\n\n            # determine current task control\n\n            input_ratio = (cur_task_strength + priming_default * (1 - is_switch)) / (\n                alt_task_strength + priming_default * (is_switch)\n            )\n\n            cur_task_control = inverse(input_ratio, 2.61541389, 0.7042097)\n            cur_task_control = np.max([cur_task_control, minimum_task_control])\n\n            cur_task_input = (\n                cur_task_strength\n                + priming_default * (1 - is_switch)\n                + cur_task_control\n                + rng.normal(0, added_noise)\n            )\n\n            alt_task_input = (\n                alt_task_strength\n                + priming_default * (is_switch)\n                + rng.normal(0, added_noise)\n            )\n\n            cur_task_activation = 1 - np.exp(-constant * cur_task_input)\n            alt_task_activation = 1 - np.exp(-constant * alt_task_input)\n\n            cur_task_performance = np.exp(cur_task_activation * 1 / temperature) / (\n                np.exp(cur_task_activation * 1 / temperature)\n                + np.exp(alt_task_activation * 1 / temperature)\n            )\n\n            Y[idx] = cur_task_performance\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[variables.dependent_variables[0].name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        s1_values = variables.independent_variables[0].allowed_values\n        s2_values = variables.independent_variables[1].allowed_values\n        is_switch_values = variables.independent_variables[2].allowed_values\n        X = np.array(np.meshgrid(s1_values, s2_values, is_switch_values)).T.reshape(-1, 3)\n        # remove all combinations where s1 &gt; s2\n        # X = X[X[:, 0] &lt;= X[:, 1]]\n        return X\n\n    def plotter(\n        model=None,\n    ):\n        X = np.zeros((4, 3))\n\n        # Values taken from Table 4 in Yeung &amp; Monsell (2003)\n\n        # word switch\n        X[0, 0] = 0.5  # current task strength\n        X[0, 1] = 0.1  # alternative task strength\n        # X[0, 2] = 0.2 # current task control\n        X[0, 2] = 1  # is switch\n\n        # word repetition\n        X[1, 0] = 0.5  # current task strength\n        X[1, 1] = 0.1  # alternative task strength\n        # X[1, 2] = 0.15 # current task control\n        X[1, 2] = 0  # is switch\n\n        # color switch\n        X[2, 0] = 0.1  # current task strength\n        X[2, 1] = 0.5  # alternative task strength\n        # X[2, 2] = 0.97  # current task control\n        X[2, 2] = 1  # is switch\n\n        # color repetition\n        X[3, 0] = 0.1  # current task strength\n        X[3, 1] = 0.5  # alternative task strength\n        # X[3, 2] = 0.38  # current task control\n        X[3, 2] = 0  # is switch\n\n        y = ground_truth(X)\n\n        word_switch_performance = y.at[0, 'cur_task_performance']\n        word_repetition_performance = y.at[1, 'cur_task_performance']\n        color_switch_performance = y.at[2, 'cur_task_performance']\n        color_repetition_performance = y.at[3, 'cur_task_performance']\n\n        x_data = [1, 2]\n        word_performance = (\n            1 - np.array([word_repetition_performance, word_switch_performance])\n        ) * 100\n        color_performance = (\n            1 - np.array([color_repetition_performance, color_switch_performance])\n        ) * 100\n\n        if model is not None:\n            y_pred = model.predict(X)\n            word_switch_performance_pred = y_pred[0][0]\n            word_repetition_performance_pred = y_pred[1][0]\n            color_switch_performance_pred = y_pred[2][0]\n            color_repetition_performance_pred = y_pred[3][0]\n            word_performance_recovered = (\n                1\n                - np.array(\n                    [word_repetition_performance_pred, word_switch_performance_pred]\n                )\n            ) * 100\n            color_performance_recovered = (\n                1\n                - np.array(\n                    [color_repetition_performance_pred, color_switch_performance_pred]\n                )\n            ) * 100\n\n        legend = (\n            \"Word Task (Original)\",\n            \"Color Task (Original)\",\n            \"Word Task (Recovered)\",\n            \"Color Task (Recovered)\",\n        )\n\n        # plot\n        import matplotlib.colors as mcolors\n        import matplotlib.pyplot as plt\n\n        colors = mcolors.TABLEAU_COLORS\n        col_keys = list(colors.keys())\n\n        plt.plot(x_data, word_performance, label=legend[0], c=colors[col_keys[0]])\n        plt.plot(x_data, color_performance, label=legend[1], c=colors[col_keys[1]])\n        if model is not None:\n            plt.plot(\n                x_data,\n                word_performance_recovered,\n                \"--\",\n                label=legend[2],\n                c=colors[col_keys[0]],\n            )\n            plt.plot(\n                x_data,\n                color_performance_recovered,\n                \"--\",\n                label=legend[3],\n                c=colors[col_keys[1]],\n            )\n        plt.xlim([0.5, 2.5])\n        plt.ylim([0, 50])\n        plt.ylabel(\"Error Rate (%)\", fontsize=\"large\")\n        plt.legend(loc=2, fontsize=\"large\")\n        plt.title(\"Task Switching\", fontsize=\"large\")\n        plt.xticks(x_data, [\"Repetition\", \"Switch\"], rotation=\"horizontal\")\n        plt.show()\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=task_switching.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=task_switching,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/exp_learning/","title":"autora.experiment_runner.synthetic.psychology.exp_learning","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/exp_learning/#autora.experiment_runner.synthetic.psychology.exp_learning.exp_learning","title":"<code>exp_learning(name='Exponential Learning', resolution=100, minimum_trial=1, minimum_initial_value=0, maximum_initial_value=0.5, lr=0.03, p_asymptotic=1.0)</code>","text":"<p>Exponential Learning</p> <p>Parameters:</p> Name Type Description Default <code>p_asymptotic</code> <p>additive bias on constant multiplier</p> <code>1.0</code> <code>lr</code> <p>learning rate</p> <code>0.03</code> <code>maximum_initial_value</code> <p>upper bound for initial p value</p> <code>0.5</code> <code>minimum_initial_value</code> <p>lower bound for initial p value</p> <code>0</code> <code>minimum_trial</code> <p>upper bound for exponential constant</p> <code>1</code> <code>name</code> <p>name of the experiment</p> <code>'Exponential Learning'</code> <code>resolution</code> <p>number of allowed values for stimulus</p> <code>100</code> <code>Examples</code> required Source code in <code>autora/experiment_runner/synthetic/psychology/exp_learning.py</code> <pre><code>def exp_learning(\n    name=\"Exponential Learning\",\n    resolution=100,\n    minimum_trial=1,\n    minimum_initial_value=0,\n    maximum_initial_value=0.5,\n    lr=0.03,\n    p_asymptotic=1.0,\n):\n    \"\"\"\n    Exponential Learning\n\n    Args:\n        p_asymptotic: additive bias on constant multiplier\n        lr: learning rate\n        maximum_initial_value: upper bound for initial p value\n        minimum_initial_value: lower bound for initial p value\n        minimum_trial: upper bound for exponential constant\n        name: name of the experiment\n        resolution: number of allowed values for stimulus\n        Examples:\n        &gt;&gt;&gt; s = exp_learning()\n        &gt;&gt;&gt; s.run(np.array([[.2,.1]]), random_state=42)\n           P_asymptotic  trial  performance\n        0           0.2    0.1     0.205444\n    \"\"\"\n\n    maximum_trial = resolution\n\n    params = dict(\n        name=\"Exponential Learning\",\n        resolution=resolution,\n        minimum_trial=minimum_trial,\n        maximum_trial=maximum_trial,\n        minimum_initial_value=minimum_initial_value,\n        maximum_initial_value=maximum_initial_value,\n        lr=lr,\n        p_asymptotic=p_asymptotic,\n    )\n\n    p_initial = IV(\n        name=\"P_asymptotic\",\n        allowed_values=np.linspace(\n            minimum_initial_value, maximum_initial_value, resolution\n        ),\n        value_range=(minimum_initial_value, maximum_initial_value),\n        units=\"performance\",\n        variable_label=\"Asymptotic Performance\",\n        type=ValueType.REAL,\n    )\n\n    trial = IV(\n        name=\"trial\",\n        allowed_values=np.linspace(minimum_trial, maximum_trial, resolution),\n        value_range=(minimum_trial, maximum_trial),\n        units=\"trials\",\n        variable_label=\"Trials\",\n        type=ValueType.REAL,\n    )\n\n    performance = DV(\n        name=\"performance\",\n        value_range=(0, p_asymptotic),\n        units=\"performance\",\n        variable_label=\"Performance\",\n        type=ValueType.REAL,\n    )\n\n    variables = VariableCollection(\n        independent_variables=[p_initial, trial],\n        dependent_variables=[performance],\n    )\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise: float = 0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n        Y = np.zeros((X.shape[0], 1))\n\n        # exp learning function according to\n        # Heathcote, A., Brown, S., &amp; Mewhort, D. J. (2000). The power law repealed:\n        # The case for an exponential law of practice. Psychonomic bulletin &amp; review, 7(2), 185\u2013207.\n\n        # Thurstone, L. L. (1919). The learning curve equation.\n        # Psy- chological Monographs, 26(3), i.\n\n        for idx, x in enumerate(X):\n            p_initial_exp = x[0]\n            trial_exp = x[1]\n            y = (\n                p_asymptotic\n                - (p_asymptotic - p_initial_exp) * np.exp(-lr * trial_exp)\n                + rng.normal(0, added_noise)\n            )\n            Y[idx] = y\n\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[variables.dependent_variables[0].name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        p_initial_values = variables.independent_variables[0].allowed_values\n        trial_values = variables.independent_variables[1].allowed_values\n\n        X = np.array(np.meshgrid(p_initial_values, trial_values)).T.reshape(-1, 2)\n        return X\n\n    def plotter(\n        model=None,\n    ):\n        import matplotlib.pyplot as plt\n\n        P_0_list = [0, 0.25, 0.5]\n\n        for P_0 in P_0_list:\n            X = np.zeros((len(trial.allowed_values), 2))\n            X[:, 0] = P_0\n            X[:, 1] = trial.allowed_values\n\n            dvs = [dv.name for dv in variables.dependent_variables]\n            y = ground_truth(X)[dvs]\n\n            plt.plot(trial.allowed_values, y, label=f\"$P_0 = {P_0}$ (Original)\")\n            if model is not None:\n                y = model.predict(X)\n                plt.plot(trial.allowed_values, y, label=f\"$P_0 = {P_0}$ (Recovered)\", linestyle=\"--\")\n\n        x_limit = [0, variables.independent_variables[1].value_range[1]]\n        y_limit = [0, 1]\n        x_label = \"Trial $t$\"\n        y_label = \"Performance $P_n$\"\n\n        plt.xlim(x_limit)\n        plt.ylim(y_limit)\n        plt.xlabel(x_label, fontsize=\"large\")\n        plt.ylabel(y_label, fontsize=\"large\")\n        plt.legend(loc=4, fontsize=\"medium\")\n        plt.title(\"Exponential Learning\", fontsize=\"x-large\")\n        plt.show()\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=exp_learning.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=exp_learning,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/luce_choice_ratio/","title":"autora.experiment_runner.synthetic.psychology.luce_choice_ratio","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/luce_choice_ratio/#autora.experiment_runner.synthetic.psychology.luce_choice_ratio.luce_choice_ratio","title":"<code>luce_choice_ratio(name='Luce-Choice-Ratio', resolution=8, maximum_similarity=10, focus=0.8)</code>","text":"<p>Luce-Choice-Ratio</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the experiment</p> <code>'Luce-Choice-Ratio'</code> <code>added_noise</code> <p>standard deviation of normally distributed noise added to y-values</p> required <code>resolution</code> <p>number of allowed values for stimulus DVs</p> <code>8</code> <code>maximum_similarity</code> <p>upperbound for DVs</p> <code>10</code> <code>focus</code> <p>parameter measuring participant focus</p> <code>0.8</code> <code>random_state</code> <p>integer used to seed the random number generator</p> required Shepard-Luce Choice Rule according to <ul> <li>Equation (4) in Logan, G. D., &amp; Gordon, R. D. (2001).</li> <li>and in Executive control of visual attention in dual-task situations.     Psychological review, 108(2), 393.</li> <li>Equation (5) in Luce, R. D. (1963). Detection and recognition.</li> </ul> <p>Examples:</p> <p>We can instantiate a Shepard-Cue Choice Experiment. We use a seed to get replicable results:</p> <pre><code>&gt;&gt;&gt; l_s_experiment = luce_choice_ratio()\n</code></pre> <p>We can look at the name of the experiment:</p> <pre><code>&gt;&gt;&gt; l_s_experiment.name\n'Luce-Choice-Ratio'\n</code></pre> <p>To call the ground truth, we can use an attribute of the experiment:</p> <pre><code>&gt;&gt;&gt; l_s_experiment.ground_truth(np.array([[1,2,3,4]]))\n   similarity_category_A1  ...  choose_A1\n0                       1  ...   0.210526\n\n[1 rows x 5 columns]\n</code></pre> <p>We can also run an experiment:</p> <pre><code>&gt;&gt;&gt; l_s_experiment.run(np.array([[1,2,3,4]]), random_state=42)\n   similarity_category_A1  ...  choose_A1\n0                       1  ...   0.211328\n\n[1 rows x 5 columns]\n</code></pre> <p>To plot the experiment use:</p> <pre><code>&gt;&gt;&gt; l_s_experiment.plotter()\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>autora/experiment_runner/synthetic/psychology/luce_choice_ratio.py</code> <pre><code>def luce_choice_ratio(\n    name=\"Luce-Choice-Ratio\",\n    resolution=8,\n    maximum_similarity=10,\n    focus=0.8,\n):\n    \"\"\"\n    Luce-Choice-Ratio\n\n    Args:\n        name: name of the experiment\n        added_noise: standard deviation of normally distributed noise added to y-values\n        resolution: number of allowed values for stimulus DVs\n        maximum_similarity: upperbound for DVs\n        focus: parameter measuring participant focus\n        random_state: integer used to seed the random number generator\n\n    Shepard-Luce Choice Rule according to:\n        - Equation (4) in Logan, G. D., &amp; Gordon, R. D. (2001).\n        - and in Executive control of visual attention in dual-task situations.\n            Psychological review, 108(2), 393.\n        - Equation (5) in Luce, R. D. (1963). Detection and recognition.\n\n    Examples:\n        We can instantiate a Shepard-Cue Choice Experiment. We use a seed to get replicable results:\n        &gt;&gt;&gt; l_s_experiment = luce_choice_ratio()\n\n        We can look at the name of the experiment:\n        &gt;&gt;&gt; l_s_experiment.name\n        'Luce-Choice-Ratio'\n\n        To call the ground truth, we can use an attribute of the experiment:\n        &gt;&gt;&gt; l_s_experiment.ground_truth(np.array([[1,2,3,4]]))\n           similarity_category_A1  ...  choose_A1\n        0                       1  ...   0.210526\n        &lt;BLANKLINE&gt;\n        [1 rows x 5 columns]\n\n        We can also run an experiment:\n        &gt;&gt;&gt; l_s_experiment.run(np.array([[1,2,3,4]]), random_state=42)\n           similarity_category_A1  ...  choose_A1\n        0                       1  ...   0.211328\n        &lt;BLANKLINE&gt;\n        [1 rows x 5 columns]\n\n        To plot the experiment use:\n        &gt;&gt;&gt; l_s_experiment.plotter()\n        &gt;&gt;&gt; plt.show()  # doctest: +SKIP\n\n    \"\"\"\n\n    minimum_similarity = 1 / maximum_similarity\n\n    params = dict(\n        name=name,\n        maximum_similarity=maximum_similarity,\n        minimum_similarity=minimum_similarity,\n        resolution=resolution,\n        focus=focus,\n    )\n\n    similarity_category_A1 = IV(\n        name=\"similarity_category_A1\",\n        allowed_values=np.linspace(minimum_similarity, maximum_similarity, resolution),\n        value_range=(minimum_similarity, maximum_similarity),\n        units=\"similarity\",\n        variable_label=\"Similarity with Category A1\",\n        type=ValueType.REAL,\n    )\n\n    similarity_category_A2 = IV(\n        name=\"similarity_category_A2\",\n        allowed_values=np.linspace(minimum_similarity, maximum_similarity, resolution),\n        value_range=(minimum_similarity, maximum_similarity),\n        units=\"similarity\",\n        variable_label=\"Similarity with Category A2\",\n        type=ValueType.REAL,\n    )\n\n    similarity_category_B1 = IV(\n        name=\"similarity_category_B1\",\n        allowed_values=np.linspace(minimum_similarity, maximum_similarity, resolution),\n        value_range=(minimum_similarity, maximum_similarity),\n        units=\"similarity\",\n        variable_label=\"Similarity with Category B1\",\n        type=ValueType.REAL,\n    )\n\n    similarity_category_B2 = IV(\n        name=\"similarity_category_B2\",\n        allowed_values=np.linspace(minimum_similarity, maximum_similarity, resolution),\n        value_range=(minimum_similarity, maximum_similarity),\n        units=\"similarity\",\n        variable_label=\"Similarity with Category B2\",\n        type=ValueType.REAL,\n    )\n\n    choose_A1 = DV(\n        name=\"choose_A1\",\n        value_range=(0, 1),\n        units=\"probability\",\n        variable_label=\"Probability of Choosing A1\",\n        type=ValueType.PROBABILITY,\n    )\n\n    variables = VariableCollection(\n        independent_variables=[\n            similarity_category_A1,\n            similarity_category_A2,\n            similarity_category_B1,\n            similarity_category_B2,\n        ],\n        dependent_variables=[choose_A1],\n    )\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        focus_: float = focus,\n        added_noise=0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n        Y = np.zeros((X.shape[0], 1))\n        for idx, x in enumerate(X):\n            similarity_A1 = x[0]\n            similarity_A2 = x[1]\n            similarity_B1 = x[2]\n            similarity_B2 = x[3]\n\n            y = (similarity_A1 * focus + rng.normal(0, added_noise)) / (\n                similarity_A1 * focus\n                + similarity_A2 * focus\n                + similarity_B1 * (1 - focus_)\n                + similarity_B2 * (1 - focus_)\n            )\n            # probability can't be negative or larger than 1 (the noise can make it so)\n            if y &lt;= 0:\n                y = 0.0001\n            elif y &gt;= 1:\n                y = 0.9999\n            Y[idx] = y\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[choose_A1.name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        similarity_A1 = variables.independent_variables[0].allowed_values\n        similarity_A2 = variables.independent_variables[1].allowed_values\n        similarity_B1 = variables.independent_variables[2].allowed_values\n        similarity_B2 = variables.independent_variables[3].allowed_values\n\n        X = np.array(\n            np.meshgrid(\n                similarity_A1,\n                similarity_A2,\n                similarity_B1,\n                similarity_B2,\n            )\n        ).T.reshape(-1, 4)\n\n        # remove all conditions from X where the focus is 0 and the similarity of A1 is 0\n        # or the similarity of A2 is 0\n        X = X[~((X[:, 0] == 0) &amp; (X[:, 1] == 0) &amp; (X[:, 2] == 0) &amp; (X[:, 3] == 0))]\n        return X\n\n    def plotter(\n        model=None,\n    ):\n        import matplotlib.colors as mcolors\n        import matplotlib.pyplot as plt\n\n        similarity_A1 = np.linspace(\n            variables.independent_variables[0].value_range[0],\n            variables.independent_variables[0].value_range[1],\n            100,\n        )\n\n        similarity_A2 = 0.5  # 1 - similarity_A1\n\n        similarity_B1_list = [0.5, 0.75, 1]\n        similarity_B2 = 0\n\n        colors = mcolors.TABLEAU_COLORS\n        col_keys = list(colors.keys())\n        for idx, similarity_B1 in enumerate(similarity_B1_list):\n            # similarity_B2 = 1 - similarity_B1\n            X = np.zeros((len(similarity_A1), 4))\n\n            X[:, 0] = similarity_A1\n            X[:, 1] = similarity_A2\n            X[:, 2] = similarity_B1\n            X[:, 3] = similarity_B2\n\n            y = ground_truth(X)[choose_A1.name]\n            plt.plot(\n                similarity_A1.reshape((len(similarity_A1), 1)),\n                y,\n                label=f\"Similarity to B1 = {similarity_B1} (Original)\",\n                c=colors[col_keys[idx]],\n            )\n\n            if model is not None:\n                y = model.predict(X)\n                plt.plot(\n                    similarity_A1,\n                    y,\n                    label=f\"Similarity to B1 = {similarity_B1} (Recovered)\",\n                    c=colors[col_keys[idx]],\n                    linestyle=\"--\",\n                )\n\n        x_limit = [np.min(similarity_A1), np.max(similarity_A1)]\n        y_limit = [0, 1]\n        x_label = \"Similarity to Category A1\"\n        y_label = \"Probability of Selecting Category A1\"\n\n        plt.xlim(x_limit)\n        plt.ylim(y_limit)\n        plt.xlabel(x_label, fontsize=\"large\")\n        plt.ylabel(y_label, fontsize=\"large\")\n        plt.legend(loc=4, fontsize=\"medium\")\n        plt.title(\"Shepard-Luce Choice Ratio\", fontsize=\"x-large\")\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=luce_choice_ratio.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=luce_choice_ratio,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/","title":"autora.experiment_runner.synthetic.psychology.q_learning","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.AgentQ","title":"<code>AgentQ</code>","text":"<p>An agent that runs simple Q-learning for an n-armed bandits tasks.</p> <p>Attributes:</p> Name Type Description <code>alpha</code> <p>The agent's learning rate</p> <code>beta</code> <p>The agent's softmax temperature</p> <code>q</code> <p>The agent's current estimate of the reward probability on each arm</p> Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>class AgentQ:\n    \"\"\"An agent that runs simple Q-learning for an n-armed bandits tasks.\n\n    Attributes:\n      alpha: The agent's learning rate\n      beta: The agent's softmax temperature\n      q: The agent's current estimate of the reward probability on each arm\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 0.2,\n        beta: float = 3.0,\n        n_actions: int = 2,\n        forget_rate: float = 0.0,\n        perseverance_bias: float = 0.0,\n        correlated_reward: bool = False,\n    ):\n        \"\"\"Update the agent after one step of the task.\n\n        Args:\n          alpha: scalar learning rate\n          beta: scalar softmax inverse temperature parameter.\n          n_actions: number of actions (default=2)\n          forgetting_rate: rate at which q values decay toward the initial values (default=0)\n          perseveration_bias: rate at which q values move toward previous action (default=0)\n        \"\"\"\n        self._prev_choice = -1\n        self._alpha = alpha\n        self._beta = beta\n        self._n_actions = n_actions\n        self._forget_rate = forget_rate\n        self._perseverance_bias = perseverance_bias\n        self._correlated_reward = correlated_reward\n        self._q_init = 0.5\n        self.new_sess()\n\n        _check_in_0_1_range(alpha, \"alpha\")\n        _check_in_0_1_range(forget_rate, \"forget_rate\")\n\n    def new_sess(self):\n        \"\"\"Reset the agent for the beginning of a new session.\"\"\"\n        self._q = self._q_init * np.ones(self._n_actions)\n        self._prev_choice = -1\n\n    def get_choice_probs(self) -&gt; np.ndarray:\n        \"\"\"Compute the choice probabilities as softmax over q.\"\"\"\n        decision_variable = np.exp(self.q * self._beta)\n        choice_probs = decision_variable / np.sum(decision_variable)\n        return choice_probs\n\n    def get_choice(self) -&gt; int:\n        \"\"\"Sample a choice, given the agent's current internal state.\"\"\"\n        choice_probs = self.get_choice_probs()\n        choice = np.random.choice(self._n_actions, p=choice_probs)\n        return choice\n\n    def update(self, choice: int, reward: float):\n        \"\"\"Update the agent after one step of the task.\n\n        Args:\n          choice: The choice made by the agent. 0 or 1\n          reward: The reward received by the agent. 0 or 1\n        \"\"\"\n\n        # Forgetting - restore q-values of non-chosen actions towards the initial value\n        non_chosen_action = np.arange(self._n_actions) != choice\n        self._q[non_chosen_action] = (1 - self._forget_rate) * self._q[\n            non_chosen_action\n        ] + self._forget_rate * self._q_init\n\n        # Reward-based update - Update chosen q for chosen action with observed reward\n        q_reward_update = -self._alpha * self._q[choice] + self._alpha * reward\n\n        # Correlated update - Update non-chosen q for non-chosen action with observed reward\n        if self._correlated_reward:\n            # index_correlated_update = self._n_actions - choice - 1\n            # self._q[index_correlated_update] =\n            # (1 - self._alpha) * self._q[index_correlated_update] + self._alpha * (1 - reward)\n            # alternative implementation - not dependent on reward but on reward-based update\n            index_correlated_update = self._n_actions - 1 - choice\n            self._q[index_correlated_update] -= 0.5 * q_reward_update\n\n        # Memorize current choice for perseveration\n        self._prev_choice = choice\n\n        self._q[choice] += q_reward_update\n\n    @property\n    def q(self):\n        q = self._q.copy()\n        if self._prev_choice != -1:\n            q[self._prev_choice] += self._perseverance_bias\n        return q\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.AgentQ.__init__","title":"<code>__init__(alpha=0.2, beta=3.0, n_actions=2, forget_rate=0.0, perseverance_bias=0.0, correlated_reward=False)</code>","text":"<p>Update the agent after one step of the task.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>scalar learning rate</p> <code>0.2</code> <code>beta</code> <code>float</code> <p>scalar softmax inverse temperature parameter.</p> <code>3.0</code> <code>n_actions</code> <code>int</code> <p>number of actions (default=2)</p> <code>2</code> <code>forgetting_rate</code> <p>rate at which q values decay toward the initial values (default=0)</p> required <code>perseveration_bias</code> <p>rate at which q values move toward previous action (default=0)</p> required Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>def __init__(\n    self,\n    alpha: float = 0.2,\n    beta: float = 3.0,\n    n_actions: int = 2,\n    forget_rate: float = 0.0,\n    perseverance_bias: float = 0.0,\n    correlated_reward: bool = False,\n):\n    \"\"\"Update the agent after one step of the task.\n\n    Args:\n      alpha: scalar learning rate\n      beta: scalar softmax inverse temperature parameter.\n      n_actions: number of actions (default=2)\n      forgetting_rate: rate at which q values decay toward the initial values (default=0)\n      perseveration_bias: rate at which q values move toward previous action (default=0)\n    \"\"\"\n    self._prev_choice = -1\n    self._alpha = alpha\n    self._beta = beta\n    self._n_actions = n_actions\n    self._forget_rate = forget_rate\n    self._perseverance_bias = perseverance_bias\n    self._correlated_reward = correlated_reward\n    self._q_init = 0.5\n    self.new_sess()\n\n    _check_in_0_1_range(alpha, \"alpha\")\n    _check_in_0_1_range(forget_rate, \"forget_rate\")\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.AgentQ.get_choice","title":"<code>get_choice()</code>","text":"<p>Sample a choice, given the agent's current internal state.</p> Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>def get_choice(self) -&gt; int:\n    \"\"\"Sample a choice, given the agent's current internal state.\"\"\"\n    choice_probs = self.get_choice_probs()\n    choice = np.random.choice(self._n_actions, p=choice_probs)\n    return choice\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.AgentQ.get_choice_probs","title":"<code>get_choice_probs()</code>","text":"<p>Compute the choice probabilities as softmax over q.</p> Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>def get_choice_probs(self) -&gt; np.ndarray:\n    \"\"\"Compute the choice probabilities as softmax over q.\"\"\"\n    decision_variable = np.exp(self.q * self._beta)\n    choice_probs = decision_variable / np.sum(decision_variable)\n    return choice_probs\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.AgentQ.new_sess","title":"<code>new_sess()</code>","text":"<p>Reset the agent for the beginning of a new session.</p> Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>def new_sess(self):\n    \"\"\"Reset the agent for the beginning of a new session.\"\"\"\n    self._q = self._q_init * np.ones(self._n_actions)\n    self._prev_choice = -1\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.AgentQ.update","title":"<code>update(choice, reward)</code>","text":"<p>Update the agent after one step of the task.</p> <p>Parameters:</p> Name Type Description Default <code>choice</code> <code>int</code> <p>The choice made by the agent. 0 or 1</p> required <code>reward</code> <code>float</code> <p>The reward received by the agent. 0 or 1</p> required Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>def update(self, choice: int, reward: float):\n    \"\"\"Update the agent after one step of the task.\n\n    Args:\n      choice: The choice made by the agent. 0 or 1\n      reward: The reward received by the agent. 0 or 1\n    \"\"\"\n\n    # Forgetting - restore q-values of non-chosen actions towards the initial value\n    non_chosen_action = np.arange(self._n_actions) != choice\n    self._q[non_chosen_action] = (1 - self._forget_rate) * self._q[\n        non_chosen_action\n    ] + self._forget_rate * self._q_init\n\n    # Reward-based update - Update chosen q for chosen action with observed reward\n    q_reward_update = -self._alpha * self._q[choice] + self._alpha * reward\n\n    # Correlated update - Update non-chosen q for non-chosen action with observed reward\n    if self._correlated_reward:\n        # index_correlated_update = self._n_actions - choice - 1\n        # self._q[index_correlated_update] =\n        # (1 - self._alpha) * self._q[index_correlated_update] + self._alpha * (1 - reward)\n        # alternative implementation - not dependent on reward but on reward-based update\n        index_correlated_update = self._n_actions - 1 - choice\n        self._q[index_correlated_update] -= 0.5 * q_reward_update\n\n    # Memorize current choice for perseveration\n    self._prev_choice = choice\n\n    self._q[choice] += q_reward_update\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning","title":"<code>q_learning(name='Q-Learning', learning_rate=0.2, decision_noise=3.0, n_actions=2, forget_rate=0.0, perseverance_bias=0.0, correlated_reward=False)</code>","text":"<p>An agent that runs simple Q-learning for an n-armed bandits tasks.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the experiment</p> <code>'Q-Learning'</code> <code>trials</code> <p>number of trials</p> required <code>learning_rate</code> <code>float</code> <p>learning rate for Q-learning</p> <code>0.2</code> <code>decision_noise</code> <code>float</code> <p>softmax parameter for decision noise</p> <code>3.0</code> <code>n_actions</code> <code>int</code> <p>number of actions</p> <code>2</code> <code>forget_rate</code> <code>float</code> <p>rate of forgetting</p> <code>0.0</code> <code>perseverance_bias</code> <code>float</code> <p>bias towards choosing the previously chosen action</p> <code>0.0</code> <code>correlated_reward</code> <code>bool</code> <p>whether rewards are correlated</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; experiment = q_learning()\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning--the-runner-can-accept-numpy-arrays-or-pandas-dataframes-but-the-return-value-will","title":"The runner can accept numpy arrays or pandas DataFrames, but the return value will","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning--always-be-a-list-of-numpy-arrays-each-array-corresponds-to-the-choices-made-by-the-agent","title":"always be a list of numpy arrays. Each array corresponds to the choices made by the agent","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning--for-each-trial-in-the-input-thus-arrays-have-shape-n_trials-n_actions","title":"for each trial in the input. Thus, arrays have shape (n_trials, n_actions).","text":"<pre><code>&gt;&gt;&gt; experiment.run(np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]),\n...                random_state=42)\n[array([[1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.]])]\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning--the-runner-can-accept-pandas-dataframes-each-cell-of-the-dataframe-should-contain-a","title":"The runner can accept pandas DataFrames. Each cell of the DataFrame should contain a","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning--numpy-array-with-shape-n_trials-n_actions-the-return-value-will-be-a-list-of-numpy","title":"numpy array with shape (n_trials, n_actions). The return value will be a list of numpy","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychology/q_learning/#autora.experiment_runner.synthetic.psychology.q_learning.q_learning--arrays-each-corresponding-to-the-choices-made-by-the-agent-for-each-trial-in-the-input","title":"arrays, each corresponding to the choices made by the agent for each trial in the input.","text":"<pre><code>&gt;&gt;&gt; experiment.run(\n...     pd.DataFrame(\n...         {'reward array': [np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]])]}),\n...     random_state = 42)\n[array([[1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.]])]\n</code></pre> Source code in <code>autora/experiment_runner/synthetic/psychology/q_learning.py</code> <pre><code>def q_learning(\n    name=\"Q-Learning\",\n    learning_rate: float = 0.2,\n    decision_noise: float = 3.0,\n    n_actions: int = 2,\n    forget_rate: float = 0.0,\n    perseverance_bias: float = 0.0,\n    correlated_reward: bool = False,\n):\n    \"\"\"\n    An agent that runs simple Q-learning for an n-armed bandits tasks.\n\n    Args:\n        name: name of the experiment\n        trials: number of trials\n        learning_rate: learning rate for Q-learning\n        decision_noise: softmax parameter for decision noise\n        n_actions: number of actions\n        forget_rate: rate of forgetting\n        perseverance_bias: bias towards choosing the previously chosen action\n        correlated_reward: whether rewards are correlated\n\n    Examples:\n        &gt;&gt;&gt; experiment = q_learning()\n\n        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n        # always be a list of numpy arrays. Each array corresponds to the choices made by the agent\n        # for each trial in the input. Thus, arrays have shape (n_trials, n_actions).\n        &gt;&gt;&gt; experiment.run(np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]),\n        ...                random_state=42)\n        [array([[1., 0.],\n               [0., 1.],\n               [0., 1.],\n               [0., 1.],\n               [1., 0.],\n               [1., 0.]])]\n\n        # The runner can accept pandas DataFrames. Each cell of the DataFrame should contain a\n        # numpy array with shape (n_trials, n_actions). The return value will be a list of numpy\n        # arrays, each corresponding to the choices made by the agent for each trial in the input.\n        &gt;&gt;&gt; experiment.run(\n        ...     pd.DataFrame(\n        ...         {'reward array': [np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]])]}),\n        ...     random_state = 42)\n        [array([[1., 0.],\n               [0., 1.],\n               [0., 1.],\n               [0., 1.],\n               [1., 0.],\n               [1., 0.]])]\n    \"\"\"\n\n    params = dict(\n        name=name,\n        trials=100,\n        learning_rate=learning_rate,\n        decision_noise=decision_noise,\n        n_actions=n_actions,\n        forget_rate=forget_rate,\n        perseverance_bias=perseverance_bias,\n        correlated_reward=correlated_reward,\n    )\n\n    iv1 = IV(\n        name=\"reward array\",\n        units=\"reward\",\n        variable_label=\"Reward Sequence\",\n        type=ValueType.BOOLEAN,\n    )\n\n    dv1 = DV(\n        name=\"choice array\",\n        units=\"actions\",\n        variable_label=\"Action Sequence\",\n        type=ValueType.REAL,\n    )\n\n    variables = VariableCollection(\n        independent_variables=[iv1],\n        dependent_variables=[dv1],\n    )\n\n    def run_AgentQ(rewards):\n        if rewards.shape[1] != n_actions:\n            Warning(\n                \"Number of actions in rewards does not match n_actions. Will use \"\n                + str(rewards.shape[1] + \" actions.\")\n            )\n        num_trials = rewards.shape[0]\n\n        y = np.zeros(rewards.shape)\n        choice_proba = np.zeros(rewards.shape)\n\n        agent = AgentQ(\n            alpha=learning_rate,\n            beta=decision_noise,\n            n_actions=rewards.shape[1],\n            forget_rate=forget_rate,\n            perseverance_bias=perseverance_bias,\n            correlated_reward=correlated_reward,\n        )\n\n        for i in range(num_trials):\n            proba = agent.get_choice_probs()\n            choice = agent.get_choice()\n            y[i, choice] = 1\n            choice_proba[i] = proba\n            reward = rewards[i, choice]\n            agent.update(choice, reward)\n        return y, choice_proba\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        random_state: Optional[int] = None,\n        return_choice_probabilities=False,\n    ):\n        if random_state is not None:\n            np.random.seed(random_state)\n\n        Y = list()\n        Y_proba = list()\n        if isinstance(conditions, pd.DataFrame):\n            for index, session in conditions.iterrows():\n                rewards = session[0]\n                choice, choice_proba = run_AgentQ(rewards)\n                Y.append(choice)\n                Y_proba.append(choice_proba)\n        elif isinstance(conditions, np.ndarray):\n            choice, choice_proba = run_AgentQ(conditions)\n            Y.append(choice)\n            Y_proba.append(choice_proba)\n\n        if return_choice_probabilities:\n            return Y, Y_proba\n        else:\n            return Y\n\n    ground_truth = partial(run)\n\n    def domain():\n        return None\n\n    def plotter():\n        raise NotImplementedError\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=q_learning.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=q_learning,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychophysics/stevens_power_law/","title":"autora.experiment_runner.synthetic.psychophysics.stevens_power_law","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychophysics/stevens_power_law/#autora.experiment_runner.synthetic.psychophysics.stevens_power_law.stevens_power_law","title":"<code>stevens_power_law(name=\"Stevens' Power Law\", resolution=100, proportionality_constant=1.0, modality_constant=0.8, maximum_stimulus_intensity=5.0)</code>","text":"<p>Stevens' Power Law</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the experiment</p> <code>\"Stevens' Power Law\"</code> <code>resolution</code> <p>number of allowed values for stimulus</p> <code>100</code> <code>modality_constant</code> <p>power constant</p> <code>0.8</code> <code>proportionality_constant</code> <p>constant multiplier</p> <code>1.0</code> <code>maximum_stimulus_intensity</code> <p>maximum value for stimulus</p> <code>5.0</code> <p>Examples:     &gt;&gt;&gt; s = stevens_power_law()     &gt;&gt;&gt; s.run(np.array([[.9]]), random_state=42)          S  perceived_intensity     0  0.9             0.922213</p> Source code in <code>autora/experiment_runner/synthetic/psychophysics/stevens_power_law.py</code> <pre><code>def stevens_power_law(\n    name=\"Stevens' Power Law\",\n    resolution=100,\n    proportionality_constant=1.0,\n    modality_constant=0.8,\n    maximum_stimulus_intensity=5.0,\n):\n    \"\"\"\n    Stevens' Power Law\n\n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus\n        modality_constant: power constant\n        proportionality_constant: constant multiplier\n        maximum_stimulus_intensity: maximum value for stimulus\n    Examples:\n        &gt;&gt;&gt; s = stevens_power_law()\n        &gt;&gt;&gt; s.run(np.array([[.9]]), random_state=42)\n             S  perceived_intensity\n        0  0.9             0.922213\n    \"\"\"\n\n    params = dict(\n        name=name,\n        resolution=resolution,\n        proportionality_constant=proportionality_constant,\n        modality_constant=modality_constant,\n        maximum_stimulus_intensity=maximum_stimulus_intensity,\n    )\n\n    iv1 = IV(\n        name=\"S\",\n        allowed_values=np.linspace(\n            1 / resolution, maximum_stimulus_intensity, resolution\n        ),\n        value_range=(1 / resolution, maximum_stimulus_intensity),\n        units=\"intensity\",\n        variable_label=\"Stimulus Intensity\",\n        type=ValueType.REAL,\n    )\n\n    dv1 = DV(\n        name=\"perceived_intensity\",\n        value_range=(0, maximum_stimulus_intensity),\n        units=\"sensation\",\n        variable_label=\"Perceived Intensity\",\n        type=ValueType.REAL,\n    )\n\n    variables = VariableCollection(\n        independent_variables=[iv1],\n        dependent_variables=[dv1],\n    )\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise: float = 0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n        Y = np.zeros((X.shape[0], 1))\n        for idx, x in enumerate(X):\n            y = proportionality_constant * x[0] ** modality_constant + rng.normal(\n                0, added_noise\n            )\n            Y[idx] = y\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[variables.dependent_variables[0].name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        s_values = variables.independent_variables[0].allowed_values\n\n        X = np.array(np.meshgrid(s_values)).T.reshape(-1, 1)\n        return X\n\n    def plotter(\n        model=None,\n    ):\n        import matplotlib.colors as mcolors\n        import matplotlib.pyplot as plt\n\n        colors = mcolors.TABLEAU_COLORS\n        col_keys = list(colors.keys())\n        X = domain()\n        y = ground_truth(X)[dv1.name]\n        plt.plot(X, y, label=\"Original\", c=colors[col_keys[0]])\n        if model is not None:\n            y = model.predict(X)\n            plt.plot(X, y, label=\"Recovered\", c=colors[col_keys[0]], linestyle=\"--\")\n        x_limit = [0, variables.independent_variables[0].value_range[1]]\n        y_limit = [0, 4]\n        x_label = \"Stimulus Intensity\"\n        y_label = \"Perceived Stimulus Intensity\"\n\n        plt.xlim(x_limit)\n        plt.ylim(y_limit)\n        plt.xlabel(x_label, fontsize=\"large\")\n        plt.ylabel(y_label, fontsize=\"large\")\n        plt.legend(loc=2, fontsize=\"medium\")\n        plt.title(\"Stevens' Power Law\", fontsize=\"x-large\")\n        plt.show()\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=stevens_power_law.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=stevens_power_law,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychophysics/weber_fechner_law/","title":"autora.experiment_runner.synthetic.psychophysics.weber_fechner_law","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychophysics/weber_fechner_law/#autora.experiment_runner.synthetic.psychophysics.weber_fechner_law.weber_fechner_law","title":"<code>weber_fechner_law(name='Weber-Fechner Law', resolution=100, constant=1.0, maximum_stimulus_intensity=5.0)</code>","text":"<p>Weber-Fechner Law</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the experiment</p> <code>'Weber-Fechner Law'</code> <code>resolution</code> <p>number of allowed values for stimulus 1 and 2</p> <code>100</code> <code>constant</code> <p>constant multiplier</p> <code>1.0</code> <code>maximum_stimulus_intensity</code> <p>maximum value for stimulus 1 and 2</p> <code>5.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; experiment = weber_fechner_law()\n</code></pre>"},{"location":"reference/autora/experiment_runner/synthetic/psychophysics/weber_fechner_law/#autora.experiment_runner.synthetic.psychophysics.weber_fechner_law.weber_fechner_law--the-runner-can-accept-numpy-arrays-or-pandas-dataframes-but-the-return-value-will","title":"The runner can accept numpy arrays or pandas DataFrames, but the return value will","text":""},{"location":"reference/autora/experiment_runner/synthetic/psychophysics/weber_fechner_law/#autora.experiment_runner.synthetic.psychophysics.weber_fechner_law.weber_fechner_law--always-be-a-pandas-dataframe","title":"always be a pandas DataFrame.","text":"<pre><code>&gt;&gt;&gt; experiment.run(np.array([[.1,.2]]), random_state=42)\n    S1   S2  difference_detected\n0  0.1  0.2             0.696194\n</code></pre> <pre><code>&gt;&gt;&gt; experiment.run(pd.DataFrame({'S1': [0.1], 'S2': [0.2]}), random_state=42)\n    S1   S2  difference_detected\n0  0.1  0.2             0.696194\n</code></pre> Source code in <code>autora/experiment_runner/synthetic/psychophysics/weber_fechner_law.py</code> <pre><code>def weber_fechner_law(\n    name=\"Weber-Fechner Law\",\n    resolution=100,\n    constant=1.0,\n    maximum_stimulus_intensity=5.0,\n):\n    \"\"\"\n    Weber-Fechner Law\n\n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus 1 and 2\n        constant: constant multiplier\n        maximum_stimulus_intensity: maximum value for stimulus 1 and 2\n\n    Examples:\n        &gt;&gt;&gt; experiment = weber_fechner_law()\n\n        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n        # always be a pandas DataFrame.\n        &gt;&gt;&gt; experiment.run(np.array([[.1,.2]]), random_state=42)\n            S1   S2  difference_detected\n        0  0.1  0.2             0.696194\n\n        &gt;&gt;&gt; experiment.run(pd.DataFrame({'S1': [0.1], 'S2': [0.2]}), random_state=42)\n            S1   S2  difference_detected\n        0  0.1  0.2             0.696194\n\n    \"\"\"\n\n    params = dict(\n        name=name,\n        resolution=resolution,\n        constant=constant,\n        maximum_stimulus_intensity=maximum_stimulus_intensity,\n    )\n\n    iv1 = IV(\n        name=\"S1\",\n        allowed_values=np.linspace(\n            1 / resolution, maximum_stimulus_intensity, resolution\n        ),\n        value_range=(1 / resolution, maximum_stimulus_intensity),\n        units=\"intensity\",\n        variable_label=\"Stimulus 1 Intensity\",\n        type=ValueType.REAL,\n    )\n\n    iv2 = IV(\n        name=\"S2\",\n        allowed_values=np.linspace(\n            1 / resolution, maximum_stimulus_intensity, resolution\n        ),\n        value_range=(1 / resolution, maximum_stimulus_intensity),\n        units=\"intensity\",\n        variable_label=\"Stimulus 2 Intensity\",\n        type=ValueType.REAL,\n    )\n\n    dv1 = DV(\n        name=\"difference_detected\",\n        value_range=(0, maximum_stimulus_intensity),\n        units=\"sensation\",\n        variable_label=\"Sensation\",\n        type=ValueType.REAL,\n    )\n\n    variables = VariableCollection(\n        independent_variables=[iv1, iv2],\n        dependent_variables=[dv1],\n    )\n\n    def run(\n        conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n        added_noise=0.01,\n        random_state: Optional[int] = None,\n    ):\n        rng = np.random.default_rng(random_state)\n        X = np.array(conditions)\n\n        Y = np.zeros((X.shape[0], 1))\n        for idx, x in enumerate(X):\n            y = constant * np.log(x[1] / x[0]) + rng.normal(0, added_noise)\n            Y[idx] = y\n\n        experiment_data = pd.DataFrame(conditions)\n        experiment_data.columns = [v.name for v in variables.independent_variables]\n        experiment_data[dv1.name] = Y\n        return experiment_data\n\n    ground_truth = partial(run, added_noise=0.0)\n\n    def domain():\n        s1_values = variables.independent_variables[0].allowed_values\n        s2_values = variables.independent_variables[1].allowed_values\n        X = np.array(np.meshgrid(s1_values, s2_values)).T.reshape(-1, 2)\n        # remove all combinations where s1 &gt; s2\n        X = X[X[:, 0] &lt;= X[:, 1]]\n        return X\n\n    def plotter(\n        model=None,\n    ):\n        import matplotlib.colors as mcolors\n        import matplotlib.pyplot as plt\n\n        colors = mcolors.TABLEAU_COLORS\n        col_keys = list(colors.keys())\n\n        S0_list = [1, 2, 4]\n        delta_S = np.linspace(0, 5, 100)\n\n        for idx, S0_value in enumerate(S0_list):\n            S0 = S0_value + np.zeros(delta_S.shape)\n            S1 = S0 + delta_S\n            X = np.array([S0, S1]).T\n            y = ground_truth(X)[dv1.name]\n            plt.plot(\n                delta_S,\n                y,\n                label=f\"$S_0 = {S0_value}$ (Original)\",\n                c=colors[col_keys[idx]],\n            )\n            if model is not None:\n                y = model.predict(X)\n                plt.plot(\n                    delta_S,\n                    y,\n                    label=f\"$S_0 = {S0_value}$ (Recovered)\",\n                    c=colors[col_keys[idx]],\n                    linestyle=\"--\",\n                )\n\n        x_limit = [0, variables.independent_variables[0].value_range[1]]\n        y_limit = [0, 2]\n        x_label = r\"Stimulus Intensity Difference $\\Delta S = S_1 - S_0$\"\n        y_label = \"Perceived Intensity of Stimulus $S_1$\"\n\n        plt.xlim(x_limit)\n        plt.ylim(y_limit)\n        plt.xlabel(x_label, fontsize=\"large\")\n        plt.ylabel(y_label, fontsize=\"large\")\n        plt.legend(loc=2, fontsize=\"medium\")\n        plt.title(\"Weber-Fechner Law\", fontsize=\"x-large\")\n\n    collection = SyntheticExperimentCollection(\n        name=name,\n        description=weber_fechner_law.__doc__,\n        variables=variables,\n        run=run,\n        ground_truth=ground_truth,\n        domain=domain,\n        plotter=plotter,\n        params=params,\n        factory_function=weber_fechner_law,\n    )\n    return collection\n</code></pre>"},{"location":"reference/autora/experimentalist/grid/","title":"autora.experimentalist.grid","text":"<p>Tools to make grids of experimental conditions.</p>"},{"location":"reference/autora/experimentalist/grid/#autora.experimentalist.grid.grid_pool","title":"<code>grid_pool = pool</code>  <code>module-attribute</code>","text":"<p>Alias for pool</p>"},{"location":"reference/autora/experimentalist/grid/#autora.experimentalist.grid.pool","title":"<code>pool(variables)</code>","text":"<p>Creates exhaustive pool of conditions given a definition of variables with allowed_values.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>VariableCollection</code> <p>a VariableCollection with <code>independent_variables</code> \u2013 a sequence of Variable objects, each of which has an attribute <code>allowed_values</code> containing a sequence of values.</p> required <p>Returns: a Result / Delta object with the conditions as a pd.DataFrame in the <code>conditions</code> field</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from autora.state import State\n&gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n</code></pre> <p>With one independent variable \"x\", and some allowed values, we get exactly those values back when running the experimentalist:</p> <pre><code>&gt;&gt;&gt; pool(VariableCollection(\n...     independent_variables=[Variable(name=\"x\", allowed_values=[1, 2, 3])]\n... ))\n   x\n0  1\n1  2\n2  3\n</code></pre> <p>The allowed_values must be specified:</p> <pre><code>&gt;&gt;&gt; pool(VariableCollection(independent_variables=[Variable(name=\"x\")]))\nTraceback (most recent call last):\n...\nAssertionError: grid_pool only supports independent variables with discrete...\n</code></pre> <p>With two independent variables, we get the cartesian product:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(independent_variables=[\n...         Variable(name=\"x1\", allowed_values=[1, 2]),\n...         Variable(name=\"x2\", allowed_values=[3, 4]),\n... ]))\n   x1  x2\n0   1   3\n1   1   4\n2   2   3\n3   2   4\n</code></pre> <p>If any of the variables have unspecified allowed_values, we get an error:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(independent_variables=[\n...         Variable(name=\"x1\", allowed_values=[1, 2]),\n...         Variable(name=\"x2\"),\n... ]))\nTraceback (most recent call last):\n...\nAssertionError: grid_pool only supports independent variables with discrete...\n</code></pre> <p>We can specify arrays of allowed values:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(independent_variables=[\n...         Variable(name=\"x\", allowed_values=np.linspace(-10, 10, 101)),\n...         Variable(name=\"y\", allowed_values=[3, 4]),\n...         Variable(name=\"z\", allowed_values=np.linspace(20, 30, 11)),\n... ]))\n         x  y     z\n0    -10.0  3  20.0\n1    -10.0  3  21.0\n2    -10.0  3  22.0\n3    -10.0  3  23.0\n4    -10.0  3  24.0\n...    ... ..   ...\n2217  10.0  4  26.0\n2218  10.0  4  27.0\n2219  10.0  4  28.0\n2220  10.0  4  29.0\n2221  10.0  4  30.0\n\n[2222 rows x 3 columns]\n</code></pre> Source code in <code>autora/experimentalist/grid.py</code> <pre><code>def pool(variables: VariableCollection) -&gt; pd.DataFrame:\n    \"\"\"Creates exhaustive pool of conditions given a definition of variables with allowed_values.\n\n    Args:\n        variables: a VariableCollection with `independent_variables` \u2013 a sequence of Variable\n            objects, each of which has an attribute `allowed_values` containing a sequence of\n            values.\n\n    Returns: a Result / Delta object with the conditions as a pd.DataFrame in the `conditions` field\n\n    Examples:\n        &gt;&gt;&gt; from autora.state import State\n        &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n\n        With one independent variable \"x\", and some allowed values, we get exactly those values\n        back when running the experimentalist:\n        &gt;&gt;&gt; pool(VariableCollection(\n        ...     independent_variables=[Variable(name=\"x\", allowed_values=[1, 2, 3])]\n        ... ))\n           x\n        0  1\n        1  2\n        2  3\n\n        The allowed_values must be specified:\n        &gt;&gt;&gt; pool(VariableCollection(independent_variables=[Variable(name=\"x\")]))\n        Traceback (most recent call last):\n        ...\n        AssertionError: grid_pool only supports independent variables with discrete...\n\n        With two independent variables, we get the cartesian product:\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(independent_variables=[\n        ...         Variable(name=\"x1\", allowed_values=[1, 2]),\n        ...         Variable(name=\"x2\", allowed_values=[3, 4]),\n        ... ]))\n           x1  x2\n        0   1   3\n        1   1   4\n        2   2   3\n        3   2   4\n\n        If any of the variables have unspecified allowed_values, we get an error:\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(independent_variables=[\n        ...         Variable(name=\"x1\", allowed_values=[1, 2]),\n        ...         Variable(name=\"x2\"),\n        ... ]))\n        Traceback (most recent call last):\n        ...\n        AssertionError: grid_pool only supports independent variables with discrete...\n\n\n        We can specify arrays of allowed values:\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(independent_variables=[\n        ...         Variable(name=\"x\", allowed_values=np.linspace(-10, 10, 101)),\n        ...         Variable(name=\"y\", allowed_values=[3, 4]),\n        ...         Variable(name=\"z\", allowed_values=np.linspace(20, 30, 11)),\n        ... ]))\n                 x  y     z\n        0    -10.0  3  20.0\n        1    -10.0  3  21.0\n        2    -10.0  3  22.0\n        3    -10.0  3  23.0\n        4    -10.0  3  24.0\n        ...    ... ..   ...\n        2217  10.0  4  26.0\n        2218  10.0  4  27.0\n        2219  10.0  4  28.0\n        2220  10.0  4  29.0\n        2221  10.0  4  30.0\n        &lt;BLANKLINE&gt;\n        [2222 rows x 3 columns]\n\n    \"\"\"\n    ivs = variables.independent_variables\n    # Get allowed values for each IV\n    l_iv_values = []\n    l_iv_names = []\n    for iv in ivs:\n        assert iv.allowed_values is not None, (\n            f\"grid_pool only supports independent variables with discrete allowed values, \"\n            f\"but allowed_values is None on {iv=} \"\n        )\n        l_iv_values.append(iv.allowed_values)\n        l_iv_names.append(iv.name)\n\n    # Return Cartesian product of all IV values\n    pool = product(*l_iv_values)\n    conditions = pd.DataFrame(pool, columns=l_iv_names)\n\n    return conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/","title":"autora.experimentalist.pipeline","text":"<p>Provides tools to chain functions used to create experiment sequences.</p>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.Pipe","title":"<code>Pipe</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Takes in an _ExperimentalSequence and modifies it before returning it.</p> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>@runtime_checkable\nclass Pipe(Protocol):\n    \"\"\"Takes in an _ExperimentalSequence and modifies it before returning it.\"\"\"\n\n    def __call__(self, ex: _ExperimentalSequence) -&gt; _ExperimentalSequence:\n        ...\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Processes (\"pipelines\") a series of ExperimentalSequences through a pipeline.</p> <p>Examples:</p> <p>A pipeline which filters even values 0 to 9:</p> <pre><code>&gt;&gt;&gt; p = Pipeline(\n... [(\"is_even\", lambda values: filter(lambda i: i % 2 == 0, values))]  # a \"pipe\" function\n... )\n&gt;&gt;&gt; list(p(range(10)))\n[0, 2, 4, 6, 8]\n</code></pre> <p>A pipeline which filters for square, odd numbers:</p> <pre><code>&gt;&gt;&gt; from math import sqrt\n&gt;&gt;&gt; p = Pipeline([\n... (\"is_odd\", lambda values: filter(lambda i: i % 2 != 0, values)),\n... (\"is_sqrt\", lambda values: filter(lambda i: sqrt(i) % 1 == 0., values))\n... ])\n&gt;&gt;&gt; list(p(range(100)))\n[1, 9, 25, 49, 81]\n</code></pre> <pre><code>&gt;&gt;&gt; from itertools import product\n&gt;&gt;&gt; Pipeline([(\"pool\", lambda: product(range(5), [\"a\", \"b\"]))])\nPipeline(steps=[('pool', &lt;function &lt;lambda&gt; at 0x...&gt;)], params={})\n</code></pre> <pre><code>&gt;&gt;&gt; Pipeline([\n... (\"pool\", lambda: product(range(5), [\"a\", \"b\"])),\n... (\"filter\", lambda values: filter(lambda i: i[0] % 2 == 0, values))\n... ])\nPipeline(steps=[('pool', &lt;function &lt;lambda&gt; at 0x...&gt;),         ('filter', &lt;function &lt;lambda&gt; at 0x...&gt;)],         params={})\n</code></pre> <pre><code>&gt;&gt;&gt; pipeline = Pipeline([\n... (\"pool\", lambda maximum: product(range(maximum), [\"a\", \"b\"])),\n... (\"filter\", lambda values, divisor: filter(lambda i: i[0] % divisor == 0, values))\n... ] ,\n... params = {\"pool\": {\"maximum\":5}, \"filter\": {\"divisor\": 2}})\n&gt;&gt;&gt; pipeline\nPipeline(steps=[('pool', &lt;function &lt;lambda&gt; at 0x...&gt;),         ('filter', &lt;function &lt;lambda&gt; at 0x...&gt;)],         params={'pool': {'maximum': 5}, 'filter': {'divisor': 2}})\n&gt;&gt;&gt; list(pipeline.run())\n[(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b'), (4, 'a'), (4, 'b')]\n</code></pre> <pre><code>&gt;&gt;&gt; pipeline.params = {\"pool\": {\"maximum\":7}, \"filter\": {\"divisor\": 3}}\n&gt;&gt;&gt; list(pipeline())\n[(0, 'a'), (0, 'b'), (3, 'a'), (3, 'b'), (6, 'a'), (6, 'b')]\n</code></pre> <pre><code>&gt;&gt;&gt; pipeline.params = {\"pool\": {\"maximum\":7}}\n&gt;&gt;&gt; list(pipeline())\nTraceback (most recent call last):\n...\nTypeError: &lt;lambda&gt;() missing 1 required positional argument: 'divisor'\n</code></pre> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Processes (\"pipelines\") a series of ExperimentalSequences through a pipeline.\n\n    Examples:\n        A pipeline which filters even values 0 to 9:\n        &gt;&gt;&gt; p = Pipeline(\n        ... [(\"is_even\", lambda values: filter(lambda i: i % 2 == 0, values))]  # a \"pipe\" function\n        ... )\n        &gt;&gt;&gt; list(p(range(10)))\n        [0, 2, 4, 6, 8]\n\n        A pipeline which filters for square, odd numbers:\n        &gt;&gt;&gt; from math import sqrt\n        &gt;&gt;&gt; p = Pipeline([\n        ... (\"is_odd\", lambda values: filter(lambda i: i % 2 != 0, values)),\n        ... (\"is_sqrt\", lambda values: filter(lambda i: sqrt(i) % 1 == 0., values))\n        ... ])\n        &gt;&gt;&gt; list(p(range(100)))\n        [1, 9, 25, 49, 81]\n\n\n        &gt;&gt;&gt; from itertools import product\n        &gt;&gt;&gt; Pipeline([(\"pool\", lambda: product(range(5), [\"a\", \"b\"]))]) # doctest: +ELLIPSIS\n        Pipeline(steps=[('pool', &lt;function &lt;lambda&gt; at 0x...&gt;)], params={})\n\n        &gt;&gt;&gt; Pipeline([\n        ... (\"pool\", lambda: product(range(5), [\"a\", \"b\"])),\n        ... (\"filter\", lambda values: filter(lambda i: i[0] % 2 == 0, values))\n        ... ]) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('pool', &lt;function &lt;lambda&gt; at 0x...&gt;), \\\n        ('filter', &lt;function &lt;lambda&gt; at 0x...&gt;)], \\\n        params={})\n\n        &gt;&gt;&gt; pipeline = Pipeline([\n        ... (\"pool\", lambda maximum: product(range(maximum), [\"a\", \"b\"])),\n        ... (\"filter\", lambda values, divisor: filter(lambda i: i[0] % divisor == 0, values))\n        ... ] ,\n        ... params = {\"pool\": {\"maximum\":5}, \"filter\": {\"divisor\": 2}})\n        &gt;&gt;&gt; pipeline # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('pool', &lt;function &lt;lambda&gt; at 0x...&gt;), \\\n        ('filter', &lt;function &lt;lambda&gt; at 0x...&gt;)], \\\n        params={'pool': {'maximum': 5}, 'filter': {'divisor': 2}})\n        &gt;&gt;&gt; list(pipeline.run())\n        [(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b'), (4, 'a'), (4, 'b')]\n\n        &gt;&gt;&gt; pipeline.params = {\"pool\": {\"maximum\":7}, \"filter\": {\"divisor\": 3}}\n        &gt;&gt;&gt; list(pipeline())\n        [(0, 'a'), (0, 'b'), (3, 'a'), (3, 'b'), (6, 'a'), (6, 'b')]\n\n        &gt;&gt;&gt; pipeline.params = {\"pool\": {\"maximum\":7}}\n        &gt;&gt;&gt; list(pipeline()) # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        TypeError: &lt;lambda&gt;() missing 1 required positional argument: 'divisor'\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: Optional[Sequence[_StepType]] = None,\n        params: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize the pipeline with a series of Pipe objects.\"\"\"\n        if steps is None:\n            steps = list()\n        self.steps = steps\n\n        if params is None:\n            params = dict()\n        self.params = params\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(steps={self.steps}, params={self.params})\"\n\n    def __call__(\n        self,\n        ex: Optional[_ExperimentalSequence] = None,\n        **params,\n    ) -&gt; _ExperimentalSequence:\n        \"\"\"Successively pass the input values through the Pipe.\"\"\"\n\n        # Initialize the parameters objects.\n        merged_params = self._merge_params_with_self_params(params)\n\n        try:\n            # Check we have steps to use\n            assert len(self.steps) &gt; 0\n        except AssertionError:\n            # If the pipeline doesn't have any steps...\n            if ex is not None:\n                # ...the output is the input\n                return ex\n            elif ex is None:\n                # ... unless the input was None, in which case it's an emtpy list\n                return []\n\n        # Make an iterator from the steps, so that we can be sure to only go through them once\n        # (Otherwise if we handle the \"pool\" as a special case, we have to track our starting point)\n        pipes_iterator = iter(self.steps)\n\n        # Initialize our results object\n        if ex is None:\n            # ... there's no input, so presumably the first element in the steps is a pool\n            # which should generate our initial values.\n            name, pool = next(pipes_iterator)\n            if isinstance(pool, Pool):\n                # Here, the pool is a Pool callable, which we can pass parameters.\n                all_params_for_pool = merged_params.get(name, dict())\n                results = [pool(**all_params_for_pool)]\n            elif isinstance(pool, Iterable):\n                # Otherwise, the pool should be an iterable which we can just use as is.\n                results = [pool]\n\n        else:\n            # ... there's some input, so we can use that as the initial value\n            results = [ex]\n\n        # Run the successive steps over the last result\n        for name, pipe in pipes_iterator:\n            assert isinstance(pipe, Pipe)\n            all_params_for_pipe = merged_params.get(name, dict())\n            results.append(pipe(results[-1], **all_params_for_pipe))\n\n        return results[-1]\n\n    def _merge_params_with_self_params(self, params):\n        pipeline_params = _parse_params_to_nested_dict(\n            self.params, divider=PARAM_DIVIDER\n        )\n        call_params = _parse_params_to_nested_dict(params, divider=PARAM_DIVIDER)\n        merged_params = _merge_dicts(pipeline_params, call_params)\n        return merged_params\n\n    run = __call__\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.Pipeline.__call__","title":"<code>__call__(ex=None, **params)</code>","text":"<p>Successively pass the input values through the Pipe.</p> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>def __call__(\n    self,\n    ex: Optional[_ExperimentalSequence] = None,\n    **params,\n) -&gt; _ExperimentalSequence:\n    \"\"\"Successively pass the input values through the Pipe.\"\"\"\n\n    # Initialize the parameters objects.\n    merged_params = self._merge_params_with_self_params(params)\n\n    try:\n        # Check we have steps to use\n        assert len(self.steps) &gt; 0\n    except AssertionError:\n        # If the pipeline doesn't have any steps...\n        if ex is not None:\n            # ...the output is the input\n            return ex\n        elif ex is None:\n            # ... unless the input was None, in which case it's an emtpy list\n            return []\n\n    # Make an iterator from the steps, so that we can be sure to only go through them once\n    # (Otherwise if we handle the \"pool\" as a special case, we have to track our starting point)\n    pipes_iterator = iter(self.steps)\n\n    # Initialize our results object\n    if ex is None:\n        # ... there's no input, so presumably the first element in the steps is a pool\n        # which should generate our initial values.\n        name, pool = next(pipes_iterator)\n        if isinstance(pool, Pool):\n            # Here, the pool is a Pool callable, which we can pass parameters.\n            all_params_for_pool = merged_params.get(name, dict())\n            results = [pool(**all_params_for_pool)]\n        elif isinstance(pool, Iterable):\n            # Otherwise, the pool should be an iterable which we can just use as is.\n            results = [pool]\n\n    else:\n        # ... there's some input, so we can use that as the initial value\n        results = [ex]\n\n    # Run the successive steps over the last result\n    for name, pipe in pipes_iterator:\n        assert isinstance(pipe, Pipe)\n        all_params_for_pipe = merged_params.get(name, dict())\n        results.append(pipe(results[-1], **all_params_for_pipe))\n\n    return results[-1]\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.Pipeline.__init__","title":"<code>__init__(steps=None, params=None)</code>","text":"<p>Initialize the pipeline with a series of Pipe objects.</p> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>def __init__(\n    self,\n    steps: Optional[Sequence[_StepType]] = None,\n    params: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize the pipeline with a series of Pipe objects.\"\"\"\n    if steps is None:\n        steps = list()\n    self.steps = steps\n\n    if params is None:\n        params = dict()\n    self.params = params\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.PipelineUnion","title":"<code>PipelineUnion</code>","text":"<p>               Bases: <code>Pipeline</code></p> <p>Run several Pipes in parallel and concatenate all their results.</p> <p>Examples:</p> <p>You can use the ParallelPipeline to parallelize a group of poolers:</p> <pre><code>&gt;&gt;&gt; union_pipeline_0 = PipelineUnion([\n...      (\"pool_1\", make_pipeline([range(5)])),\n...      (\"pool_2\", make_pipeline([range(25, 30)])),\n...     ]\n... )\n&gt;&gt;&gt; list(union_pipeline_0.run())\n[0, 1, 2, 3, 4, 25, 26, 27, 28, 29]\n</code></pre> <pre><code>&gt;&gt;&gt; union_pipeline_1 = PipelineUnion([\n...      (\"pool_1\", range(5)),\n...      (\"pool_2\", range(25, 30)),\n...     ]\n... )\n&gt;&gt;&gt; list(union_pipeline_1.run())\n[0, 1, 2, 3, 4, 25, 26, 27, 28, 29]\n</code></pre> <p>You can use the ParallelPipeline to parallelize a group of pipes \u2013 each of which gets the same input.</p> <pre><code>&gt;&gt;&gt; pipeline_with_embedded_union = Pipeline([\n...      (\"pool\", range(22)),\n...      (\"filters\",  PipelineUnion([\n...          (\"div_5_filter\", lambda x: filter(lambda i: i % 5 == 0, x)),\n...          (\"div_7_filter\", lambda x: filter(lambda i: i % 7 == 0, x))\n...         ]))\n... ])\n&gt;&gt;&gt; list(pipeline_with_embedded_union.run())\n[0, 5, 10, 15, 20, 0, 7, 14, 21]\n</code></pre> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>class PipelineUnion(Pipeline):\n    \"\"\"\n    Run several Pipes in parallel and concatenate all their results.\n\n    Examples:\n        You can use the ParallelPipeline to parallelize a group of poolers:\n        &gt;&gt;&gt; union_pipeline_0 = PipelineUnion([\n        ...      (\"pool_1\", make_pipeline([range(5)])),\n        ...      (\"pool_2\", make_pipeline([range(25, 30)])),\n        ...     ]\n        ... )\n        &gt;&gt;&gt; list(union_pipeline_0.run())\n        [0, 1, 2, 3, 4, 25, 26, 27, 28, 29]\n\n        &gt;&gt;&gt; union_pipeline_1 = PipelineUnion([\n        ...      (\"pool_1\", range(5)),\n        ...      (\"pool_2\", range(25, 30)),\n        ...     ]\n        ... )\n        &gt;&gt;&gt; list(union_pipeline_1.run())\n        [0, 1, 2, 3, 4, 25, 26, 27, 28, 29]\n\n        You can use the ParallelPipeline to parallelize a group of pipes \u2013 each of which gets\n        the same input.\n        &gt;&gt;&gt; pipeline_with_embedded_union = Pipeline([\n        ...      (\"pool\", range(22)),\n        ...      (\"filters\",  PipelineUnion([\n        ...          (\"div_5_filter\", lambda x: filter(lambda i: i % 5 == 0, x)),\n        ...          (\"div_7_filter\", lambda x: filter(lambda i: i % 7 == 0, x))\n        ...         ]))\n        ... ])\n        &gt;&gt;&gt; list(pipeline_with_embedded_union.run())\n        [0, 5, 10, 15, 20, 0, 7, 14, 21]\n\n    \"\"\"\n\n    def __call__(\n        self,\n        ex: Optional[_ExperimentalSequence] = None,\n        **params,\n    ) -&gt; _ExperimentalSequence:\n        \"\"\"Pass the input values in parallel through the steps.\"\"\"\n\n        # Initialize the parameters objects.\n        merged_params = self._merge_params_with_self_params(params)\n\n        results = []\n\n        # Run the parallel steps over the input\n        for name, pipe in self.steps:\n            all_params_for_step = merged_params.get(name, dict())\n            if ex is None:\n                if isinstance(pipe, Pool):\n                    results.append(pipe(**all_params_for_step))\n                elif isinstance(pipe, Iterable):\n                    results.append(pipe)\n                else:\n                    raise NotImplementedError(\n                        f\"{pipe=} cannot be used in the PipelineUnion\"\n                    )\n            else:\n                assert isinstance(\n                    pipe, Pipe\n                ), f\"{pipe=} is incompatible with the Pipe interface\"\n                results.append(pipe(ex, **all_params_for_step))\n\n        union_results = chain.from_iterable(results)\n\n        return union_results\n\n    run = __call__\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.PipelineUnion.__call__","title":"<code>__call__(ex=None, **params)</code>","text":"<p>Pass the input values in parallel through the steps.</p> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>def __call__(\n    self,\n    ex: Optional[_ExperimentalSequence] = None,\n    **params,\n) -&gt; _ExperimentalSequence:\n    \"\"\"Pass the input values in parallel through the steps.\"\"\"\n\n    # Initialize the parameters objects.\n    merged_params = self._merge_params_with_self_params(params)\n\n    results = []\n\n    # Run the parallel steps over the input\n    for name, pipe in self.steps:\n        all_params_for_step = merged_params.get(name, dict())\n        if ex is None:\n            if isinstance(pipe, Pool):\n                results.append(pipe(**all_params_for_step))\n            elif isinstance(pipe, Iterable):\n                results.append(pipe)\n            else:\n                raise NotImplementedError(\n                    f\"{pipe=} cannot be used in the PipelineUnion\"\n                )\n        else:\n            assert isinstance(\n                pipe, Pipe\n            ), f\"{pipe=} is incompatible with the Pipe interface\"\n            results.append(pipe(ex, **all_params_for_step))\n\n    union_results = chain.from_iterable(results)\n\n    return union_results\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.Pool","title":"<code>Pool</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Creates an experimental sequence from scratch.</p> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>@runtime_checkable\nclass Pool(Protocol):\n    \"\"\"Creates an experimental sequence from scratch.\"\"\"\n\n    def __call__(self) -&gt; _ExperimentalSequence:\n        ...\n</code></pre>"},{"location":"reference/autora/experimentalist/pipeline/#autora.experimentalist.pipeline.make_pipeline","title":"<code>make_pipeline(steps=None, params=None, kind='serial')</code>","text":"<p>A factory function to make pipeline objects.</p> <p>The pipe objects' names will be set to the lowercase of their types, plus an index starting from 0 for non-unique names.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>Optional[Sequence[Union[Pool, Pipe]]]</code> <p>a sequence of Pipe-compatible objects</p> <code>None</code> <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>a dictionary of parameters passed to each Pipe by its inferred name</p> <code>None</code> <code>kind</code> <code>Literal['serial', 'union']</code> <p>whether the steps should run in \"serial\", passing data from one to the next, or in \"union\", where all the steps get the same data and the output is the union of all the results.</p> <code>'serial'</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>A pipeline object</p> <p>Examples:</p> <pre><code>You can create pipelines using purely anonymous functions:\n&gt;&gt;&gt; from itertools import product\n&gt;&gt;&gt; make_pipeline([lambda: product(range(5), [\"a\", \"b\"])]) # doctest: +ELLIPSIS\nPipeline(steps=[('&lt;lambda&gt;', &lt;function &lt;lambda&gt; at 0x...&gt;)], params={})\n\nYou can create pipelines with normal functions.\n&gt;&gt;&gt; def ab_pool(maximum=5): return product(range(maximum), [\"a\", \"b\"])\n&gt;&gt;&gt; def even_filter(values): return filter(lambda i: i[0] % 2 == 0, values)\n&gt;&gt;&gt; make_pipeline([ab_pool, even_filter]) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\nPipeline(steps=[('ab_pool', &lt;function ab_pool at 0x...&gt;),         ('even_filter', &lt;function even_filter at 0x...&gt;)], params={})\n\nYou can create pipelines with generators as their first elements functions.\n&gt;&gt;&gt; ab_pool_gen = product(range(3), [\"a\", \"b\"])\n&gt;&gt;&gt; pl = make_pipeline([ab_pool_gen, even_filter])\n&gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\nPipeline(steps=[('step', &lt;itertools.product object at 0x...&gt;),\n('even_filter', &lt;function even_filter at 0x...&gt;)], params={})\n&gt;&gt;&gt; list(pl.run())\n[(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b')]\n\nYou can pass parameters into the different steps of the pl using the \"params\"\nargument:\n&gt;&gt;&gt; def divisor_filter(x, divisor): return filter(lambda i: i[0] % divisor == 0, x)\n&gt;&gt;&gt; pl = make_pipeline([ab_pool, divisor_filter],\n... params = {\"ab_pool\": {\"maximum\":5}, \"divisor_filter\": {\"divisor\": 2}})\n&gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\nPipeline(steps=[('ab_pool', &lt;function ab_pool at 0x...&gt;),         ('divisor_filter', &lt;function divisor_filter at 0x...&gt;)],         params={'ab_pool': {'maximum': 5}, 'divisor_filter': {'divisor': 2}})\n\nYou can evaluate the pipeline means calling its `run` method:\n&gt;&gt;&gt; list(pl.run())\n[(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b'), (4, 'a'), (4, 'b')]\n\n... or calling it directly:\n&gt;&gt;&gt; list(pl())\n[(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b'), (4, 'a'), (4, 'b')]\n\nYou can update the parameters and evaluate again, giving different results:\n&gt;&gt;&gt; pl.params = {\"ab_pool\": {\"maximum\": 7}, \"divisor_filter\": {\"divisor\": 3}}\n&gt;&gt;&gt; list(pl())\n[(0, 'a'), (0, 'b'), (3, 'a'), (3, 'b'), (6, 'a'), (6, 'b')]\n\nIf the pipeline needs parameters, then removing them will break the pipeline:\n&gt;&gt;&gt; pl.params = {}\n&gt;&gt;&gt; list(pl()) # doctest: +ELLIPSIS\nTraceback (most recent call last):\n...\nTypeError: divisor_filter() missing 1 required positional argument: 'divisor'\n\nIf multiple steps have the same inferred name, then they are given a suffix automatically,\nwhich has to be reflected in the params if used:\n&gt;&gt;&gt; pl = make_pipeline([ab_pool, divisor_filter, divisor_filter])\n&gt;&gt;&gt; pl.params = {\n...     \"ab_pool\": {\"maximum\": 22},\n...     \"divisor_filter_0\": {\"divisor\": 3},\n...     \"divisor_filter_1\": {\"divisor\": 7}\n... }\n&gt;&gt;&gt; list(pl())\n[(0, 'a'), (0, 'b'), (21, 'a'), (21, 'b')]\n\nYou can also use \"partial\" functions to include Pipes with defaults in the pipeline.\nBecause the `partial` function doesn't inherit the __name__ of the original function,\nthese steps are renamed to \"step\".\n&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; pl = make_pipeline([partial(ab_pool, maximum=100)])\n&gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\nPipeline(steps=[('step', functools.partial(&lt;function ab_pool at 0x...&gt;, maximum=100))],         params={})\n\nIf there are multiple steps with the same name, they get suffixes as usual:\n&gt;&gt;&gt; pl = make_pipeline([partial(range, stop=10), partial(divisor_filter, divisor=3)])\n&gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\nPipeline(steps=[('step_0', functools.partial(&lt;class 'range'&gt;, stop=10)),         ('step_1', functools.partial(&lt;function divisor_filter at 0x...&gt;, divisor=3))],         params={})\n\nIt is possible to create parallel pipelines too:\n&gt;&gt;&gt; pl = make_pipeline([range(5), range(10,15)], kind=\"union\")\n&gt;&gt;&gt; pl\nPipelineUnion(steps=[('step_0', range(0, 5)), ('step_1', range(10, 15))], params={})\n\n&gt;&gt;&gt; list(pl.run())\n[0, 1, 2, 3, 4, 10, 11, 12, 13, 14]\n</code></pre> Source code in <code>autora/experimentalist/pipeline.py</code> <pre><code>def make_pipeline(\n    steps: Optional[Sequence[Union[Pool, Pipe]]] = None,\n    params: Optional[Dict[str, Any]] = None,\n    kind: Literal[\"serial\", \"union\"] = \"serial\",\n) -&gt; Pipeline:\n    \"\"\"\n    A factory function to make pipeline objects.\n\n    The pipe objects' names will be set to the lowercase of their types, plus an index\n    starting from 0 for non-unique names.\n\n    Args:\n        steps: a sequence of Pipe-compatible objects\n        params: a dictionary of parameters passed to each Pipe by its inferred name\n        kind: whether the steps should run in \"serial\", passing data from one to the next,\n            or in \"union\", where all the steps get the same data and the output is the union\n            of all the results.\n\n    Returns:\n        A pipeline object\n\n    Examples:\n\n        You can create pipelines using purely anonymous functions:\n        &gt;&gt;&gt; from itertools import product\n        &gt;&gt;&gt; make_pipeline([lambda: product(range(5), [\"a\", \"b\"])]) # doctest: +ELLIPSIS\n        Pipeline(steps=[('&lt;lambda&gt;', &lt;function &lt;lambda&gt; at 0x...&gt;)], params={})\n\n        You can create pipelines with normal functions.\n        &gt;&gt;&gt; def ab_pool(maximum=5): return product(range(maximum), [\"a\", \"b\"])\n        &gt;&gt;&gt; def even_filter(values): return filter(lambda i: i[0] % 2 == 0, values)\n        &gt;&gt;&gt; make_pipeline([ab_pool, even_filter]) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('ab_pool', &lt;function ab_pool at 0x...&gt;), \\\n        ('even_filter', &lt;function even_filter at 0x...&gt;)], params={})\n\n        You can create pipelines with generators as their first elements functions.\n        &gt;&gt;&gt; ab_pool_gen = product(range(3), [\"a\", \"b\"])\n        &gt;&gt;&gt; pl = make_pipeline([ab_pool_gen, even_filter])\n        &gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('step', &lt;itertools.product object at 0x...&gt;),\n        ('even_filter', &lt;function even_filter at 0x...&gt;)], params={})\n        &gt;&gt;&gt; list(pl.run())\n        [(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b')]\n\n        You can pass parameters into the different steps of the pl using the \"params\"\n        argument:\n        &gt;&gt;&gt; def divisor_filter(x, divisor): return filter(lambda i: i[0] % divisor == 0, x)\n        &gt;&gt;&gt; pl = make_pipeline([ab_pool, divisor_filter],\n        ... params = {\"ab_pool\": {\"maximum\":5}, \"divisor_filter\": {\"divisor\": 2}})\n        &gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('ab_pool', &lt;function ab_pool at 0x...&gt;), \\\n        ('divisor_filter', &lt;function divisor_filter at 0x...&gt;)], \\\n        params={'ab_pool': {'maximum': 5}, 'divisor_filter': {'divisor': 2}})\n\n        You can evaluate the pipeline means calling its `run` method:\n        &gt;&gt;&gt; list(pl.run())\n        [(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b'), (4, 'a'), (4, 'b')]\n\n        ... or calling it directly:\n        &gt;&gt;&gt; list(pl())\n        [(0, 'a'), (0, 'b'), (2, 'a'), (2, 'b'), (4, 'a'), (4, 'b')]\n\n        You can update the parameters and evaluate again, giving different results:\n        &gt;&gt;&gt; pl.params = {\"ab_pool\": {\"maximum\": 7}, \"divisor_filter\": {\"divisor\": 3}}\n        &gt;&gt;&gt; list(pl())\n        [(0, 'a'), (0, 'b'), (3, 'a'), (3, 'b'), (6, 'a'), (6, 'b')]\n\n        If the pipeline needs parameters, then removing them will break the pipeline:\n        &gt;&gt;&gt; pl.params = {}\n        &gt;&gt;&gt; list(pl()) # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        TypeError: divisor_filter() missing 1 required positional argument: 'divisor'\n\n        If multiple steps have the same inferred name, then they are given a suffix automatically,\n        which has to be reflected in the params if used:\n        &gt;&gt;&gt; pl = make_pipeline([ab_pool, divisor_filter, divisor_filter])\n        &gt;&gt;&gt; pl.params = {\n        ...     \"ab_pool\": {\"maximum\": 22},\n        ...     \"divisor_filter_0\": {\"divisor\": 3},\n        ...     \"divisor_filter_1\": {\"divisor\": 7}\n        ... }\n        &gt;&gt;&gt; list(pl())\n        [(0, 'a'), (0, 'b'), (21, 'a'), (21, 'b')]\n\n        You can also use \"partial\" functions to include Pipes with defaults in the pipeline.\n        Because the `partial` function doesn't inherit the __name__ of the original function,\n        these steps are renamed to \"step\".\n        &gt;&gt;&gt; from functools import partial\n        &gt;&gt;&gt; pl = make_pipeline([partial(ab_pool, maximum=100)])\n        &gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('step', functools.partial(&lt;function ab_pool at 0x...&gt;, maximum=100))], \\\n        params={})\n\n        If there are multiple steps with the same name, they get suffixes as usual:\n        &gt;&gt;&gt; pl = make_pipeline([partial(range, stop=10), partial(divisor_filter, divisor=3)])\n        &gt;&gt;&gt; pl # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n        Pipeline(steps=[('step_0', functools.partial(&lt;class 'range'&gt;, stop=10)), \\\n        ('step_1', functools.partial(&lt;function divisor_filter at 0x...&gt;, divisor=3))], \\\n        params={})\n\n        It is possible to create parallel pipelines too:\n        &gt;&gt;&gt; pl = make_pipeline([range(5), range(10,15)], kind=\"union\")\n        &gt;&gt;&gt; pl\n        PipelineUnion(steps=[('step_0', range(0, 5)), ('step_1', range(10, 15))], params={})\n\n        &gt;&gt;&gt; list(pl.run())\n        [0, 1, 2, 3, 4, 10, 11, 12, 13, 14]\n\n    \"\"\"\n\n    if steps is None:\n        steps = []\n    steps_: List[_StepType] = []\n    raw_names_ = [getattr(pipe, \"__name__\", \"step\").lower() for pipe in steps]\n    names_tally_ = dict([(name, raw_names_.count(name)) for name in set(raw_names_)])\n    names_index_ = dict([(name, 0) for name in set(raw_names_)])\n\n    for name, pipe in zip(raw_names_, steps):\n        assert isinstance(pipe, get_args(Union[Pipe, Pool, Iterable]))\n\n        if names_tally_[name] &gt; 1:\n            current_index_for_this_name = names_index_.get(name, 0)\n            name_in_pipeline = f\"{name}_{current_index_for_this_name}\"\n            names_index_[name] += 1\n        else:\n            name_in_pipeline = name\n\n        steps_.append((name_in_pipeline, pipe))\n\n    if kind == \"serial\":\n        pipeline = Pipeline(steps_, params=params)\n    elif kind == \"union\":\n        pipeline = PipelineUnion(steps_, params=params)\n    else:\n        raise NotImplementedError(f\"{kind=} is not implemented\")\n\n    return pipeline\n</code></pre>"},{"location":"reference/autora/experimentalist/random/","title":"autora.experimentalist.random","text":""},{"location":"reference/autora/experimentalist/random/#autora.experimentalist.random.random_pool","title":"<code>random_pool = pool</code>  <code>module-attribute</code>","text":"<p>Alias for <code>pool</code></p>"},{"location":"reference/autora/experimentalist/random/#autora.experimentalist.random.random_sample","title":"<code>random_sample = sample</code>  <code>module-attribute</code>","text":"<p>Alias for <code>sample</code></p>"},{"location":"reference/autora/experimentalist/random/#autora.experimentalist.random.pool","title":"<code>pool(variables, num_samples=5, random_state=None, replace=True)</code>","text":"<p>Create a sequence of conditions randomly sampled from independent variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>VariableCollection</code> <p>the description of all the variables in the AER experiment.</p> required <code>num_samples</code> <code>int</code> <p>the number of conditions to produce</p> <code>5</code> <code>random_state</code> <code>Optional[int]</code> <p>the seed value for the random number generator</p> <code>None</code> <code>replace</code> <code>bool</code> <p>if True, allow repeated values</p> <code>True</code> <p>Returns: the generated conditions as a dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from autora.state import State\n&gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n</code></pre> <p>With one independent variable \"x\", and some allowed_values we get some of those values back when running the experimentalist:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(\n...         independent_variables=[Variable(name=\"x\", allowed_values=range(10))\n... ]), random_state=1)\n   x\n0  4\n1  5\n2  7\n3  9\n4  0\n</code></pre> <p>... with one independent variable \"x\", and a value_range, we get a sample of the range back when running the experimentalist:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(independent_variables=[\n...         Variable(name=\"x\", value_range=(-5, 5))\n... ]), random_state=1)\n          x\n0  0.118216\n1  4.504637\n2 -3.558404\n3  4.486494\n4 -1.881685\n</code></pre> <p>The allowed_values or value_range must be specified:</p> <pre><code>&gt;&gt;&gt; pool(VariableCollection(independent_variables=[Variable(name=\"x\")]))\nTraceback (most recent call last):\n...\nValueError: allowed_values or [value_range and type==REAL] needs to be set...\n</code></pre> <p>With two independent variables, we get independent samples on both axes:</p> <pre><code>&gt;&gt;&gt; pool(VariableCollection(independent_variables=[\n...         Variable(name=\"x1\", allowed_values=range(1, 5)),\n...         Variable(name=\"x2\", allowed_values=range(1, 500)),\n... ]), num_samples=10, replace=True, random_state=1)\n   x1   x2\n0   2  434\n1   3  212\n2   4  137\n3   4  414\n4   1  129\n5   1  205\n6   4  322\n7   4  275\n8   1   43\n9   2   14\n</code></pre> <p>If any of the variables have unspecified allowed_values, we get an error:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(independent_variables=[\n...         Variable(name=\"x1\", allowed_values=[1, 2]),\n...         Variable(name=\"x2\"),\n... ]))\nTraceback (most recent call last):\n...\nValueError: allowed_values or [value_range and type==REAL] needs to be set...\n</code></pre> <p>We can specify arrays of allowed values:</p> <pre><code>&gt;&gt;&gt; pool(\n...     VariableCollection(independent_variables=[\n...         Variable(name=\"x\", allowed_values=np.linspace(-10, 10, 101)),\n...         Variable(name=\"y\", allowed_values=[3, 4]),\n...         Variable(name=\"z\", allowed_values=np.linspace(20, 30, 11)),\n... ]), random_state=1)\n     x  y     z\n0 -0.6  3  29.0\n1  0.2  4  24.0\n2  5.2  4  23.0\n3  9.0  3  29.0\n4 -9.4  3  22.0\n</code></pre> Source code in <code>autora/experimentalist/random.py</code> <pre><code>def pool(\n    variables: VariableCollection,\n    num_samples: int = 5,\n    random_state: Optional[int] = None,\n    replace: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a sequence of conditions randomly sampled from independent variables.\n\n    Args:\n        variables: the description of all the variables in the AER experiment.\n        num_samples: the number of conditions to produce\n        random_state: the seed value for the random number generator\n        replace: if True, allow repeated values\n\n    Returns: the generated conditions as a dataframe\n\n    Examples:\n        &gt;&gt;&gt; from autora.state import State\n        &gt;&gt;&gt; from autora.variable import VariableCollection, Variable\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n\n        With one independent variable \"x\", and some allowed_values we get some of those values\n        back when running the experimentalist:\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(\n        ...         independent_variables=[Variable(name=\"x\", allowed_values=range(10))\n        ... ]), random_state=1)\n           x\n        0  4\n        1  5\n        2  7\n        3  9\n        4  0\n\n\n        ... with one independent variable \"x\", and a value_range,\n        we get a sample of the range back when running the experimentalist:\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(independent_variables=[\n        ...         Variable(name=\"x\", value_range=(-5, 5))\n        ... ]), random_state=1)\n                  x\n        0  0.118216\n        1  4.504637\n        2 -3.558404\n        3  4.486494\n        4 -1.881685\n\n\n\n        The allowed_values or value_range must be specified:\n        &gt;&gt;&gt; pool(VariableCollection(independent_variables=[Variable(name=\"x\")]))\n        Traceback (most recent call last):\n        ...\n        ValueError: allowed_values or [value_range and type==REAL] needs to be set...\n\n        With two independent variables, we get independent samples on both axes:\n        &gt;&gt;&gt; pool(VariableCollection(independent_variables=[\n        ...         Variable(name=\"x1\", allowed_values=range(1, 5)),\n        ...         Variable(name=\"x2\", allowed_values=range(1, 500)),\n        ... ]), num_samples=10, replace=True, random_state=1)\n           x1   x2\n        0   2  434\n        1   3  212\n        2   4  137\n        3   4  414\n        4   1  129\n        5   1  205\n        6   4  322\n        7   4  275\n        8   1   43\n        9   2   14\n\n        If any of the variables have unspecified allowed_values, we get an error:\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(independent_variables=[\n        ...         Variable(name=\"x1\", allowed_values=[1, 2]),\n        ...         Variable(name=\"x2\"),\n        ... ]))\n        Traceback (most recent call last):\n        ...\n        ValueError: allowed_values or [value_range and type==REAL] needs to be set...\n\n\n        We can specify arrays of allowed values:\n\n        &gt;&gt;&gt; pool(\n        ...     VariableCollection(independent_variables=[\n        ...         Variable(name=\"x\", allowed_values=np.linspace(-10, 10, 101)),\n        ...         Variable(name=\"y\", allowed_values=[3, 4]),\n        ...         Variable(name=\"z\", allowed_values=np.linspace(20, 30, 11)),\n        ... ]), random_state=1)\n             x  y     z\n        0 -0.6  3  29.0\n        1  0.2  4  24.0\n        2  5.2  4  23.0\n        3  9.0  3  29.0\n        4 -9.4  3  22.0\n\n\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n\n    raw_conditions = {}\n    for iv in variables.independent_variables:\n        if iv.allowed_values is not None:\n            raw_conditions[iv.name] = rng.choice(\n                iv.allowed_values, size=num_samples, replace=replace\n            )\n        elif (iv.value_range is not None) and (iv.type == ValueType.REAL):\n            raw_conditions[iv.name] = rng.uniform(*iv.value_range, size=num_samples)\n\n        else:\n            raise ValueError(\n                \"allowed_values or [value_range and type==REAL] needs to be set for \"\n                \"%s\" % (iv)\n            )\n\n    return pd.DataFrame(raw_conditions)\n</code></pre>"},{"location":"reference/autora/experimentalist/random/#autora.experimentalist.random.sample","title":"<code>sample(conditions, num_samples=1, random_state=None, replace=False)</code>","text":"<p>Take a random sample from some input conditions.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray, recarray]</code> <p>the conditions to sample from</p> required <code>num_samples</code> <code>int</code> <p>the number of conditions to produce</p> <code>1</code> <code>random_state</code> <code>Optional[int]</code> <p>the seed value for the random number generator</p> <code>None</code> <code>replace</code> <code>bool</code> <p>if True, allow repeated values</p> <code>False</code> <p>Returns: a Result object with a field <code>conditions</code> containing a DataFrame of the sampled conditions</p> <p>Examples:</p> <p>From a pd.DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; sample(\n...     pd.DataFrame({\"x\": range(100, 200)}), num_samples=5, random_state=180)\n     x\n0  167\n1  171\n2  164\n3  163\n4  196\n</code></pre> <p>From a list (returns a DataFrame):</p> <pre><code>&gt;&gt;&gt; sample(range(1000), num_samples=5, random_state=180)\n     0\n0  270\n1  908\n2  109\n3  331\n4  978\n</code></pre> Source code in <code>autora/experimentalist/random.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray, np.recarray],\n    num_samples: int = 1,\n    random_state: Optional[int] = None,\n    replace: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Take a random sample from some input conditions.\n\n    Args:\n        conditions: the conditions to sample from\n        num_samples: the number of conditions to produce\n        random_state: the seed value for the random number generator\n        replace: if True, allow repeated values\n\n    Returns: a Result object with a field `conditions` containing a DataFrame of the sampled\n    conditions\n\n    Examples:\n        From a pd.DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; sample(\n        ...     pd.DataFrame({\"x\": range(100, 200)}), num_samples=5, random_state=180)\n             x\n        0  167\n        1  171\n        2  164\n        3  163\n        4  196\n\n        From a list (returns a DataFrame):\n        &gt;&gt;&gt; sample(range(1000), num_samples=5, random_state=180)\n             0\n        0  270\n        1  908\n        2  109\n        3  331\n        4  978\n    \"\"\"\n    conditions_ = pd.DataFrame(conditions)\n    return pd.DataFrame.sample(\n        conditions_,\n        random_state=random_state,\n        n=num_samples,\n        replace=replace,\n        ignore_index=True,\n    )\n</code></pre>"},{"location":"reference/autora/experimentalist/utils/","title":"autora.experimentalist.utils","text":""},{"location":"reference/autora/experimentalist/utils/#autora.experimentalist.utils.array_to_sequence","title":"<code>array_to_sequence(input)</code>","text":"<p>Convert an array of experimental conditions into an iterable of smaller arrays.</p> See also <ul> <li>sequence_to_array</li> <li>sequence_to_array</li> </ul> <p>Examples:</p> <pre><code>We start with an array:\n&gt;&gt;&gt; a0 = np.arange(10).reshape(-1,2)\n&gt;&gt;&gt; a0\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\nThe sequence is created as a generator object\n&gt;&gt;&gt; array_to_sequence(a0)  # doctest: +ELLIPSIS\n&lt;generator object array_to_sequence at 0x...&gt;\n\nTo see the sequence, we can convert it into a list:\n&gt;&gt;&gt; l0 = list(array_to_sequence(a0))\n&gt;&gt;&gt; l0\n[array([0, 1]), array([2, 3]), array([4, 5]), array([6, 7]), array([8, 9])]\n\nThe individual rows are themselves 1-dimensional arrays:\n&gt;&gt;&gt; l0[0]\narray([0, 1])\n\nThe rows can be subscripted as usual:\n&gt;&gt;&gt; int(l0[2][1])\n5\n\nWe can also use a record array:\n&gt;&gt;&gt; a1 = np.rec.fromarrays([range(5), list(\"abcde\")])\n&gt;&gt;&gt; a1 # doctest: +ELLIPSIS\nrec.array([(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')],\n          dtype=[('f0', '&lt;i...'), ('f1', '&lt;U1')])\n\nThis is converted into records:\n&gt;&gt;&gt; l1 = list(array_to_sequence(a1))\n&gt;&gt;&gt; l1 # doctest: +NORMALIZE_WHITESPACE\n[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]\n\nThe elements of the list are numpy.records\n&gt;&gt;&gt; type(l1[0])\n&lt;class 'numpy.record'&gt;\n</code></pre> Source code in <code>autora/experimentalist/utils.py</code> <pre><code>def array_to_sequence(input: numpy.typing.ArrayLike):\n    \"\"\"\n    Convert an array of experimental conditions into an iterable of smaller arrays.\n\n    See also:\n        - [sequence_to_array][autora.experimentalist.utils.sequence_to_array]\n        - [sequence_to_array][autora.experimentalist.utils.sequence_to_recarray]\n\n    Examples:\n\n        We start with an array:\n        &gt;&gt;&gt; a0 = np.arange(10).reshape(-1,2)\n        &gt;&gt;&gt; a0\n        array([[0, 1],\n               [2, 3],\n               [4, 5],\n               [6, 7],\n               [8, 9]])\n\n        The sequence is created as a generator object\n        &gt;&gt;&gt; array_to_sequence(a0)  # doctest: +ELLIPSIS\n        &lt;generator object array_to_sequence at 0x...&gt;\n\n        To see the sequence, we can convert it into a list:\n        &gt;&gt;&gt; l0 = list(array_to_sequence(a0))\n        &gt;&gt;&gt; l0\n        [array([0, 1]), array([2, 3]), array([4, 5]), array([6, 7]), array([8, 9])]\n\n        The individual rows are themselves 1-dimensional arrays:\n        &gt;&gt;&gt; l0[0]\n        array([0, 1])\n\n        The rows can be subscripted as usual:\n        &gt;&gt;&gt; int(l0[2][1])\n        5\n\n        We can also use a record array:\n        &gt;&gt;&gt; a1 = np.rec.fromarrays([range(5), list(\"abcde\")])\n        &gt;&gt;&gt; a1 # doctest: +ELLIPSIS\n        rec.array([(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')],\n                  dtype=[('f0', '&lt;i...'), ('f1', '&lt;U1')])\n\n        This is converted into records:\n        &gt;&gt;&gt; l1 = list(array_to_sequence(a1))\n        &gt;&gt;&gt; l1 # doctest: +NORMALIZE_WHITESPACE\n        [(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]\n\n        The elements of the list are numpy.records\n        &gt;&gt;&gt; type(l1[0])\n        &lt;class 'numpy.record'&gt;\n\n    \"\"\"\n    assert isinstance(input, (np.ndarray, np.recarray))\n\n    for a in input:\n        yield a\n</code></pre>"},{"location":"reference/autora/experimentalist/utils/#autora.experimentalist.utils.sequence_to_array","title":"<code>sequence_to_array(iterable)</code>","text":"<p>Converts a finite sequence of experimental conditions into a 2D numpy.array.</p> <p>See also: array_to_sequence</p> <p>Examples:</p> <pre><code>A simple range object can be converted into an array of dimension 2:\n&gt;&gt;&gt; sequence_to_array(range(5)) # doctest: +NORMALIZE_WHITESPACE\narray([[0], [1], [2], [3], [4]])\n\nFor mixed datatypes, the highest-level type common to all the inputs will be used, so\nconsider using [_sequence_to_recarray][autora.experimentalist.utils._sequence_to_recarray]\ninstead.\n&gt;&gt;&gt; sequence_to_array(zip(range(5), \"abcde\"))  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\narray([['0', 'a'], ['1', 'b'], ['2', 'c'], ['3', 'd'], ['4', 'e']],  dtype='&lt;U...')\n\nSingle strings are broken into characters:\n&gt;&gt;&gt; sequence_to_array(\"abcde\")  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\narray([['a'], ['b'], ['c'], ['d'], ['e']], dtype='&lt;U...')\n\nMultiple strings are treated as individual entries:\n&gt;&gt;&gt; sequence_to_array([\"abc\", \"de\"])  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\narray([['abc'], ['de']], dtype='&lt;U...')\n</code></pre> Source code in <code>autora/experimentalist/utils.py</code> <pre><code>def sequence_to_array(iterable):\n    \"\"\"\n    Converts a finite sequence of experimental conditions into a 2D numpy.array.\n\n    See also: [array_to_sequence][autora.experimentalist.utils.array_to_sequence]\n\n    Examples:\n\n        A simple range object can be converted into an array of dimension 2:\n        &gt;&gt;&gt; sequence_to_array(range(5)) # doctest: +NORMALIZE_WHITESPACE\n        array([[0], [1], [2], [3], [4]])\n\n        For mixed datatypes, the highest-level type common to all the inputs will be used, so\n        consider using [_sequence_to_recarray][autora.experimentalist.utils._sequence_to_recarray]\n        instead.\n        &gt;&gt;&gt; sequence_to_array(zip(range(5), \"abcde\"))  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        array([['0', 'a'], ['1', 'b'], ['2', 'c'], ['3', 'd'], ['4', 'e']],  dtype='&lt;U...')\n\n        Single strings are broken into characters:\n        &gt;&gt;&gt; sequence_to_array(\"abcde\")  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        array([['a'], ['b'], ['c'], ['d'], ['e']], dtype='&lt;U...')\n\n        Multiple strings are treated as individual entries:\n        &gt;&gt;&gt; sequence_to_array([\"abc\", \"de\"])  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        array([['abc'], ['de']], dtype='&lt;U...')\n\n    \"\"\"\n    deque = collections.deque(iterable)\n    array = np.array(deque).reshape((len(deque), -1))\n    return array\n</code></pre>"},{"location":"reference/autora/experimentalist/utils/#autora.experimentalist.utils.sequence_to_recarray","title":"<code>sequence_to_recarray(iterable)</code>","text":"<p>Converts a finite sequence of experimental conditions into a numpy recarray.</p> <p>See also: array_to_sequence</p> <p>Examples:</p> <pre><code>A simple range object is converted into a recarray of dimension 2:\n&gt;&gt;&gt; sequence_to_recarray(range(5)) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\nrec.array([(0,), (1,), (2,), (3,), (4,)], dtype=[('f0', '&lt;i...')])\n\nMixed datatypes lead to multiple output types:\n&gt;&gt;&gt; sequence_to_recarray(zip(range(5), \"abcde\"))  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\nrec.array([(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')],\n    dtype=[('f0', '&lt;i...'), ('f1', '&lt;U1')])\n\nSingle strings are broken into characters:\n&gt;&gt;&gt; sequence_to_recarray(\"abcde\")  # doctest: +NORMALIZE_WHITESPACE\nrec.array([('a',), ('b',), ('c',), ('d',), ('e',)], dtype=[('f0', '&lt;U1')])\n\nMultiple strings are treated as individual entries:\n&gt;&gt;&gt; sequence_to_recarray([\"abc\", \"de\"])  # doctest: +NORMALIZE_WHITESPACE\nrec.array([('abc',), ('de',)], dtype=[('f0', '&lt;U3')])\n</code></pre> Source code in <code>autora/experimentalist/utils.py</code> <pre><code>def sequence_to_recarray(iterable):\n    \"\"\"\n    Converts a finite sequence of experimental conditions into a numpy recarray.\n\n    See also: [array_to_sequence][autora.experimentalist.utils.array_to_sequence]\n\n    Examples:\n\n        A simple range object is converted into a recarray of dimension 2:\n        &gt;&gt;&gt; sequence_to_recarray(range(5)) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        rec.array([(0,), (1,), (2,), (3,), (4,)], dtype=[('f0', '&lt;i...')])\n\n        Mixed datatypes lead to multiple output types:\n        &gt;&gt;&gt; sequence_to_recarray(zip(range(5), \"abcde\"))  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        rec.array([(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')],\n            dtype=[('f0', '&lt;i...'), ('f1', '&lt;U1')])\n\n        Single strings are broken into characters:\n        &gt;&gt;&gt; sequence_to_recarray(\"abcde\")  # doctest: +NORMALIZE_WHITESPACE\n        rec.array([('a',), ('b',), ('c',), ('d',), ('e',)], dtype=[('f0', '&lt;U1')])\n\n        Multiple strings are treated as individual entries:\n        &gt;&gt;&gt; sequence_to_recarray([\"abc\", \"de\"])  # doctest: +NORMALIZE_WHITESPACE\n        rec.array([('abc',), ('de',)], dtype=[('f0', '&lt;U3')])\n\n    \"\"\"\n    deque = collections.deque(iterable)\n\n    if isinstance(deque[0], (str, int, float, complex)):\n        recarray = np.core.records.fromrecords([(d,) for d in deque])\n    else:\n        recarray = np.core.records.fromrecords(deque)\n\n    return recarray\n</code></pre>"},{"location":"reference/autora/experimentalist/bandit_random/","title":"autora.experimentalist.bandit_random","text":"<p>Experimentalist that returns probability sequences: Sequences of vectors with elements between 0 and 1 or reward sequences: Sequences of vectors with binary elements</p>"},{"location":"reference/autora/experimentalist/bandit_random/#autora.experimentalist.bandit_random.pool","title":"<code>pool(num_rewards, sequence_length, initial_probabilities=None, sigmas=None, num_samples=1, random_state=None)</code>","text":"<p>Returns a list of rewards. A reward sequence is a sequence of vectors of dimension <code>num_probabilities</code>. Each entry of this vector is a number between 0 and 1. We can set a fixed initial value for the reward probability of the first vector of each sequence and a constant drif rate. We can also set a range to randomly sample these values.</p> <p>Parameters:</p> Name Type Description Default <code>num_rewards</code> <code>int</code> <p>The number of rewards/ dimention of each element of the sequence</p> required <code>sequence_length</code> <code>int</code> <p>The length of the sequence</p> required <code>initial_probabilities</code> <code>Optional[Iterable[Union[float, Iterable]]]</code> <p>A list of initial reward-probabilities. Each</p> <code>None</code> <code>sigmas</code> <code>Optional[Iterable[Union[float, Iterable]]]</code> <p>A list of constant drift rate for each element of the probabilites. Each</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>number of experimental conditions to select</p> <code>1</code> <code>random_state</code> <code>Optional[int]</code> <p>the seed value for the random number generator</p> <code>None</code> <p>Returns:     Sampled pool of experimental conditions</p> <p>Examples:</p> <p>We create a reward sequence for five two arm bandit tasks. The reward probabilities for each arm should be .5 and constant.</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=2, sequence_length=3, num_samples=1, random_state=42)\n[[[1, 0], [1, 1], [0, 1]]]\n</code></pre> <p>If we want more arms:</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=4, sequence_length=3, num_samples=1, random_state=42)\n[[[1, 0, 1, 1], [0, 1, 1, 1], [0, 0, 0, 1]]]\n</code></pre> <p>longer sequence:</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=2, sequence_length=5, num_samples=1, random_state=42)\n[[[1, 0], [1, 1], [0, 1], [1, 1], [0, 0]]]\n</code></pre> <p>more sequences:</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=2, sequence_length=3, num_samples=2, random_state=42)\n[[[1, 0], [1, 1], [0, 1]], [[1, 1], [0, 0], [0, 1]]]\n</code></pre> <p>We  can set fixed initial values:</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=2, sequence_length=3,\n...     initial_probabilities=[0.,.4],\n...     random_state=42)\n[[[0, 0], [0, 1], [0, 1]]]\n</code></pre> <p>And drift rates:</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=2, sequence_length=3,\n...     initial_probabilities=[0.,.4],\n...     sigmas=[.2, .3],\n...     random_state=42)\n[[[0, 0], [0, 1], [0, 1]]]\n</code></pre> <p>We can also sample the initial values by passing a range:</p> <pre><code>&gt;&gt;&gt; pool(num_rewards=2, sequence_length=3,\n...     initial_probabilities=[[0, .2],[.8, 1.]],\n...     sigmas=[[0., .2], [0., .3]],\n...     random_state=42)\n[[[0, 1], [1, 1], [0, 1]]]\n</code></pre> Source code in <code>temp_dir/bandit-random/src/autora/experimentalist/bandit_random/__init__.py</code> <pre><code>def pool(\n        num_rewards: int,\n        sequence_length: int,\n        initial_probabilities: Optional[Iterable[Union[float, Iterable]]] = None,\n        sigmas: Optional[Iterable[Union[float, Iterable]]] = None,\n        num_samples: int = 1,\n        random_state: Optional[int] = None,\n) -&gt; List[List[List[float]]]:\n    \"\"\"\n    Returns a list of rewards.\n    A reward sequence is a sequence of vectors of dimension `num_probabilities`. Each entry\n    of this vector is a number between 0 and 1.\n    We can set a fixed initial value for the reward probability of the first vector of each sequence\n    and a constant drif rate.\n    We can also set a range to randomly sample these values.\n\n\n    Args:\n        num_rewards: The number of rewards/ dimention of each element of the sequence\n        sequence_length: The length of the sequence\n        initial_probabilities: A list of initial reward-probabilities. Each\n        entry can be a range.\n        sigmas: A list of constant drift rate for each element of the probabilites. Each\n        entry can be a range. The drift rate is defined as change per step\n        num_samples: number of experimental conditions to select\n        random_state: the seed value for the random number generator\n    Returns:\n        Sampled pool of experimental conditions\n\n    Examples:\n        We create a reward sequence for five two arm bandit tasks. The reward\n        probabilities for each arm should be .5 and constant.\n        &gt;&gt;&gt; pool(num_rewards=2, sequence_length=3, num_samples=1, random_state=42)\n        [[[1, 0], [1, 1], [0, 1]]]\n\n        If we want more arms:\n        &gt;&gt;&gt; pool(num_rewards=4, sequence_length=3, num_samples=1, random_state=42)\n        [[[1, 0, 1, 1], [0, 1, 1, 1], [0, 0, 0, 1]]]\n\n        longer sequence:\n        &gt;&gt;&gt; pool(num_rewards=2, sequence_length=5, num_samples=1, random_state=42)\n        [[[1, 0], [1, 1], [0, 1], [1, 1], [0, 0]]]\n\n        more sequences:\n        &gt;&gt;&gt; pool(num_rewards=2, sequence_length=3, num_samples=2, random_state=42)\n        [[[1, 0], [1, 1], [0, 1]], [[1, 1], [0, 0], [0, 1]]]\n\n        We  can set fixed initial values:\n        &gt;&gt;&gt; pool(num_rewards=2, sequence_length=3,\n        ...     initial_probabilities=[0.,.4],\n        ...     random_state=42)\n        [[[0, 0], [0, 1], [0, 1]]]\n\n        And drift rates:\n        &gt;&gt;&gt; pool(num_rewards=2, sequence_length=3,\n        ...     initial_probabilities=[0.,.4],\n        ...     sigmas=[.2, .3],\n        ...     random_state=42)\n        [[[0, 0], [0, 1], [0, 1]]]\n\n        We can also sample the initial values by passing a range:\n        &gt;&gt;&gt; pool(num_rewards=2, sequence_length=3,\n        ...     initial_probabilities=[[0, .2],[.8, 1.]],\n        ...     sigmas=[[0., .2], [0., .3]],\n        ...     random_state=42)\n        [[[0, 1], [1, 1], [0, 1]]]\n    \"\"\"\n    _sequence = pool_proba(num_rewards,\n                           sequence_length,\n                           initial_probabilities,\n                           sigmas,\n                           num_samples,\n                           random_state)\n    return pool_from_proba(_sequence, random_state)\n</code></pre>"},{"location":"reference/autora/experimentalist/bandit_random/#autora.experimentalist.bandit_random.pool_from_proba","title":"<code>pool_from_proba(probability_sequence, random_state=None)</code>","text":"<p>From a given probability sequence sample rewards (0 or 1)</p> Example <p>proba_sequence = pool_proba(num_probabilities=2, sequence_length=3, ...     initial_probabilities=[.2,.8], ...     sigmas=[.2, .1], random_state=42) proba_sequence [[[0.2, 0.8], [0.26094341595088627, 0.8750451195806458], [0.05294659470278715, 0.9691015912197671]]] pool_from_proba(proba_sequence, 42) [[[0, 1], [1, 1], [0, 1]]]</p> Source code in <code>temp_dir/bandit-random/src/autora/experimentalist/bandit_random/__init__.py</code> <pre><code>def pool_from_proba(\n        probability_sequence: Iterable,\n        random_state: Optional[int] = None,\n) -&gt; List[List[List[float]]]:\n    \"\"\"\n    From a given probability sequence sample rewards (0 or 1)\n\n    Example:\n        &gt;&gt;&gt; proba_sequence = pool_proba(num_probabilities=2, sequence_length=3,\n        ...     initial_probabilities=[.2,.8],\n        ...     sigmas=[.2, .1], random_state=42)\n        &gt;&gt;&gt; proba_sequence\n        [[[0.2, 0.8], [0.26094341595088627, 0.8750451195806458], \\\n[0.05294659470278715, 0.9691015912197671]]]\n        &gt;&gt;&gt; pool_from_proba(proba_sequence, 42)\n        [[[0, 1], [1, 1], [0, 1]]]\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    probability_sequence_array = _sample_from_probabilities(probability_sequence, rng)\n    probability_sequence_lst = [el for el in probability_sequence_array]\n    return probability_sequence_lst\n</code></pre>"},{"location":"reference/autora/experimentalist/bandit_random/#autora.experimentalist.bandit_random.pool_proba","title":"<code>pool_proba(num_probabilities, sequence_length, initial_probabilities=None, sigmas=None, num_samples=1, random_state=None)</code>","text":"<p>Returns a list of probability sequences. A probability sequence is a sequence of vectors of dimension <code>num_probabilities</code>. Each entry of this vector is a number between 0 and 1. We can set a fixed initial value for the first vector of each sequence and a constant drif rate. We can also set a range to randomly sample these values.</p> <p>Parameters:</p> Name Type Description Default <code>num_probabilities</code> <code>int</code> <p>The number of probilities/ dimention of each element of the sequence</p> required <code>sequence_length</code> <code>int</code> <p>The length of the sequence</p> required <code>initial_probabilities</code> <code>Optional[Iterable[Union[float, Iterable]]]</code> <p>A list of initial values for each element of the probalities. Each</p> <code>None</code> <code>sigmas</code> <code>Optional[Iterable[Union[float, Iterable]]]</code> <p>A list of sigma of the normal distribution for the drift rate of each arm. Each entry can be a range to be sampled from. The drift rate is defined as change per step</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>number of experimental conditions to select</p> <code>1</code> <code>random_state</code> <code>Optional[int]</code> <p>the seed value for the random number generator</p> <code>None</code> <p>Returns:     Sampled pool of experimental conditions</p> <p>Examples:</p> <p>We create a reward probabilty sequence for five two arm bandit tasks. The reward probabilities for each arm should be .5 and constant.</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3, num_samples=1, random_state=42)\n[[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]\n</code></pre> <p>If we want more arms:</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=4, sequence_length=3, num_samples=1, random_state=42)\n[[[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]]]\n</code></pre> <p>longer sequence:</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=5, num_samples=1, random_state=42)\n[[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]\n</code></pre> <p>more sequences:</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3, num_samples=2, random_state=42)\n[[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]], [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]\n</code></pre> <p>We  can set fixed initial values:</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3,\n...     initial_probabilities=[0.,.4], random_state=42)\n[[[0.0, 0.4], [0.0, 0.4], [0.0, 0.4]]]\n</code></pre> <p>And drift rates:</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3,\n...     initial_probabilities=[0.,.4],\n...     sigmas=[.1, .5], random_state=42)\n[[[0.0, 0.4], [0.030471707975443137, 0.7752255979032286], [0.0, 1.0]]]\n</code></pre> <p>We can also sample the initial values by passing a range:</p> <pre><code>&gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3,\n...     initial_probabilities=[[0, .2],[.8, 1.]],\n...     sigmas=[[0., .25], [0., .5]],\n...     random_state=42)\n[[[0.15479120971119267, 0.81883546957753], [0.23713042219259264, 0.8811974469636589], [0.34032881599649456, 0.7269307761486841]]]\n</code></pre> Source code in <code>temp_dir/bandit-random/src/autora/experimentalist/bandit_random/__init__.py</code> <pre><code>def pool_proba(\n        num_probabilities: int,\n        sequence_length: int,\n        initial_probabilities: Optional[Iterable[Union[float, Iterable]]] = None,\n        sigmas: Optional[Iterable[Union[float, Iterable]]] = None,\n        num_samples: int = 1,\n        random_state: Optional[int] = None,\n) -&gt; List[List[List[float]]]:\n    \"\"\"\n    Returns a list of probability sequences.\n    A probability sequence is a sequence of vectors of dimension `num_probabilities`. Each entry\n    of this vector is a number between 0 and 1.\n    We can set a fixed initial value for the first vector of each sequence and a constant drif rate.\n    We can also set a range to randomly sample these values.\n\n\n    Args:\n        num_probabilities: The number of probilities/ dimention of each element of the sequence\n        sequence_length: The length of the sequence\n        initial_probabilities: A list of initial values for each element of the probalities. Each\n        entry can be a range.\n        sigmas: A list of sigma of the normal distribution for the drift rate of each arm. Each\n            entry can be a range to be sampled from. The drift rate is defined as change per step\n        num_samples: number of experimental conditions to select\n        random_state: the seed value for the random number generator\n    Returns:\n        Sampled pool of experimental conditions\n\n    Examples:\n        We create a reward probabilty sequence for five two arm bandit tasks. The reward\n        probabilities for each arm should be .5 and constant.\n        &gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3, num_samples=1, random_state=42)\n        [[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]\n\n        If we want more arms:\n        &gt;&gt;&gt; pool_proba(num_probabilities=4, sequence_length=3, num_samples=1, random_state=42)\n        [[[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]]]\n\n        longer sequence:\n        &gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=5, num_samples=1, random_state=42)\n        [[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]\n\n        more sequences:\n        &gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3, num_samples=2, random_state=42)\n        [[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]], [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]\n\n        We  can set fixed initial values:\n        &gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3,\n        ...     initial_probabilities=[0.,.4], random_state=42)\n        [[[0.0, 0.4], [0.0, 0.4], [0.0, 0.4]]]\n\n        And drift rates:\n        &gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3,\n        ...     initial_probabilities=[0.,.4],\n        ...     sigmas=[.1, .5], random_state=42)\n        [[[0.0, 0.4], [0.030471707975443137, 0.7752255979032286], [0.0, 1.0]]]\n\n        We can also sample the initial values by passing a range:\n        &gt;&gt;&gt; pool_proba(num_probabilities=2, sequence_length=3,\n        ...     initial_probabilities=[[0, .2],[.8, 1.]],\n        ...     sigmas=[[0., .25], [0., .5]],\n        ...     random_state=42)\n        [[[0.15479120971119267, 0.81883546957753], \\\n[0.23713042219259264, 0.8811974469636589], \\\n[0.34032881599649456, 0.7269307761486841]]]\n    \"\"\"\n    rng = np.random.default_rng(random_state)\n    if initial_probabilities:\n        assert len(initial_probabilities) == num_probabilities\n    else:\n        initial_probabilities = [.5 for _ in range(num_probabilities)]\n    if sigmas:\n        assert len(sigmas) == num_probabilities\n    else:\n        sigmas = [0 for _ in range(num_probabilities)]\n    res = []\n    for _ in range(num_samples):\n        seq = []\n        for idx, el in enumerate(initial_probabilities):\n\n            if _is_iterable(el):\n                start = rng.uniform(el[0], el[1])\n            else:\n                start = el\n            if _is_iterable(sigmas[idx]):\n                sigma = rng.uniform(sigmas[idx][0], sigmas[idx][1])\n            else:\n                sigma = sigmas[idx]\n            prob = [start]\n            for _ in range(sequence_length - 1):\n                start += rng.normal(loc=0, scale=sigma)\n                start = max(0., min(start, 1.))\n                prob.append(start)\n            seq.append(prob)\n        res.append(seq)\n    for idx in range(len(res)):\n        res[idx] = _transpose_matrix(res[idx])\n    return res\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/","title":"autora.experimentalist.falsification","text":""},{"location":"reference/autora/experimentalist/falsification/#autora.experimentalist.falsification.falsification_score_sample","title":"<code>falsification_score_sample(conditions, model, reference_conditions, reference_observations, metadata=None, num_samples=None, training_epochs=1000, training_lr=0.001, plot=False)</code>","text":"<p>A Sampler that generates samples of experimental conditions with the objective of maximizing the (approximated) loss of a model relating experimental conditions to observations. The samples are generated by first training a neural network to approximate the loss of a model for all patterns in the training data. Once trained, the network is then provided with the candidate samples of experimental conditions and the selects those with the highest loss.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>The candidate samples of experimental conditions to be evaluated.</p> required <code>model</code> <p>Scikit-learn model, could be either a classification or regression model</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>Experimental conditions that the model was trained on</p> required <code>reference_observations</code> <code>Union[DataFrame, ndarray]</code> <p>Observations that the model was trained to predict</p> required <code>metadata</code> <code>Optional[VariableCollection]</code> <p>Meta-data about the dependent and independent variables specifying the experimental conditions</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>Number of samples to return</p> <code>None</code> <code>training_epochs</code> <code>int</code> <p>Number of epochs to train the popper network for approximating the</p> <code>1000</code> <code>training_lr</code> <code>float</code> <p>Learning rate for training the popper network</p> <code>0.001</code> <code>plot</code> <code>bool</code> <p>Print out the prediction of the popper network as well as its training loss</p> <code>False</code> <p>Returns:</p> Name Type Description <code>new_conditions</code> <p>Samples of experimental conditions with the highest loss</p> <code>scores</code> <p>Normalized falsification scores for the samples</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/__init__.py</code> <pre><code>def falsification_score_sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    model,\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    reference_observations: Union[pd.DataFrame, np.ndarray],\n    metadata: Optional[VariableCollection] = None,\n    num_samples: Optional[int] = None,\n    training_epochs: int = 1000,\n    training_lr: float = 1e-3,\n    plot: bool = False,\n):\n    \"\"\"\n    A Sampler that generates samples of experimental conditions with the objective of maximizing the\n    (approximated) loss of a model relating experimental conditions to observations. The samples are\n    generated by first training a neural network to approximate the loss of a model for all patterns\n    in the training data. Once trained, the network is then provided with the candidate samples of\n    experimental conditions and the selects those with the highest loss.\n\n    Args:\n        conditions: The candidate samples of experimental conditions to be evaluated.\n        model: Scikit-learn model, could be either a classification or regression model\n        reference_conditions: Experimental conditions that the model was trained on\n        reference_observations: Observations that the model was trained to predict\n        metadata: Meta-data about the dependent and independent variables specifying\n            the experimental conditions\n        num_samples: Number of samples to return\n        training_epochs: Number of epochs to train the popper network for approximating the\n        error of the model\n        training_lr: Learning rate for training the popper network\n        plot: Print out the prediction of the popper network as well as its training loss\n\n    Returns:\n        new_conditions: Samples of experimental conditions with the highest loss\n        scores: Normalized falsification scores for the samples\n\n    \"\"\"\n\n    if isinstance(conditions, Iterable) and not isinstance(conditions, pd.DataFrame):\n        conditions = np.array(list(conditions))\n\n    condition_pool_copy = conditions.copy()\n    conditions = np.array(conditions)\n    reference_conditions = np.array(reference_conditions)\n    reference_observations = np.array(reference_observations)\n\n    if len(reference_conditions.shape) == 1:\n        reference_conditions = reference_conditions.reshape(-1, 1)\n\n    predicted_observations = model.predict(reference_conditions)\n\n    new_conditions, new_scores = falsification_score_sample_from_predictions(\n        conditions,\n        predicted_observations,\n        reference_conditions,\n        reference_observations,\n        metadata,\n        num_samples,\n        training_epochs,\n        training_lr,\n        plot,\n    )\n\n    if isinstance(condition_pool_copy, pd.DataFrame):\n        sorted_conditions = pd.DataFrame(\n            new_conditions, columns=condition_pool_copy.columns\n        )\n    else:\n        sorted_conditions = pd.DataFrame(new_conditions)\n\n    sorted_conditions[\"score\"] = new_scores\n\n    return sorted_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/#autora.experimentalist.falsification.falsification_score_sample_from_predictions","title":"<code>falsification_score_sample_from_predictions(conditions, predicted_observations, reference_conditions, reference_observations, metadata=None, num_samples=None, training_epochs=1000, training_lr=0.001, plot=False)</code>","text":"<p>A Sampler that generates samples of experimental conditions with the objective of maximizing the (approximated) loss of a model relating experimental conditions to observations. The samples are generated by first training a neural network to approximate the loss of a model for all patterns in the training data. Once trained, the network is then provided with the candidate samples of experimental conditions and the selects those with the highest loss.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>The candidate samples of experimental conditions to be evaluated.</p> required <code>predicted_observations</code> <code>Union[DataFrame, ndarray]</code> <p>Prediction obtained from the model for the set of reference experimental conditions</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>Experimental conditions that the model was trained on</p> required <code>reference_observations</code> <code>ndarray</code> <p>Observations that the model was trained to predict</p> required <code>metadata</code> <code>Optional[VariableCollection]</code> <p>Meta-data about the dependent and independent variables specifying the experimental conditions</p> <code>None</code> <code>num_samples</code> <code>Optional[int]</code> <p>Number of samples to return</p> <code>None</code> <code>training_epochs</code> <code>int</code> <p>Number of epochs to train the popper network for approximating the</p> <code>1000</code> <code>training_lr</code> <code>float</code> <p>Learning rate for training the popper network</p> <code>0.001</code> <code>plot</code> <code>bool</code> <p>Print out the prediction of the popper network as well as its training loss</p> <code>False</code> <p>Returns:</p> Name Type Description <code>new_conditions</code> <p>Samples of experimental conditions with the highest loss</p> <code>scores</code> <p>Normalized falsification scores for the samples</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/__init__.py</code> <pre><code>def falsification_score_sample_from_predictions(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    predicted_observations: Union[pd.DataFrame, np.ndarray],\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    reference_observations: np.ndarray,\n    metadata: Optional[VariableCollection] = None,\n    num_samples: Optional[int] = None,\n    training_epochs: int = 1000,\n    training_lr: float = 1e-3,\n    plot: bool = False,\n):\n    \"\"\"\n    A Sampler that generates samples of experimental conditions with the objective of maximizing the\n    (approximated) loss of a model relating experimental conditions to observations. The samples are\n    generated by first training a neural network to approximate the loss of a model for all patterns\n    in the training data. Once trained, the network is then provided with the candidate samples of\n    experimental conditions and the selects those with the highest loss.\n\n    Args:\n        conditions: The candidate samples of experimental conditions to be evaluated.\n        predicted_observations: Prediction obtained from the model for the set of\n            reference experimental conditions\n        reference_conditions: Experimental conditions that the model was trained on\n        reference_observations: Observations that the model was trained to predict\n        metadata: Meta-data about the dependent and independent variables specifying\n            the experimental conditions\n        num_samples: Number of samples to return\n        training_epochs: Number of epochs to train the popper network for approximating the\n        error of the model\n        training_lr: Learning rate for training the popper network\n        plot: Print out the prediction of the popper network as well as its training loss\n\n    Returns:\n        new_conditions: Samples of experimental conditions with the highest loss\n        scores: Normalized falsification scores for the samples\n\n    \"\"\"\n\n    conditions = np.array(conditions)\n    reference_conditions = np.array(reference_conditions)\n    reference_observations = np.array(reference_observations)\n\n    if len(conditions.shape) == 1:\n        conditions = conditions.reshape(-1, 1)\n\n    reference_conditions = np.array(reference_conditions)\n    if len(reference_conditions.shape) == 1:\n        reference_conditions = reference_conditions.reshape(-1, 1)\n\n    reference_observations = np.array(reference_observations)\n    if len(reference_observations.shape) == 1:\n        reference_observations = reference_observations.reshape(-1, 1)\n\n    if num_samples is None:\n        num_samples = conditions.shape[0]\n\n    if metadata is not None:\n        if metadata.dependent_variables[0].type == ValueType.CLASS:\n            # find all unique values in reference_observations\n            num_classes = len(np.unique(reference_observations))\n            reference_observations = class_to_onehot(\n                reference_observations, n_classes=num_classes\n            )\n\n    # create list of IV limits\n    iv_limit_list = get_iv_limits(reference_conditions, metadata)\n\n    popper_net, model_loss = train_popper_net(\n        predicted_observations,\n        reference_conditions,\n        reference_observations,\n        metadata,\n        iv_limit_list,\n        training_epochs,\n        training_lr,\n        plot,\n    )\n\n    # now that the popper network is trained we can assign losses to all data points to be evaluated\n    popper_input = Variable(torch.from_numpy(conditions)).float()\n    Y = popper_net(popper_input).detach().numpy().flatten()\n    scaler = StandardScaler()\n    score = scaler.fit_transform(Y.reshape(-1, 1)).flatten()\n\n    # order rows in Y from highest to lowest\n    sorted_conditions = conditions[np.argsort(score)[::-1]]\n    sorted_score = score[np.argsort(score)[::-1]]\n\n    return sorted_conditions[0:num_samples], sorted_score[0:num_samples]\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/#autora.experimentalist.falsification.pool","title":"<code>pool(model, reference_conditions, reference_observations, metadata, num_samples=100, training_epochs=1000, optimization_epochs=1000, training_lr=0.001, optimization_lr=0.001, limit_offset=0, limit_repulsion=0, plot=False)</code>","text":"<p>A pooler that generates samples for independent variables with the objective of maximizing the (approximated) loss of the model. The samples are generated by first training a neural network to approximate the loss of a model for all patterns in the training data. Once trained, the network is then inverted to generate samples that maximize the approximated loss of the model.</p> <p>Note: If the pooler returns samples that are close to the boundaries of the variable space, then it is advisable to increase the limit_repulsion parameter (e.g., to 0.000001).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Scikit-learn model, could be either a classification or regression model</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>data that the model was trained on</p> required <code>reference_observations</code> <code>Union[DataFrame, ndarray]</code> <p>labels that the model was trained on</p> required <code>metadata</code> <code>VariableCollection</code> <p>Meta-data about the dependent and independent variables</p> required <code>num_samples</code> <code>int</code> <p>number of samples to return</p> <code>100</code> <code>training_epochs</code> <code>int</code> <p>number of epochs to train the popper network for approximating the</p> <code>1000</code> <code>optimization_epochs</code> <code>int</code> <p>number of epochs to optimize the samples based on the trained</p> <code>1000</code> <code>training_lr</code> <code>float</code> <p>learning rate for training the popper network</p> <code>0.001</code> <code>optimization_lr</code> <code>float</code> <p>learning rate for optimizing the samples</p> <code>0.001</code> <code>limit_offset</code> <code>float</code> <p>a limited offset to prevent the samples from being too close to the value</p> <code>0</code> <code>limit_repulsion</code> <code>float</code> <p>a limited repulsion to prevent the samples from being too close to the</p> <code>0</code> <code>plot</code> <code>bool</code> <p>print out the prediction of the popper network as well as its training loss</p> <code>False</code> <p>Returns: Sampled pool</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/__init__.py</code> <pre><code>def pool(\n    model,\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    reference_observations: Union[pd.DataFrame, np.ndarray],\n    metadata: VariableCollection,\n    num_samples: int = 100,\n    training_epochs: int = 1000,\n    optimization_epochs: int = 1000,\n    training_lr: float = 1e-3,\n    optimization_lr: float = 1e-3,\n    limit_offset: float = 0,  # 10**-10,\n    limit_repulsion: float = 0,\n    plot: bool = False,\n):\n    \"\"\"\n    A pooler that generates samples for independent variables with the objective of maximizing the\n    (approximated) loss of the model. The samples are generated by first training a neural network\n    to approximate the loss of a model for all patterns in the training data.\n    Once trained, the network is then inverted to generate samples that maximize the approximated\n    loss of the model.\n\n    Note: If the pooler returns samples that are close to the boundaries of the variable space,\n    then it is advisable to increase the limit_repulsion parameter (e.g., to 0.000001).\n\n    Args:\n        model: Scikit-learn model, could be either a classification or regression model\n        reference_conditions: data that the model was trained on\n        reference_observations: labels that the model was trained on\n        metadata: Meta-data about the dependent and independent variables\n        num_samples: number of samples to return\n        training_epochs: number of epochs to train the popper network for approximating the\n        error fo the model\n        optimization_epochs: number of epochs to optimize the samples based on the trained\n        popper network\n        training_lr: learning rate for training the popper network\n        optimization_lr: learning rate for optimizing the samples\n        limit_offset: a limited offset to prevent the samples from being too close to the value\n        boundaries\n        limit_repulsion: a limited repulsion to prevent the samples from being too close to the\n        allowed value boundaries\n        plot: print out the prediction of the popper network as well as its training loss\n\n    Returns: Sampled pool\n\n    \"\"\"\n\n    # format input\n\n    if isinstance(reference_conditions, pd.DataFrame):\n        reference_conditions = align_dataframe_to_ivs(\n            reference_conditions, metadata.independent_variables\n        )\n\n    reference_conditions_np = np.array(reference_conditions)\n    if len(reference_conditions_np.shape) == 1:\n        reference_conditions_np = reference_conditions_np.reshape(-1, 1)\n\n    x = np.empty([num_samples, reference_conditions_np.shape[1]])\n\n    reference_observations = np.array(reference_observations)\n    if len(reference_observations.shape) == 1:\n        reference_observations = reference_observations.reshape(-1, 1)\n\n    if metadata.dependent_variables[0].type == ValueType.CLASS:\n        # find all unique values in reference_observations\n        num_classes = len(np.unique(reference_observations))\n        reference_observations = class_to_onehot(\n            reference_observations, n_classes=num_classes\n        )\n\n    reference_conditions_tensor = torch.from_numpy(reference_conditions_np).float()\n\n    iv_limit_list = get_iv_limits(reference_conditions_np, metadata)\n\n    popper_net, model_loss = train_popper_net_with_model(\n        model,\n        reference_conditions_np,\n        reference_observations,\n        metadata,\n        iv_limit_list,\n        training_epochs,\n        training_lr,\n        plot,\n    )\n\n    # now that the popper network is trained we can sample new data points\n    # to sample data points we need to provide the popper network with an initial\n    # condition we will sample those initial conditions proportional to the loss of the current\n    # model\n\n    # feed average model losses through softmax\n    # model_loss_avg= torch.from_numpy(np.mean(model_loss.detach().numpy(), axis=1)).float()\n    softmax_func = torch.nn.Softmax(dim=0)\n    probabilities = softmax_func(model_loss)\n    # sample data point in proportion to model loss\n    transform_category = torch.distributions.categorical.Categorical(probabilities)\n\n    popper_net.freeze_weights()\n\n    for condition in range(num_samples):\n\n        index = transform_category.sample()\n        input_sample = torch.flatten(reference_conditions_tensor[index, :])\n        popper_input = Variable(input_sample, requires_grad=True)\n\n        # invert the popper network to determine optimal experiment conditions\n        for optimization_epoch in range(optimization_epochs):\n            # feedforward pass on popper network\n            popper_prediction = popper_net(popper_input)\n            # compute gradient that maximizes output of popper network\n            # (i.e. predicted loss of original model)\n            popper_loss_optim = -popper_prediction\n            popper_loss_optim.backward()\n\n            with torch.no_grad():\n\n                # first add repulsion from variable limits\n                for idx in range(len(input_sample)):\n                    iv_value = popper_input[idx]\n                    iv_limits = iv_limit_list[idx]\n                    dist_to_min = np.abs(iv_value - np.min(iv_limits))\n                    dist_to_max = np.abs(iv_value - np.max(iv_limits))\n                    # deal with boundary case where distance is 0 or very small\n                    dist_to_min = np.max([dist_to_min, 0.00000001])\n                    dist_to_max = np.max([dist_to_max, 0.00000001])\n                    repulsion_from_min = limit_repulsion / (dist_to_min**2)\n                    repulsion_from_max = limit_repulsion / (dist_to_max**2)\n                    iv_value_repulsed = (\n                        iv_value + repulsion_from_min - repulsion_from_max\n                    )\n                    popper_input[idx] = iv_value_repulsed\n\n                # now add gradient for theory loss maximization\n                delta = -optimization_lr * popper_input.grad\n                popper_input += delta\n\n                # finally, clip input variable from its limits\n                for idx in range(len(input_sample)):\n                    iv_raw_value = input_sample[idx]\n                    iv_limits = iv_limit_list[idx]\n                    iv_clipped_value = np.min(\n                        [iv_raw_value, np.max(iv_limits) - limit_offset]\n                    )\n                    iv_clipped_value = np.max(\n                        [\n                            iv_clipped_value,\n                            np.min(iv_limits) + limit_offset,\n                        ]\n                    )\n                    popper_input[idx] = iv_clipped_value\n                popper_input.grad.zero_()\n\n        # add condition to new experiment sequence\n        for idx in range(len(input_sample)):\n            iv_limits = iv_limit_list[idx]\n\n            # first clip value\n            iv_clipped_value = np.min([iv_raw_value, np.max(iv_limits) - limit_offset])\n            iv_clipped_value = np.max(\n                [iv_clipped_value, np.min(iv_limits) + limit_offset]\n            )\n            # make sure to convert variable to original scale\n            iv_clipped_scaled_value = iv_clipped_value\n\n            x[condition, idx] = iv_clipped_scaled_value\n    if isinstance(reference_conditions, pd.DataFrame):\n        new_conditions = pd.DataFrame(x, columns=reference_conditions.columns)\n    else:\n        new_conditions = pd.DataFrame(x)\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/#autora.experimentalist.falsification.sample","title":"<code>sample(conditions, model, reference_conditions, reference_observations, metadata, num_samples=None, training_epochs=1000, training_lr=0.001, plot=False)</code>","text":"<p>A Sampler that generates samples of experimental conditions with the objective of maximizing the (approximated) loss of a model relating experimental conditions to observations. The samples are generated by first training a neural network to approximate the loss of a model for all patterns in the training data. Once trained, the network is then provided with the candidate samples of experimental conditions and the selects those with the highest loss.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>The candidate samples of experimental conditions to be evaluated.</p> required <code>model</code> <p>Scikit-learn model, could be either a classification or regression model</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>Experimental conditions that the model was trained on</p> required <code>reference_observations</code> <code>Union[DataFrame, ndarray]</code> <p>Observations that the model was trained to predict</p> required <code>metadata</code> <code>VariableCollection</code> <p>Meta-data about the dependent and independent variables specifying the experimental conditions</p> required <code>num_samples</code> <code>Optional[int]</code> <p>Number of samples to return</p> <code>None</code> <code>training_epochs</code> <code>int</code> <p>Number of epochs to train the popper network for approximating the</p> <code>1000</code> <code>training_lr</code> <code>float</code> <p>Learning rate for training the popper network</p> <code>0.001</code> <code>plot</code> <code>bool</code> <p>Print out the prediction of the popper network as well as its training loss</p> <code>False</code> <p>Returns: Samples with the highest loss</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    model,\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    reference_observations: Union[pd.DataFrame, np.ndarray],\n    metadata: VariableCollection,\n    num_samples: Optional[int] = None,\n    training_epochs: int = 1000,\n    training_lr: float = 1e-3,\n    plot: bool = False,\n):\n    \"\"\"\n    A Sampler that generates samples of experimental conditions with the objective of maximizing the\n    (approximated) loss of a model relating experimental conditions to observations. The samples are\n    generated by first training a neural network to approximate the loss of a model for all patterns\n    in the training data. Once trained, the network is then provided with the candidate samples of\n    experimental conditions and the selects those with the highest loss.\n\n    Args:\n        conditions: The candidate samples of experimental conditions to be evaluated.\n        model: Scikit-learn model, could be either a classification or regression model\n        reference_conditions: Experimental conditions that the model was trained on\n        reference_observations: Observations that the model was trained to predict\n        metadata: Meta-data about the dependent and independent variables specifying the\n            experimental conditions\n        num_samples: Number of samples to return\n        training_epochs: Number of epochs to train the popper network for approximating the\n        error of the model\n        training_lr: Learning rate for training the popper network\n        plot: Print out the prediction of the popper network as well as its training loss\n\n    Returns: Samples with the highest loss\n\n    \"\"\"\n\n    # format input\n\n    if isinstance(conditions, Iterable) and not isinstance(conditions, pd.DataFrame):\n        conditions = np.array(list(conditions))\n\n    condition_pool_copy = conditions.copy()\n    conditions = np.array(conditions)\n    reference_observations = np.array(reference_observations)\n    reference_conditions = np.array(reference_conditions)\n    if len(reference_conditions.shape) == 1:\n        reference_conditions = reference_conditions.reshape(-1, 1)\n\n    # get target pattern for popper net\n    model_predict = getattr(model, \"predict_proba\", None)\n    if callable(model_predict) is False:\n        model_predict = getattr(model, \"predict\", None)\n\n    if callable(model_predict) is False or model_predict is None:\n        raise Exception(\"Model must have `predict` or `predict_proba` method.\")\n\n    predicted_observations = model_predict(reference_conditions)\n    if isinstance(predicted_observations, np.ndarray) is False:\n        try:\n            predicted_observations = np.array(predicted_observations)\n        except Exception:\n            raise Exception(\"Model prediction must be convertable to numpy array.\")\n    if predicted_observations.ndim == 1:\n        predicted_observations = predicted_observations.reshape(-1, 1)\n\n    new_conditions, scores = falsification_score_sample_from_predictions(\n        conditions,\n        predicted_observations,\n        reference_conditions,\n        reference_observations,\n        metadata,\n        num_samples,\n        training_epochs,\n        training_lr,\n        plot,\n    )\n\n    if isinstance(condition_pool_copy, pd.DataFrame):\n        new_conditions = pd.DataFrame(\n            new_conditions, columns=condition_pool_copy.columns\n        )\n    else:\n        new_conditions = pd.DataFrame(new_conditions)\n\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/popper_net/","title":"autora.experimentalist.falsification.popper_net","text":""},{"location":"reference/autora/experimentalist/falsification/popper_net/#autora.experimentalist.falsification.popper_net.PopperNet","title":"<code>PopperNet</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/popper_net.py</code> <pre><code>class PopperNet(nn.Module):\n    def __init__(self, n_input: torch.Tensor, n_output: torch.Tensor):\n        # Perform initialization of the pytorch superclass\n        super(PopperNet, self).__init__()\n\n        # Define network layer dimensions\n        D_in, H1, H2, H3, D_out = [n_input, 64, 64, 64, n_output]\n\n        # Define layer types\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, H3)\n        self.linear4 = nn.Linear(H3, D_out)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        This method defines the network layering and activation functions\n        \"\"\"\n        x = self.linear1(x)  # hidden layer\n        x = torch.tanh(x)  # activation function\n\n        x = self.linear2(x)  # hidden layer\n        x = torch.tanh(x)  # activation function\n\n        x = self.linear3(x)  # hidden layer\n        x = torch.tanh(x)  # activation function\n\n        x = self.linear4(x)  # output layer\n\n        return x\n\n    def freeze_weights(self):\n        for param in self.parameters():\n            param.requires_grad = False\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/popper_net/#autora.experimentalist.falsification.popper_net.PopperNet.forward","title":"<code>forward(x)</code>","text":"<p>This method defines the network layering and activation functions</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/popper_net.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"\n    This method defines the network layering and activation functions\n    \"\"\"\n    x = self.linear1(x)  # hidden layer\n    x = torch.tanh(x)  # activation function\n\n    x = self.linear2(x)  # hidden layer\n    x = torch.tanh(x)  # activation function\n\n    x = self.linear3(x)  # hidden layer\n    x = torch.tanh(x)  # activation function\n\n    x = self.linear4(x)  # output layer\n\n    return x\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/popper_net/#autora.experimentalist.falsification.popper_net.train_popper_net","title":"<code>train_popper_net(model_prediction, reference_conditions, reference_observations, metadata, iv_limit_list, training_epochs=1000, training_lr=0.001, plot=False)</code>","text":"<p>Trains a neural network to approximate the loss of a model for all patterns in the training data Once trained, the network is then inverted to generate samples that maximize the approximated loss of the model.</p> <p>Note: If the pooler returns samples that are close to the boundaries of the variable space, then it is advisable to increase the limit_repulsion parameter (e.g., to 0.000001).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Scikit-learn model, could be either a classification or regression model</p> required <code>reference_conditions</code> <code>ndarray</code> <p>data that the model was trained on</p> required <code>reference_observations</code> <code>ndarray</code> <p>labels that the model was trained on</p> required <code>metadata</code> <code>VariableCollection</code> <p>Meta-data about the dependent and independent variables</p> required <code>training_epochs</code> <code>int</code> <p>number of epochs to train the popper network for approximating the</p> <code>1000</code> <code>training_lr</code> <code>float</code> <p>learning rate for training the popper network</p> <code>0.001</code> <code>plot</code> <code>bool</code> <p>print out the prediction of the popper network as well as its training loss</p> <code>False</code> <p>Returns: Trained popper net.</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/popper_net.py</code> <pre><code>def train_popper_net(\n    model_prediction,\n    reference_conditions: np.ndarray,\n    reference_observations: np.ndarray,\n    metadata: VariableCollection,\n    iv_limit_list: List,\n    training_epochs: int = 1000,\n    training_lr: float = 1e-3,\n    plot: bool = False,\n):\n    \"\"\"\n    Trains a neural network to approximate the loss of a model for all patterns in the training data\n    Once trained, the network is then inverted to generate samples that maximize the approximated\n    loss of the model.\n\n    Note: If the pooler returns samples that are close to the boundaries of the variable space,\n    then it is advisable to increase the limit_repulsion parameter (e.g., to 0.000001).\n\n    Args:\n        model: Scikit-learn model, could be either a classification or regression model\n        reference_conditions: data that the model was trained on\n        reference_observations: labels that the model was trained on\n        metadata: Meta-data about the dependent and independent variables\n        training_epochs: number of epochs to train the popper network for approximating the\n        error fo the model\n        training_lr: learning rate for training the popper network\n        plot: print out the prediction of the popper network as well as its training loss\n\n    Returns: Trained popper net.\n\n    \"\"\"\n\n    # get dimensions of input and output\n    n_input = reference_conditions.shape[1]\n    n_output = 1  # only predicting one MSE\n\n    # get input pattern for popper net\n    popper_input = Variable(\n        torch.from_numpy(reference_conditions), requires_grad=False\n    ).float()\n\n    # get target pattern for popper net\n    if isinstance(model_prediction, np.ndarray) is False:\n        try:\n            model_prediction = np.array(model_prediction)\n        except Exception:\n            raise Exception(\"Model prediction must be convertable to numpy array.\")\n    if model_prediction.ndim == 1:\n        model_prediction = model_prediction.reshape(-1, 1)\n\n    criterion = nn.MSELoss()\n    model_loss = (model_prediction - reference_observations) ** 2\n    model_loss = np.mean(model_loss, axis=1)\n\n    # standardize the loss\n    scaler = StandardScaler()\n    model_loss = scaler.fit_transform(model_loss.reshape(-1, 1)).flatten()\n\n    model_loss = torch.from_numpy(model_loss).float()\n    popper_target = Variable(model_loss, requires_grad=False)\n\n    # create the network\n    popper_net = PopperNet(n_input, n_output)\n\n    # reformat input in case it is 1D\n    if len(popper_input.shape) == 1:\n        popper_input = popper_input.flatten()\n        popper_input = popper_input.reshape(-1, 1)\n\n    # define the optimizer\n    popper_optimizer = torch.optim.Adam(popper_net.parameters(), lr=training_lr)\n\n    # train the network\n    losses = []\n    for epoch in range(training_epochs):\n        popper_prediction = popper_net(popper_input)\n        loss = criterion(popper_prediction, popper_target.reshape(-1, 1))\n        popper_optimizer.zero_grad()\n        loss.backward()\n        popper_optimizer.step()\n        losses.append(loss.item())\n\n    if plot:\n        if len(iv_limit_list) &gt; 1:\n            Warning(\n                \"Plotting currently not supported for more than two independent variables.\"\n            )\n        else:\n            popper_input_full = np.linspace(\n                iv_limit_list[0][0], iv_limit_list[0][1], 1000\n            ).reshape(-1, 1)\n            popper_input_full = Variable(\n                torch.from_numpy(popper_input_full), requires_grad=False\n            ).float()\n            popper_prediction = popper_net(popper_input_full)\n            plot_falsification_diagnostics(\n                losses,\n                popper_input,\n                popper_input_full,\n                popper_prediction,\n                popper_target,\n                model_prediction,\n                reference_observations,\n            )\n\n    return popper_net, model_loss\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/popper_net/#autora.experimentalist.falsification.popper_net.train_popper_net_with_model","title":"<code>train_popper_net_with_model(model, reference_conditions, reference_observations, metadata, iv_limit_list, training_epochs=1000, training_lr=0.001, plot=False)</code>","text":"<p>Trains a neural network to approximate the loss of a model for all patterns in the training data Once trained, the network is then inverted to generate samples that maximize the approximated loss of the model.</p> <p>Note: If the pooler returns samples that are close to the boundaries of the variable space, then it is advisable to increase the limit_repulsion parameter (e.g., to 0.000001).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Scikit-learn model, could be either a classification or regression model</p> required <code>reference_conditions</code> <code>ndarray</code> <p>data that the model was trained on</p> required <code>reference_observations</code> <code>ndarray</code> <p>labels that the model was trained on</p> required <code>metadata</code> <code>VariableCollection</code> <p>Meta-data about the dependent and independent variables</p> required <code>training_epochs</code> <code>int</code> <p>number of epochs to train the popper network for approximating the</p> <code>1000</code> <code>training_lr</code> <code>float</code> <p>learning rate for training the popper network</p> <code>0.001</code> <code>plot</code> <code>bool</code> <p>print out the prediction of the popper network as well as its training loss</p> <code>False</code> <p>Returns: Trained popper net.</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/popper_net.py</code> <pre><code>def train_popper_net_with_model(\n    model,\n    reference_conditions: np.ndarray,\n    reference_observations: np.ndarray,\n    metadata: VariableCollection,\n    iv_limit_list: List,\n    training_epochs: int = 1000,\n    training_lr: float = 1e-3,\n    plot: bool = False,\n):\n    \"\"\"\n    Trains a neural network to approximate the loss of a model for all patterns in the training data\n    Once trained, the network is then inverted to generate samples that maximize the approximated\n    loss of the model.\n\n    Note: If the pooler returns samples that are close to the boundaries of the variable space,\n    then it is advisable to increase the limit_repulsion parameter (e.g., to 0.000001).\n\n    Args:\n        model: Scikit-learn model, could be either a classification or regression model\n        reference_conditions: data that the model was trained on\n        reference_observations: labels that the model was trained on\n        metadata: Meta-data about the dependent and independent variables\n        training_epochs: number of epochs to train the popper network for approximating the\n        error fo the model\n        training_lr: learning rate for training the popper network\n        plot: print out the prediction of the popper network as well as its training loss\n\n    Returns: Trained popper net.\n\n    \"\"\"\n\n    model_predict = getattr(model, \"predict_proba\", None)\n    if callable(model_predict) is False:\n        model_predict = getattr(model, \"predict\", None)\n\n    if callable(model_predict) is False or model_predict is None:\n        raise Exception(\"Model must have `predict` or `predict_proba` method.\")\n\n    model_prediction = model_predict(reference_conditions)\n\n    return train_popper_net(\n        model_prediction,\n        reference_conditions,\n        reference_observations,\n        metadata,\n        iv_limit_list,\n        training_epochs,\n        training_lr,\n        plot,\n    )\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/utils/","title":"autora.experimentalist.falsification.utils","text":""},{"location":"reference/autora/experimentalist/falsification/utils/#autora.experimentalist.falsification.utils.align_dataframe_to_ivs","title":"<code>align_dataframe_to_ivs(dataframe, independent_variables)</code>","text":"<p>Aligns a dataframe to a metadata object, ensuring that the columns are in the same order as the independent variables in the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>a dataframe with columns to align</p> required <code>independent_variables</code> <code>List[IV]</code> <p>a list of independent variables</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>a dataframe with columns in the same order as the independent variables in the metadata</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/utils.py</code> <pre><code>def align_dataframe_to_ivs(\n    dataframe: pd.DataFrame, independent_variables: List[IV]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aligns a dataframe to a metadata object, ensuring that the columns are in the same order\n    as the independent variables in the metadata.\n\n    Args:\n        dataframe: a dataframe with columns to align\n        independent_variables: a list of independent variables\n\n    Returns:\n        a dataframe with columns in the same order as the independent variables in the metadata\n    \"\"\"\n    variable_names = list()\n    for variable in independent_variables:\n        variable_names.append(variable.name)\n    return dataframe[variable_names]\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/utils/#autora.experimentalist.falsification.utils.class_to_onehot","title":"<code>class_to_onehot(y, n_classes=None)</code>","text":"<p>Converts a class vector (integers) to binary class matrix.</p> <p>E.g. for use with categorical_crossentropy.</p>"},{"location":"reference/autora/experimentalist/falsification/utils/#autora.experimentalist.falsification.utils.class_to_onehot--arguments","title":"Arguments","text":"<pre><code>y: class vector to be converted into a matrix\n    (integers from 0 to num_classes).\nn_classes: total number of classes.\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/utils/#autora.experimentalist.falsification.utils.class_to_onehot--returns","title":"Returns","text":"<pre><code>A binary matrix representation of the input.\n</code></pre> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/utils.py</code> <pre><code>def class_to_onehot(y: np.array, n_classes: Optional[int] = None):\n    \"\"\"Converts a class vector (integers) to binary class matrix.\n\n    E.g. for use with categorical_crossentropy.\n\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        n_classes: total number of classes.\n\n    # Returns\n        A binary matrix representation of the input.\n    \"\"\"\n    y = np.array(y, dtype=\"int\")\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) &gt; 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not n_classes:\n        n_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, n_classes))\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (n_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical\n</code></pre>"},{"location":"reference/autora/experimentalist/falsification/utils/#autora.experimentalist.falsification.utils.get_iv_limits","title":"<code>get_iv_limits(reference_conditions, metadata)</code>","text":"<p>Get the limits of the independent variables</p> <p>Parameters:</p> Name Type Description Default <code>reference_conditions</code> <code>ndarray</code> <p>data that the model was trained on</p> required <code>metadata</code> <code>VariableCollection</code> <p>Meta-data about the dependent and independent variables</p> required <p>Returns: List of limits for each independent variable</p> Source code in <code>temp_dir/falsification/src/autora/experimentalist/falsification/utils.py</code> <pre><code>def get_iv_limits(\n    reference_conditions: np.ndarray,\n    metadata: VariableCollection,\n):\n    \"\"\"\n    Get the limits of the independent variables\n\n    Args:\n        reference_conditions: data that the model was trained on\n        metadata: Meta-data about the dependent and independent variables\n\n    Returns: List of limits for each independent variable\n    \"\"\"\n\n    # create list of IV limits\n    iv_limit_list = list()\n    if metadata is not None:\n        ivs = metadata.independent_variables\n        for iv in ivs:\n            if hasattr(iv, \"value_range\"):\n                value_range = cast(Tuple, iv.value_range)\n                lower_bound = value_range[0]\n                upper_bound = value_range[1]\n                iv_limit_list.append(([lower_bound, upper_bound]))\n    else:\n        for col in range(reference_conditions.shape[1]):\n            min = np.min(reference_conditions[:, col])\n            max = np.max(reference_conditions[:, col])\n            iv_limit_list.append(([min, max]))\n\n    return iv_limit_list\n</code></pre>"},{"location":"reference/autora/experimentalist/inequality/","title":"autora.experimentalist.inequality","text":""},{"location":"reference/autora/experimentalist/inequality/#autora.experimentalist.inequality.sample","title":"<code>sample(conditions, reference_conditions, num_samples=1, equality_distance=0, metric='euclidean')</code>","text":"<p>This inequality experimentalist chooses from the pool of IV conditions according to their inequality with respect to a reference pool reference_conditions. Two IVs are considered equal if their distance is less than the equality_distance. The IVs chosen first are feed back into reference_conditions and are included in the summed equality calculation.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of IV conditions to evaluate inequality</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>reference pool of IV conditions</p> required <code>num_samples</code> <code>int</code> <p>number of samples to select</p> <code>1</code> <code>equality_distance</code> <code>float</code> <p>the distance to decide if two data points are equal.</p> <code>0</code> <code>metric</code> <code>str</code> <p>inequality measure. Options: 'euclidean', 'manhattan', 'chebyshev', 'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine', 'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice', 'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'yule'. See <code>sklearn.metrics.DistanceMetric</code> for more details.</p> <code>'euclidean'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Sampled pool</p> <p>Examples:</p> <p>The value 1 is not in the reference. Therefore it is choosen.</p> <pre><code>&gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [2, 3, 4])\n   0\n0  1\n</code></pre> <p>The equality distance is set to 0.4. 1 and 1.3 are considered equal, so are 3 and 3.1. Therefore 2 is choosen.</p> <pre><code>&gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [1.3, 2.7, 3.1], 1, .4)\n   0\n0  2\n</code></pre> <p>The value 3 appears least often in the reference.</p> <pre><code>&gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [1, 1, 1, 2, 2, 2, 3, 3])\n   0\n0  3\n</code></pre> <p>The experimentalist \"fills up\" the reference array so the values are contributed evenly</p> <pre><code>&gt;&gt;&gt; summed_inequality_sample([1, 1, 1, 2, 2, 2, 3, 3, 3], [1, 1, 2, 2, 2, 2, 3, 3, 3], 3)\n   0\n0  1\n1  3\n2  1\n</code></pre> <p>The experimentalist samples without replacemnt!</p> <pre><code>&gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [1, 1, 1], 3)\n   0\n0  3\n1  2\n2  1\n</code></pre> Source code in <code>temp_dir/inequality/src/autora/experimentalist/inequality/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    num_samples: int = 1,\n    equality_distance: float = 0,\n    metric: str = \"euclidean\",\n) -&gt; np.ndarray:\n    \"\"\"\n    This inequality experimentalist chooses from the pool of IV conditions according to their\n    inequality with respect to a reference pool reference_conditions. Two IVs are considered\n    equal if their distance is less than the equality_distance. The IVs chosen first are feed back\n    into reference_conditions and are included in the summed equality calculation.\n\n    Args:\n        conditions: pool of IV conditions to evaluate inequality\n        reference_conditions: reference pool of IV conditions\n        num_samples: number of samples to select\n        equality_distance: the distance to decide if two data points are equal.\n        metric: inequality measure. Options: 'euclidean', 'manhattan', 'chebyshev',\n            'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine',\n            'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice',\n            'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener',\n            'sokalsneath', 'yule'. See `sklearn.metrics.DistanceMetric` for more details.\n\n    Returns:\n        Sampled pool\n\n    Examples:\n        The value 1 is not in the reference. Therefore it is choosen.\n        &gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [2, 3, 4])\n           0\n        0  1\n\n        The equality distance is set to 0.4. 1 and 1.3 are considered equal, so are 3 and 3.1.\n        Therefore 2 is choosen.\n        &gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [1.3, 2.7, 3.1], 1, .4)\n           0\n        0  2\n\n        The value 3 appears least often in the reference.\n        &gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [1, 1, 1, 2, 2, 2, 3, 3])\n           0\n        0  3\n\n        The experimentalist \"fills up\" the reference array so the values are contributed evenly\n        &gt;&gt;&gt; summed_inequality_sample([1, 1, 1, 2, 2, 2, 3, 3, 3], [1, 1, 2, 2, 2, 2, 3, 3, 3], 3)\n           0\n        0  1\n        1  3\n        2  1\n\n        The experimentalist samples without replacemnt!\n        &gt;&gt;&gt; summed_inequality_sample([1, 2, 3], [1, 1, 1], 3)\n           0\n        0  3\n        1  2\n        2  1\n\n    \"\"\"\n\n    X = np.array(conditions)\n\n    _reference_conditions = reference_conditions.copy()\n    if isinstance(reference_conditions, pd.DataFrame):\n        if set(conditions.columns) != set(reference_conditions.columns):\n            raise Exception(\n                f\"Variable names {set(conditions.columns)} in conditions\"\n                f\"and {set(reference_conditions.columns)} in allowed values don't match. \"\n            )\n\n        _reference_conditions = _reference_conditions[conditions.columns]\n\n    X_reference_conditions = np.array(_reference_conditions)\n\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    if X_reference_conditions.ndim == 1:\n        X_reference_conditions = X_reference_conditions.reshape(-1, 1)\n\n    if X.shape[1] != X_reference_conditions.shape[1]:\n        raise ValueError(\n            f\"conditions and reference_conditions must have the same number of columns.\\n\"\n            f\"conditions has {X.shape[1]} columns, \"\n            f\"while reference_conditions has {X_reference_conditions.shape[1]} columns.\"\n        )\n\n    if X.shape[0] &lt; num_samples:\n        raise ValueError(\n            f\"conditions must have at least {num_samples} rows matching the number \"\n            f\"of requested samples.\"\n        )\n\n    dist = DistanceMetric.get_metric(metric)\n\n    # create a list to store the n conditions values with the highest inequality scores\n    condition_pool_res = []\n    # choose the canditate with the highest inequality score n-times\n    for _ in range(num_samples):\n        summed_equalities = []\n        # loop over all IV values\n        for row in X:\n\n            # calculate the distances between the current row in matrix1\n            # and all other rows in matrix2\n            summed_equality = 0\n            for reference_conditions_row in X_reference_conditions:\n                distance = dist.pairwise([row, reference_conditions_row])[0, 1]\n                summed_equality += distance &gt; equality_distance\n\n            # store the summed distance for the current row\n            summed_equalities.append(summed_equality)\n\n        # sort the rows in matrix1 by their summed distances\n        X = X[np.argsort(summed_equalities)[::-1]]\n        # append the first value of the sorted list to the result\n        condition_pool_res.append(X[0])\n        # add the chosen value to reference_conditions\n        X_reference_conditions = np.append(X_reference_conditions, [X[0]], axis=0)\n        # remove the chosen value from X\n        X = X[1:]\n\n    new_conditions = np.array(condition_pool_res[:num_samples])\n    if isinstance(conditions, pd.DataFrame):\n        new_conditions = pd.DataFrame(new_conditions, columns=conditions.columns)\n    else:\n        new_conditions = pd.DataFrame(new_conditions)\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/leverage/","title":"autora.experimentalist.leverage","text":""},{"location":"reference/autora/experimentalist/leverage/#autora.experimentalist.leverage.sample","title":"<code>sample(conditions, Y, models, fit='both', num_samples=5, sd=0.1, random_state=None)</code>","text":""},{"location":"reference/autora/experimentalist/leverage/#autora.experimentalist.leverage.sample--the-leverage-experimentalist","title":"The Leverage Experimentalist","text":"<p>This experimentalist uses the statistical concept of leverage by refitting the provided models iteratively with the leave-one-out method.</p> <p>WARNING: This experimentalist needs to fit each model you provide it n times, where n corresponds to the number of datapoints you have. As such, the computational time and power needed to run this experimentalist increases exponentially with increasing number of models and datapoints.</p> <p>In each iteration, it computes the degree to which the currently removed datapoint     has influence on the model. If the model remains stable, the datapoint is deemed to have little influence on the model,     and as such will have a low likelihood of being selected for further investigation. In contrast, if the model changes, the datapoint is influential on the model,     and has a higher likelihood of being selected for further investigation.</p> <p>Specifically, you provide the experimentalist with a model that has been trained on all of the data. On each iteration, the experimentalist fits a new model with all data aside from one datapoint. Both models then predict Y scores from the original X variable and compute a mean squared error (MSE) for each X score.</p> <p>The experimentalist then computes a ratio of the MSE scores between the experimentalist model     and the original model that you provided:</p> <p>As such, values above one indicates that the original model fit the data better     than the experimentalist model when removing that datapoint. In contrast, values below one indicates that the experimentalist model fit the data better     than the original model when removing that datapoint. And a value of one indicates that both models fit the data equally.     If you provide multiple models, it will then average across these models to result     in an aggregate MSE score for each X score. In the future,     it might be a good idea to incorporate multiple models in a more sophisticated way.</p> <p>Finally, the experimentalist then uses these aggregated ratios to select the next set of datapoints to explore in one of three ways, declared with the 'fit' parameter.     -'increase' will choose samples focused on X scores where the fits got better         (i.e., the smallest MSE ratios)     -'decrease' will choose samples focused on X scores where the fits got worse         (i.e., the largest MSE ratios)     -'both' will do both of the above, or in other words focus on X scores with         the most extreme scores.</p> <pre><code>Args:\n    conditions: pool of IV conditions to evaluate leverage\n    Y: pool of DV conditions to evaluate leverage\n    models: List of Scikit-learn (regression or classification) model(s) to compare\n        -can be a single model, or a list of models.\n    fit: method to evaluate leverage. Options:\n        -both: This will choose samples that caused the most change in the model,\n            regardless of whether it got better or worse\n        -increase: This will choose samples focused on iterations where the fits got better\n        -decrease: This will choose samples focused on iterations where the fits got worse\n    num_samples: number of samples to select\n    sd: A noise parameter around the selected samples to allow for the selection\n        of datapoints that are not part of the original dataset.\n        This is not currently constrained by the pipelines IV resolution.\n    random_state:\n\nReturns:\n    Sampled pool of experimental conditions\n</code></pre> Source code in <code>temp_dir/leverage/src/autora/experimentalist/leverage/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    Y: np.array,\n    models: list,\n    fit: str = \"both\",\n    num_samples: int = 5,\n    sd: float = 0.1,\n    random_state: Optional[int] = None,\n):\n    \"\"\"\n\n    # The Leverage Experimentalist\n\n    This experimentalist uses the statistical concept of leverage by refitting the provided models\n    iteratively with the leave-one-out method.\n\n    ---\n    WARNING:\n    This experimentalist needs to fit each model you provide it n times, where n corresponds to the\n    number of datapoints you have.\n    As such, the computational time and power needed to run this experimentalist increases\n    exponentially with increasing number of models and datapoints.\n\n    ---\n\n    In each iteration, it computes the degree to which the currently removed datapoint\n        has influence on the model.\n    If the model remains stable, the datapoint is deemed to have little influence on the model,\n        and as such will have a low likelihood of being selected for further investigation.\n    In contrast, if the model changes, the datapoint is influential on the model,\n        and has a higher likelihood of being selected for further investigation.\n\n    Specifically, you provide the experimentalist with a model that has been trained on all of\n    the data. On each iteration, the experimentalist fits a new model with all data aside from one\n    datapoint. Both models then predict Y scores from the original X variable and compute a mean\n    squared error (MSE) for each X score.\n\n    The experimentalist then computes a ratio of the MSE scores between the experimentalist model\n        and the original model that you provided:\n\n    As such, values above one indicates that the original model fit the data better\n        than the experimentalist model when removing that datapoint.\n    In contrast, values below one indicates that the experimentalist model fit the data better\n        than the original model when removing that datapoint.\n    And a value of one indicates that both models fit the data equally.\n        If you provide multiple models, it will then average across these models to result\n        in an aggregate MSE score for each X score. In the future,\n        it might be a good idea to incorporate multiple models in a more sophisticated way.\n\n    Finally, the experimentalist then uses these aggregated ratios to select the next set of\n    datapoints to explore in one of three ways, declared with the 'fit' parameter.\n        -'increase' will choose samples focused on X scores where the fits got better\n            (i.e., the smallest MSE ratios)\n        -'decrease' will choose samples focused on X scores where the fits got worse\n            (i.e., the largest MSE ratios)\n        -'both' will do both of the above, or in other words focus on X scores with\n            the most extreme scores.\n\n        Args:\n            conditions: pool of IV conditions to evaluate leverage\n            Y: pool of DV conditions to evaluate leverage\n            models: List of Scikit-learn (regression or classification) model(s) to compare\n                -can be a single model, or a list of models.\n            fit: method to evaluate leverage. Options:\n                -both: This will choose samples that caused the most change in the model,\n                    regardless of whether it got better or worse\n                -increase: This will choose samples focused on iterations where the fits got better\n                -decrease: This will choose samples focused on iterations where the fits got worse\n            num_samples: number of samples to select\n            sd: A noise parameter around the selected samples to allow for the selection\n                of datapoints that are not part of the original dataset.\n                This is not currently constrained by the pipelines IV resolution.\n            random_state:\n\n        Returns:\n            Sampled pool of experimental conditions\n\n    \"\"\"\n    # Force data into required formats\n    if not isinstance(models, list):\n        models = list(models)\n\n    X = np.array(conditions)\n\n    # Determine the leverage\n    leverage_mse = np.zeros((len(models), X.shape[0]))\n    for mi, model in enumerate(models):\n        current_model = copy.deepcopy(model)\n        current_model.fit(X, Y)\n        original_mse = np.mean(np.power(current_model.predict(X) - Y, 2))\n        for xi, x in enumerate(X):\n            # Remove a datapoint for each iteration\n            current_X = X\n            current_X = np.delete(current_X, xi).reshape(-1, 1)\n            current_Y = Y\n            current_Y = np.delete(current_Y, xi).reshape(-1, 1)\n\n            # Refit the model with the truncated (n-1) data\n            current_model = copy.deepcopy(model)\n            current_model.fit(current_X, current_Y)\n\n            # Determine current models mean squared error from original data\n            current_mse = np.mean(np.power(current_model.predict(X) - Y, 2))\n\n            # Determine the change of fit between original and truncated model\n            # Greater than 1 means the fit got worse in this iteration\n            # Smaller than 1 means the fit got better in this iteration\n            leverage_mse[mi, xi] = current_mse / original_mse\n\n    # Determine the samples to propose\n    leverage_mse = np.mean(leverage_mse, 0)  # Average across models\n    if fit == \"both\":\n        leverage_mse[leverage_mse &lt; 1] = (\n            1 / leverage_mse[leverage_mse &lt; 1]\n        )  # Transform numbers under 1 to parallel numbers over 1\n        new_conditions_index = np.argsort(leverage_mse)[::-1]\n    elif fit == \"increase\":\n        new_conditions_index = np.argsort(leverage_mse)[::-1]\n    elif fit == \"decrease\":\n        new_conditions_index = np.argsort(leverage_mse)\n    else:\n        raise AttributeError(\n            \"The fit parameter was not recognized. Accepted parameters include:\"\n            \" 'both', 'increase', and 'decrease'.\"\n        )\n\n    rng = np.random.default_rng(random_state)\n\n    noise = np.array([rng.normal(0, sd) for r in range(len(new_conditions_index))])\n    new_conditions = X[new_conditions_index].reshape(-1) + noise\n    new_conditions = new_conditions[:num_samples]\n\n    if isinstance(conditions, pd.DataFrame):\n        new_conditions = pd.DataFrame(new_conditions, columns=conditions.columns)\n    else:\n        new_conditions = pd.DataFrame(new_conditions)\n\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/mixture/","title":"autora.experimentalist.mixture","text":"<p>Mixture Experimentalist Sampler</p>"},{"location":"reference/autora/experimentalist/mixture/#autora.experimentalist.mixture.sample","title":"<code>sample(conditions, temperature, samplers, params, num_samples=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of experimental conditions to evaluate: pd.Dataframe</p> required <code>temperature</code> <code>float</code> <p>how random is selection of conditions (cannot be 0; (0:1) - the choices are more deterministic than the choices made wrt</p> required <code>samplers</code> <code>list</code> <p>tuple containing sampler functions, their names, and weights</p> required <code>for</code> <code>sampler functions that return both positive and negative scores, user can provide a list with two weights</code> <p>the first one will be applied to positive scores, the second one -- to the negative</p> required <code>params</code> <code>dict</code> <p>nested dictionary. keys correspond to the sampler function names (same as provided in samplers),</p> required <code>values</code> <code>correspond to the dictionaries of function arguments (argument name</code> <p>its value)</p> required <code>num_samples</code> <code>Optional[int]</code> <p>number of experimental conditions to select</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sampled pool of experimental conditions with the scores attached to them</p> Source code in <code>temp_dir/mixture/src/autora/experimentalist/mixture/__init__.py</code> <pre><code>def sample(conditions: Union[pd.DataFrame, np.ndarray], temperature: float,\n                   samplers: list, params: dict,\n                   num_samples: Optional[int] = None) -&gt; pd.DataFrame:\n    \"\"\"\n\n    Args:\n        conditions: pool of experimental conditions to evaluate: pd.Dataframe\n        temperature: how random is selection of conditions (cannot be 0; (0:1) - the choices are more deterministic than the choices made wrt\n        samplers: tuple containing sampler functions, their names, and weights\n        for sampler functions that return both positive and negative scores, user can provide a list with two weights: the first one will be applied to positive scores, the second one -- to the negative\n        params: nested dictionary. keys correspond to the sampler function names (same as provided in samplers),\n        values correspond to the dictionaries of function arguments (argument name: its value)\n        num_samples: number of experimental conditions to select\n\n    Returns:\n        Sampled pool of experimental conditions with the scores attached to them\n    \"\"\"\n\n    condition_pool = pd.DataFrame(conditions)\n\n    rankings = pd.DataFrame()\n    mixture_scores = np.zeros(len(condition_pool))\n    ## getting rankings and weighted scores from each function\n    for (function, name, weight) in samplers:\n\n        try:\n            sampler_params = params[name]\n            pd_ranking = function(conditions=condition_pool, **sampler_params)\n        except:\n            pd_ranking = function(conditions=condition_pool)\n        # sorting by index\n        pd_ranking = pd_ranking.sort_index()\n\n        if len(weight) == 1:\n            weight = weight[0]\n\n        # if only one weight is provided, use it for both negative and positive dimensions\n        if isinstance(weight, float) or isinstance(weight, int):\n            pd_ranking[\"score\"] = pd_ranking[\"score\"] * weight\n        else:\n            if len(pd_ranking[\"score\"] &lt; 0) &gt; 0 and len(pd_ranking[\"score\"] &gt; 0) &gt; 0:  # there are both positive and negative values\n\n                pd_ranking.loc[pd_ranking[\"score\"] &gt; 0][\"score\"] = pd_ranking.loc[pd_ranking[\"score\"] &gt; 0][\"score\"] * weight[0]  # positive dimension gets the first weight\n                pd_ranking.loc[pd_ranking[\"score\"] &lt; 0][\"score\"] = pd_ranking.loc[pd_ranking[\"score\"] &lt; 0][\"score\"] * weight[1]  # negative dimension gets the second weight\n            else:\n                pd_ranking[\"score\"] = pd_ranking[\"score\"] * weight[0]\n\n        pd_ranking.rename(columns={\"score\": f\"{name}_score\"}, inplace=True)\n        # sum_scores are arranged based on the original conditions_ indices\n        mixture_scores = mixture_scores + pd_ranking[f\"{name}_score\"]\n\n        rankings = pd.merge(rankings, pd_ranking, left_index=True, right_index=True, how=\"outer\")\n\n    # adjust mixture scores wrt temperature\n    weighted_mixture_scores_adjusted = adjust_distribution(mixture_scores, temperature)\n\n    if num_samples is None:\n        num_samples = condition_pool.shape[0]\n\n    condition_indices = np.random.choice(np.arange(len(condition_pool)), num_samples,\n                                         p=weighted_mixture_scores_adjusted, replace=False)\n    conditions_ = condition_pool.iloc[condition_indices]\n    conditions_[\"score\"] = mixture_scores\n\n    return conditions_\n</code></pre>"},{"location":"reference/autora/experimentalist/model_disagreement/","title":"autora.experimentalist.model_disagreement","text":""},{"location":"reference/autora/experimentalist/model_disagreement/#autora.experimentalist.model_disagreement.sample","title":"<code>sample(conditions, models, num_samples=1)</code>","text":"<p>A experimentalist that returns selected samples for independent variables for which the models disagree the most in terms of their predictions.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of IV conditions to evaluate in terms of model disagreement</p> required <code>models</code> <code>List</code> <p>List of Scikit-learn (regression or classification) models to compare</p> required <code>num_samples</code> <code>int</code> <p>number of samples to select</p> <code>1</code> <p>Returns: Sampled pool</p> Source code in <code>temp_dir/disagreement/src/autora/experimentalist/model_disagreement/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray], models: List, num_samples: int = 1\n):\n    \"\"\"\n    A experimentalist that returns selected samples for independent variables\n    for which the models disagree the most in terms of their predictions.\n\n    Args:\n        conditions: pool of IV conditions to evaluate in terms of model disagreement\n        models: List of Scikit-learn (regression or classification) models to compare\n        num_samples: number of samples to select\n\n    Returns: Sampled pool\n    \"\"\"\n\n    selected_conditions = score_sample(conditions, models, num_samples)\n    selected_conditions.drop(columns=[\"score\"], inplace=True)\n\n    return selected_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/model_disagreement/#autora.experimentalist.model_disagreement.sample_custom_distance","title":"<code>sample_custom_distance(conditions, models, distance_fct=lambda x, y: (x - y) ** 2, aggregate_fct=lambda x: np.sum(x, axis=0), num_samples=1)</code>","text":"<p>An experimentalist that returns selected samples for independent variables for which the models disagree the most in terms of their predictions. The disagreement measurement is customizable.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of IV conditions to evaluate in terms of model disagreement</p> required <code>models</code> <code>List</code> <p>List of Scikit-learn (regression or classification) models to compare</p> required <code>distance_fct</code> <code>Callable</code> <p>distance function to use on the predictions</p> <code>lambda x, y: (x - y) ** 2</code> <code>aggregate_fct</code> <code>Callable</code> <p>aggregate function to use on the pairwise distances of the models</p> <code>lambda x: sum(x, axis=0)</code> <code>num_samples</code> <code>Optional[int]</code> <p>number of samples to select</p> <code>1</code> <p>Returns: Sampled pool</p> Source code in <code>temp_dir/disagreement/src/autora/experimentalist/model_disagreement/__init__.py</code> <pre><code>def sample_custom_distance(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    models: List,\n    distance_fct: Callable = lambda x, y: (x - y) ** 2,\n    aggregate_fct: Callable = lambda x: np.sum(x, axis=0),\n    num_samples: Optional[int] = 1,\n):\n    \"\"\"\n    An experimentalist that returns selected samples for independent variables\n    for which the models disagree the most in terms of their predictions. The disagreement\n    measurement is customizable.\n\n    Args:\n        conditions: pool of IV conditions to evaluate in terms of model disagreement\n        models: List of Scikit-learn (regression or classification) models to compare\n        distance_fct: distance function to use on the predictions\n        aggregate_fct: aggregate function to use on the pairwise distances of the models\n        num_samples: number of samples to select\n\n    Returns: Sampled pool\n    \"\"\"\n\n    selected_conditions = score_sample_custom_distance(\n        conditions, models, distance_fct, aggregate_fct, num_samples\n    )\n    selected_conditions.drop(columns=[\"score\"], inplace=True)\n    return selected_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/model_disagreement/#autora.experimentalist.model_disagreement.score_sample","title":"<code>score_sample(conditions, models, num_samples=None)</code>","text":"<p>A experimentalist that returns selected samples for independent variables for which the models disagree the most in terms of their predictions.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of IV conditions to evaluate in terms of model disagreement</p> required <code>models</code> <code>List</code> <p>List of Scikit-learn (regression or classification) models to compare</p> required <code>num_samples</code> <code>Optional[int]</code> <p>number of samples to select</p> <code>None</code> <p>Returns: Sampled pool</p> <p>Examples:</p> <p>If a model is undefined at a certain condition, the disagreement on that point is set to 0:</p> <pre><code>&gt;&gt;&gt; class ModelUndefined:\n...     def predict(self, X):\n...         return np.log(X)\n&gt;&gt;&gt; class ModelDefinined:\n...     def predict(self, X):\n...         return X\n&gt;&gt;&gt; modelUndefined = ModelUndefined()\n&gt;&gt;&gt; modelDefined = ModelDefinined()\n&gt;&gt;&gt; conditions_defined = np.array([1, 2, 3])\n&gt;&gt;&gt; score_sample(conditions_defined, [modelUndefined, modelDefined], 3)\n   0     score\n2  3  1.364948\n1  2 -0.362023\n0  1 -1.002924\n</code></pre> <pre><code>&gt;&gt;&gt; conditions_undefined = np.array([-1, 0, 1, 2, 3])\n&gt;&gt;&gt; score_sample(conditions_undefined, [modelUndefined, modelDefined], 5)\n   0     score\n4  3  1.752985\n3  2  0.330542\n2  1 -0.197345\n0 -1 -0.943091\n1  0 -0.943091\n</code></pre> Source code in <code>temp_dir/disagreement/src/autora/experimentalist/model_disagreement/__init__.py</code> <pre><code>def score_sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    models: List,\n    num_samples: Optional[int] = None,\n):\n    \"\"\"\n    A experimentalist that returns selected samples for independent variables\n    for which the models disagree the most in terms of their predictions.\n\n    Args:\n        conditions: pool of IV conditions to evaluate in terms of model disagreement\n        models: List of Scikit-learn (regression or classification) models to compare\n        num_samples: number of samples to select\n\n    Returns: Sampled pool\n\n    Examples:\n        If a model is undefined at a certain condition, the disagreement on that point is set to 0:\n        &gt;&gt;&gt; class ModelUndefined:\n        ...     def predict(self, X):\n        ...         return np.log(X)\n        &gt;&gt;&gt; class ModelDefinined:\n        ...     def predict(self, X):\n        ...         return X\n        &gt;&gt;&gt; modelUndefined = ModelUndefined()\n        &gt;&gt;&gt; modelDefined = ModelDefinined()\n        &gt;&gt;&gt; conditions_defined = np.array([1, 2, 3])\n        &gt;&gt;&gt; score_sample(conditions_defined, [modelUndefined, modelDefined], 3)\n           0     score\n        2  3  1.364948\n        1  2 -0.362023\n        0  1 -1.002924\n\n        &gt;&gt;&gt; conditions_undefined = np.array([-1, 0, 1, 2, 3])\n        &gt;&gt;&gt; score_sample(conditions_undefined, [modelUndefined, modelDefined], 5)\n           0     score\n        4  3  1.752985\n        3  2  0.330542\n        2  1 -0.197345\n        0 -1 -0.943091\n        1  0 -0.943091\n    \"\"\"\n\n    if (\n        isinstance(conditions, Iterable)\n        and not isinstance(conditions, pd.DataFrame)\n        and not isinstance(conditions, list)\n    ):\n        conditions = np.array(list(conditions))\n\n    condition_pool_copy = conditions.copy()\n\n    if isinstance(conditions, list):\n        X_predict = conditions\n    else:\n        conditions = np.array(conditions)\n        X_predict = np.array(conditions)\n        if len(X_predict.shape) == 1:\n            X_predict = X_predict.reshape(-1, 1)\n\n    model_disagreement = list()\n\n    # collect diagreements for each model pair\n    for model_a, model_b in itertools.combinations(models, 2):\n\n        # determine the prediction method\n        predict_proba = False\n        if hasattr(model_a, \"predict_proba\") and hasattr(model_b, \"predict_proba\"):\n            predict_proba = True\n        elif hasattr(model_a, \"predict\") and hasattr(model_b, \"predict\"):\n            predict_proba = False\n        else:\n            raise AttributeError(\n                \"Models must both have `predict_proba` or `predict` method.\"\n            )\n\n        if isinstance(X_predict, list):\n            disagreement_part_list = list()\n            for element in X_predict:\n                if not isinstance(element, np.ndarray):\n                    raise ValueError(\n                        \"X_predict must be a list of numpy arrays if it is a list.\"\n                    )\n                else:\n                    disagreement_part = compute_disagreement(\n                        model_a, model_b, element, predict_proba\n                    )\n                    disagreement_part_list.append(disagreement_part)\n            disagreement = np.sum(disagreement_part_list, axis=1)\n        else:\n            disagreement = compute_disagreement(\n                model_a, model_b, X_predict, predict_proba\n            )\n        model_disagreement.append(disagreement)\n\n    assert len(model_disagreement) &gt;= 1, \"No disagreements to compare.\"\n\n    # sum up all model disagreements\n    summed_disagreement = np.sum(model_disagreement, axis=0)\n\n    if isinstance(condition_pool_copy, pd.DataFrame):\n        conditions = pd.DataFrame(conditions, columns=condition_pool_copy.columns)\n    elif isinstance(condition_pool_copy, list):\n        conditions = pd.DataFrame({\"X\": conditions})\n    else:\n        conditions = pd.DataFrame(conditions)\n\n    # normalize the distances\n    scaler = StandardScaler()\n    score = scaler.fit_transform(summed_disagreement.reshape(-1, 1)).flatten()\n\n    # order rows in Y from highest to lowest\n    conditions[\"score\"] = score\n    conditions = conditions.sort_values(by=\"score\", ascending=False)\n\n    if num_samples is None:\n        return conditions\n    else:\n        return conditions.head(num_samples)\n</code></pre>"},{"location":"reference/autora/experimentalist/model_disagreement/#autora.experimentalist.model_disagreement.score_sample_custom_distance","title":"<code>score_sample_custom_distance(conditions, models, distance_fct=lambda x, y: (x - y) ** 2, aggregate_fct=lambda x: np.sum(x, axis=0), num_samples=None)</code>","text":"<p>An experimentalist that returns selected samples for independent variables for which the models disagree the most in terms of their predictions. The disagreement measurement is customizable.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of IV conditions to evaluate in terms of model disagreement</p> required <code>models</code> <code>List</code> <p>List of Scikit-learn (regression or classification) models to compare</p> required <code>distance_fct</code> <code>Callable</code> <p>distance function to use on the predictions</p> <code>lambda x, y: (x - y) ** 2</code> <code>aggregate_fct</code> <code>Callable</code> <p>aggregate function to use on the pairwise distances of the models</p> <code>lambda x: sum(x, axis=0)</code> <code>num_samples</code> <code>Optional[int]</code> <p>number of samples to select</p> <code>None</code> <p>Returns:</p> Type Description <p>Sampled pool with score</p> <p>Examples:</p> <p>We can use this without passing in a distance function (squared distance as default) ...</p> <pre><code>&gt;&gt;&gt; class IdentityModel:\n...     def predict(self, X):\n...         return X\n&gt;&gt;&gt; class SquareModel:\n...     def predict(self, X):\n...         return X**2\n&gt;&gt;&gt; id_model = IdentityModel()\n&gt;&gt;&gt; sq_model = SquareModel()\n&gt;&gt;&gt; _conditions = np.array([1, 2, 3])\n&gt;&gt;&gt; id_model.predict(_conditions)\narray([1, 2, 3])\n&gt;&gt;&gt; sq_model.predict(_conditions)\narray([1, 4, 9])\n&gt;&gt;&gt; score_sample_custom_distance(_conditions, [id_model, sq_model])\n   0  score\n2  3     36\n1  2      4\n0  1      0\n</code></pre> <p>... we can use our own distance function (for example binary 1 and 0 for different or equal)</p> <pre><code>&gt;&gt;&gt; score_sample_custom_distance(_conditions, [id_model, sq_model], lambda x,y : x != y)\n   0  score\n1  2      1\n2  3      1\n0  1      0\n</code></pre> <p>... this is mostly usefull if the predict function of the model doesn't return a standard one-dimensional array:</p> <pre><code>&gt;&gt;&gt; _conditions = np.array([[0, 1], [1, 0], [1, 1], [.5, .5]])\n&gt;&gt;&gt; id_model.predict(_conditions)\narray([[0. , 1. ],\n       [1. , 0. ],\n       [1. , 1. ],\n       [0.5, 0.5]])\n&gt;&gt;&gt; sq_model.predict(_conditions)\narray([[0.  , 1.  ],\n       [1.  , 0.  ],\n       [1.  , 1.  ],\n       [0.25, 0.25]])\n</code></pre> <pre><code>&gt;&gt;&gt; def distance(x, y):\n...     return np.sqrt((x[0] - y[0])**2 + (x[1] - y[1])**2)\n</code></pre> <pre><code>&gt;&gt;&gt; score_sample_custom_distance(_conditions, [id_model, sq_model], distance)\n     0    1     score\n3  0.5  0.5  0.353553\n0  0.0  1.0  0.000000\n1  1.0  0.0  0.000000\n2  1.0  1.0  0.000000\n</code></pre> Source code in <code>temp_dir/disagreement/src/autora/experimentalist/model_disagreement/__init__.py</code> <pre><code>def score_sample_custom_distance(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    models: List,\n    distance_fct: Callable = lambda x, y: (x - y) ** 2,\n    aggregate_fct: Callable = lambda x: np.sum(x, axis=0),\n    num_samples: Optional[int] = None,\n):\n    \"\"\"\n    An experimentalist that returns selected samples for independent variables\n    for which the models disagree the most in terms of their predictions. The disagreement\n    measurement is customizable.\n\n\n    Args:\n        conditions: pool of IV conditions to evaluate in terms of model disagreement\n        models: List of Scikit-learn (regression or classification) models to compare\n        distance_fct: distance function to use on the predictions\n        aggregate_fct: aggregate function to use on the pairwise distances of the models\n        num_samples: number of samples to select\n\n    Returns:\n        Sampled pool with score\n\n\n    Examples:\n        We can use this without passing in a distance function (squared distance as default) ...\n        &gt;&gt;&gt; class IdentityModel:\n        ...     def predict(self, X):\n        ...         return X\n        &gt;&gt;&gt; class SquareModel:\n        ...     def predict(self, X):\n        ...         return X**2\n        &gt;&gt;&gt; id_model = IdentityModel()\n        &gt;&gt;&gt; sq_model = SquareModel()\n        &gt;&gt;&gt; _conditions = np.array([1, 2, 3])\n        &gt;&gt;&gt; id_model.predict(_conditions)\n        array([1, 2, 3])\n        &gt;&gt;&gt; sq_model.predict(_conditions)\n        array([1, 4, 9])\n        &gt;&gt;&gt; score_sample_custom_distance(_conditions, [id_model, sq_model])\n           0  score\n        2  3     36\n        1  2      4\n        0  1      0\n\n        ... we can use our own distance function (for example binary 1 and 0 for different or equal)\n        &gt;&gt;&gt; score_sample_custom_distance(_conditions, [id_model, sq_model], lambda x,y : x != y)\n           0  score\n        1  2      1\n        2  3      1\n        0  1      0\n\n        ... this is mostly usefull if the predict function of the model doesn't return a\n        standard one-dimensional array:\n        &gt;&gt;&gt; _conditions = np.array([[0, 1], [1, 0], [1, 1], [.5, .5]])\n        &gt;&gt;&gt; id_model.predict(_conditions)\n        array([[0. , 1. ],\n               [1. , 0. ],\n               [1. , 1. ],\n               [0.5, 0.5]])\n        &gt;&gt;&gt; sq_model.predict(_conditions)\n        array([[0.  , 1.  ],\n               [1.  , 0.  ],\n               [1.  , 1.  ],\n               [0.25, 0.25]])\n\n        &gt;&gt;&gt; def distance(x, y):\n        ...     return np.sqrt((x[0] - y[0])**2 + (x[1] - y[1])**2)\n\n        &gt;&gt;&gt; score_sample_custom_distance(_conditions, [id_model, sq_model], distance)\n             0    1     score\n        3  0.5  0.5  0.353553\n        0  0.0  1.0  0.000000\n        1  1.0  0.0  0.000000\n        2  1.0  1.0  0.000000\n    \"\"\"\n    disagreements = []\n    for model_a, model_b in itertools.combinations(models, 2):\n        if hasattr(model_a, \"predict_proba\") and hasattr(model_b, \"predict_proba\"):\n            model_a_predict = model_a.predict_proba\n            model_b_predict = model_b.predict_proba\n        else:\n            model_a_predict = model_a.predict\n            model_b_predict = model_b.predict\n        y_A = model_a_predict(conditions)\n        y_B = model_b_predict(conditions)\n        disagreements.append([distance_fct(y_a, y_b) for y_a, y_b in zip(y_A, y_B)])\n    score = aggregate_fct(disagreements)\n\n    conditions_new = pd.DataFrame(conditions)\n    conditions_new[\"score\"] = np.array(score).tolist()\n    conditions_new = conditions_new.sort_values(by=\"score\", ascending=False)\n    if num_samples is None:\n        return conditions_new\n    else:\n        return conditions_new.head(num_samples)\n</code></pre>"},{"location":"reference/autora/experimentalist/nearest_value/","title":"autora.experimentalist.nearest_value","text":""},{"location":"reference/autora/experimentalist/nearest_value/#autora.experimentalist.nearest_value.sample","title":"<code>sample(conditions, reference_conditions, num_samples)</code>","text":"<p>A experimentalist which returns the nearest values between the input samples and the allowed values, without replacement.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>The candidate samples of experimental conditions to be evaluated.</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>Experimental conditions to which the distance is calculated</p> required <code>num_samples</code> <code>int</code> <p>number of samples</p> required <p>Returns:</p> Type Description <p>the nearest values from <code>allowed_samples</code> to the <code>samples</code></p> Source code in <code>temp_dir/nearest-value/src/autora/experimentalist/nearest_value/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    num_samples: int,\n):\n    \"\"\"\n    A experimentalist which returns the nearest values between the input samples and the allowed\n    values, without replacement.\n\n    Args:\n        conditions: The candidate samples of experimental conditions to be evaluated.\n        reference_conditions: Experimental conditions to which the distance is calculated\n        num_samples: number of samples\n\n    Returns:\n        the nearest values from `allowed_samples` to the `samples`\n\n    \"\"\"\n\n    if isinstance(conditions, Iterable):\n        conditions = np.array(list(conditions))\n\n    if len(conditions.shape) == 1:\n        conditions = conditions.reshape(-1, 1)\n\n    if conditions.shape[0] &lt; num_samples:\n        raise Exception(\n            \"More samples requested than samples available in the set allowed of values.\"\n        )\n\n    X = np.array(reference_conditions)\n\n    if X.shape[0] &lt; num_samples:\n        raise Exception(\"More samples requested than samples available in the pool.\")\n\n    x_new = np.empty((num_samples, conditions.shape[1]))\n\n    # get index of row in x that is closest to each sample\n    for row, sample in enumerate(X):\n\n        if row &gt;= num_samples:\n            break\n\n        dist = np.linalg.norm(conditions - sample, axis=1)\n        idx = np.argmin(dist)\n        x_new[row, :] = conditions[idx, :]\n        conditions = np.delete(conditions, idx, axis=0)\n\n    if isinstance(reference_conditions, pd.DataFrame):\n        x_new = pd.DataFrame(x_new, columns=reference_conditions.columns)\n    else:\n        x_new = pd.DataFrame(x_new)\n\n    return x_new\n</code></pre>"},{"location":"reference/autora/experimentalist/novelty/","title":"autora.experimentalist.novelty","text":"<p>Novelty Experimentalist</p>"},{"location":"reference/autora/experimentalist/novelty/#autora.experimentalist.novelty.sample","title":"<code>sample(conditions, reference_conditions, num_samples=None, metric='euclidean', integration='min')</code>","text":"<p>This novelty experimentalist re-arranges the pool of experimental conditions according to their dissimilarity with respect to a reference pool. The default dissimilarity is calculated as the average of the pairwise distances between the conditions in the pool and the reference conditions. If no number of samples are specified, all samples will be ordered and returned from the pool.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of experimental conditions to evaluate dissimilarity</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>reference pool of experimental conditions</p> required <code>num_samples</code> <code>Optional[int]</code> <p>number of samples to select from the pool of experimental conditions</p> <code>None</code> <code>metric</code> <code>str</code> <p>dissimilarity measure. Options: 'euclidean', 'manhattan', 'chebyshev', 'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine', 'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice', 'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'yule'. See sklearn.metrics.DistanceMetric for more details.</p> <code>'euclidean'</code> <p>Returns:</p> Type Description <p>Sampled pool of conditions</p> Source code in <code>temp_dir/novelty/src/autora/experimentalist/novelty/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    num_samples: Optional[int] = None,\n    metric: AllowedMetrics = \"euclidean\",\n    integration: str = \"min\",\n):\n    \"\"\"\n    This novelty experimentalist re-arranges the pool of experimental conditions according to their\n    dissimilarity with respect to a reference pool. The default dissimilarity is calculated\n    as the average of the pairwise distances between the conditions in the pool and the reference\n    conditions.\n    If no number of samples are specified, all samples will be ordered and returned from the pool.\n\n    Args:\n        conditions: pool of experimental conditions to evaluate dissimilarity\n        reference_conditions: reference pool of experimental conditions\n        num_samples: number of samples to select from the pool of experimental conditions\n        (the default is to select all)\n        metric (str): dissimilarity measure. Options: 'euclidean', 'manhattan', 'chebyshev',\n            'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine',\n            'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice',\n            'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener',\n            'sokalsneath', 'yule'. See [sklearn.metrics.DistanceMetric][] for more details.\n\n    Returns:\n        Sampled pool of conditions\n    \"\"\"\n\n    condition_pool_copy = conditions.copy()\n\n    new_conditions = novelty_score_sample(\n        conditions, reference_conditions, num_samples, metric, integration\n    )\n    new_conditions.drop(\"score\", axis=1, inplace=True)\n\n    if isinstance(condition_pool_copy, pd.DataFrame):\n        new_conditions = pd.DataFrame(\n            new_conditions, columns=condition_pool_copy.columns\n        )\n\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/novelty/#autora.experimentalist.novelty.score_sample","title":"<code>score_sample(conditions, reference_conditions, num_samples=None, metric='euclidean', integration='sum')</code>","text":"<p>This dissimilarity samples re-arranges the pool of experimental conditions according to their dissimilarity with respect to a reference pool. The default dissimilarity is calculated as the average of the pairwise distances between the conditions in the pool and the reference conditions. If no number of samples are specified, all samples will be ordered and returned from the pool.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of experimental conditions to evaluate dissimilarity</p> required <code>reference_conditions</code> <code>Union[DataFrame, ndarray]</code> <p>reference pool of experimental conditions</p> required <code>num_samples</code> <code>Optional[int]</code> <p>number of samples to select from the pool of experimental conditions</p> <code>None</code> <code>metric</code> <code>str</code> <p>dissimilarity measure. Options: 'euclidean', 'manhattan', 'chebyshev', 'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine', 'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice', 'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'yule'. See sklearn.metrics.DistanceMetric for more details.</p> <code>'euclidean'</code> <code>integration</code> <code>str</code> <p>Distance integration method used to compute the overall dissimilarity score</p> <code>'sum'</code> <code>for</code> <code>a given data point. Options</code> <p>'sum', 'prod', 'mean', 'min', 'max'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sampled pool of conditions and dissimilarity scores</p> Source code in <code>temp_dir/novelty/src/autora/experimentalist/novelty/__init__.py</code> <pre><code>def score_sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    reference_conditions: Union[pd.DataFrame, np.ndarray],\n    num_samples: Optional[int] = None,\n    metric: AllowedMetrics = \"euclidean\",\n    integration: str = \"sum\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    This dissimilarity samples re-arranges the pool of experimental conditions according to their\n    dissimilarity with respect to a reference pool. The default dissimilarity is calculated\n    as the average of the pairwise distances between the conditions in the pool and the reference\n    conditions.\n    If no number of samples are specified, all samples will be ordered and returned from the pool.\n\n    Args:\n        conditions: pool of experimental conditions to evaluate dissimilarity\n        reference_conditions: reference pool of experimental conditions\n        num_samples: number of samples to select from the pool of experimental conditions\n        (the default is to select all)\n        metric (str): dissimilarity measure. Options: 'euclidean', 'manhattan', 'chebyshev',\n            'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine',\n            'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice',\n            'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener',\n            'sokalsneath', 'yule'. See [sklearn.metrics.DistanceMetric][] for more details.\n        integration: Distance integration method used to compute the overall dissimilarity score\n        for a given data point. Options: 'sum', 'prod', 'mean', 'min', 'max'.\n\n    Returns:\n        Sampled pool of conditions and dissimilarity scores\n    \"\"\"\n    conditions = pd.DataFrame(conditions)\n    reference_conditions = pd.DataFrame(reference_conditions)\n\n    dist = DistanceMetric.get_metric(metric)\n    distances = dist.pairwise(reference_conditions, conditions)\n\n    if integration == \"sum\":\n        integrated_distance = np.sum(distances, axis=0)\n    elif integration == \"mean\":\n        integrated_distance = np.mean(distances, axis=0)\n    elif integration == \"max\":\n        integrated_distance = np.max(distances, axis=0)\n    elif integration == \"min\":\n        integrated_distance = np.min(distances, axis=0)\n    elif integration == \"prod\":\n        integrated_distance = np.prod(distances, axis=0)\n    else:\n        raise ValueError(f\"Integration method {integration} not supported.\")\n\n    # normalize the distances\n    scaler = StandardScaler()\n    score = scaler.fit_transform(integrated_distance.reshape(-1, 1)).flatten()\n\n    # order rows in Y from highest to lowest\n    conditions[\"score\"] = score\n    conditions = conditions.sort_values(by=\"score\", ascending=False)\n\n    if num_samples is not None:\n        return conditions[:num_samples]\n    else:\n        return conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/prediction_filter/","title":"autora.experimentalist.prediction_filter","text":"<p>Example Experimentalist</p>"},{"location":"reference/autora/experimentalist/prediction_filter/#autora.experimentalist.prediction_filter.filter","title":"<code>filter(conditions, model, filter_function)</code>","text":"<p>Filter conditions based on the expected outcome io the mdeol</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>The pool to filter</p> required <code>model</code> <code>BaseEstimator</code> <p>The model to make the prediction</p> required <code>filter_function</code> <code>Callable</code> <p>A function that returns True if a prediciton should be included</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered pool of experimental conditions</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class ModelLinear:\n...     def predict(self, X):\n...         c_array = np.array(X)\n...         return 2 * c_array + 1\n&gt;&gt;&gt; model = ModelLinear()\n&gt;&gt;&gt; model.predict(4)\n9\n</code></pre> <p>For the filter function, be aware of the output type of the predict function. For example, here, we expect a list with a single entry</p> <pre><code>&gt;&gt;&gt; filter_fct = lambda x: 5 &lt; x &lt; 10\n&gt;&gt;&gt; pool = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6]})\n&gt;&gt;&gt; filter(pool, model, filter_fct)\n   x\n0  3\n1  4\n</code></pre> <pre><code>&gt;&gt;&gt; filter_fct_2d = lambda x: 4 &lt; x[0] + x[1] &lt; 10\n&gt;&gt;&gt; pool = np.array([[1, 0], [0, 1], [0, 1], [1 ,1], [2, 2]])\n&gt;&gt;&gt; model.predict(pool)\narray([[3, 1],\n       [1, 3],\n       [1, 3],\n       [3, 3],\n       [5, 5]])\n</code></pre> <pre><code>&gt;&gt;&gt; filter(pool, model, filter_fct_2d)\n   0  1\n0  1  1\n</code></pre> <pre><code>&gt;&gt;&gt; pool = pd.DataFrame({'x': [1, 0, 0, 1, 2], 'y': [0, 1, 1, 1, 2]})\n&gt;&gt;&gt; model.predict(pool)\narray([[3, 1],\n       [1, 3],\n       [1, 3],\n       [3, 3],\n       [5, 5]])\n</code></pre> <pre><code>&gt;&gt;&gt; filter(pool, model, filter_fct_2d)\n   x  y\n0  1  1\n</code></pre> Source code in <code>temp_dir/prediction-filter/src/autora/experimentalist/prediction_filter/__init__.py</code> <pre><code>def filter(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    model: BaseEstimator,\n    filter_function: Callable,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter conditions based on the expected outcome io the mdeol\n\n    Args:\n        conditions: The pool to filter\n        model: The model to make the prediction\n        filter_function: A function that returns True if a prediciton should be included\n\n    Returns:\n        Filtered pool of experimental conditions\n\n    Examples:\n        &gt;&gt;&gt; class ModelLinear:\n        ...     def predict(self, X):\n        ...         c_array = np.array(X)\n        ...         return 2 * c_array + 1\n        &gt;&gt;&gt; model = ModelLinear()\n        &gt;&gt;&gt; model.predict(4)\n        9\n\n        For the filter function, be aware of the output type of the predict function. For example,\n        here, we expect a list with a single entry\n        &gt;&gt;&gt; filter_fct = lambda x: 5 &lt; x &lt; 10\n        &gt;&gt;&gt; pool = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6]})\n        &gt;&gt;&gt; filter(pool, model, filter_fct)\n           x\n        0  3\n        1  4\n\n        &gt;&gt;&gt; filter_fct_2d = lambda x: 4 &lt; x[0] + x[1] &lt; 10\n        &gt;&gt;&gt; pool = np.array([[1, 0], [0, 1], [0, 1], [1 ,1], [2, 2]])\n        &gt;&gt;&gt; model.predict(pool)\n        array([[3, 1],\n               [1, 3],\n               [1, 3],\n               [3, 3],\n               [5, 5]])\n\n        &gt;&gt;&gt; filter(pool, model, filter_fct_2d)\n           0  1\n        0  1  1\n\n        &gt;&gt;&gt; pool = pd.DataFrame({'x': [1, 0, 0, 1, 2], 'y': [0, 1, 1, 1, 2]})\n        &gt;&gt;&gt; model.predict(pool)\n        array([[3, 1],\n               [1, 3],\n               [1, 3],\n               [3, 3],\n               [5, 5]])\n\n        &gt;&gt;&gt; filter(pool, model, filter_fct_2d)\n           x  y\n        0  1  1\n    \"\"\"\n    _pred = model.predict(conditions)\n    _filter = np.apply_along_axis(filter_function, 1, _pred)\n    _filter = _filter.reshape(1, -1)\n\n    new_conditions = conditions[list(_filter[0])]\n\n    if isinstance(conditions, pd.DataFrame):\n        new_conditions = pd.DataFrame(\n            new_conditions, columns=conditions.columns\n        ).reset_index(drop=True)\n    else:\n        new_conditions = pd.DataFrame(new_conditions)\n\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/experimentalist/uncertainty/","title":"autora.experimentalist.uncertainty","text":""},{"location":"reference/autora/experimentalist/uncertainty/#autora.experimentalist.uncertainty.sample","title":"<code>sample(conditions, model, num_samples, measure='least_confident')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Union[DataFrame, ndarray]</code> <p>pool of IV conditions to evaluate uncertainty</p> required <code>model</code> <p>Scikit-learn model, must have <code>predict_proba</code> method.</p> required <code>num_samples</code> <p>number of samples to select</p> required <code>measure</code> <p>method to evaluate uncertainty. Options:</p> <ul> <li><code>'least_confident'</code>: \\(x* = \\operatorname{argmax} \\left( 1-P(\\hat{y}|x) \\right)\\),   where \\(\\hat{y} = \\operatorname{argmax} P(y_i|x)\\)</li> <li><code>'margin'</code>:   \\(x* = \\operatorname{argmax} \\left( P(\\hat{y}_1|x) - P(\\hat{y}_2|x) \\right)\\),   where \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) are the first and second most probable   class labels under the model, respectively.</li> <li><code>'entropy'</code>:   \\(x* = \\operatorname{argmax} \\left( - \\sum P(y_i|x)   \\operatorname{log} P(y_i|x) \\right)\\)</li> </ul> <code>'least_confident'</code> <p>Returns: Sampled conditions</p> Source code in <code>temp_dir/uncertainty/src/autora/experimentalist/uncertainty/__init__.py</code> <pre><code>def sample(\n    conditions: Union[pd.DataFrame, np.ndarray],\n    model,\n    num_samples,\n    measure=\"least_confident\",\n):\n    \"\"\"\n\n    Args:\n        conditions: pool of IV conditions to evaluate uncertainty\n        model: Scikit-learn model, must have `predict_proba` method.\n        num_samples: number of samples to select\n        measure: method to evaluate uncertainty. Options:\n\n            - `'least_confident'`: $x* = \\\\operatorname{argmax} \\\\left( 1-P(\\\\hat{y}|x) \\\\right)$,\n              where $\\\\hat{y} = \\\\operatorname{argmax} P(y_i|x)$\n            - `'margin'`:\n              $x* = \\\\operatorname{argmax} \\\\left( P(\\\\hat{y}_1|x) - P(\\\\hat{y}_2|x) \\\\right)$,\n              where $\\\\hat{y}_1$ and $\\\\hat{y}_2$ are the first and second most probable\n              class labels under the model, respectively.\n            - `'entropy'`:\n              $x* = \\\\operatorname{argmax} \\\\left( - \\\\sum P(y_i|x)\n              \\\\operatorname{log} P(y_i|x) \\\\right)$\n\n    Returns: Sampled conditions\n\n    \"\"\"\n    X = np.array(conditions)\n\n    a_prob = model.predict_proba(X)\n\n    if measure == \"least_confident\":\n        # Calculate uncertainty of max probability class\n        a_uncertainty = 1 - a_prob.max(axis=1)\n        # Get index of largest uncertainties\n        idx = np.flip(a_uncertainty.argsort()[-num_samples:])\n\n    elif measure == \"margin\":\n        # Sort values by row descending\n        a_part = np.partition(-a_prob, 1, axis=1)\n        # Calculate difference between 2 largest probabilities\n        a_margin = -a_part[:, 0] + a_part[:, 1]\n        # Determine index of smallest margins\n        idx = a_margin.argsort()[:num_samples]\n\n    elif measure == \"entropy\":\n        # Calculate entropy\n        a_entropy = entropy(a_prob.T)\n        # Get index of largest entropies\n        idx = np.flip(a_entropy.argsort()[-num_samples:])\n\n    else:\n        raise ValueError(\n            f\"Unsupported uncertainty measure: '{measure}'\\n\"\n            f\"Only 'least_confident', 'margin', or 'entropy' is supported.\"\n        )\n\n    new_conditions = X[idx]\n    if isinstance(conditions, pd.DataFrame):\n        new_conditions = pd.DataFrame(new_conditions, columns=conditions.columns)\n    else:\n        new_conditions = pd.DataFrame(new_conditions)\n\n    return new_conditions\n</code></pre>"},{"location":"reference/autora/serializer/","title":"autora.serializer","text":"<p>A submodule which handles importing supported serializers.</p>"},{"location":"reference/autora/serializer/#autora.serializer.SupportedSerializer","title":"<code>SupportedSerializer</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Listing of allowed serializers.</p> Source code in <code>temp_dir/core/src/autora/serializer/__init__.py</code> <pre><code>class SupportedSerializer(str, Enum):\n    \"\"\"Listing of allowed serializers.\"\"\"\n\n    pickle = \"pickle\"\n    dill = \"dill\"\n    yaml = \"yaml\"\n</code></pre>"},{"location":"reference/autora/serializer/#autora.serializer.dump_state","title":"<code>dump_state(state_, path, dumper=default_serializer)</code>","text":"<p>Write a State object to a path.</p> Source code in <code>temp_dir/core/src/autora/serializer/__init__.py</code> <pre><code>def dump_state(\n    state_: State,\n    path: Optional[pathlib.Path],\n    dumper: SupportedSerializer = default_serializer,\n) -&gt; None:\n    \"\"\"Write a State object to a path.\"\"\"\n    serializer = load_serializer(dumper)\n    if path is not None:\n        _logger.debug(f\"dump_state: dumping to {path=}\")\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with open(path, f\"w{serializer.file_mode}\") as f:\n            serializer.module.dump(state_, f)\n    else:\n        _logger.debug(f\"dump_state: {path=} so writing to stdout\")\n        print(serializer.module.dumps(state_))\n    return\n</code></pre>"},{"location":"reference/autora/serializer/#autora.serializer.load_serializer","title":"<code>load_serializer(serializer=default_serializer)</code>","text":"<p>Load a serializer, returning an object which includes data on the file mode it expects (regular or binary).</p> <p>Examples:</p> <p>The default serializer is pickle:</p> <pre><code>&gt;&gt;&gt; load_serializer()\nLOADED_SERIALIZER(module=&lt;module 'pickle' from '...'&gt;, file_mode='b')\n</code></pre> <p>All supported serializers can be loaded explictly:</p> <pre><code>&gt;&gt;&gt; p_ = load_serializer(\"pickle\")\n&gt;&gt;&gt; p_\nLOADED_SERIALIZER(module=&lt;module 'pickle' from '...'&gt;, file_mode='b')\n</code></pre> <pre><code>&gt;&gt;&gt; d_ = load_serializer(\"dill\")\n&gt;&gt;&gt; d_\nLOADED_SERIALIZER(module=&lt;module 'dill' from '...'&gt;, file_mode='b')\n</code></pre> <pre><code>&gt;&gt;&gt; y_ = load_serializer(\"yaml\")\n&gt;&gt;&gt; y_\nLOADED_SERIALIZER(module=&lt;module 'autora.serializer.yaml_' from '...'&gt;, file_mode='')\n</code></pre> <p>Note that the yaml serializer is a wrapped version of the <code>pyyaml</code> package, which conforms to the same interface as <code>pickle</code>.</p> <p>This function loads the modules lazily and caches them, and returns the same object if the same serializer is loaded more than once.</p> <pre><code>&gt;&gt;&gt; p_ is load_serializer(\"pickle\")\nTrue\n&gt;&gt;&gt; d_ is load_serializer(\"dill\")\nTrue\n&gt;&gt;&gt; y_ is load_serializer(\"yaml\")\nTrue\n</code></pre> <p>The Serializer can be specified by the SerializersSupported enum:</p> <pre><code>&gt;&gt;&gt; load_serializer(SupportedSerializer.pickle)\nLOADED_SERIALIZER(module=&lt;module 'pickle'...\n</code></pre> <pre><code>&gt;&gt;&gt; load_serializer(SupportedSerializer.dill)\nLOADED_SERIALIZER(module=&lt;module 'dill'...\n</code></pre> <pre><code>&gt;&gt;&gt; load_serializer(SupportedSerializer.yaml)\nLOADED_SERIALIZER(module=&lt;module 'autora.serializer.yaml_'...\n</code></pre> Source code in <code>temp_dir/core/src/autora/serializer/__init__.py</code> <pre><code>def load_serializer(\n    serializer: Union[SupportedSerializer, str] = default_serializer\n) -&gt; LOADED_SERIALIZER:\n    \"\"\"\n    Load a serializer, returning an object which includes data on the file mode it expects\n    (regular or binary).\n\n    Examples:\n        The default serializer is pickle:\n        &gt;&gt;&gt; load_serializer()  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'pickle' from '...'&gt;, file_mode='b')\n\n        All supported serializers can be loaded explictly:\n        &gt;&gt;&gt; p_ = load_serializer(\"pickle\")\n        &gt;&gt;&gt; p_  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'pickle' from '...'&gt;, file_mode='b')\n\n        &gt;&gt;&gt; d_ = load_serializer(\"dill\")\n        &gt;&gt;&gt; d_  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'dill' from '...'&gt;, file_mode='b')\n\n        &gt;&gt;&gt; y_ = load_serializer(\"yaml\")\n        &gt;&gt;&gt; y_  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'autora.serializer.yaml_' from '...'&gt;, file_mode='')\n\n        Note that the yaml serializer is a wrapped version of the `pyyaml` package,\n        which conforms to the same interface as `pickle`.\n\n        This function loads the modules lazily and caches them, and returns the same object if the\n        same serializer is loaded more than once.\n        &gt;&gt;&gt; p_ is load_serializer(\"pickle\")\n        True\n        &gt;&gt;&gt; d_ is load_serializer(\"dill\")\n        True\n        &gt;&gt;&gt; y_ is load_serializer(\"yaml\")\n        True\n\n        The Serializer can be specified by the SerializersSupported enum:\n        &gt;&gt;&gt; load_serializer(SupportedSerializer.pickle)  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'pickle'...\n\n        &gt;&gt;&gt; load_serializer(SupportedSerializer.dill)  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'dill'...\n\n        &gt;&gt;&gt; load_serializer(SupportedSerializer.yaml)  # doctest: +ELLIPSIS\n        LOADED_SERIALIZER(module=&lt;module 'autora.serializer.yaml_'...\n\n    \"\"\"\n    serializer = SupportedSerializer(serializer)\n    try:\n        serializer_def = _LOADED_SERIALIZERS[serializer]\n\n    except KeyError:\n        serializer_info = _SERIALIZER_INFO[serializer]\n        module_ = importlib.import_module(serializer_info.fully_qualified_module_name)\n        serializer_def = LOADED_SERIALIZER(module_, serializer_info.file_mode)\n        _LOADED_SERIALIZERS[serializer] = serializer_def\n\n    return serializer_def\n</code></pre>"},{"location":"reference/autora/serializer/#autora.serializer.load_state","title":"<code>load_state(path, loader=default_serializer)</code>","text":"<p>Load a State object from a path.</p> Source code in <code>temp_dir/core/src/autora/serializer/__init__.py</code> <pre><code>def load_state(\n    path: Optional[pathlib.Path],\n    loader: SupportedSerializer = default_serializer,\n) -&gt; Union[State, None]:\n    \"\"\"Load a State object from a path.\"\"\"\n    serializer = load_serializer(loader)\n    if path is not None:\n        _logger.debug(f\"load_state: loading from {path=}\")\n        with open(path, f\"r{serializer.file_mode}\") as f:\n            state_ = serializer.module.load(f)\n    else:\n        _logger.debug(f\"load_state: {path=} -&gt; returning None\")\n        state_ = None\n    return state_\n</code></pre>"},{"location":"reference/autora/serializer/yaml_/","title":"autora.serializer.yaml_","text":""},{"location":"reference/autora/theorist/bms/","title":"autora.theorist.bms","text":""},{"location":"reference/autora/theorist/bms/fit_prior/","title":"autora.theorist.bms.fit_prior","text":""},{"location":"reference/autora/theorist/bms/fit_prior/#autora.theorist.bms.fit_prior.parse_options","title":"<code>parse_options()</code>","text":"<p>Parse command-line arguments.</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/fit_prior.py</code> <pre><code>def parse_options():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = OptionParser()\n    parser.add_option(\n        \"-s\",\n        \"--source\",\n        dest=\"source\",\n        default=\"named_equations\",\n        help=\"formula dataset to use ('full' or 'named_equations' (default))\",\n    )\n    parser.add_option(\n        \"-n\",\n        \"--nvar\",\n        dest=\"nvar\",\n        type=\"int\",\n        default=5,\n        help=\"number of variables to include (default 5)\",\n    )\n    parser.add_option(\n        \"-m\",\n        \"--npar\",\n        dest=\"npar\",\n        type=\"int\",\n        default=None,\n        help=\"number of parameters to include (default: 2*NVAR)\",\n    )\n    parser.add_option(\n        \"-f\",\n        \"--factor\",\n        dest=\"fact\",\n        type=\"float\",\n        default=0.05,\n        help=\"factor for the parameter adjustment (default 0.05)\",\n    )\n    parser.add_option(\n        \"-r\",\n        \"--repetitions\",\n        type=\"int\",\n        default=1000000,\n        dest=\"nrep\",\n        help=\"formulas to generate between parameter updates\",\n    )\n    parser.add_option(\n        \"-M\",\n        \"--maxsize\",\n        type=\"int\",\n        default=50,\n        dest=\"max_size\",\n        help=\"maximum tree (formula) size\",\n    )\n    parser.add_option(\n        \"-c\",\n        \"--continue\",\n        dest=\"contfile\",\n        default=None,\n        help=\"continue from parameter values in CONTFILE (default: start from scratch)\",\n    )\n    parser.add_option(\n        \"-q\",\n        \"--quadratic\",\n        action=\"store_true\",\n        dest=\"quadratic\",\n        default=False,\n        help=\"fit parameters for quadratic terms (default: False)\",\n    )\n    return parser\n</code></pre>"},{"location":"reference/autora/theorist/bms/fit_prior/#autora.theorist.bms.fit_prior.read_target_values","title":"<code>read_target_values(source, quadratic=False)</code>","text":"<p>Read the target proportions for each type of operation.</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/fit_prior.py</code> <pre><code>def read_target_values(source, quadratic=False):\n    \"\"\"Read the target proportions for each type of operation.\"\"\"\n    # Number of formulas\n    infn1 = \"./data/%s.wiki.parsed__num_operations.dat\" % source\n    with open(infn1) as inf1:\n        lines = inf1.readlines()\n        nform = sum([int(line.strip().split()[1]) for line in lines])\n    # Fraction of each of the operations\n    infn2 = \"./data/%s.wiki.parsed__operation_type.dat\" % source\n    with open(infn2) as inf2:\n        lines = inf2.readlines()\n        target = dict(\n            [\n                (\n                    \"Nopi_%s\" % line.strip().split()[0],\n                    float(line.strip().split()[1]) / nform,\n                )\n                for line in lines\n            ]\n        )\n    # Fraction of each of the operations squared\n    if quadratic:\n        infn3 = \"./data/%s.wiki.parsed__operation_type_sq.dat\" % (source)\n        with open(infn3) as inf3:\n            lines = inf3.readlines()\n            target2 = dict(\n                [\n                    (\n                        \"Nopi2_%s\" % line.strip().split()[0],\n                        float(line.strip().split()[1]) / nform,\n                    )\n                    for line in lines\n                ]\n            )\n        for k, v in list(target2.items()):\n            target[k] = v\n    # Done\n    return target, nform\n</code></pre>"},{"location":"reference/autora/theorist/bms/fit_prior/#autora.theorist.bms.fit_prior.update_ppar","title":"<code>update_ppar(tree, current, target, terms=None, step=0.05)</code>","text":"<p>Update the prior parameters using a gradient descend of sorts.</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/fit_prior.py</code> <pre><code>def update_ppar(tree, current, target, terms=None, step=0.05):\n    \"\"\"Update the prior parameters using a gradient descend of sorts.\"\"\"\n\n    # Which terms should we update? (Default: all)\n    if terms is None:\n        terms = list(current.keys())\n    # Update\n    for t in terms:\n        if current[t] &gt; target[t]:\n            tree.prior_par[t] += min(\n                0.5,\n                random() * step * float(current[t] - target[t]) / (target[t] + 1e-10),\n            )\n        elif current[t] &lt; target[t]:\n            tree.prior_par[t] -= min(\n                0.5,\n                random() * step * float(target[t] - current[t]) / (target[t] + 1e-10),\n            )\n        else:\n            pass\n    # Make sure quadratic terms are not below the minimum allowed\n    for t in [t for t in terms if t.startswith(\"Nopi2_\")]:\n        \"\"\"\n        lint = t.replace('Nopi2_', 'Nopi_')\n        op = t[6:]\n        nopmax = float(tree.max_size) / tree.ops[op] - 1.\n        minval = - tree.prior_par[lint] / nopmax\n        \"\"\"\n        minval = 0.0\n        if tree.prior_par[t] &lt; minval:\n            tree.prior_par[t] = minval\n\n    return\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/","title":"autora.theorist.bms.mcmc","text":"<p>A Markov-Chain Monte-Carlo module.</p> Module constants <p><code>get_ops()</code>:     A dictionary of accepted operations: <code>{operation_name: offspring}</code></p> <pre><code>`operation_name`: the operation name, e.g. 'sin' for the sinusoid function\n\n`offspring`: the number of arguments the function requires.\n\nFor instance, `get_ops() = {\"sin\": 1, \"**\": 2 }` means for\n`sin` the function call looks like `sin(x1)` whereas for\nthe exponentiation operator `**`, the function call looks like `x1 ** x2`\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Node","title":"<code>Node</code>","text":"<p>Object that holds algebraic term. This could be a function, variable, or parameter.</p> <p>Attributes:</p> Name Type Description <code>order</code> <code>int</code> <p>number of children nodes this term has     e.g. cos(x) has one child, whereas add(x,y) has two children</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>class Node:\n    \"\"\"\n    Object that holds algebraic term. This could be a function, variable, or parameter.\n\n    Attributes:\n        order: number of children nodes this term has\n                e.g. cos(x) has one child, whereas add(x,y) has two children\n    \"\"\"\n\n    def __init__(self, value, parent=None, offspring=[]):\n        \"\"\"\n        Initialises the node object.\n\n        Arguments:\n            parent: parent node - unless this node is the root, this will be whichever node contains\n                    the function this node's term is most immediately nested within\n                    e.g. f(x) is the parent of g(x) in f(g(x))\n            offspring: list of child nodes\n            value: the specific term held by this node\n        \"\"\"\n        self.parent: Node = parent\n        self.offspring: List[Node] = offspring\n        self.value: str = value\n        self.order: int = len(self.offspring)\n\n    def pr(self, custom_ops, show_pow=False):\n        \"\"\"\n        Converts expression in readable form\n\n        Returns: String\n        \"\"\"\n        if self.offspring == []:\n            return \"%s\" % self.value\n        elif len(self.offspring) == 2 and self.value not in custom_ops:\n            return \"(%s %s %s)\" % (\n                self.offspring[0].pr(custom_ops=custom_ops, show_pow=show_pow),\n                self.value,\n                self.offspring[1].pr(custom_ops=custom_ops, show_pow=show_pow),\n            )\n        else:\n            if show_pow:\n                return \"%s(%s)\" % (\n                    self.value,\n                    \",\".join(\n                        [\n                            o.pr(custom_ops=custom_ops, show_pow=show_pow)\n                            for o in self.offspring\n                        ]\n                    ),\n                )\n            else:\n                if self.value == \"pow2\":\n                    return \"(%s ** 2)\" % (\n                        self.offspring[0].pr(custom_ops=custom_ops, show_pow=show_pow)\n                    )\n                elif self.value == \"pow3\":\n                    return \"(%s ** 3)\" % (\n                        self.offspring[0].pr(custom_ops=custom_ops, show_pow=show_pow)\n                    )\n                else:\n                    return \"%s(%s)\" % (\n                        self.value,\n                        \",\".join(\n                            [\n                                o.pr(custom_ops=custom_ops, show_pow=show_pow)\n                                for o in self.offspring\n                            ]\n                        ),\n                    )\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Node.__init__","title":"<code>__init__(value, parent=None, offspring=[])</code>","text":"<p>Initialises the node object.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <p>parent node - unless this node is the root, this will be whichever node contains     the function this node's term is most immediately nested within     e.g. f(x) is the parent of g(x) in f(g(x))</p> <code>None</code> <code>offspring</code> <p>list of child nodes</p> <code>[]</code> <code>value</code> <p>the specific term held by this node</p> required Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def __init__(self, value, parent=None, offspring=[]):\n    \"\"\"\n    Initialises the node object.\n\n    Arguments:\n        parent: parent node - unless this node is the root, this will be whichever node contains\n                the function this node's term is most immediately nested within\n                e.g. f(x) is the parent of g(x) in f(g(x))\n        offspring: list of child nodes\n        value: the specific term held by this node\n    \"\"\"\n    self.parent: Node = parent\n    self.offspring: List[Node] = offspring\n    self.value: str = value\n    self.order: int = len(self.offspring)\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Node.pr","title":"<code>pr(custom_ops, show_pow=False)</code>","text":"<p>Converts expression in readable form</p> <p>Returns: String</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def pr(self, custom_ops, show_pow=False):\n    \"\"\"\n    Converts expression in readable form\n\n    Returns: String\n    \"\"\"\n    if self.offspring == []:\n        return \"%s\" % self.value\n    elif len(self.offspring) == 2 and self.value not in custom_ops:\n        return \"(%s %s %s)\" % (\n            self.offspring[0].pr(custom_ops=custom_ops, show_pow=show_pow),\n            self.value,\n            self.offspring[1].pr(custom_ops=custom_ops, show_pow=show_pow),\n        )\n    else:\n        if show_pow:\n            return \"%s(%s)\" % (\n                self.value,\n                \",\".join(\n                    [\n                        o.pr(custom_ops=custom_ops, show_pow=show_pow)\n                        for o in self.offspring\n                    ]\n                ),\n            )\n        else:\n            if self.value == \"pow2\":\n                return \"(%s ** 2)\" % (\n                    self.offspring[0].pr(custom_ops=custom_ops, show_pow=show_pow)\n                )\n            elif self.value == \"pow3\":\n                return \"(%s ** 3)\" % (\n                    self.offspring[0].pr(custom_ops=custom_ops, show_pow=show_pow)\n                )\n            else:\n                return \"%s(%s)\" % (\n                    self.value,\n                    \",\".join(\n                        [\n                            o.pr(custom_ops=custom_ops, show_pow=show_pow)\n                            for o in self.offspring\n                        ]\n                    ),\n                )\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree","title":"<code>Tree</code>","text":"<p>Object that manages the model equation. It contains the root node, which in turn iteratively holds children nodes. Collectively this represents the model equation tree</p> <p>Attributes:</p> Name Type Description <code>root</code> <p>the root node of the equation tree</p> <code>parameters</code> <p>the settable parameters for this trees model search</p> <code>op_orders</code> <p>order of each function within the ops</p> <code>nops</code> <p>number of operations of each type</p> <code>move_types</code> <p>possible combinations of function nesting</p> <code>ets</code> <p>possible elementary equation trees</p> <code>dist_par</code> <p>distinct parameters used</p> <code>nodes</code> <p>nodes of the tree (operations and leaves)</p> <code>et_space</code> <p>space of all possible leaves and elementary trees</p> <code>rr_space</code> <p>space of all possible root replacement trees</p> <code>num_rr</code> <p>number of possible root replacement trees</p> <code>x</code> <p>independent variable data</p> <code>y</code> <p>depedent variable data</p> <code>par_values</code> <p>The values of the model parameters (one set of values for each dataset)</p> <code>fit_par</code> <p>past successful parameter fittings</p> <code>sse</code> <p>sum of squared errors (measure of goodness of fit)</p> <code>bic</code> <p>bayesian information criterion (measure of goodness of fit)</p> <code>E</code> <p>total energy of model</p> <code>EB</code> <p>fraction of energy derived from bic score of model</p> <code>EP</code> <p>fraction of energy derived from model given prior</p> <code>representative</code> <p>representative tree for each canonical formula</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>class Tree:\n    \"\"\"\n    Object that manages the model equation. It contains the root node, which in turn iteratively\n    holds children nodes. Collectively this represents the model equation tree\n\n    Attributes:\n        root: the root node of the equation tree\n        parameters: the settable parameters for this trees model search\n        op_orders: order of each function within the ops\n        nops: number of operations of each type\n        move_types: possible combinations of function nesting\n        ets: possible elementary equation trees\n        dist_par: distinct parameters used\n        nodes: nodes of the tree (operations and leaves)\n        et_space: space of all possible leaves and elementary trees\n        rr_space: space of all possible root replacement trees\n        num_rr: number of possible root replacement trees\n        x: independent variable data\n        y: depedent variable data\n        par_values: The values of the model parameters (one set of values for each dataset)\n        fit_par: past successful parameter fittings\n        sse: sum of squared errors (measure of goodness of fit)\n        bic: bayesian information criterion (measure of goodness of fit)\n        E: total energy of model\n        EB: fraction of energy derived from bic score of model\n        EP: fraction of energy derived from model given prior\n        representative: representative tree for each canonical formula\n    \"\"\"\n\n    prior, ops = get_priors()\n\n    def __init__(\n        self,\n        ops=ops,\n        variables=[\"x\"],\n        parameters=[\"a\"],\n        prior_par=prior,\n        x=None,\n        y=None,\n        BT=1.0,\n        PT=1.0,\n        max_size=50,\n        root_value=None,\n        fixed_root=False,\n        custom_ops={},\n        random_state=None,\n    ):\n        \"\"\"\n        Initialises the tree object\n\n        Args:\n            ops: allowed operations to compose equation\n            variables: dependent variable names\n            parameters: parameters that can be used to better fit the equation to the data\n            prior_par: hyperparameter values over operations within ops\n            x: dependent variables\n            y: independent variables\n            BT: BIC value corresponding to equation\n            PT: prior temperature\n            max_size: maximum size of tree (maximum number of nodes)\n            root_value: algebraic term held at root of equation\n        \"\"\"\n        if random_state is not None:\n            seed(random_state)\n            np.random.seed(random_state)\n        # The variables and parameters\n        if custom_ops is None:\n            custom_ops = dict()\n        self.variables = variables\n        self.parameters = [\n            p if p.startswith(\"_\") and p.endswith(\"_\") else \"_%s_\" % p\n            for p in parameters\n        ]\n        # The root\n        self.fixed_root = fixed_root\n        if root_value is None:\n            self.root = Node(\n                choice(self.variables + self.parameters), offspring=[], parent=None\n            )\n        else:\n            self.root = Node(root_value, offspring=[], parent=None)\n            root_order = len(signature(custom_ops[root_value]).parameters)\n            self.root.order = root_order\n            for _ in range(root_order):\n                self.root.offspring.append(\n                    Node(\n                        choice(self.variables + self.parameters),\n                        offspring=[],\n                        parent=self.root,\n                    )\n                )\n\n        # The possible operations\n        self.ops = ops\n        self.custom_ops = custom_ops\n        # The possible orders of the operations, move types, and move\n        # type probabilities\n        self.op_orders = list(set([0] + [n for n in list(ops.values())]))\n        self.move_types = [p for p in permutations(self.op_orders, 2)]\n        # Elementary trees (including leaves), indexed by order\n        self.ets = dict([(o, []) for o in self.op_orders])\n        self.ets[0] = [x for x in self.root.offspring]\n        self.ets[self.root.order] = [self.root]\n        # Distinct parameters used\n        self.dist_par = list(\n            set([n.value for n in self.ets[0] if n.value in self.parameters])\n        )\n        self.n_dist_par = len(self.dist_par)\n        # Nodes of the tree (operations + leaves)\n        self.nodes = [self.root]\n        # Tree size and other properties of the model\n        self.size = 1\n        self.max_size = max_size\n        # Space of all possible leaves and elementary trees\n        # (dict. indexed by order)\n        self.et_space = self.build_et_space()\n        # Space of all possible root replacement trees\n        self.rr_space = self.build_rr_space()\n        self.num_rr = len(self.rr_space)\n        # Number of operations of each type\n        self.nops = dict([[o, 0] for o in ops])\n        if root_value is not None:\n            self.nops[self.root.value] += 1\n        # The parameters of the prior probability (default: 5 everywhere)\n        if prior_par == {}:\n            self.prior_par = dict([(\"Nopi_%s\" % t, 10.0) for t in self.ops])\n        else:\n            self.prior_par = prior_par\n        # The datasets\n        if x is None:\n            self.x = {\"d0\": pd.DataFrame()}\n            self.y = {\"d0\": pd.Series(dtype=float)}\n        elif isinstance(x, pd.DataFrame):\n            self.x = {\"d0\": x}\n            self.y = {\"d0\": y}\n        elif isinstance(x, dict):\n            self.x = x\n            if y is None:\n                self.y = dict([(ds, pd.Series(dtype=float)) for ds in self.x])\n            else:\n                self.y = y\n        else:\n            raise TypeError(\"x must be either a dict or a pandas.DataFrame\")\n        # The values of the model parameters (one set of values for each dataset)\n        self.par_values = dict(\n            [(ds, deepcopy(dict([(p, 1.0) for p in self.parameters]))) for ds in self.x]\n        )\n        # BIC and prior temperature\n        self.BT = float(BT)\n        self.PT = float(PT)\n        # For fast fitting, we save past successful fits to this formula\n        self.fit_par = {}\n        # Goodness of fit measures\n        self.sse = self.get_sse()\n        self.bic = self.get_bic()\n        self.E, self.EB, self.EP = self.get_energy()\n        # To control formula degeneracy (i.e. different trees that\n        # correspond to the same canonical formula), we store the\n        # representative tree for each canonical formula\n        self.representative = {}\n        self.representative[self.canonical()] = (\n            str(self),\n            self.E,\n            deepcopy(self.par_values),\n        )\n        # Done\n        return\n\n    # -------------------------------------------------------------------------\n    def __repr__(self):\n        \"\"\"\n        Updates tree's internal representation\n\n        Returns: root node representation\n\n        \"\"\"\n        return self.root.pr(custom_ops=self.custom_ops)\n\n    # -------------------------------------------------------------------------\n    def pr(self, show_pow=True):\n        \"\"\"\n        Returns readable representation of tree's root node\n\n        Returns: root node representation\n\n        \"\"\"\n        return self.root.pr(custom_ops=self.custom_ops, show_pow=show_pow)\n\n    # -------------------------------------------------------------------------\n    def canonical(self, verbose=False):\n        \"\"\"\n        Provides canonical form of tree's equation so that functionally equivalent trees\n        are made into structurally equivalent trees\n\n        Return: canonical form of a tree\n        \"\"\"\n        try:\n            cansp = sympify(str(self).replace(\" \", \"\"))\n            can = str(cansp)\n            ps = list([str(s) for s in cansp.free_symbols])\n            positions = []\n            for p in ps:\n                if p.startswith(\"_\") and p.endswith(\"_\"):\n                    positions.append((can.find(p), p))\n            positions.sort()\n            pcount = 1\n            for pos, p in positions:\n                can = can.replace(p, \"c%d\" % pcount)\n                pcount += 1\n        except SyntaxError:\n            if verbose:\n                print(\n                    \"WARNING: Could not get canonical form for\",\n                    str(self),\n                    \"(using full form!)\",\n                    file=sys.stderr,\n                )\n            can = str(self)\n        return can.replace(\" \", \"\")\n\n    # -------------------------------------------------------------------------\n    def latex(self):\n        \"\"\"\n        translate equation into latex\n\n        Returns: canonical latex form of equation\n        \"\"\"\n        return latex(sympify(self.canonical()))\n\n    # -------------------------------------------------------------------------\n    def build_et_space(self):\n        \"\"\"\n        Build the space of possible elementary trees,\n        which is a dictionary indexed by the order of the elementary tree\n\n        Returns: space of elementary trees\n        \"\"\"\n        et_space = dict([(o, []) for o in self.op_orders])\n        et_space[0] = [[x, []] for x in self.variables + self.parameters]\n        for op, noff in list(self.ops.items()):\n            for vs in product(et_space[0], repeat=noff):\n                et_space[noff].append([op, [v[0] for v in vs]])\n        return et_space\n\n    # -------------------------------------------------------------------------\n    def build_rr_space(self):\n        \"\"\"\n        Build the space of possible trees for the root replacement move\n\n        Returns: space of possible root replacements\n        \"\"\"\n        rr_space = []\n        for op, noff in list(self.ops.items()):\n            if noff == 1:\n                rr_space.append([op, []])\n            else:\n                for vs in product(self.et_space[0], repeat=(noff - 1)):\n                    rr_space.append([op, [v[0] for v in vs]])\n        return rr_space\n\n    # -------------------------------------------------------------------------\n    def replace_root(self, rr=None, update_gof=True, verbose=False):\n        \"\"\"\n        Replace the root with a \"root replacement\" rr (if provided;\n        otherwise choose one at random from self.rr_space)\n\n        Returns: new root (if move was possible) or None (otherwise)\n        \"\"\"\n        # If no RR is provided, randomly choose one\n        if rr is None:\n            rr = choice(self.rr_space)\n        # Return None if the replacement is too big\n        if (self.size + self.ops[rr[0]]) &gt; self.max_size:\n            return None\n        # Create the new root and replace existing root\n        newRoot = Node(rr[0], offspring=[], parent=None)\n        newRoot.order = 1 + len(rr[1])\n        if newRoot.order != self.ops[rr[0]]:\n            raise\n        newRoot.offspring.append(self.root)\n        self.root.parent = newRoot\n        self.root = newRoot\n        self.nops[self.root.value] += 1\n        self.nodes.append(self.root)\n        self.size += 1\n        oldRoot = self.root.offspring[0]\n        for leaf in rr[1]:\n            self.root.offspring.append(Node(leaf, offspring=[], parent=self.root))\n            self.nodes.append(self.root.offspring[-1])\n            self.ets[0].append(self.root.offspring[-1])\n            self.size += 1\n        # Add new root to elementary trees if necessary (that is, iff\n        # the old root was a leaf)\n        if oldRoot.offspring is []:\n            self.ets[self.root.order].append(self.root)\n        # Update list of distinct parameters\n        self.dist_par = list(\n            set([n.value for n in self.ets[0] if n.value in self.parameters])\n        )\n        self.n_dist_par = len(self.dist_par)\n        # Update goodness of fit measures, if necessary\n        if update_gof:\n            self.sse = self.get_sse(verbose=verbose)\n            self.bic = self.get_bic(verbose=verbose)\n            self.E = self.get_energy(verbose=verbose)\n        return self.root\n\n    # -------------------------------------------------------------------------\n    def is_root_prunable(self):\n        \"\"\"\n        Check if the root is \"prunable\"\n\n        Returns: boolean of root \"prunability\"\n        \"\"\"\n        if self.size == 1:\n            isPrunable = False\n        elif self.size == 2:\n            isPrunable = True\n        else:\n            isPrunable = True\n            for o in self.root.offspring[1:]:\n                if o.offspring != []:\n                    isPrunable = False\n                    break\n        return isPrunable\n\n    # -------------------------------------------------------------------------\n    def prune_root(self, update_gof=True, verbose=False):\n        \"\"\"\n        Cut the root and its rightmost leaves (provided they are, indeed, leaves),\n        leaving the leftmost branch as the new tree. Returns the pruned root with the same format\n        as the replacement roots in self.rr_space (or None if pruning was impossible)\n\n        Returns: the replacement root\n        \"\"\"\n        # Check if the root is \"prunable\" (and return None if not)\n        if not self.is_root_prunable():\n            return None\n        # Let's do it!\n        rr = [self.root.value, []]\n        self.nodes.remove(self.root)\n        try:\n            self.ets[len(self.root.offspring)].remove(self.root)\n        except ValueError:\n            pass\n        self.nops[self.root.value] -= 1\n        self.size -= 1\n        for o in self.root.offspring[1:]:\n            rr[1].append(o.value)\n            self.nodes.remove(o)\n            self.size -= 1\n            self.ets[0].remove(o)\n        self.root = self.root.offspring[0]\n        self.root.parent = None\n        # Update list of distinct parameters\n        self.dist_par = list(\n            set([n.value for n in self.ets[0] if n.value in self.parameters])\n        )\n        self.n_dist_par = len(self.dist_par)\n        # Update goodness of fit measures, if necessary\n        if update_gof:\n            self.sse = self.get_sse(verbose=verbose)\n            self.bic = self.get_bic(verbose=verbose)\n            self.E = self.get_energy(verbose=verbose)\n        # Done\n        return rr\n\n    # -------------------------------------------------------------------------\n    def _add_et(self, node, et_order=None, et=None, update_gof=True, verbose=False):\n        \"\"\"\n        Add an elementary tree replacing the node, which must be a leaf\n\n        Returns: the input node\n        \"\"\"\n        if node.offspring != []:\n            raise\n        # If no ET is provided, randomly choose one (of the specified\n        # order if given, or totally at random otherwise)\n        if et is None:\n            if et_order is not None:\n                et = choice(self.et_space[et_order])\n            else:\n                all_ets = []\n                for o in [o for o in self.op_orders if o &gt; 0]:\n                    all_ets += self.et_space[o]\n                et = choice(all_ets)\n                et_order = len(et[1])\n        else:\n            et_order = len(et[1])\n        # Update the node and its offspring\n        node.value = et[0]\n        try:\n            self.nops[node.value] += 1\n        except KeyError:\n            pass\n        node.offspring = [Node(v, parent=node, offspring=[]) for v in et[1]]\n        self.ets[et_order].append(node)\n        try:\n            self.ets[len(node.parent.offspring)].remove(node.parent)\n        except ValueError:\n            pass\n        except AttributeError:\n            pass\n        # Add the offspring to the list of nodes\n        for n in node.offspring:\n            self.nodes.append(n)\n        # Remove the node from the list of leaves and add its offspring\n        self.ets[0].remove(node)\n        for o in node.offspring:\n            self.ets[0].append(o)\n            self.size += 1\n        # Update list of distinct parameters\n        self.dist_par = list(\n            set([n.value for n in self.ets[0] if n.value in self.parameters])\n        )\n        self.n_dist_par = len(self.dist_par)\n        # Update goodness of fit measures, if necessary\n        if update_gof:\n            self.sse = self.get_sse(verbose=verbose)\n            self.bic = self.get_bic(verbose=verbose)\n            self.E = self.get_energy(verbose=verbose)\n        return node\n\n    # -------------------------------------------------------------------------\n    def _del_et(self, node, leaf=None, update_gof=True, verbose=False):\n        \"\"\"\n        Remove an elementary tree, replacing it by a leaf\n\n        Returns: input node\n        \"\"\"\n        if self.size == 1:\n            return None\n        if leaf is None:\n            leaf = choice(self.et_space[0])[0]\n        self.nops[node.value] -= 1\n        node.value = leaf\n        self.ets[len(node.offspring)].remove(node)\n        self.ets[0].append(node)\n        for o in node.offspring:\n            self.ets[0].remove(o)\n            self.nodes.remove(o)\n            self.size -= 1\n        node.offspring = []\n        if node.parent is not None:\n            is_parent_et = True\n            for o in node.parent.offspring:\n                if o not in self.ets[0]:\n                    is_parent_et = False\n                    break\n            if is_parent_et:\n                self.ets[len(node.parent.offspring)].append(node.parent)\n        # Update list of distinct parameters\n        self.dist_par = list(\n            set([n.value for n in self.ets[0] if n.value in self.parameters])\n        )\n        self.n_dist_par = len(self.dist_par)\n        # Update goodness of fit measures, if necessary\n        if update_gof:\n            self.sse = self.get_sse(verbose=verbose)\n            self.bic = self.get_bic(verbose=verbose)\n            self.E = self.get_energy(verbose=verbose)\n        return node\n\n    # -------------------------------------------------------------------------\n    def et_replace(self, target, new, update_gof=True, verbose=False):\n        \"\"\"\n        Replace one elementary tree with another one, both of arbitrary order. target is a\n        Node and new is a tuple [node_value, [list, of, offspring, values]]\n\n        Returns: target\n        \"\"\"\n        oini, ofin = len(target.offspring), len(new[1])\n        if oini == 0:\n            added = self._add_et(target, et=new, update_gof=False, verbose=verbose)\n        else:\n            if ofin == 0:\n                added = self._del_et(\n                    target, leaf=new[0], update_gof=False, verbose=verbose\n                )\n            else:\n                self._del_et(target, update_gof=False, verbose=verbose)\n                added = self._add_et(target, et=new, update_gof=False, verbose=verbose)\n        # Update goodness of fit measures, if necessary\n        if update_gof:\n            self.sse = self.get_sse(verbose=verbose)\n            self.bic = self.get_bic(verbose=verbose)\n        # Done\n        return added\n\n    # -------------------------------------------------------------------------\n    def get_sse(self, fit=True, verbose=False):\n        \"\"\"\n        Get the sum of squared errors, fitting the expression represented by the Tree\n        to the existing data, if specified (by default, yes)\n\n        Returns: sum of square errors (sse)\n        \"\"\"\n        # Return 0 if there is no data\n        if list(self.x.values())[0].empty or list(self.y.values())[0].empty:\n            self.sse = 0\n            return 0\n        # Convert the Tree into a SymPy expression\n        ex = sympify(str(self))\n        # Convert the expression to a function that can be used by\n        # curve_fit, i.e. that takes as arguments (x, a0, a1, ..., an)\n        atomd = dict([(a.name, a) for a in ex.atoms() if a.is_Symbol])\n        variables = [atomd[v] for v in self.variables if v in list(atomd.keys())]\n        parameters = [atomd[p] for p in self.parameters if p in list(atomd.keys())]\n        dic: dict = dict(\n            {\n                \"fac\": scipy.special.factorial,\n                \"sig\": scipy.special.expit,\n                \"relu\": relu,\n            },\n            **self.custom_ops,\n        )\n        try:\n            flam = lambdify(\n                variables + parameters,\n                ex,\n                [\n                    \"numpy\",\n                    dic,\n                ],\n            )\n        except (SyntaxError, KeyError):\n            self.sse = dict([(ds, np.inf) for ds in self.x])\n            return self.sse\n        if fit:\n            if len(parameters) == 0:  # Nothing to fit\n                for ds in self.x:\n                    for p in self.parameters:\n                        self.par_values[ds][p] = 1.0\n            elif str(self) in self.fit_par:  # Recover previously fit parameters\n                self.par_values = self.fit_par[str(self)]\n            else:  # Do the fit for all datasets\n                self.fit_par[str(self)] = {}\n                for ds in self.x:\n                    this_x, this_y = self.x[ds], self.y[ds]\n                    xmat = [this_x[v.name] for v in variables]\n\n                    def feval(x, *params):\n                        args = [xi for xi in x] + [p for p in params]\n                        return flam(*args)\n\n                    try:\n                        # Fit the parameters\n                        res = curve_fit(\n                            feval,\n                            xmat,\n                            this_y,\n                            p0=[self.par_values[ds][p.name] for p in parameters],\n                            maxfev=10000,\n                        )\n                        # Reassign the values of the parameters\n                        self.par_values[ds] = dict(\n                            [\n                                (parameters[i].name, res[0][i])\n                                for i in range(len(res[0]))\n                            ]\n                        )\n                        for p in self.parameters:\n                            if p not in self.par_values[ds]:\n                                self.par_values[ds][p] = 1.0\n                        # Save this fit\n                        self.fit_par[str(self)][ds] = deepcopy(self.par_values[ds])\n                    except RuntimeError:\n                        # Save this (unsuccessful) fit and print warning\n                        self.fit_par[str(self)][ds] = deepcopy(self.par_values[ds])\n                        if verbose:\n                            print(\n                                \"#Cannot_fit:%s # # # # #\" % str(self).replace(\" \", \"\"),\n                                file=sys.stderr,\n                            )\n\n        # Sum of squared errors\n        self.sse = {}\n        for ds in self.x:\n            this_x, this_y = self.x[ds], self.y[ds]\n            xmat = [this_x[v.name] for v in variables]\n            ar = [np.array(xi) for xi in xmat] + [\n                self.par_values[ds][p.name] for p in parameters\n            ]\n            try:\n                se = np.square(this_y - flam(*ar))\n                if sum(np.isnan(se)) &gt; 0:\n                    raise ValueError\n                else:\n                    self.sse[ds] = np.sum(se)\n            except ValueError:\n                if verbose:\n                    print(\"&gt; Cannot calculate SSE for %s: inf\" % self, file=sys.stderr)\n                self.sse[ds] = np.inf\n            except TypeError:\n                if verbose:\n                    print(\"Complex-valued parameters are invalid\")\n                self.sse[ds] = np.inf\n\n        # Done\n        return self.sse\n\n    # -------------------------------------------------------------------------\n    def get_bic(self, reset=True, fit=False, verbose=False):\n        \"\"\"\n        Calculate the Bayesian information criterion (BIC) of the current expression,\n        given the data. If reset==False, the value of self.bic will not be updated\n        (by default, it will)\n\n        Returns: Bayesian information criterion (BIC)\n        \"\"\"\n        if list(self.x.values())[0].empty or list(self.y.values())[0].empty:\n            if reset:\n                self.bic = 0\n            return 0\n        # Get the sum of squared errors (fitting, if required)\n        sse = self.get_sse(fit=fit, verbose=verbose)\n        # Calculate the BIC\n        parameters = set([p.value for p in self.ets[0] if p.value in self.parameters])\n        k = 1 + len(parameters)\n        BIC = 0.0\n        for ds in self.y:\n            if sse[ds] == 0.0:\n                BIC = -np.inf\n                break\n            else:\n                n = len(self.y[ds])\n                BIC += (k - n) * np.log(n) + n * (\n                    np.log(2.0 * np.pi) + log(sse[ds]) + 1\n                )\n        if reset:\n            self.bic = BIC\n        return BIC\n\n    # -------------------------------------------------------------------------\n    def get_energy(self, bic=False, reset=False, verbose=False):\n        \"\"\"\n        Calculate the \"energy\" of a given formula, that is, approximate minus log-posterior\n        of the formula given the data (the approximation coming from the use of the BIC\n        instead of the exactly integrated likelihood)\n\n        Returns: Energy of formula (as E, EB, and EP)\n        \"\"\"\n        # Contribution of the data (recalculating BIC if necessary)\n        if bic:\n            EB = self.get_bic(reset=reset, verbose=verbose) / 2.0\n        else:\n            EB = self.bic / 2.0\n        # Contribution from the prior\n        EP = 0.0\n        for op, nop in list(self.nops.items()):\n            try:\n                EP += self.prior_par[\"Nopi_%s\" % op] * nop\n            except KeyError:\n                pass\n            try:\n                EP += self.prior_par[\"Nopi2_%s\" % op] * nop**2\n            except KeyError:\n                pass\n        # Reset the value, if necessary\n        if reset:\n            self.EB = EB\n            self.EP = EP\n            self.E = EB + EP\n        # Done\n        return EB + EP, EB, EP\n\n    # -------------------------------------------------------------------------\n    def update_representative(self, verbose=False):\n        \"\"\"Check if we've seen this formula before, either in its current form\n        or in another form.\n\n        *If we haven't seen it, save it and return 1.\n\n        *If we have seen it and this IS the representative, just return 0.\n\n        *If we have seen it and the representative has smaller energy, just return -1.\n\n        *If we have seen it and the representative has higher energy, update\n        the representatitve and return -2.\n\n        Returns: Integer value (0, 1, or -1) corresponding to:\n            0: we have seen this canonical form before\n            1: we haven't seen this canonical form before\n            -1: we have seen this equation's canonical form before but it isn't in that form yet\n        \"\"\"\n        # Check for canonical representative\n        canonical = self.canonical(verbose=verbose)\n        try:  # We've seen this canonical before!\n            rep, rep_energy, rep_par_values = self.representative[canonical]\n        except TypeError:\n            return -1  # Complex-valued parameters are invalid\n        except KeyError:  # Never seen this canonical formula before:\n            # save it and return 1\n            self.get_bic(reset=True, fit=True, verbose=verbose)\n            new_energy = self.get_energy(bic=False, verbose=verbose)\n            self.representative[canonical] = (\n                str(self),\n                new_energy,\n                deepcopy(self.par_values),\n            )\n            return 1\n\n        # If we've seen this canonical before, check if the\n        # representative needs to be updated\n        if rep == str(self):  # This IS the representative: return 0\n            return 0\n        else:\n            return -1\n\n    # -------------------------------------------------------------------------\n    def dE_et(self, target, new, verbose=False):\n        \"\"\"\n        Calculate the energy change associated to the replacement of one elementary tree\n        with another, both of arbitrary order. \"target\" is a Node() and \"new\" is\n        a tuple [node_value, [list, of, offspring, values]].\n\n        Returns: change in energy associated with an elementary tree replacement move\n        \"\"\"\n        dEB, dEP = 0.0, 0.0\n\n        # Some terms of the acceptance (number of possible move types\n        # from initial and final configurations), as well as checking\n        # if the tree is canonically acceptable.\n\n        # number of possible move types from initial\n        nif = sum(\n            [\n                int(len(self.ets[oi]) &gt; 0 and (self.size + of - oi) &lt;= self.max_size)\n                for oi, of in self.move_types\n            ]\n        )\n        # replace\n        old = [target.value, [o.value for o in target.offspring]]\n        old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n        old_par_values = deepcopy(self.par_values)\n        added = self.et_replace(target, new, update_gof=False, verbose=verbose)\n        # number of possible move types from final\n        nfi = sum(\n            [\n                int(len(self.ets[oi]) &gt; 0 and (self.size + of - oi) &lt;= self.max_size)\n                for oi, of in self.move_types\n            ]\n        )\n        # check/update canonical representative\n        rep_res = self.update_representative(verbose=verbose)\n        if rep_res == -1:\n            # this formula is forbidden\n            self.et_replace(added, old, update_gof=False, verbose=verbose)\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n            return np.inf, np.inf, np.inf, deepcopy(self.par_values), nif, nfi\n        # leave the whole thing as it was before the back &amp; fore\n        self.et_replace(added, old, update_gof=False, verbose=verbose)\n        self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n        self.par_values = old_par_values\n        # Prior: change due to the numbers of each operation\n        try:\n            dEP -= self.prior_par[\"Nopi_%s\" % target.value]\n        except KeyError:\n            pass\n        try:\n            dEP += self.prior_par[\"Nopi_%s\" % new[0]]\n        except KeyError:\n            pass\n        try:\n            dEP += self.prior_par[\"Nopi2_%s\" % target.value] * (\n                (self.nops[target.value] - 1) ** 2 - (self.nops[target.value]) ** 2\n            )\n        except KeyError:\n            pass\n        try:\n            dEP += self.prior_par[\"Nopi2_%s\" % new[0]] * (\n                (self.nops[new[0]] + 1) ** 2 - (self.nops[new[0]]) ** 2\n            )\n        except KeyError:\n            pass\n\n        # Data\n        if not list(self.x.values())[0].empty:\n            bicOld = self.bic\n            sseOld = deepcopy(self.sse)\n            par_valuesOld = deepcopy(self.par_values)\n            old = [target.value, [o.value for o in target.offspring]]\n            # replace\n            added = self.et_replace(target, new, update_gof=True, verbose=verbose)\n            bicNew = self.bic\n            par_valuesNew = deepcopy(self.par_values)\n            # leave the whole thing as it was before the back &amp; fore\n            self.et_replace(added, old, update_gof=False, verbose=verbose)\n            self.bic = bicOld\n            self.sse = deepcopy(sseOld)\n            self.par_values = par_valuesOld\n            dEB += (bicNew - bicOld) / 2.0\n        else:\n            par_valuesNew = deepcopy(self.par_values)\n        # Done\n        try:\n            dEB = float(dEB)\n            dEP = float(dEP)\n            dE = dEB + dEP\n        except (ValueError, TypeError):\n            dEB, dEP, dE = np.inf, np.inf, np.inf\n        return dE, dEB, dEP, par_valuesNew, nif, nfi\n\n    # -------------------------------------------------------------------------\n    def dE_lr(self, target, new, verbose=False):\n        \"\"\"\n        Calculate the energy change associated to a long-range move\n        (the replacement of the value of a node. \"target\" is a Node() and \"new\" is a node_value\n\n        Returns: energy change associated with a long-range move\n        \"\"\"\n        dEB, dEP = 0.0, 0.0\n        par_valuesNew = deepcopy(self.par_values)\n\n        if target.value != new:\n            # Check if the new tree is canonically acceptable.\n            old = target.value\n            old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n            old_par_values = deepcopy(self.par_values)\n            target.value = new\n            try:\n                self.nops[old] -= 1\n                self.nops[new] += 1\n            except KeyError:\n                pass\n            # check/update canonical representative\n            rep_res = self.update_representative(verbose=verbose)\n            if rep_res == -1:\n                # this formula is forbidden\n                target.value = old\n                try:\n                    self.nops[old] += 1\n                    self.nops[new] -= 1\n                except KeyError:\n                    pass\n                self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n                self.par_values = old_par_values\n                return np.inf, np.inf, np.inf, None\n            # leave the whole thing as it was before the back &amp; fore\n            target.value = old\n            try:\n                self.nops[old] += 1\n                self.nops[new] -= 1\n            except KeyError:\n                pass\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n\n            # Prior: change due to the numbers of each operation\n            try:\n                dEP -= self.prior_par[\"Nopi_%s\" % target.value]\n            except KeyError:\n                pass\n            try:\n                dEP += self.prior_par[\"Nopi_%s\" % new]\n            except KeyError:\n                pass\n            try:\n                dEP += self.prior_par[\"Nopi2_%s\" % target.value] * (\n                    (self.nops[target.value] - 1) ** 2 - (self.nops[target.value]) ** 2\n                )\n            except KeyError:\n                pass\n            try:\n                dEP += self.prior_par[\"Nopi2_%s\" % new] * (\n                    (self.nops[new] + 1) ** 2 - (self.nops[new]) ** 2\n                )\n            except KeyError:\n                pass\n\n            # Data\n            if not list(self.x.values())[0].empty:\n                bicOld = self.bic\n                sseOld = deepcopy(self.sse)\n                par_valuesOld = deepcopy(self.par_values)\n                old = target.value\n                target.value = new\n                bicNew = self.get_bic(reset=True, fit=True, verbose=verbose)\n                par_valuesNew = deepcopy(self.par_values)\n                # leave the whole thing as it was before the back &amp; fore\n                target.value = old\n                self.bic = bicOld\n                self.sse = deepcopy(sseOld)\n                self.par_values = par_valuesOld\n                dEB += (bicNew - bicOld) / 2.0\n            else:\n                par_valuesNew = deepcopy(self.par_values)\n\n        # Done\n        try:\n            dEB = float(dEB)\n            dEP = float(dEP)\n            dE = dEB + dEP\n            return dE, dEB, dEP, par_valuesNew\n        except (ValueError, TypeError):\n            return np.inf, np.inf, np.inf, None\n\n    # -------------------------------------------------------------------------\n    def dE_rr(self, rr=None, verbose=False):\n        \"\"\"\n        Calculate the energy change associated to a root replacement move.\n        If rr==None, then it returns the energy change associated to pruning the root; otherwise,\n        it returns the energy change associated to adding the root replacement \"rr\"\n\n        Returns: energy change associated with a root replacement move\n        \"\"\"\n        dEB, dEP = 0.0, 0.0\n\n        # Root pruning\n        if rr is None:\n            if not self.is_root_prunable():\n                return np.inf, np.inf, np.inf, self.par_values\n\n            # Check if the new tree is canonically acceptable.\n            # replace\n            old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n            old_par_values = deepcopy(self.par_values)\n            oldrr = [self.root.value, [o.value for o in self.root.offspring[1:]]]\n            self.prune_root(update_gof=False, verbose=verbose)\n            # check/update canonical representative\n            rep_res = self.update_representative(verbose=verbose)\n            if rep_res == -1:\n                # this formula is forbidden\n                self.replace_root(rr=oldrr, update_gof=False, verbose=verbose)\n                self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n                self.par_values = old_par_values\n                return np.inf, np.inf, np.inf, deepcopy(self.par_values)\n            # leave the whole thing as it was before the back &amp; fore\n            self.replace_root(rr=oldrr, update_gof=False, verbose=verbose)\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n\n            # Prior: change due to the numbers of each operation\n            dEP -= self.prior_par[\"Nopi_%s\" % self.root.value]\n            try:\n                dEP += self.prior_par[\"Nopi2_%s\" % self.root.value] * (\n                    (self.nops[self.root.value] - 1) ** 2\n                    - (self.nops[self.root.value]) ** 2\n                )\n            except KeyError:\n                pass\n\n            # Data correction\n            if not list(self.x.values())[0].empty:\n                bicOld = self.bic\n                sseOld = deepcopy(self.sse)\n                par_valuesOld = deepcopy(self.par_values)\n                oldrr = [self.root.value, [o.value for o in self.root.offspring[1:]]]\n                # replace\n                self.prune_root(update_gof=False, verbose=verbose)\n                bicNew = self.get_bic(reset=True, fit=True, verbose=verbose)\n                par_valuesNew = deepcopy(self.par_values)\n                # leave the whole thing as it was before the back &amp; fore\n                self.replace_root(rr=oldrr, update_gof=False, verbose=verbose)\n                self.bic = bicOld\n                self.sse = deepcopy(sseOld)\n                self.par_values = par_valuesOld\n                dEB += (bicNew - bicOld) / 2.0\n            else:\n                par_valuesNew = deepcopy(self.par_values)\n            # Done\n            try:\n                dEB = float(dEB)\n                dEP = float(dEP)\n                dE = dEB + dEP\n            except (ValueError, TypeError):\n                dEB, dEP, dE = np.inf, np.inf, np.inf\n            return dE, dEB, dEP, par_valuesNew\n\n        # Root replacement\n        else:\n            # Check if the new tree is canonically acceptable.\n            # replace\n            old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n            old_par_values = deepcopy(self.par_values)\n            newroot = self.replace_root(rr=rr, update_gof=False, verbose=verbose)\n            if newroot is None:  # Root cannot be replaced (due to max_size)\n                return np.inf, np.inf, np.inf, deepcopy(self.par_values)\n            # check/update canonical representative\n            rep_res = self.update_representative(verbose=verbose)\n            if rep_res == -1:\n                # this formula is forbidden\n                self.prune_root(update_gof=False, verbose=verbose)\n                self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n                self.par_values = old_par_values\n                return np.inf, np.inf, np.inf, deepcopy(self.par_values)\n            # leave the whole thing as it was before the back &amp; fore\n            self.prune_root(update_gof=False, verbose=verbose)\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n\n            # Prior: change due to the numbers of each operation\n            dEP += self.prior_par[\"Nopi_%s\" % rr[0]]\n            try:\n                dEP += self.prior_par[\"Nopi2_%s\" % rr[0]] * (\n                    (self.nops[rr[0]] + 1) ** 2 - (self.nops[rr[0]]) ** 2\n                )\n            except KeyError:\n                pass\n\n            # Data\n            if not list(self.x.values())[0].empty:\n                bicOld = self.bic\n                sseOld = deepcopy(self.sse)\n                par_valuesOld = deepcopy(self.par_values)\n                # replace\n                newroot = self.replace_root(rr=rr, update_gof=False, verbose=verbose)\n                if newroot is None:\n                    return np.inf, np.inf, np.inf, self.par_values\n                bicNew = self.get_bic(reset=True, fit=True, verbose=verbose)\n                par_valuesNew = deepcopy(self.par_values)\n                # leave the whole thing as it was before the back &amp; fore\n                self.prune_root(update_gof=False, verbose=verbose)\n                self.bic = bicOld\n                self.sse = deepcopy(sseOld)\n                self.par_values = par_valuesOld\n                dEB += (bicNew - bicOld) / 2.0\n            else:\n                par_valuesNew = deepcopy(self.par_values)\n            # Done\n            try:\n                dEB = float(dEB)\n                dEP = float(dEP)\n                dE = dEB + dEP\n            except (ValueError, TypeError):\n                dEB, dEP, dE = np.inf, np.inf, np.inf\n            return dE, dEB, dEP, par_valuesNew\n\n    # -------------------------------------------------------------------------\n    def mcmc_step(self, verbose=False, p_rr=0.05, p_long=0.45):\n        \"\"\"\n        Make a single MCMC step\n\n        Returns: None or expression list\n        \"\"\"\n        topDice = random()\n        # Root replacement move\n        if topDice &lt; p_rr:\n            if random() &lt; 0.5:\n                # Try to prune the root\n                dE, dEB, dEP, par_valuesNew = self.dE_rr(rr=None, verbose=verbose)\n                if -dEB / self.BT - dEP / self.PT &gt; 300:\n                    paccept = 1\n                else:\n                    paccept = np.exp(-dEB / self.BT - dEP / self.PT) / float(\n                        self.num_rr\n                    )\n                dice = random()\n                if dice &lt; paccept:\n                    # Accept move\n                    self.prune_root(update_gof=False, verbose=verbose)\n                    self.par_values = par_valuesNew\n                    self.get_bic(reset=True, fit=False, verbose=verbose)\n                    self.E += dE\n                    self.EB += dEB\n                    self.EP += dEP\n            else:\n                # Try to replace the root\n                newrr = choice(self.rr_space)\n                dE, dEB, dEP, par_valuesNew = self.dE_rr(rr=newrr, verbose=verbose)\n                if self.num_rr &gt; 0 and -dEB / self.BT - dEP / self.PT &gt; 0:\n                    paccept = 1.0\n                elif self.num_rr == 0:\n                    paccept = 0.0\n                else:\n                    paccept = self.num_rr * np.exp(-dEB / self.BT - dEP / self.PT)\n                dice = random()\n                if dice &lt; paccept:\n                    # Accept move\n                    self.replace_root(rr=newrr, update_gof=False, verbose=verbose)\n                    self.par_values = par_valuesNew\n                    self.get_bic(reset=True, fit=False, verbose=verbose)\n                    self.E += dE\n                    self.EB += dEB\n                    self.EP += dEP\n\n        # Long-range move\n        elif topDice &lt; (p_rr + p_long) and not (\n            self.fixed_root and len(self.nodes) == 1\n        ):\n            # Choose a random node in the tree, and a random new operation\n            target = choice(self.nodes)\n            if self.fixed_root:\n                while target is self.root:\n                    target = choice(self.nodes)\n            nready = False\n            while not nready:\n                if len(target.offspring) == 0:\n                    new = choice(self.variables + self.parameters)\n                    nready = True\n                else:\n                    new = choice(list(self.ops.keys()))\n                    if self.ops[new] == self.ops[target.value]:\n                        nready = True\n            dE, dEB, dEP, par_valuesNew = self.dE_lr(target, new, verbose=verbose)\n            try:\n                paccept = np.exp(-dEB / self.BT - dEP / self.PT)\n            except ValueError:\n                _logger.warning(\"Potentially failing to set paccept properly\")\n                if (dEB / self.BT + dEP / self.PT) &lt; 0:\n                    paccept = 1.0\n            # Accept move, if necessary\n            dice = random()\n            if dice &lt; paccept:\n                # update number of operations\n                if target.offspring != []:\n                    self.nops[target.value] -= 1\n                    self.nops[new] += 1\n                # move\n                target.value = new\n                # recalculate distinct parameters\n                self.dist_par = list(\n                    set([n.value for n in self.ets[0] if n.value in self.parameters])\n                )\n                self.n_dist_par = len(self.dist_par)\n                # update others\n                self.par_values = deepcopy(par_valuesNew)\n                self.get_bic(reset=True, fit=False, verbose=verbose)\n                self.E += dE\n                self.EB += dEB\n                self.EP += dEP\n\n        # Elementary tree (short-range) move\n        else:\n            target = None\n            while target is None or self.fixed_root and target is self.root:\n                # Choose a feasible move (doable and keeping size&lt;=max_size)\n                while True:\n                    oini, ofin = choice(self.move_types)\n                    if len(self.ets[oini]) &gt; 0 and (\n                        self.size - oini + ofin &lt;= self.max_size\n                    ):\n                        break\n                # target and new ETs\n                target = choice(self.ets[oini])\n                new = choice(self.et_space[ofin])\n            # omegai and omegaf\n            omegai = len(self.ets[oini])\n            omegaf = len(self.ets[ofin]) + 1\n            if ofin == 0:\n                omegaf -= oini\n            if oini == 0 and target.parent in self.ets[ofin]:\n                omegaf -= 1\n            # size of et_space of each type\n            si = len(self.et_space[oini])\n            sf = len(self.et_space[ofin])\n            # Probability of acceptance\n            dE, dEB, dEP, par_valuesNew, nif, nfi = self.dE_et(\n                target, new, verbose=verbose\n            )\n            try:\n                paccept = (\n                    float(nif) * omegai * sf * np.exp(-dEB / self.BT - dEP / self.PT)\n                ) / (float(nfi) * omegaf * si)\n            except ValueError:\n                if (dEB / self.BT + dEP / self.PT) &lt; -200:\n                    paccept = 1.0\n            # Accept / reject\n            dice = random()\n            if dice &lt; paccept:\n                # Accept move\n                self.et_replace(target, new, verbose=verbose)\n                self.par_values = par_valuesNew\n                self.get_bic(verbose=verbose)\n                self.E += dE\n                self.EB += dEB\n                self.EP += dEP\n\n        # Done\n        return\n\n    # -------------------------------------------------------------------------\n    def mcmc(\n        self,\n        tracefn=\"trace.dat\",\n        progressfn=\"progress.dat\",\n        write_files=True,\n        reset_files=True,\n        burnin=2000,\n        thin=10,\n        samples=10000,\n        verbose=False,\n        progress=True,\n    ):\n        \"\"\"\n        Sample the space of formula trees using MCMC, and write the trace and some progress\n        information to files (unless write_files is False)\n\n        Returns: None or expression list\n        \"\"\"\n        self.get_energy(reset=True, verbose=verbose)\n\n        # Burning\n        if progress:\n            sys.stdout.write(\"# Burning in\\t\")\n            sys.stdout.write(\"[%s]\" % (\" \" * 50))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\b\" * (50 + 1))\n        for i in range(burnin):\n            self.mcmc_step(verbose=verbose)\n            if progress and (i % (burnin / 50) == 0):\n                sys.stdout.write(\"=\")\n                sys.stdout.flush()\n        # Sample\n        if write_files:\n            if reset_files:\n                tracef = open(tracefn, \"w\")\n                progressf = open(progressfn, \"w\")\n            else:\n                tracef = open(tracefn, \"a\")\n                progressf = open(progressfn, \"a\")\n        if progress:\n            sys.stdout.write(\"\\n# Sampling\\t\")\n            sys.stdout.write(\"[%s]\" % (\" \" * 50))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\b\" * (50 + 1))\n        for s in range(samples):\n            for i in range(thin):\n                self.mcmc_step(verbose=verbose)\n            if progress and (s % (samples / 50) == 0):\n                sys.stdout.write(\"=\")\n                sys.stdout.flush()\n            if write_files:\n                json.dump(\n                    [\n                        s,\n                        float(self.bic),\n                        float(self.E),\n                        str(self.get_energy(verbose=verbose)),\n                        str(self),\n                        self.par_values,\n                    ],\n                    tracef,\n                )\n                tracef.write(\"\\n\")\n                tracef.flush()\n                progressf.write(\"%d %lf %lf\\n\" % (s, self.E, self.bic))\n                progressf.flush()\n        # Done\n        if progress:\n            sys.stdout.write(\"\\n\")\n        return\n\n    # -------------------------------------------------------------------------\n    def predict(self, x):\n        \"\"\"\n        Calculate the value of the formula at the given data x. The data x\n        must have the same format as the training data and, in particular, it\n        it must specify to which dataset the example data belongs, if multiple\n        datasets where used for training.\n\n        Returns: predicted y values\n        \"\"\"\n        if isinstance(x, np.ndarray):\n            columns = list()\n            for col in range(x.shape[1]):\n                columns.append(\"X\" + str(col))\n            x = pd.DataFrame(x, columns=columns)\n\n        if isinstance(x, pd.DataFrame):\n            this_x = {\"d0\": x}\n            input_type = \"df\"\n        elif isinstance(x, dict):\n            this_x = x\n            input_type = \"dict\"\n        else:\n            raise TypeError(\"x must be either a dict or a pandas.DataFrame\")\n\n        # Convert the Tree into a SymPy expression\n        ex = sympify(str(self))\n        # Convert the expression to a function\n        atomd = dict([(a.name, a) for a in ex.atoms() if a.is_Symbol])\n        variables = [atomd[v] for v in self.variables if v in list(atomd.keys())]\n        parameters = [atomd[p] for p in self.parameters if p in list(atomd.keys())]\n        flam = lambdify(\n            variables + parameters,\n            ex,\n            [\n                \"numpy\",\n                dict(\n                    {\n                        \"fac\": scipy.special.factorial,\n                        \"sig\": scipy.special.expit,\n                        \"relu\": relu,\n                    },\n                    **self.custom_ops,\n                ),\n            ],\n        )\n        # Loop over datasets\n        predictions = {}\n        for ds in this_x:\n            # Prepare variables and parameters\n            xmat = [this_x[ds][v.name] for v in variables]\n            params = [self.par_values[ds][p.name] for p in parameters]\n            args = [xi for xi in xmat] + [p for p in params]\n            # Predict\n            try:\n                prediction = flam(*args)\n            except SyntaxError:\n                # Do it point by point\n                prediction = [np.nan for i in range(len(this_x[ds]))]\n            predictions[ds] = pd.Series(prediction, index=list(this_x[ds].index))\n\n        if input_type == \"df\":\n            return predictions[\"d0\"]\n        else:\n            return predictions\n\n    # -------------------------------------------------------------------------\n    def trace_predict(\n        self,\n        x,\n        burnin=1000,\n        thin=2000,\n        samples=1000,\n        tracefn=\"trace.dat\",\n        progressfn=\"progress.dat\",\n        write_files=False,\n        reset_files=True,\n        verbose=False,\n        progress=True,\n    ):\n        \"\"\"\n        Sample the space of formula trees using MCMC,\n        and predict y(x) for each of the sampled formula trees\n\n        Returns: predicted y values for each of the sampled formula trees\n        \"\"\"\n        ypred = {}\n        # Burning\n        if progress:\n            sys.stdout.write(\"# Burning in\\t\")\n            sys.stdout.write(\"[%s]\" % (\" \" * 50))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\b\" * (50 + 1))\n        for i in range(burnin):\n            self.mcmc_step(verbose=verbose)\n            if progress and (i % (burnin / 50) == 0):\n                sys.stdout.write(\"=\")\n                sys.stdout.flush()\n        # Sample\n        if write_files:\n            if reset_files:\n                tracef = open(tracefn, \"w\")\n                progressf = open(progressfn, \"w\")\n            else:\n                tracef = open(tracefn, \"a\")\n                progressf = open(progressfn, \"a\")\n        if progress:\n            sys.stdout.write(\"\\n# Sampling\\t\")\n            sys.stdout.write(\"[%s]\" % (\" \" * 50))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\b\" * (50 + 1))\n\n        for s in range(samples):\n            for kk in range(thin):\n                self.mcmc_step(verbose=verbose)\n            # Make prediction\n            ypred[s] = self.predict(x)\n            # Output\n            if progress and (s % (samples / 50) == 0):\n                sys.stdout.write(\"=\")\n                sys.stdout.flush()\n            if write_files:\n                json.dump(\n                    [\n                        s,\n                        float(self.bic),\n                        float(self.E),\n                        float(self.get_energy(verbose=verbose)),\n                        str(self),\n                        self.par_values,\n                    ],\n                    tracef,\n                )\n                tracef.write(\"\\n\")\n                tracef.flush()\n                progressf.write(\"%d %lf %lf\\n\" % (s, self.E, self.bic))\n                progressf.flush()\n        # Done\n        if progress:\n            sys.stdout.write(\"\\n\")\n        return pd.DataFrame.from_dict(ypred)\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.__init__","title":"<code>__init__(ops=ops, variables=['x'], parameters=['a'], prior_par=prior, x=None, y=None, BT=1.0, PT=1.0, max_size=50, root_value=None, fixed_root=False, custom_ops={}, random_state=None)</code>","text":"<p>Initialises the tree object</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <p>allowed operations to compose equation</p> <code>ops</code> <code>variables</code> <p>dependent variable names</p> <code>['x']</code> <code>parameters</code> <p>parameters that can be used to better fit the equation to the data</p> <code>['a']</code> <code>prior_par</code> <p>hyperparameter values over operations within ops</p> <code>prior</code> <code>x</code> <p>dependent variables</p> <code>None</code> <code>y</code> <p>independent variables</p> <code>None</code> <code>BT</code> <p>BIC value corresponding to equation</p> <code>1.0</code> <code>PT</code> <p>prior temperature</p> <code>1.0</code> <code>max_size</code> <p>maximum size of tree (maximum number of nodes)</p> <code>50</code> <code>root_value</code> <p>algebraic term held at root of equation</p> <code>None</code> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def __init__(\n    self,\n    ops=ops,\n    variables=[\"x\"],\n    parameters=[\"a\"],\n    prior_par=prior,\n    x=None,\n    y=None,\n    BT=1.0,\n    PT=1.0,\n    max_size=50,\n    root_value=None,\n    fixed_root=False,\n    custom_ops={},\n    random_state=None,\n):\n    \"\"\"\n    Initialises the tree object\n\n    Args:\n        ops: allowed operations to compose equation\n        variables: dependent variable names\n        parameters: parameters that can be used to better fit the equation to the data\n        prior_par: hyperparameter values over operations within ops\n        x: dependent variables\n        y: independent variables\n        BT: BIC value corresponding to equation\n        PT: prior temperature\n        max_size: maximum size of tree (maximum number of nodes)\n        root_value: algebraic term held at root of equation\n    \"\"\"\n    if random_state is not None:\n        seed(random_state)\n        np.random.seed(random_state)\n    # The variables and parameters\n    if custom_ops is None:\n        custom_ops = dict()\n    self.variables = variables\n    self.parameters = [\n        p if p.startswith(\"_\") and p.endswith(\"_\") else \"_%s_\" % p\n        for p in parameters\n    ]\n    # The root\n    self.fixed_root = fixed_root\n    if root_value is None:\n        self.root = Node(\n            choice(self.variables + self.parameters), offspring=[], parent=None\n        )\n    else:\n        self.root = Node(root_value, offspring=[], parent=None)\n        root_order = len(signature(custom_ops[root_value]).parameters)\n        self.root.order = root_order\n        for _ in range(root_order):\n            self.root.offspring.append(\n                Node(\n                    choice(self.variables + self.parameters),\n                    offspring=[],\n                    parent=self.root,\n                )\n            )\n\n    # The possible operations\n    self.ops = ops\n    self.custom_ops = custom_ops\n    # The possible orders of the operations, move types, and move\n    # type probabilities\n    self.op_orders = list(set([0] + [n for n in list(ops.values())]))\n    self.move_types = [p for p in permutations(self.op_orders, 2)]\n    # Elementary trees (including leaves), indexed by order\n    self.ets = dict([(o, []) for o in self.op_orders])\n    self.ets[0] = [x for x in self.root.offspring]\n    self.ets[self.root.order] = [self.root]\n    # Distinct parameters used\n    self.dist_par = list(\n        set([n.value for n in self.ets[0] if n.value in self.parameters])\n    )\n    self.n_dist_par = len(self.dist_par)\n    # Nodes of the tree (operations + leaves)\n    self.nodes = [self.root]\n    # Tree size and other properties of the model\n    self.size = 1\n    self.max_size = max_size\n    # Space of all possible leaves and elementary trees\n    # (dict. indexed by order)\n    self.et_space = self.build_et_space()\n    # Space of all possible root replacement trees\n    self.rr_space = self.build_rr_space()\n    self.num_rr = len(self.rr_space)\n    # Number of operations of each type\n    self.nops = dict([[o, 0] for o in ops])\n    if root_value is not None:\n        self.nops[self.root.value] += 1\n    # The parameters of the prior probability (default: 5 everywhere)\n    if prior_par == {}:\n        self.prior_par = dict([(\"Nopi_%s\" % t, 10.0) for t in self.ops])\n    else:\n        self.prior_par = prior_par\n    # The datasets\n    if x is None:\n        self.x = {\"d0\": pd.DataFrame()}\n        self.y = {\"d0\": pd.Series(dtype=float)}\n    elif isinstance(x, pd.DataFrame):\n        self.x = {\"d0\": x}\n        self.y = {\"d0\": y}\n    elif isinstance(x, dict):\n        self.x = x\n        if y is None:\n            self.y = dict([(ds, pd.Series(dtype=float)) for ds in self.x])\n        else:\n            self.y = y\n    else:\n        raise TypeError(\"x must be either a dict or a pandas.DataFrame\")\n    # The values of the model parameters (one set of values for each dataset)\n    self.par_values = dict(\n        [(ds, deepcopy(dict([(p, 1.0) for p in self.parameters]))) for ds in self.x]\n    )\n    # BIC and prior temperature\n    self.BT = float(BT)\n    self.PT = float(PT)\n    # For fast fitting, we save past successful fits to this formula\n    self.fit_par = {}\n    # Goodness of fit measures\n    self.sse = self.get_sse()\n    self.bic = self.get_bic()\n    self.E, self.EB, self.EP = self.get_energy()\n    # To control formula degeneracy (i.e. different trees that\n    # correspond to the same canonical formula), we store the\n    # representative tree for each canonical formula\n    self.representative = {}\n    self.representative[self.canonical()] = (\n        str(self),\n        self.E,\n        deepcopy(self.par_values),\n    )\n    # Done\n    return\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.__repr__","title":"<code>__repr__()</code>","text":"<p>Updates tree's internal representation</p> <p>Returns: root node representation</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Updates tree's internal representation\n\n    Returns: root node representation\n\n    \"\"\"\n    return self.root.pr(custom_ops=self.custom_ops)\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.build_et_space","title":"<code>build_et_space()</code>","text":"<p>Build the space of possible elementary trees, which is a dictionary indexed by the order of the elementary tree</p> <p>Returns: space of elementary trees</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def build_et_space(self):\n    \"\"\"\n    Build the space of possible elementary trees,\n    which is a dictionary indexed by the order of the elementary tree\n\n    Returns: space of elementary trees\n    \"\"\"\n    et_space = dict([(o, []) for o in self.op_orders])\n    et_space[0] = [[x, []] for x in self.variables + self.parameters]\n    for op, noff in list(self.ops.items()):\n        for vs in product(et_space[0], repeat=noff):\n            et_space[noff].append([op, [v[0] for v in vs]])\n    return et_space\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.build_rr_space","title":"<code>build_rr_space()</code>","text":"<p>Build the space of possible trees for the root replacement move</p> <p>Returns: space of possible root replacements</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def build_rr_space(self):\n    \"\"\"\n    Build the space of possible trees for the root replacement move\n\n    Returns: space of possible root replacements\n    \"\"\"\n    rr_space = []\n    for op, noff in list(self.ops.items()):\n        if noff == 1:\n            rr_space.append([op, []])\n        else:\n            for vs in product(self.et_space[0], repeat=(noff - 1)):\n                rr_space.append([op, [v[0] for v in vs]])\n    return rr_space\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.canonical","title":"<code>canonical(verbose=False)</code>","text":"<p>Provides canonical form of tree's equation so that functionally equivalent trees are made into structurally equivalent trees</p> <p>Return: canonical form of a tree</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def canonical(self, verbose=False):\n    \"\"\"\n    Provides canonical form of tree's equation so that functionally equivalent trees\n    are made into structurally equivalent trees\n\n    Return: canonical form of a tree\n    \"\"\"\n    try:\n        cansp = sympify(str(self).replace(\" \", \"\"))\n        can = str(cansp)\n        ps = list([str(s) for s in cansp.free_symbols])\n        positions = []\n        for p in ps:\n            if p.startswith(\"_\") and p.endswith(\"_\"):\n                positions.append((can.find(p), p))\n        positions.sort()\n        pcount = 1\n        for pos, p in positions:\n            can = can.replace(p, \"c%d\" % pcount)\n            pcount += 1\n    except SyntaxError:\n        if verbose:\n            print(\n                \"WARNING: Could not get canonical form for\",\n                str(self),\n                \"(using full form!)\",\n                file=sys.stderr,\n            )\n        can = str(self)\n    return can.replace(\" \", \"\")\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.dE_et","title":"<code>dE_et(target, new, verbose=False)</code>","text":"<p>Calculate the energy change associated to the replacement of one elementary tree with another, both of arbitrary order. \"target\" is a Node() and \"new\" is a tuple [node_value, [list, of, offspring, values]].</p> <p>Returns: change in energy associated with an elementary tree replacement move</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def dE_et(self, target, new, verbose=False):\n    \"\"\"\n    Calculate the energy change associated to the replacement of one elementary tree\n    with another, both of arbitrary order. \"target\" is a Node() and \"new\" is\n    a tuple [node_value, [list, of, offspring, values]].\n\n    Returns: change in energy associated with an elementary tree replacement move\n    \"\"\"\n    dEB, dEP = 0.0, 0.0\n\n    # Some terms of the acceptance (number of possible move types\n    # from initial and final configurations), as well as checking\n    # if the tree is canonically acceptable.\n\n    # number of possible move types from initial\n    nif = sum(\n        [\n            int(len(self.ets[oi]) &gt; 0 and (self.size + of - oi) &lt;= self.max_size)\n            for oi, of in self.move_types\n        ]\n    )\n    # replace\n    old = [target.value, [o.value for o in target.offspring]]\n    old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n    old_par_values = deepcopy(self.par_values)\n    added = self.et_replace(target, new, update_gof=False, verbose=verbose)\n    # number of possible move types from final\n    nfi = sum(\n        [\n            int(len(self.ets[oi]) &gt; 0 and (self.size + of - oi) &lt;= self.max_size)\n            for oi, of in self.move_types\n        ]\n    )\n    # check/update canonical representative\n    rep_res = self.update_representative(verbose=verbose)\n    if rep_res == -1:\n        # this formula is forbidden\n        self.et_replace(added, old, update_gof=False, verbose=verbose)\n        self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n        self.par_values = old_par_values\n        return np.inf, np.inf, np.inf, deepcopy(self.par_values), nif, nfi\n    # leave the whole thing as it was before the back &amp; fore\n    self.et_replace(added, old, update_gof=False, verbose=verbose)\n    self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n    self.par_values = old_par_values\n    # Prior: change due to the numbers of each operation\n    try:\n        dEP -= self.prior_par[\"Nopi_%s\" % target.value]\n    except KeyError:\n        pass\n    try:\n        dEP += self.prior_par[\"Nopi_%s\" % new[0]]\n    except KeyError:\n        pass\n    try:\n        dEP += self.prior_par[\"Nopi2_%s\" % target.value] * (\n            (self.nops[target.value] - 1) ** 2 - (self.nops[target.value]) ** 2\n        )\n    except KeyError:\n        pass\n    try:\n        dEP += self.prior_par[\"Nopi2_%s\" % new[0]] * (\n            (self.nops[new[0]] + 1) ** 2 - (self.nops[new[0]]) ** 2\n        )\n    except KeyError:\n        pass\n\n    # Data\n    if not list(self.x.values())[0].empty:\n        bicOld = self.bic\n        sseOld = deepcopy(self.sse)\n        par_valuesOld = deepcopy(self.par_values)\n        old = [target.value, [o.value for o in target.offspring]]\n        # replace\n        added = self.et_replace(target, new, update_gof=True, verbose=verbose)\n        bicNew = self.bic\n        par_valuesNew = deepcopy(self.par_values)\n        # leave the whole thing as it was before the back &amp; fore\n        self.et_replace(added, old, update_gof=False, verbose=verbose)\n        self.bic = bicOld\n        self.sse = deepcopy(sseOld)\n        self.par_values = par_valuesOld\n        dEB += (bicNew - bicOld) / 2.0\n    else:\n        par_valuesNew = deepcopy(self.par_values)\n    # Done\n    try:\n        dEB = float(dEB)\n        dEP = float(dEP)\n        dE = dEB + dEP\n    except (ValueError, TypeError):\n        dEB, dEP, dE = np.inf, np.inf, np.inf\n    return dE, dEB, dEP, par_valuesNew, nif, nfi\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.dE_lr","title":"<code>dE_lr(target, new, verbose=False)</code>","text":"<p>Calculate the energy change associated to a long-range move (the replacement of the value of a node. \"target\" is a Node() and \"new\" is a node_value</p> <p>Returns: energy change associated with a long-range move</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def dE_lr(self, target, new, verbose=False):\n    \"\"\"\n    Calculate the energy change associated to a long-range move\n    (the replacement of the value of a node. \"target\" is a Node() and \"new\" is a node_value\n\n    Returns: energy change associated with a long-range move\n    \"\"\"\n    dEB, dEP = 0.0, 0.0\n    par_valuesNew = deepcopy(self.par_values)\n\n    if target.value != new:\n        # Check if the new tree is canonically acceptable.\n        old = target.value\n        old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n        old_par_values = deepcopy(self.par_values)\n        target.value = new\n        try:\n            self.nops[old] -= 1\n            self.nops[new] += 1\n        except KeyError:\n            pass\n        # check/update canonical representative\n        rep_res = self.update_representative(verbose=verbose)\n        if rep_res == -1:\n            # this formula is forbidden\n            target.value = old\n            try:\n                self.nops[old] += 1\n                self.nops[new] -= 1\n            except KeyError:\n                pass\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n            return np.inf, np.inf, np.inf, None\n        # leave the whole thing as it was before the back &amp; fore\n        target.value = old\n        try:\n            self.nops[old] += 1\n            self.nops[new] -= 1\n        except KeyError:\n            pass\n        self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n        self.par_values = old_par_values\n\n        # Prior: change due to the numbers of each operation\n        try:\n            dEP -= self.prior_par[\"Nopi_%s\" % target.value]\n        except KeyError:\n            pass\n        try:\n            dEP += self.prior_par[\"Nopi_%s\" % new]\n        except KeyError:\n            pass\n        try:\n            dEP += self.prior_par[\"Nopi2_%s\" % target.value] * (\n                (self.nops[target.value] - 1) ** 2 - (self.nops[target.value]) ** 2\n            )\n        except KeyError:\n            pass\n        try:\n            dEP += self.prior_par[\"Nopi2_%s\" % new] * (\n                (self.nops[new] + 1) ** 2 - (self.nops[new]) ** 2\n            )\n        except KeyError:\n            pass\n\n        # Data\n        if not list(self.x.values())[0].empty:\n            bicOld = self.bic\n            sseOld = deepcopy(self.sse)\n            par_valuesOld = deepcopy(self.par_values)\n            old = target.value\n            target.value = new\n            bicNew = self.get_bic(reset=True, fit=True, verbose=verbose)\n            par_valuesNew = deepcopy(self.par_values)\n            # leave the whole thing as it was before the back &amp; fore\n            target.value = old\n            self.bic = bicOld\n            self.sse = deepcopy(sseOld)\n            self.par_values = par_valuesOld\n            dEB += (bicNew - bicOld) / 2.0\n        else:\n            par_valuesNew = deepcopy(self.par_values)\n\n    # Done\n    try:\n        dEB = float(dEB)\n        dEP = float(dEP)\n        dE = dEB + dEP\n        return dE, dEB, dEP, par_valuesNew\n    except (ValueError, TypeError):\n        return np.inf, np.inf, np.inf, None\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.dE_rr","title":"<code>dE_rr(rr=None, verbose=False)</code>","text":"<p>Calculate the energy change associated to a root replacement move. If rr==None, then it returns the energy change associated to pruning the root; otherwise, it returns the energy change associated to adding the root replacement \"rr\"</p> <p>Returns: energy change associated with a root replacement move</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def dE_rr(self, rr=None, verbose=False):\n    \"\"\"\n    Calculate the energy change associated to a root replacement move.\n    If rr==None, then it returns the energy change associated to pruning the root; otherwise,\n    it returns the energy change associated to adding the root replacement \"rr\"\n\n    Returns: energy change associated with a root replacement move\n    \"\"\"\n    dEB, dEP = 0.0, 0.0\n\n    # Root pruning\n    if rr is None:\n        if not self.is_root_prunable():\n            return np.inf, np.inf, np.inf, self.par_values\n\n        # Check if the new tree is canonically acceptable.\n        # replace\n        old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n        old_par_values = deepcopy(self.par_values)\n        oldrr = [self.root.value, [o.value for o in self.root.offspring[1:]]]\n        self.prune_root(update_gof=False, verbose=verbose)\n        # check/update canonical representative\n        rep_res = self.update_representative(verbose=verbose)\n        if rep_res == -1:\n            # this formula is forbidden\n            self.replace_root(rr=oldrr, update_gof=False, verbose=verbose)\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n            return np.inf, np.inf, np.inf, deepcopy(self.par_values)\n        # leave the whole thing as it was before the back &amp; fore\n        self.replace_root(rr=oldrr, update_gof=False, verbose=verbose)\n        self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n        self.par_values = old_par_values\n\n        # Prior: change due to the numbers of each operation\n        dEP -= self.prior_par[\"Nopi_%s\" % self.root.value]\n        try:\n            dEP += self.prior_par[\"Nopi2_%s\" % self.root.value] * (\n                (self.nops[self.root.value] - 1) ** 2\n                - (self.nops[self.root.value]) ** 2\n            )\n        except KeyError:\n            pass\n\n        # Data correction\n        if not list(self.x.values())[0].empty:\n            bicOld = self.bic\n            sseOld = deepcopy(self.sse)\n            par_valuesOld = deepcopy(self.par_values)\n            oldrr = [self.root.value, [o.value for o in self.root.offspring[1:]]]\n            # replace\n            self.prune_root(update_gof=False, verbose=verbose)\n            bicNew = self.get_bic(reset=True, fit=True, verbose=verbose)\n            par_valuesNew = deepcopy(self.par_values)\n            # leave the whole thing as it was before the back &amp; fore\n            self.replace_root(rr=oldrr, update_gof=False, verbose=verbose)\n            self.bic = bicOld\n            self.sse = deepcopy(sseOld)\n            self.par_values = par_valuesOld\n            dEB += (bicNew - bicOld) / 2.0\n        else:\n            par_valuesNew = deepcopy(self.par_values)\n        # Done\n        try:\n            dEB = float(dEB)\n            dEP = float(dEP)\n            dE = dEB + dEP\n        except (ValueError, TypeError):\n            dEB, dEP, dE = np.inf, np.inf, np.inf\n        return dE, dEB, dEP, par_valuesNew\n\n    # Root replacement\n    else:\n        # Check if the new tree is canonically acceptable.\n        # replace\n        old_bic, old_sse, old_energy = self.bic, deepcopy(self.sse), self.E\n        old_par_values = deepcopy(self.par_values)\n        newroot = self.replace_root(rr=rr, update_gof=False, verbose=verbose)\n        if newroot is None:  # Root cannot be replaced (due to max_size)\n            return np.inf, np.inf, np.inf, deepcopy(self.par_values)\n        # check/update canonical representative\n        rep_res = self.update_representative(verbose=verbose)\n        if rep_res == -1:\n            # this formula is forbidden\n            self.prune_root(update_gof=False, verbose=verbose)\n            self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n            self.par_values = old_par_values\n            return np.inf, np.inf, np.inf, deepcopy(self.par_values)\n        # leave the whole thing as it was before the back &amp; fore\n        self.prune_root(update_gof=False, verbose=verbose)\n        self.bic, self.sse, self.E = old_bic, deepcopy(old_sse), old_energy\n        self.par_values = old_par_values\n\n        # Prior: change due to the numbers of each operation\n        dEP += self.prior_par[\"Nopi_%s\" % rr[0]]\n        try:\n            dEP += self.prior_par[\"Nopi2_%s\" % rr[0]] * (\n                (self.nops[rr[0]] + 1) ** 2 - (self.nops[rr[0]]) ** 2\n            )\n        except KeyError:\n            pass\n\n        # Data\n        if not list(self.x.values())[0].empty:\n            bicOld = self.bic\n            sseOld = deepcopy(self.sse)\n            par_valuesOld = deepcopy(self.par_values)\n            # replace\n            newroot = self.replace_root(rr=rr, update_gof=False, verbose=verbose)\n            if newroot is None:\n                return np.inf, np.inf, np.inf, self.par_values\n            bicNew = self.get_bic(reset=True, fit=True, verbose=verbose)\n            par_valuesNew = deepcopy(self.par_values)\n            # leave the whole thing as it was before the back &amp; fore\n            self.prune_root(update_gof=False, verbose=verbose)\n            self.bic = bicOld\n            self.sse = deepcopy(sseOld)\n            self.par_values = par_valuesOld\n            dEB += (bicNew - bicOld) / 2.0\n        else:\n            par_valuesNew = deepcopy(self.par_values)\n        # Done\n        try:\n            dEB = float(dEB)\n            dEP = float(dEP)\n            dE = dEB + dEP\n        except (ValueError, TypeError):\n            dEB, dEP, dE = np.inf, np.inf, np.inf\n        return dE, dEB, dEP, par_valuesNew\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.et_replace","title":"<code>et_replace(target, new, update_gof=True, verbose=False)</code>","text":"<p>Replace one elementary tree with another one, both of arbitrary order. target is a Node and new is a tuple [node_value, [list, of, offspring, values]]</p> <p>Returns: target</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def et_replace(self, target, new, update_gof=True, verbose=False):\n    \"\"\"\n    Replace one elementary tree with another one, both of arbitrary order. target is a\n    Node and new is a tuple [node_value, [list, of, offspring, values]]\n\n    Returns: target\n    \"\"\"\n    oini, ofin = len(target.offspring), len(new[1])\n    if oini == 0:\n        added = self._add_et(target, et=new, update_gof=False, verbose=verbose)\n    else:\n        if ofin == 0:\n            added = self._del_et(\n                target, leaf=new[0], update_gof=False, verbose=verbose\n            )\n        else:\n            self._del_et(target, update_gof=False, verbose=verbose)\n            added = self._add_et(target, et=new, update_gof=False, verbose=verbose)\n    # Update goodness of fit measures, if necessary\n    if update_gof:\n        self.sse = self.get_sse(verbose=verbose)\n        self.bic = self.get_bic(verbose=verbose)\n    # Done\n    return added\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.get_bic","title":"<code>get_bic(reset=True, fit=False, verbose=False)</code>","text":"<p>Calculate the Bayesian information criterion (BIC) of the current expression, given the data. If reset==False, the value of self.bic will not be updated (by default, it will)</p> <p>Returns: Bayesian information criterion (BIC)</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def get_bic(self, reset=True, fit=False, verbose=False):\n    \"\"\"\n    Calculate the Bayesian information criterion (BIC) of the current expression,\n    given the data. If reset==False, the value of self.bic will not be updated\n    (by default, it will)\n\n    Returns: Bayesian information criterion (BIC)\n    \"\"\"\n    if list(self.x.values())[0].empty or list(self.y.values())[0].empty:\n        if reset:\n            self.bic = 0\n        return 0\n    # Get the sum of squared errors (fitting, if required)\n    sse = self.get_sse(fit=fit, verbose=verbose)\n    # Calculate the BIC\n    parameters = set([p.value for p in self.ets[0] if p.value in self.parameters])\n    k = 1 + len(parameters)\n    BIC = 0.0\n    for ds in self.y:\n        if sse[ds] == 0.0:\n            BIC = -np.inf\n            break\n        else:\n            n = len(self.y[ds])\n            BIC += (k - n) * np.log(n) + n * (\n                np.log(2.0 * np.pi) + log(sse[ds]) + 1\n            )\n    if reset:\n        self.bic = BIC\n    return BIC\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.get_energy","title":"<code>get_energy(bic=False, reset=False, verbose=False)</code>","text":"<p>Calculate the \"energy\" of a given formula, that is, approximate minus log-posterior of the formula given the data (the approximation coming from the use of the BIC instead of the exactly integrated likelihood)</p> <p>Returns: Energy of formula (as E, EB, and EP)</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def get_energy(self, bic=False, reset=False, verbose=False):\n    \"\"\"\n    Calculate the \"energy\" of a given formula, that is, approximate minus log-posterior\n    of the formula given the data (the approximation coming from the use of the BIC\n    instead of the exactly integrated likelihood)\n\n    Returns: Energy of formula (as E, EB, and EP)\n    \"\"\"\n    # Contribution of the data (recalculating BIC if necessary)\n    if bic:\n        EB = self.get_bic(reset=reset, verbose=verbose) / 2.0\n    else:\n        EB = self.bic / 2.0\n    # Contribution from the prior\n    EP = 0.0\n    for op, nop in list(self.nops.items()):\n        try:\n            EP += self.prior_par[\"Nopi_%s\" % op] * nop\n        except KeyError:\n            pass\n        try:\n            EP += self.prior_par[\"Nopi2_%s\" % op] * nop**2\n        except KeyError:\n            pass\n    # Reset the value, if necessary\n    if reset:\n        self.EB = EB\n        self.EP = EP\n        self.E = EB + EP\n    # Done\n    return EB + EP, EB, EP\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.get_sse","title":"<code>get_sse(fit=True, verbose=False)</code>","text":"<p>Get the sum of squared errors, fitting the expression represented by the Tree to the existing data, if specified (by default, yes)</p> <p>Returns: sum of square errors (sse)</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def get_sse(self, fit=True, verbose=False):\n    \"\"\"\n    Get the sum of squared errors, fitting the expression represented by the Tree\n    to the existing data, if specified (by default, yes)\n\n    Returns: sum of square errors (sse)\n    \"\"\"\n    # Return 0 if there is no data\n    if list(self.x.values())[0].empty or list(self.y.values())[0].empty:\n        self.sse = 0\n        return 0\n    # Convert the Tree into a SymPy expression\n    ex = sympify(str(self))\n    # Convert the expression to a function that can be used by\n    # curve_fit, i.e. that takes as arguments (x, a0, a1, ..., an)\n    atomd = dict([(a.name, a) for a in ex.atoms() if a.is_Symbol])\n    variables = [atomd[v] for v in self.variables if v in list(atomd.keys())]\n    parameters = [atomd[p] for p in self.parameters if p in list(atomd.keys())]\n    dic: dict = dict(\n        {\n            \"fac\": scipy.special.factorial,\n            \"sig\": scipy.special.expit,\n            \"relu\": relu,\n        },\n        **self.custom_ops,\n    )\n    try:\n        flam = lambdify(\n            variables + parameters,\n            ex,\n            [\n                \"numpy\",\n                dic,\n            ],\n        )\n    except (SyntaxError, KeyError):\n        self.sse = dict([(ds, np.inf) for ds in self.x])\n        return self.sse\n    if fit:\n        if len(parameters) == 0:  # Nothing to fit\n            for ds in self.x:\n                for p in self.parameters:\n                    self.par_values[ds][p] = 1.0\n        elif str(self) in self.fit_par:  # Recover previously fit parameters\n            self.par_values = self.fit_par[str(self)]\n        else:  # Do the fit for all datasets\n            self.fit_par[str(self)] = {}\n            for ds in self.x:\n                this_x, this_y = self.x[ds], self.y[ds]\n                xmat = [this_x[v.name] for v in variables]\n\n                def feval(x, *params):\n                    args = [xi for xi in x] + [p for p in params]\n                    return flam(*args)\n\n                try:\n                    # Fit the parameters\n                    res = curve_fit(\n                        feval,\n                        xmat,\n                        this_y,\n                        p0=[self.par_values[ds][p.name] for p in parameters],\n                        maxfev=10000,\n                    )\n                    # Reassign the values of the parameters\n                    self.par_values[ds] = dict(\n                        [\n                            (parameters[i].name, res[0][i])\n                            for i in range(len(res[0]))\n                        ]\n                    )\n                    for p in self.parameters:\n                        if p not in self.par_values[ds]:\n                            self.par_values[ds][p] = 1.0\n                    # Save this fit\n                    self.fit_par[str(self)][ds] = deepcopy(self.par_values[ds])\n                except RuntimeError:\n                    # Save this (unsuccessful) fit and print warning\n                    self.fit_par[str(self)][ds] = deepcopy(self.par_values[ds])\n                    if verbose:\n                        print(\n                            \"#Cannot_fit:%s # # # # #\" % str(self).replace(\" \", \"\"),\n                            file=sys.stderr,\n                        )\n\n    # Sum of squared errors\n    self.sse = {}\n    for ds in self.x:\n        this_x, this_y = self.x[ds], self.y[ds]\n        xmat = [this_x[v.name] for v in variables]\n        ar = [np.array(xi) for xi in xmat] + [\n            self.par_values[ds][p.name] for p in parameters\n        ]\n        try:\n            se = np.square(this_y - flam(*ar))\n            if sum(np.isnan(se)) &gt; 0:\n                raise ValueError\n            else:\n                self.sse[ds] = np.sum(se)\n        except ValueError:\n            if verbose:\n                print(\"&gt; Cannot calculate SSE for %s: inf\" % self, file=sys.stderr)\n            self.sse[ds] = np.inf\n        except TypeError:\n            if verbose:\n                print(\"Complex-valued parameters are invalid\")\n            self.sse[ds] = np.inf\n\n    # Done\n    return self.sse\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.is_root_prunable","title":"<code>is_root_prunable()</code>","text":"<p>Check if the root is \"prunable\"</p> <p>Returns: boolean of root \"prunability\"</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def is_root_prunable(self):\n    \"\"\"\n    Check if the root is \"prunable\"\n\n    Returns: boolean of root \"prunability\"\n    \"\"\"\n    if self.size == 1:\n        isPrunable = False\n    elif self.size == 2:\n        isPrunable = True\n    else:\n        isPrunable = True\n        for o in self.root.offspring[1:]:\n            if o.offspring != []:\n                isPrunable = False\n                break\n    return isPrunable\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.latex","title":"<code>latex()</code>","text":"<p>translate equation into latex</p> <p>Returns: canonical latex form of equation</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def latex(self):\n    \"\"\"\n    translate equation into latex\n\n    Returns: canonical latex form of equation\n    \"\"\"\n    return latex(sympify(self.canonical()))\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.mcmc","title":"<code>mcmc(tracefn='trace.dat', progressfn='progress.dat', write_files=True, reset_files=True, burnin=2000, thin=10, samples=10000, verbose=False, progress=True)</code>","text":"<p>Sample the space of formula trees using MCMC, and write the trace and some progress information to files (unless write_files is False)</p> <p>Returns: None or expression list</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def mcmc(\n    self,\n    tracefn=\"trace.dat\",\n    progressfn=\"progress.dat\",\n    write_files=True,\n    reset_files=True,\n    burnin=2000,\n    thin=10,\n    samples=10000,\n    verbose=False,\n    progress=True,\n):\n    \"\"\"\n    Sample the space of formula trees using MCMC, and write the trace and some progress\n    information to files (unless write_files is False)\n\n    Returns: None or expression list\n    \"\"\"\n    self.get_energy(reset=True, verbose=verbose)\n\n    # Burning\n    if progress:\n        sys.stdout.write(\"# Burning in\\t\")\n        sys.stdout.write(\"[%s]\" % (\" \" * 50))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\b\" * (50 + 1))\n    for i in range(burnin):\n        self.mcmc_step(verbose=verbose)\n        if progress and (i % (burnin / 50) == 0):\n            sys.stdout.write(\"=\")\n            sys.stdout.flush()\n    # Sample\n    if write_files:\n        if reset_files:\n            tracef = open(tracefn, \"w\")\n            progressf = open(progressfn, \"w\")\n        else:\n            tracef = open(tracefn, \"a\")\n            progressf = open(progressfn, \"a\")\n    if progress:\n        sys.stdout.write(\"\\n# Sampling\\t\")\n        sys.stdout.write(\"[%s]\" % (\" \" * 50))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\b\" * (50 + 1))\n    for s in range(samples):\n        for i in range(thin):\n            self.mcmc_step(verbose=verbose)\n        if progress and (s % (samples / 50) == 0):\n            sys.stdout.write(\"=\")\n            sys.stdout.flush()\n        if write_files:\n            json.dump(\n                [\n                    s,\n                    float(self.bic),\n                    float(self.E),\n                    str(self.get_energy(verbose=verbose)),\n                    str(self),\n                    self.par_values,\n                ],\n                tracef,\n            )\n            tracef.write(\"\\n\")\n            tracef.flush()\n            progressf.write(\"%d %lf %lf\\n\" % (s, self.E, self.bic))\n            progressf.flush()\n    # Done\n    if progress:\n        sys.stdout.write(\"\\n\")\n    return\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.mcmc_step","title":"<code>mcmc_step(verbose=False, p_rr=0.05, p_long=0.45)</code>","text":"<p>Make a single MCMC step</p> <p>Returns: None or expression list</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def mcmc_step(self, verbose=False, p_rr=0.05, p_long=0.45):\n    \"\"\"\n    Make a single MCMC step\n\n    Returns: None or expression list\n    \"\"\"\n    topDice = random()\n    # Root replacement move\n    if topDice &lt; p_rr:\n        if random() &lt; 0.5:\n            # Try to prune the root\n            dE, dEB, dEP, par_valuesNew = self.dE_rr(rr=None, verbose=verbose)\n            if -dEB / self.BT - dEP / self.PT &gt; 300:\n                paccept = 1\n            else:\n                paccept = np.exp(-dEB / self.BT - dEP / self.PT) / float(\n                    self.num_rr\n                )\n            dice = random()\n            if dice &lt; paccept:\n                # Accept move\n                self.prune_root(update_gof=False, verbose=verbose)\n                self.par_values = par_valuesNew\n                self.get_bic(reset=True, fit=False, verbose=verbose)\n                self.E += dE\n                self.EB += dEB\n                self.EP += dEP\n        else:\n            # Try to replace the root\n            newrr = choice(self.rr_space)\n            dE, dEB, dEP, par_valuesNew = self.dE_rr(rr=newrr, verbose=verbose)\n            if self.num_rr &gt; 0 and -dEB / self.BT - dEP / self.PT &gt; 0:\n                paccept = 1.0\n            elif self.num_rr == 0:\n                paccept = 0.0\n            else:\n                paccept = self.num_rr * np.exp(-dEB / self.BT - dEP / self.PT)\n            dice = random()\n            if dice &lt; paccept:\n                # Accept move\n                self.replace_root(rr=newrr, update_gof=False, verbose=verbose)\n                self.par_values = par_valuesNew\n                self.get_bic(reset=True, fit=False, verbose=verbose)\n                self.E += dE\n                self.EB += dEB\n                self.EP += dEP\n\n    # Long-range move\n    elif topDice &lt; (p_rr + p_long) and not (\n        self.fixed_root and len(self.nodes) == 1\n    ):\n        # Choose a random node in the tree, and a random new operation\n        target = choice(self.nodes)\n        if self.fixed_root:\n            while target is self.root:\n                target = choice(self.nodes)\n        nready = False\n        while not nready:\n            if len(target.offspring) == 0:\n                new = choice(self.variables + self.parameters)\n                nready = True\n            else:\n                new = choice(list(self.ops.keys()))\n                if self.ops[new] == self.ops[target.value]:\n                    nready = True\n        dE, dEB, dEP, par_valuesNew = self.dE_lr(target, new, verbose=verbose)\n        try:\n            paccept = np.exp(-dEB / self.BT - dEP / self.PT)\n        except ValueError:\n            _logger.warning(\"Potentially failing to set paccept properly\")\n            if (dEB / self.BT + dEP / self.PT) &lt; 0:\n                paccept = 1.0\n        # Accept move, if necessary\n        dice = random()\n        if dice &lt; paccept:\n            # update number of operations\n            if target.offspring != []:\n                self.nops[target.value] -= 1\n                self.nops[new] += 1\n            # move\n            target.value = new\n            # recalculate distinct parameters\n            self.dist_par = list(\n                set([n.value for n in self.ets[0] if n.value in self.parameters])\n            )\n            self.n_dist_par = len(self.dist_par)\n            # update others\n            self.par_values = deepcopy(par_valuesNew)\n            self.get_bic(reset=True, fit=False, verbose=verbose)\n            self.E += dE\n            self.EB += dEB\n            self.EP += dEP\n\n    # Elementary tree (short-range) move\n    else:\n        target = None\n        while target is None or self.fixed_root and target is self.root:\n            # Choose a feasible move (doable and keeping size&lt;=max_size)\n            while True:\n                oini, ofin = choice(self.move_types)\n                if len(self.ets[oini]) &gt; 0 and (\n                    self.size - oini + ofin &lt;= self.max_size\n                ):\n                    break\n            # target and new ETs\n            target = choice(self.ets[oini])\n            new = choice(self.et_space[ofin])\n        # omegai and omegaf\n        omegai = len(self.ets[oini])\n        omegaf = len(self.ets[ofin]) + 1\n        if ofin == 0:\n            omegaf -= oini\n        if oini == 0 and target.parent in self.ets[ofin]:\n            omegaf -= 1\n        # size of et_space of each type\n        si = len(self.et_space[oini])\n        sf = len(self.et_space[ofin])\n        # Probability of acceptance\n        dE, dEB, dEP, par_valuesNew, nif, nfi = self.dE_et(\n            target, new, verbose=verbose\n        )\n        try:\n            paccept = (\n                float(nif) * omegai * sf * np.exp(-dEB / self.BT - dEP / self.PT)\n            ) / (float(nfi) * omegaf * si)\n        except ValueError:\n            if (dEB / self.BT + dEP / self.PT) &lt; -200:\n                paccept = 1.0\n        # Accept / reject\n        dice = random()\n        if dice &lt; paccept:\n            # Accept move\n            self.et_replace(target, new, verbose=verbose)\n            self.par_values = par_valuesNew\n            self.get_bic(verbose=verbose)\n            self.E += dE\n            self.EB += dEB\n            self.EP += dEP\n\n    # Done\n    return\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.pr","title":"<code>pr(show_pow=True)</code>","text":"<p>Returns readable representation of tree's root node</p> <p>Returns: root node representation</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def pr(self, show_pow=True):\n    \"\"\"\n    Returns readable representation of tree's root node\n\n    Returns: root node representation\n\n    \"\"\"\n    return self.root.pr(custom_ops=self.custom_ops, show_pow=show_pow)\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.predict","title":"<code>predict(x)</code>","text":"<p>Calculate the value of the formula at the given data x. The data x must have the same format as the training data and, in particular, it it must specify to which dataset the example data belongs, if multiple datasets where used for training.</p> <p>Returns: predicted y values</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def predict(self, x):\n    \"\"\"\n    Calculate the value of the formula at the given data x. The data x\n    must have the same format as the training data and, in particular, it\n    it must specify to which dataset the example data belongs, if multiple\n    datasets where used for training.\n\n    Returns: predicted y values\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        columns = list()\n        for col in range(x.shape[1]):\n            columns.append(\"X\" + str(col))\n        x = pd.DataFrame(x, columns=columns)\n\n    if isinstance(x, pd.DataFrame):\n        this_x = {\"d0\": x}\n        input_type = \"df\"\n    elif isinstance(x, dict):\n        this_x = x\n        input_type = \"dict\"\n    else:\n        raise TypeError(\"x must be either a dict or a pandas.DataFrame\")\n\n    # Convert the Tree into a SymPy expression\n    ex = sympify(str(self))\n    # Convert the expression to a function\n    atomd = dict([(a.name, a) for a in ex.atoms() if a.is_Symbol])\n    variables = [atomd[v] for v in self.variables if v in list(atomd.keys())]\n    parameters = [atomd[p] for p in self.parameters if p in list(atomd.keys())]\n    flam = lambdify(\n        variables + parameters,\n        ex,\n        [\n            \"numpy\",\n            dict(\n                {\n                    \"fac\": scipy.special.factorial,\n                    \"sig\": scipy.special.expit,\n                    \"relu\": relu,\n                },\n                **self.custom_ops,\n            ),\n        ],\n    )\n    # Loop over datasets\n    predictions = {}\n    for ds in this_x:\n        # Prepare variables and parameters\n        xmat = [this_x[ds][v.name] for v in variables]\n        params = [self.par_values[ds][p.name] for p in parameters]\n        args = [xi for xi in xmat] + [p for p in params]\n        # Predict\n        try:\n            prediction = flam(*args)\n        except SyntaxError:\n            # Do it point by point\n            prediction = [np.nan for i in range(len(this_x[ds]))]\n        predictions[ds] = pd.Series(prediction, index=list(this_x[ds].index))\n\n    if input_type == \"df\":\n        return predictions[\"d0\"]\n    else:\n        return predictions\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.prune_root","title":"<code>prune_root(update_gof=True, verbose=False)</code>","text":"<p>Cut the root and its rightmost leaves (provided they are, indeed, leaves), leaving the leftmost branch as the new tree. Returns the pruned root with the same format as the replacement roots in self.rr_space (or None if pruning was impossible)</p> <p>Returns: the replacement root</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def prune_root(self, update_gof=True, verbose=False):\n    \"\"\"\n    Cut the root and its rightmost leaves (provided they are, indeed, leaves),\n    leaving the leftmost branch as the new tree. Returns the pruned root with the same format\n    as the replacement roots in self.rr_space (or None if pruning was impossible)\n\n    Returns: the replacement root\n    \"\"\"\n    # Check if the root is \"prunable\" (and return None if not)\n    if not self.is_root_prunable():\n        return None\n    # Let's do it!\n    rr = [self.root.value, []]\n    self.nodes.remove(self.root)\n    try:\n        self.ets[len(self.root.offspring)].remove(self.root)\n    except ValueError:\n        pass\n    self.nops[self.root.value] -= 1\n    self.size -= 1\n    for o in self.root.offspring[1:]:\n        rr[1].append(o.value)\n        self.nodes.remove(o)\n        self.size -= 1\n        self.ets[0].remove(o)\n    self.root = self.root.offspring[0]\n    self.root.parent = None\n    # Update list of distinct parameters\n    self.dist_par = list(\n        set([n.value for n in self.ets[0] if n.value in self.parameters])\n    )\n    self.n_dist_par = len(self.dist_par)\n    # Update goodness of fit measures, if necessary\n    if update_gof:\n        self.sse = self.get_sse(verbose=verbose)\n        self.bic = self.get_bic(verbose=verbose)\n        self.E = self.get_energy(verbose=verbose)\n    # Done\n    return rr\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.replace_root","title":"<code>replace_root(rr=None, update_gof=True, verbose=False)</code>","text":"<p>Replace the root with a \"root replacement\" rr (if provided; otherwise choose one at random from self.rr_space)</p> <p>Returns: new root (if move was possible) or None (otherwise)</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def replace_root(self, rr=None, update_gof=True, verbose=False):\n    \"\"\"\n    Replace the root with a \"root replacement\" rr (if provided;\n    otherwise choose one at random from self.rr_space)\n\n    Returns: new root (if move was possible) or None (otherwise)\n    \"\"\"\n    # If no RR is provided, randomly choose one\n    if rr is None:\n        rr = choice(self.rr_space)\n    # Return None if the replacement is too big\n    if (self.size + self.ops[rr[0]]) &gt; self.max_size:\n        return None\n    # Create the new root and replace existing root\n    newRoot = Node(rr[0], offspring=[], parent=None)\n    newRoot.order = 1 + len(rr[1])\n    if newRoot.order != self.ops[rr[0]]:\n        raise\n    newRoot.offspring.append(self.root)\n    self.root.parent = newRoot\n    self.root = newRoot\n    self.nops[self.root.value] += 1\n    self.nodes.append(self.root)\n    self.size += 1\n    oldRoot = self.root.offspring[0]\n    for leaf in rr[1]:\n        self.root.offspring.append(Node(leaf, offspring=[], parent=self.root))\n        self.nodes.append(self.root.offspring[-1])\n        self.ets[0].append(self.root.offspring[-1])\n        self.size += 1\n    # Add new root to elementary trees if necessary (that is, iff\n    # the old root was a leaf)\n    if oldRoot.offspring is []:\n        self.ets[self.root.order].append(self.root)\n    # Update list of distinct parameters\n    self.dist_par = list(\n        set([n.value for n in self.ets[0] if n.value in self.parameters])\n    )\n    self.n_dist_par = len(self.dist_par)\n    # Update goodness of fit measures, if necessary\n    if update_gof:\n        self.sse = self.get_sse(verbose=verbose)\n        self.bic = self.get_bic(verbose=verbose)\n        self.E = self.get_energy(verbose=verbose)\n    return self.root\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.trace_predict","title":"<code>trace_predict(x, burnin=1000, thin=2000, samples=1000, tracefn='trace.dat', progressfn='progress.dat', write_files=False, reset_files=True, verbose=False, progress=True)</code>","text":"<p>Sample the space of formula trees using MCMC, and predict y(x) for each of the sampled formula trees</p> <p>Returns: predicted y values for each of the sampled formula trees</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def trace_predict(\n    self,\n    x,\n    burnin=1000,\n    thin=2000,\n    samples=1000,\n    tracefn=\"trace.dat\",\n    progressfn=\"progress.dat\",\n    write_files=False,\n    reset_files=True,\n    verbose=False,\n    progress=True,\n):\n    \"\"\"\n    Sample the space of formula trees using MCMC,\n    and predict y(x) for each of the sampled formula trees\n\n    Returns: predicted y values for each of the sampled formula trees\n    \"\"\"\n    ypred = {}\n    # Burning\n    if progress:\n        sys.stdout.write(\"# Burning in\\t\")\n        sys.stdout.write(\"[%s]\" % (\" \" * 50))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\b\" * (50 + 1))\n    for i in range(burnin):\n        self.mcmc_step(verbose=verbose)\n        if progress and (i % (burnin / 50) == 0):\n            sys.stdout.write(\"=\")\n            sys.stdout.flush()\n    # Sample\n    if write_files:\n        if reset_files:\n            tracef = open(tracefn, \"w\")\n            progressf = open(progressfn, \"w\")\n        else:\n            tracef = open(tracefn, \"a\")\n            progressf = open(progressfn, \"a\")\n    if progress:\n        sys.stdout.write(\"\\n# Sampling\\t\")\n        sys.stdout.write(\"[%s]\" % (\" \" * 50))\n        sys.stdout.flush()\n        sys.stdout.write(\"\\b\" * (50 + 1))\n\n    for s in range(samples):\n        for kk in range(thin):\n            self.mcmc_step(verbose=verbose)\n        # Make prediction\n        ypred[s] = self.predict(x)\n        # Output\n        if progress and (s % (samples / 50) == 0):\n            sys.stdout.write(\"=\")\n            sys.stdout.flush()\n        if write_files:\n            json.dump(\n                [\n                    s,\n                    float(self.bic),\n                    float(self.E),\n                    float(self.get_energy(verbose=verbose)),\n                    str(self),\n                    self.par_values,\n                ],\n                tracef,\n            )\n            tracef.write(\"\\n\")\n            tracef.flush()\n            progressf.write(\"%d %lf %lf\\n\" % (s, self.E, self.bic))\n            progressf.flush()\n    # Done\n    if progress:\n        sys.stdout.write(\"\\n\")\n    return pd.DataFrame.from_dict(ypred)\n</code></pre>"},{"location":"reference/autora/theorist/bms/mcmc/#autora.theorist.bms.mcmc.Tree.update_representative","title":"<code>update_representative(verbose=False)</code>","text":"<p>Check if we've seen this formula before, either in its current form or in another form.</p> <p>*If we haven't seen it, save it and return 1.</p> <p>*If we have seen it and this IS the representative, just return 0.</p> <p>*If we have seen it and the representative has smaller energy, just return -1.</p> <p>*If we have seen it and the representative has higher energy, update the representatitve and return -2.</p> <p>Integer value (0, 1, or -1) corresponding to:</p> Name Type Description <code>0</code> <p>we have seen this canonical form before</p> <code>1</code> <p>we haven't seen this canonical form before</p> <p>-1: we have seen this equation's canonical form before but it isn't in that form yet</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/mcmc.py</code> <pre><code>def update_representative(self, verbose=False):\n    \"\"\"Check if we've seen this formula before, either in its current form\n    or in another form.\n\n    *If we haven't seen it, save it and return 1.\n\n    *If we have seen it and this IS the representative, just return 0.\n\n    *If we have seen it and the representative has smaller energy, just return -1.\n\n    *If we have seen it and the representative has higher energy, update\n    the representatitve and return -2.\n\n    Returns: Integer value (0, 1, or -1) corresponding to:\n        0: we have seen this canonical form before\n        1: we haven't seen this canonical form before\n        -1: we have seen this equation's canonical form before but it isn't in that form yet\n    \"\"\"\n    # Check for canonical representative\n    canonical = self.canonical(verbose=verbose)\n    try:  # We've seen this canonical before!\n        rep, rep_energy, rep_par_values = self.representative[canonical]\n    except TypeError:\n        return -1  # Complex-valued parameters are invalid\n    except KeyError:  # Never seen this canonical formula before:\n        # save it and return 1\n        self.get_bic(reset=True, fit=True, verbose=verbose)\n        new_energy = self.get_energy(bic=False, verbose=verbose)\n        self.representative[canonical] = (\n            str(self),\n            new_energy,\n            deepcopy(self.par_values),\n        )\n        return 1\n\n    # If we've seen this canonical before, check if the\n    # representative needs to be updated\n    if rep == str(self):  # This IS the representative: return 0\n        return 0\n    else:\n        return -1\n</code></pre>"},{"location":"reference/autora/theorist/bms/parallel/","title":"autora.theorist.bms.parallel","text":""},{"location":"reference/autora/theorist/bms/parallel/#autora.theorist.bms.parallel.Parallel","title":"<code>Parallel</code>","text":"<p>The Parallel Machine Scientist Object, equipped with parallel tempering</p> <p>Attributes:</p> Name Type Description <code>Ts</code> <p>list of parallel temperatures</p> <code>trees</code> <p>list of parallel trees, corresponding to each parallel temperature</p> <code>t1</code> <p>equation tree which best describes the data</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/parallel.py</code> <pre><code>class Parallel:\n    \"\"\"\n    The Parallel Machine Scientist Object, equipped with parallel tempering\n\n    Attributes:\n        Ts: list of parallel temperatures\n        trees: list of parallel trees, corresponding to each parallel temperature\n        t1: equation tree which best describes the data\n    \"\"\"\n\n    # -------------------------------------------------------------------------\n    def __init__(\n        self,\n        Ts: list,\n        ops=get_priors()[1],\n        custom_ops={},\n        variables=[\"x\"],\n        parameters=[\"a\"],\n        max_size=50,\n        prior_par=get_priors()[0],\n        x=None,\n        y=None,\n        root=None,\n        random_state=None,\n    ) -&gt; None:\n        \"\"\"\n        Initialises Parallel Machine Scientist\n\n        Args:\n            Ts: list of temperature values\n            ops: allowed operations for the search task\n            variables: independent variables from data\n            parameters: settable values to improve model fit\n            max_size: maximum size (number of nodes) in a tree\n            prior_par: prior values over ops\n            x: independent variables of dataset\n            y: dependent variable of dataset\n            root: fixed root of the tree\n        \"\"\"\n        if random_state is not None:\n            seed(random_state)\n        self.root = root\n        # All trees are initialized to the same tree but with different BT\n        Ts.sort()\n        self.Ts = [str(T) for T in Ts]\n        self.trees = {\n            \"1.0\": Tree(\n                ops=ops,\n                variables=deepcopy(variables),\n                parameters=deepcopy(parameters),\n                prior_par=deepcopy(prior_par),\n                x=x,\n                y=y,\n                max_size=max_size,\n                BT=1,\n                root_value=root.__name__ if root is not None else None,\n                fixed_root=True if root is not None else False,\n                custom_ops=custom_ops,\n                random_state=random_state,\n            )\n        }\n        self.t1 = self.trees[\"1.0\"]\n        for BT in [T for T in self.Ts if T != 1]:\n            treetmp = Tree(\n                ops=ops,\n                variables=deepcopy(variables),\n                parameters=deepcopy(parameters),\n                prior_par=deepcopy(prior_par),\n                x=x,\n                y=y,\n                root_value=root.__name__ if root is not None else None,\n                fixed_root=self.t1.fixed_root,\n                custom_ops=custom_ops,\n                max_size=max_size,\n                BT=float(BT),\n                random_state=random_state,\n            )\n            self.trees[BT] = treetmp\n            # Share fitted parameters and representative with other trees\n            self.trees[BT].fit_par = self.t1.fit_par\n            self.trees[BT].representative = self.t1.representative\n\n    # -------------------------------------------------------------------------\n    def mcmc_step(self, verbose=False, p_rr=0.05, p_long=0.45) -&gt; None:\n        \"\"\"\n        Perform a MCMC step in each of the trees\n        \"\"\"\n        # Loop over all trees\n        if self.root is not None:\n            p_rr = 0.0\n        for T, tree in list(self.trees.items()):\n            # MCMC step\n            tree.mcmc_step(verbose=verbose, p_rr=p_rr, p_long=p_long)\n        self.t1 = self.trees[\"1.0\"]\n\n    # -------------------------------------------------------------------------\n    def tree_swap(self) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"\n        Choose a pair of trees of adjacent temperatures and attempt to swap their temperatures\n        based on the resultant energy change\n\n        Returns: new temperature values for the pair of trees\n        \"\"\"\n        # Choose Ts to swap\n        nT1 = randint(0, len(self.Ts) - 2)\n        nT2 = nT1 + 1\n        t1 = self.trees[self.Ts[nT1]]\n        t2 = self.trees[self.Ts[nT2]]\n        # The temperatures and energies\n        BT1, BT2 = t1.BT, t2.BT\n        EB1, EB2 = t1.EB, t2.EB\n        # The energy change\n        DeltaE = float(EB1) * (1.0 / BT2 - 1.0 / BT1) + float(EB2) * (\n            1.0 / BT1 - 1.0 / BT2\n        )\n        if DeltaE &gt; 0:\n            paccept = exp(-DeltaE)\n        else:\n            paccept = 1.0\n        # Accept/reject change\n        if random() &lt; paccept:\n            self.trees[self.Ts[nT1]] = t2\n            self.trees[self.Ts[nT2]] = t1\n            t1.BT = BT2\n            t2.BT = BT1\n            self.t1 = self.trees[\"1.0\"]\n            return self.Ts[nT1], self.Ts[nT2]\n        else:\n            return None, None\n\n    # -------------------------------------------------------------------------\n    def anneal(self, n=1000, factor=5) -&gt; None:\n        \"\"\"\n        Annealing function for the Machine Scientist\n\n        Args:\n            n: number of mcmc step &amp; tree swap iterations\n            factor: degree of annealing - how much the temperatures are raised\n\n        Returns: Nothing\n\n        \"\"\"\n        for t in list(self.trees.values()):\n            t.BT *= factor\n        for kk in range(n):\n            print(\n                \"# Annealing heating at %g: %d / %d\" % (self.trees[\"1.0\"].BT, kk, n),\n                file=sys.stderr,\n            )\n            self.mcmc_step()\n            self.tree_swap()\n        # Cool down (return to original temperatures)\n        for BT, t in list(self.trees.items()):\n            t.BT = float(BT)\n        for kk in range(2 * n):\n            print(\n                \"# Annealing cooling at %g: %d / %d\"\n                % (self.trees[\"1.0\"].BT, kk, 2 * n),\n                file=sys.stderr,\n            )\n            self.mcmc_step()\n            self.tree_swap()\n</code></pre>"},{"location":"reference/autora/theorist/bms/parallel/#autora.theorist.bms.parallel.Parallel.__init__","title":"<code>__init__(Ts, ops=get_priors()[1], custom_ops={}, variables=['x'], parameters=['a'], max_size=50, prior_par=get_priors()[0], x=None, y=None, root=None, random_state=None)</code>","text":"<p>Initialises Parallel Machine Scientist</p> <p>Parameters:</p> Name Type Description Default <code>Ts</code> <code>list</code> <p>list of temperature values</p> required <code>ops</code> <p>allowed operations for the search task</p> <code>get_priors()[1]</code> <code>variables</code> <p>independent variables from data</p> <code>['x']</code> <code>parameters</code> <p>settable values to improve model fit</p> <code>['a']</code> <code>max_size</code> <p>maximum size (number of nodes) in a tree</p> <code>50</code> <code>prior_par</code> <p>prior values over ops</p> <code>get_priors()[0]</code> <code>x</code> <p>independent variables of dataset</p> <code>None</code> <code>y</code> <p>dependent variable of dataset</p> <code>None</code> <code>root</code> <p>fixed root of the tree</p> <code>None</code> Source code in <code>temp_dir/bms/src/autora/theorist/bms/parallel.py</code> <pre><code>def __init__(\n    self,\n    Ts: list,\n    ops=get_priors()[1],\n    custom_ops={},\n    variables=[\"x\"],\n    parameters=[\"a\"],\n    max_size=50,\n    prior_par=get_priors()[0],\n    x=None,\n    y=None,\n    root=None,\n    random_state=None,\n) -&gt; None:\n    \"\"\"\n    Initialises Parallel Machine Scientist\n\n    Args:\n        Ts: list of temperature values\n        ops: allowed operations for the search task\n        variables: independent variables from data\n        parameters: settable values to improve model fit\n        max_size: maximum size (number of nodes) in a tree\n        prior_par: prior values over ops\n        x: independent variables of dataset\n        y: dependent variable of dataset\n        root: fixed root of the tree\n    \"\"\"\n    if random_state is not None:\n        seed(random_state)\n    self.root = root\n    # All trees are initialized to the same tree but with different BT\n    Ts.sort()\n    self.Ts = [str(T) for T in Ts]\n    self.trees = {\n        \"1.0\": Tree(\n            ops=ops,\n            variables=deepcopy(variables),\n            parameters=deepcopy(parameters),\n            prior_par=deepcopy(prior_par),\n            x=x,\n            y=y,\n            max_size=max_size,\n            BT=1,\n            root_value=root.__name__ if root is not None else None,\n            fixed_root=True if root is not None else False,\n            custom_ops=custom_ops,\n            random_state=random_state,\n        )\n    }\n    self.t1 = self.trees[\"1.0\"]\n    for BT in [T for T in self.Ts if T != 1]:\n        treetmp = Tree(\n            ops=ops,\n            variables=deepcopy(variables),\n            parameters=deepcopy(parameters),\n            prior_par=deepcopy(prior_par),\n            x=x,\n            y=y,\n            root_value=root.__name__ if root is not None else None,\n            fixed_root=self.t1.fixed_root,\n            custom_ops=custom_ops,\n            max_size=max_size,\n            BT=float(BT),\n            random_state=random_state,\n        )\n        self.trees[BT] = treetmp\n        # Share fitted parameters and representative with other trees\n        self.trees[BT].fit_par = self.t1.fit_par\n        self.trees[BT].representative = self.t1.representative\n</code></pre>"},{"location":"reference/autora/theorist/bms/parallel/#autora.theorist.bms.parallel.Parallel.anneal","title":"<code>anneal(n=1000, factor=5)</code>","text":"<p>Annealing function for the Machine Scientist</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>number of mcmc step &amp; tree swap iterations</p> <code>1000</code> <code>factor</code> <p>degree of annealing - how much the temperatures are raised</p> <code>5</code> <p>Returns: Nothing</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/parallel.py</code> <pre><code>def anneal(self, n=1000, factor=5) -&gt; None:\n    \"\"\"\n    Annealing function for the Machine Scientist\n\n    Args:\n        n: number of mcmc step &amp; tree swap iterations\n        factor: degree of annealing - how much the temperatures are raised\n\n    Returns: Nothing\n\n    \"\"\"\n    for t in list(self.trees.values()):\n        t.BT *= factor\n    for kk in range(n):\n        print(\n            \"# Annealing heating at %g: %d / %d\" % (self.trees[\"1.0\"].BT, kk, n),\n            file=sys.stderr,\n        )\n        self.mcmc_step()\n        self.tree_swap()\n    # Cool down (return to original temperatures)\n    for BT, t in list(self.trees.items()):\n        t.BT = float(BT)\n    for kk in range(2 * n):\n        print(\n            \"# Annealing cooling at %g: %d / %d\"\n            % (self.trees[\"1.0\"].BT, kk, 2 * n),\n            file=sys.stderr,\n        )\n        self.mcmc_step()\n        self.tree_swap()\n</code></pre>"},{"location":"reference/autora/theorist/bms/parallel/#autora.theorist.bms.parallel.Parallel.mcmc_step","title":"<code>mcmc_step(verbose=False, p_rr=0.05, p_long=0.45)</code>","text":"<p>Perform a MCMC step in each of the trees</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/parallel.py</code> <pre><code>def mcmc_step(self, verbose=False, p_rr=0.05, p_long=0.45) -&gt; None:\n    \"\"\"\n    Perform a MCMC step in each of the trees\n    \"\"\"\n    # Loop over all trees\n    if self.root is not None:\n        p_rr = 0.0\n    for T, tree in list(self.trees.items()):\n        # MCMC step\n        tree.mcmc_step(verbose=verbose, p_rr=p_rr, p_long=p_long)\n    self.t1 = self.trees[\"1.0\"]\n</code></pre>"},{"location":"reference/autora/theorist/bms/parallel/#autora.theorist.bms.parallel.Parallel.tree_swap","title":"<code>tree_swap()</code>","text":"<p>Choose a pair of trees of adjacent temperatures and attempt to swap their temperatures based on the resultant energy change</p> <p>Returns: new temperature values for the pair of trees</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/parallel.py</code> <pre><code>def tree_swap(self) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Choose a pair of trees of adjacent temperatures and attempt to swap their temperatures\n    based on the resultant energy change\n\n    Returns: new temperature values for the pair of trees\n    \"\"\"\n    # Choose Ts to swap\n    nT1 = randint(0, len(self.Ts) - 2)\n    nT2 = nT1 + 1\n    t1 = self.trees[self.Ts[nT1]]\n    t2 = self.trees[self.Ts[nT2]]\n    # The temperatures and energies\n    BT1, BT2 = t1.BT, t2.BT\n    EB1, EB2 = t1.EB, t2.EB\n    # The energy change\n    DeltaE = float(EB1) * (1.0 / BT2 - 1.0 / BT1) + float(EB2) * (\n        1.0 / BT1 - 1.0 / BT2\n    )\n    if DeltaE &gt; 0:\n        paccept = exp(-DeltaE)\n    else:\n        paccept = 1.0\n    # Accept/reject change\n    if random() &lt; paccept:\n        self.trees[self.Ts[nT1]] = t2\n        self.trees[self.Ts[nT2]] = t1\n        t1.BT = BT2\n        t2.BT = BT1\n        self.t1 = self.trees[\"1.0\"]\n        return self.Ts[nT1], self.Ts[nT2]\n    else:\n        return None, None\n</code></pre>"},{"location":"reference/autora/theorist/bms/prior/","title":"autora.theorist.bms.prior","text":""},{"location":"reference/autora/theorist/bms/regressor/","title":"autora.theorist.bms.regressor","text":""},{"location":"reference/autora/theorist/bms/regressor/#autora.theorist.bms.regressor.BMSRegressor","title":"<code>BMSRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Bayesian Machine Scientist.</p> <p>BMS finds an optimal function to explain a dataset, given a set of variables, and a pre-defined number of parameters</p> <p>This class is intended to be compatible with the Scikit-Learn Estimator API.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from autora.theorist.bms import Parallel\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; num_samples = 1000\n&gt;&gt;&gt; X = np.linspace(start=0, stop=1, num=num_samples).reshape(-1, 1)\n&gt;&gt;&gt; y = 15. * np.ones(num_samples)\n&gt;&gt;&gt; estimator = BMSRegressor()\n&gt;&gt;&gt; estimator = estimator.fit(X, y)\n&gt;&gt;&gt; estimator.predict([[15.]])\narray([[15.]])\n</code></pre> <p>Attributes:</p> Name Type Description <code>pms</code> <code>Parallel</code> <p>the bayesian (parallel) machine scientist model</p> <code>model_</code> <code>Tree</code> <p>represents the best-fit model</p> <code>loss_</code> <code>float</code> <p>represents loss associated with best-fit model</p> <code>cache_</code> <code>List</code> <p>record of loss_ over model fitting epochs</p> <code>temp_</code> <code>float</code> <p>temperature of model_</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/regressor.py</code> <pre><code>class BMSRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Bayesian Machine Scientist.\n\n    BMS finds an optimal function to explain a dataset, given a set of variables,\n    and a pre-defined number of parameters\n\n    This class is intended to be compatible with the\n    [Scikit-Learn Estimator API](https://scikit-learn.org/stable/developers/develop.html).\n\n    Examples:\n\n        &gt;&gt;&gt; from autora.theorist.bms import Parallel\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; num_samples = 1000\n        &gt;&gt;&gt; X = np.linspace(start=0, stop=1, num=num_samples).reshape(-1, 1)\n        &gt;&gt;&gt; y = 15. * np.ones(num_samples)\n        &gt;&gt;&gt; estimator = BMSRegressor()\n        &gt;&gt;&gt; estimator = estimator.fit(X, y)\n        &gt;&gt;&gt; estimator.predict([[15.]])\n        array([[15.]])\n\n\n    Attributes:\n        pms: the bayesian (parallel) machine scientist model\n        model_: represents the best-fit model\n        loss_: represents loss associated with best-fit model\n        cache_: record of loss_ over model fitting epochs\n        temp_: temperature of model_\n    \"\"\"\n\n    def __init__(\n        self,\n        prior_par: dict = PRIORS,\n        ts: List[float] = TEMPERATURES,\n        epochs: int = 1500,\n    ):\n        \"\"\"\n        Arguments:\n            prior_par: a dictionary of the prior probabilities of different functions based on\n                wikipedia data scraping\n            ts: contains a list of the temperatures that the parallel ms works at\n        \"\"\"\n        self.ts = ts\n        self.prior_par = prior_par\n        self.epochs = epochs\n        self.pms: Parallel = Parallel(Ts=ts)\n        self.ops = get_priors()[1]\n        self.custom_ops: Dict[str, Callable] = dict()\n        self.X_: Optional[np.ndarray] = None\n        self.y_: Optional[np.ndarray] = None\n        self.model_: Tree = Tree()\n        self.temp_: float = 0.0\n        self.models_: List[Tree] = [Tree()]\n        self.loss_: float = np.inf\n        self.cache_: List = []\n        self.variables: List = []\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        num_param: int = 1,\n        root=None,\n        custom_ops=None,\n        random_state=None,\n    ) -&gt; BMSRegressor:\n        \"\"\"\n        Runs the optimization for a given set of `X`s and `y`s.\n\n        Arguments:\n            X: independent variables in an n-dimensional array\n            y: dependent variables in an n-dimensional array\n            num_param: number of parameters\n            root: fixed root of the tree\n            custom_ops: user-defined functions to additionally treated as primitives\n\n        Returns:\n            self (BMS): the fitted estimator\n        \"\"\"\n        # firstly, store the column names of X since checking will\n        # cast the type of X to np.ndarray\n        if hasattr(X, \"columns\"):\n            self.variables = list(X.columns)\n        else:\n            # create variables X_1 to X_n where n is the number of columns in X\n            self.variables = [\"X%d\" % i for i in range(X.shape[1])]\n\n        X, y = check_X_y(X, y)\n\n        # cast X into pd.Pandas again to fit the need in mcmc.py\n        X = pd.DataFrame(X, columns=self.variables)\n        y = pd.Series(y)\n        _logger.info(\"BMS fitting started\")\n        if custom_ops is not None:\n            for op in custom_ops:\n                self.add_primitive(op)\n        if (root is not None) and (root not in self.ops.keys()):\n            self.add_primitive(root)\n        self.pms = Parallel(\n            Ts=self.ts,\n            variables=self.variables,\n            parameters=[\"a%d\" % i for i in range(num_param)],\n            x=X,\n            y=y,\n            prior_par=self.prior_par,\n            ops=self.ops,\n            custom_ops=self.custom_ops,\n            root=root,\n            random_state=random_state,\n        )\n        self.model_, self.loss_, self.cache_ = utils.run(self.pms, self.epochs)\n        self.models_ = list(self.pms.trees.values())\n\n        _logger.info(\"BMS fitting finished\")\n        self.X_, self.y_ = X, y\n        return self\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Applies the fitted model to a set of independent variables `X`,\n        to give predictions for the dependent variable `y`.\n\n        Arguments:\n            X: independent variables in an n-dimensional array\n\n        Returns:\n            y: predicted dependent variable values\n        \"\"\"\n        # this validation step will cast X into np.ndarray format\n        X = check_array(X)\n\n        check_is_fitted(self, attributes=[\"model_\"])\n\n        assert self.model_ is not None\n        # we need to cast it back into pd.DataFrame with the original\n        # column names (generated in `fit`).\n        # in the future, we might need to look into mcmc.py to remove\n        # these redundant type castings.\n        X = pd.DataFrame(X, columns=self.variables)\n\n        return np.expand_dims(self.model_.predict(X).to_numpy(), axis=1)\n\n    def present_results(self):\n        \"\"\"\n        Prints out the best equation, its description length,\n        along with a plot of how this has progressed over the course of the search tasks.\n        \"\"\"\n        check_is_fitted(self, attributes=[\"model_\", \"loss_\", \"cache_\"])\n        assert self.model_ is not None\n        assert self.loss_ is not None\n        assert self.cache_ is not None\n\n        utils.present_results(self.model_, self.loss_, self.cache_)\n\n    def __repr__(self):\n        return self.repr()\n\n    def repr(self, decimals=2):\n        model_str = self.model_.__repr__()\n        parameter_names = self.model_.parameters\n        parameter_values = self.model_.par_values\n        for name in parameter_names:\n            value = parameter_values[\"d0\"][name]\n            model_str = model_str.replace(name, str(np.round(value, decimals=decimals)))\n        return model_str\n\n    def get_models(self):\n        model_list = []\n        for idx, tree in enumerate(self.models_):\n            bms_model = BMSRegressor()\n            bms_model.model_ = tree\n            bms_model.temp_ = self.ts[idx]\n            bms_model.variables = (\n                list(self.X_.columns)\n                if hasattr(self.X_, \"columns\")\n                else [\"X%d\" % i for i in range(self.X_.shape[1])]\n            )\n            model_list.append(bms_model)\n        return model_list\n\n    def latex(self):\n        return self.model_.latex()\n\n    def add_primitive(self, op: Callable):\n        self.custom_ops.update({op.__name__: op})\n        self.ops.update({op.__name__: len(signature(op).parameters)})\n        self.prior_par.update({\"Nopi_\" + op.__name__: 1})\n</code></pre>"},{"location":"reference/autora/theorist/bms/regressor/#autora.theorist.bms.regressor.BMSRegressor.__init__","title":"<code>__init__(prior_par=PRIORS, ts=TEMPERATURES, epochs=1500)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>prior_par</code> <code>dict</code> <p>a dictionary of the prior probabilities of different functions based on wikipedia data scraping</p> <code>PRIORS</code> <code>ts</code> <code>List[float]</code> <p>contains a list of the temperatures that the parallel ms works at</p> <code>TEMPERATURES</code> Source code in <code>temp_dir/bms/src/autora/theorist/bms/regressor.py</code> <pre><code>def __init__(\n    self,\n    prior_par: dict = PRIORS,\n    ts: List[float] = TEMPERATURES,\n    epochs: int = 1500,\n):\n    \"\"\"\n    Arguments:\n        prior_par: a dictionary of the prior probabilities of different functions based on\n            wikipedia data scraping\n        ts: contains a list of the temperatures that the parallel ms works at\n    \"\"\"\n    self.ts = ts\n    self.prior_par = prior_par\n    self.epochs = epochs\n    self.pms: Parallel = Parallel(Ts=ts)\n    self.ops = get_priors()[1]\n    self.custom_ops: Dict[str, Callable] = dict()\n    self.X_: Optional[np.ndarray] = None\n    self.y_: Optional[np.ndarray] = None\n    self.model_: Tree = Tree()\n    self.temp_: float = 0.0\n    self.models_: List[Tree] = [Tree()]\n    self.loss_: float = np.inf\n    self.cache_: List = []\n    self.variables: List = []\n</code></pre>"},{"location":"reference/autora/theorist/bms/regressor/#autora.theorist.bms.regressor.BMSRegressor.fit","title":"<code>fit(X, y, num_param=1, root=None, custom_ops=None, random_state=None)</code>","text":"<p>Runs the optimization for a given set of <code>X</code>s and <code>y</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>independent variables in an n-dimensional array</p> required <code>y</code> <code>ndarray</code> <p>dependent variables in an n-dimensional array</p> required <code>num_param</code> <code>int</code> <p>number of parameters</p> <code>1</code> <code>root</code> <p>fixed root of the tree</p> <code>None</code> <code>custom_ops</code> <p>user-defined functions to additionally treated as primitives</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>BMS</code> <p>the fitted estimator</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/regressor.py</code> <pre><code>def fit(\n    self,\n    X: np.ndarray,\n    y: np.ndarray,\n    num_param: int = 1,\n    root=None,\n    custom_ops=None,\n    random_state=None,\n) -&gt; BMSRegressor:\n    \"\"\"\n    Runs the optimization for a given set of `X`s and `y`s.\n\n    Arguments:\n        X: independent variables in an n-dimensional array\n        y: dependent variables in an n-dimensional array\n        num_param: number of parameters\n        root: fixed root of the tree\n        custom_ops: user-defined functions to additionally treated as primitives\n\n    Returns:\n        self (BMS): the fitted estimator\n    \"\"\"\n    # firstly, store the column names of X since checking will\n    # cast the type of X to np.ndarray\n    if hasattr(X, \"columns\"):\n        self.variables = list(X.columns)\n    else:\n        # create variables X_1 to X_n where n is the number of columns in X\n        self.variables = [\"X%d\" % i for i in range(X.shape[1])]\n\n    X, y = check_X_y(X, y)\n\n    # cast X into pd.Pandas again to fit the need in mcmc.py\n    X = pd.DataFrame(X, columns=self.variables)\n    y = pd.Series(y)\n    _logger.info(\"BMS fitting started\")\n    if custom_ops is not None:\n        for op in custom_ops:\n            self.add_primitive(op)\n    if (root is not None) and (root not in self.ops.keys()):\n        self.add_primitive(root)\n    self.pms = Parallel(\n        Ts=self.ts,\n        variables=self.variables,\n        parameters=[\"a%d\" % i for i in range(num_param)],\n        x=X,\n        y=y,\n        prior_par=self.prior_par,\n        ops=self.ops,\n        custom_ops=self.custom_ops,\n        root=root,\n        random_state=random_state,\n    )\n    self.model_, self.loss_, self.cache_ = utils.run(self.pms, self.epochs)\n    self.models_ = list(self.pms.trees.values())\n\n    _logger.info(\"BMS fitting finished\")\n    self.X_, self.y_ = X, y\n    return self\n</code></pre>"},{"location":"reference/autora/theorist/bms/regressor/#autora.theorist.bms.regressor.BMSRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Applies the fitted model to a set of independent variables <code>X</code>, to give predictions for the dependent variable <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>independent variables in an n-dimensional array</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>predicted dependent variable values</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/regressor.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies the fitted model to a set of independent variables `X`,\n    to give predictions for the dependent variable `y`.\n\n    Arguments:\n        X: independent variables in an n-dimensional array\n\n    Returns:\n        y: predicted dependent variable values\n    \"\"\"\n    # this validation step will cast X into np.ndarray format\n    X = check_array(X)\n\n    check_is_fitted(self, attributes=[\"model_\"])\n\n    assert self.model_ is not None\n    # we need to cast it back into pd.DataFrame with the original\n    # column names (generated in `fit`).\n    # in the future, we might need to look into mcmc.py to remove\n    # these redundant type castings.\n    X = pd.DataFrame(X, columns=self.variables)\n\n    return np.expand_dims(self.model_.predict(X).to_numpy(), axis=1)\n</code></pre>"},{"location":"reference/autora/theorist/bms/regressor/#autora.theorist.bms.regressor.BMSRegressor.present_results","title":"<code>present_results()</code>","text":"<p>Prints out the best equation, its description length, along with a plot of how this has progressed over the course of the search tasks.</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/regressor.py</code> <pre><code>def present_results(self):\n    \"\"\"\n    Prints out the best equation, its description length,\n    along with a plot of how this has progressed over the course of the search tasks.\n    \"\"\"\n    check_is_fitted(self, attributes=[\"model_\", \"loss_\", \"cache_\"])\n    assert self.model_ is not None\n    assert self.loss_ is not None\n    assert self.cache_ is not None\n\n    utils.present_results(self.model_, self.loss_, self.cache_)\n</code></pre>"},{"location":"reference/autora/theorist/bms/utils/","title":"autora.theorist.bms.utils","text":""},{"location":"reference/autora/theorist/bms/utils/#autora.theorist.bms.utils.present_results","title":"<code>present_results(model, model_len, desc_len)</code>","text":"<p>Prints out the best equation, its description length, along with a plot of how this has progressed over the course of the search tasks</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Tree</code> <p>The equation which best describes the data</p> required <code>model_len</code> <code>float</code> <p>The equation loss (defined as description length)</p> required <code>desc_len</code> <code>List[float]</code> <p>Record of equation loss over time</p> required <p>Returns: Nothing</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/utils.py</code> <pre><code>def present_results(model: Tree, model_len: float, desc_len: List[float]) -&gt; None:\n    \"\"\"\n    Prints out the best equation, its description length,\n    along with a plot of how this has progressed over the course of the search tasks\n\n    Args:\n        model: The equation which best describes the data\n        model_len: The equation loss (defined as description length)\n        desc_len: Record of equation loss over time\n\n    Returns: Nothing\n\n    \"\"\"\n    print(\"Best model:\\t\", model)\n    print(\"Desc. length:\\t\", model_len)\n    plt.figure(figsize=(15, 5))\n    plt.plot(desc_len)\n    plt.xlabel(\"MCMC step\", fontsize=14)\n    plt.ylabel(\"Description length\", fontsize=14)\n    plt.title(\"MDL model: $%s$\" % model.latex())\n    plt.show()\n</code></pre>"},{"location":"reference/autora/theorist/bms/utils/#autora.theorist.bms.utils.run","title":"<code>run(pms, num_steps, thinning=100)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pms</code> <code>Parallel</code> <p>Parallel Machine Scientist (BMS is essentially a wrapper for pms)</p> required <code>num_steps</code> <code>int</code> <p>number of epochs / mcmc step &amp; tree swap iterations</p> required <code>thinning</code> <code>int</code> <p>number of epochs between recording model loss to the trace</p> <code>100</code> <p>Returns:</p> Name Type Description <code>model</code> <code>Tree</code> <p>The equation which best describes the data</p> <code>model_len</code> <code>float</code> <p>(defined as description length) loss function score</p> <code>desc_len</code> <code>List[float]</code> <p>Record of loss function score over time</p> Source code in <code>temp_dir/bms/src/autora/theorist/bms/utils.py</code> <pre><code>def run(\n    pms: Parallel, num_steps: int, thinning: int = 100\n) -&gt; Tuple[Tree, float, List[float]]:\n    \"\"\"\n\n    Args:\n        pms: Parallel Machine Scientist (BMS is essentially a wrapper for pms)\n        num_steps: number of epochs / mcmc step &amp; tree swap iterations\n        thinning: number of epochs between recording model loss to the trace\n\n    Returns:\n        model: The equation which best describes the data\n        model_len: (defined as description length) loss function score\n        desc_len: Record of loss function score over time\n\n    \"\"\"\n    desc_len, model, model_len = [], pms.t1, np.inf\n    for n in tqdm(range(num_steps)):\n        pms.mcmc_step()\n        pms.tree_swap()\n        if num_steps % thinning == 0:  # sample less often if we thin more\n            desc_len.append(pms.t1.E)  # Add the description length to the trace\n        if pms.t1.E &lt; model_len:  # Check if this is the MDL expression so far\n            model, model_len = deepcopy(pms.t1), pms.t1.E\n        _logger.debug(\"Finish iteration {}\".format(n))\n    return model, model_len, desc_len\n</code></pre>"},{"location":"reference/autora/theorist/bsr/","title":"autora.theorist.bsr","text":""},{"location":"reference/autora/theorist/bsr/funcs/","title":"autora.theorist.bsr.funcs","text":""},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.Action","title":"<code>Action</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> <p>Enum class that represents a MCMC step with a certain action</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>class Action(int, Enum):\n    \"\"\"\n    Enum class that represents a MCMC step with a certain action\n    \"\"\"\n\n    STAY = 0\n    GROW = 1\n    PRUNE = 2\n    DE_TRANSFORM = 3\n    TRANSFORM = 4\n    REASSIGN_OP = 5\n    REASSIGN_FEAT = 6\n\n    @classmethod\n    def rand_action(\n        cls, lt_num: int, term_num: int, de_trans_num: int\n    ) -&gt; Tuple[int, List[float]]:\n        \"\"\"\n        Draw a random action for MCMC algorithm to take a step\n\n        Arguments:\n            lt_num: the number of linear (`lt`) nodes in the tree\n            term_num: the number of terminal nodes in the tree\n            de_trans_num: the number of de-trans qualified nodes in the tree\n                          (see `propose` for details)\n\n        Returns:\n            action: the MCMC action to perform\n            weights: the probabilities for each action\n        \"\"\"\n        # from the BSR paper\n        weights = []\n        weights.append(0.25 * lt_num / (lt_num + 3))  # p_stay\n        weights.append((1 - weights[0]) * min(1, 4 / (term_num + 2)) / 3)  # p_grow\n        weights.append((1 - weights[0]) / 3 - weights[1])  # p_prune\n        weights.append(\n            ((1 - weights[0]) * (1 / 3) * de_trans_num / (3 + de_trans_num))\n        )  # p_detrans\n        weights.append((1 - weights[0]) / 3 - weights[3])  # p_trans\n        weights.append((1 - weights[0]) / 6)  # p_reassign_op\n        weights.append(1 - sum(weights))  # p_reassign_feat\n        assert weights[-1] &gt;= 0\n\n        action = random.choices(range(7), weights=weights, k=1)[0]\n        return action, weights\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.Action.rand_action","title":"<code>rand_action(lt_num, term_num, de_trans_num)</code>  <code>classmethod</code>","text":"<p>Draw a random action for MCMC algorithm to take a step</p> <p>Parameters:</p> Name Type Description Default <code>lt_num</code> <code>int</code> <p>the number of linear (<code>lt</code>) nodes in the tree</p> required <code>term_num</code> <code>int</code> <p>the number of terminal nodes in the tree</p> required <code>de_trans_num</code> <code>int</code> <p>the number of de-trans qualified nodes in the tree           (see <code>propose</code> for details)</p> required <p>Returns:</p> Name Type Description <code>action</code> <code>int</code> <p>the MCMC action to perform</p> <code>weights</code> <code>List[float]</code> <p>the probabilities for each action</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@classmethod\ndef rand_action(\n    cls, lt_num: int, term_num: int, de_trans_num: int\n) -&gt; Tuple[int, List[float]]:\n    \"\"\"\n    Draw a random action for MCMC algorithm to take a step\n\n    Arguments:\n        lt_num: the number of linear (`lt`) nodes in the tree\n        term_num: the number of terminal nodes in the tree\n        de_trans_num: the number of de-trans qualified nodes in the tree\n                      (see `propose` for details)\n\n    Returns:\n        action: the MCMC action to perform\n        weights: the probabilities for each action\n    \"\"\"\n    # from the BSR paper\n    weights = []\n    weights.append(0.25 * lt_num / (lt_num + 3))  # p_stay\n    weights.append((1 - weights[0]) * min(1, 4 / (term_num + 2)) / 3)  # p_grow\n    weights.append((1 - weights[0]) / 3 - weights[1])  # p_prune\n    weights.append(\n        ((1 - weights[0]) * (1 / 3) * de_trans_num / (3 + de_trans_num))\n    )  # p_detrans\n    weights.append((1 - weights[0]) / 3 - weights[3])  # p_trans\n    weights.append((1 - weights[0]) / 6)  # p_reassign_op\n    weights.append(1 - sum(weights))  # p_reassign_feat\n    assert weights[-1] &gt;= 0\n\n    action = random.choices(range(7), weights=weights, k=1)[0]\n    return action, weights\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.calc_aux_ll","title":"<code>calc_aux_ll(node, **hyper_params)</code>","text":"<p>Calculate the likelihood of generating auxiliary parameters</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the node from which the auxiliary params are generated</p> required <code>hyper_params</code> <p>hyperparameters for generating auxiliary params</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>log_aux</code> <code>float</code> <p>log likelihood of auxiliary params</p> <code>lt_count</code> <code>int</code> <p>number of nodes with <code>lt</code> operator in the tree with <code>node</code> as its root</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>def calc_aux_ll(node: Node, **hyper_params) -&gt; Tuple[float, int]:\n    \"\"\"\n    Calculate the likelihood of generating auxiliary parameters\n\n    Arguments:\n        node: the node from which the auxiliary params are generated\n        hyper_params: hyperparameters for generating auxiliary params\n\n    Returns:\n        log_aux: log likelihood of auxiliary params\n        lt_count: number of nodes with `lt` operator in the tree with\n            `node` as its root\n    \"\"\"\n    sigma_a, sigma_b = hyper_params[\"sigma_a\"], hyper_params[\"sigma_b\"]\n    log_aux = np.log(invgamma.pdf(sigma_a, 1)) + np.log(invgamma.pdf(sigma_b, 1))\n\n    all_nodes = get_all_nodes(node)\n    lt_count = 0\n    for i in range(all_nodes):\n        if all_nodes[i].op_name == \"ln\":\n            lt_count += 1\n            a, b = all_nodes[i].params[\"a\"], all_nodes[i].params[\"b\"]\n            log_aux += np.log(norm.pdf(a, 1, np.sqrt(sigma_a)))\n            log_aux += np.log(norm.pdf(b, 0, np.sqrt(sigma_b)))\n\n    return log_aux, lt_count\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.calc_tree_ll","title":"<code>calc_tree_ll(node, ops_priors, n_feature=1, **hyper_params)</code>","text":"<p>Calculate the likelihood-related quantities of the given tree <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node for which the calculations are done</p> required <code>ops_priors</code> <code>Dict[str, Dict]</code> <p>the dictionary that maps operation names to their prior info</p> required <code>n_feature</code> <code>int</code> <p>number of features in the input data</p> <code>1</code> <code>hyperparams</code> <p>hyperparameters for initialization</p> required <p>Returns:</p> Name Type Description <code>struct_ll</code> <p>tree structure-related likelihood</p> <code>params_ll</code> <p>tree parameters-related likelihood</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef calc_tree_ll(\n    node: Node, ops_priors: Dict[str, Dict], n_feature: int = 1, **hyper_params\n):\n    \"\"\"\n    Calculate the likelihood-related quantities of the given tree `node`.\n\n    Arguments:\n        node: the tree node for which the calculations are done\n        ops_priors: the dictionary that maps operation names to their prior info\n        n_feature: number of features in the input data\n        hyperparams: hyperparameters for initialization\n\n    Returns:\n        struct_ll: tree structure-related likelihood\n        params_ll: tree parameters-related likelihood\n    \"\"\"\n    struct_ll = 0  # log likelihood of tree structure S = (T,M)\n    params_ll = 0  # log likelihood of linear params\n    depth = node.depth\n    beta = hyper_params.get(\"beta\", -1)\n    sigma_a, sigma_b = hyper_params.get(\"sigma_a\", 1), hyper_params.get(\"sigma_b\", 1)\n\n    # contribution of hyperparameter sigma_theta\n    if not depth:  # root node\n        struct_ll += np.log(invgamma.pdf(sigma_a, 1))\n        struct_ll += np.log(invgamma.pdf(sigma_b, 1))\n\n    # contribution of splitting the node or becoming leaf node\n    if node.node_type == NodeType.LEAF:\n        # contribution of choosing terminal\n        struct_ll += np.log(1 - 1 / np.power((1 + depth), -beta))\n        # contribution of feature selection\n        struct_ll -= np.log(n_feature)\n        return struct_ll, params_ll\n    elif node.node_type == NodeType.UNARY:  # unitary operator\n        # contribution of child nodes are added since the log likelihood is additive\n        # if we assume the parameters are independent.\n        left = cast(Node, node.left)\n        struct_ll_left, params_ll_left = calc_tree_ll(\n            left, ops_priors, n_feature, **hyper_params\n        )\n        struct_ll += struct_ll_left\n        params_ll += params_ll_left\n        # contribution of parameters of linear nodes\n        # make sure the below parameter ll calculation is extendable\n        if node.op_name == \"ln\":\n            params_ll -= np.power((node.params[\"a\"] - 1), 2) / (2 * sigma_a)\n            params_ll -= np.power(node.params[\"b\"], 2) / (2 * sigma_b)\n            params_ll -= 0.5 * np.log(4 * np.pi**2 * sigma_a * sigma_b)\n    else:  # binary operator\n        left = cast(Node, node.left)\n        right = cast(Node, node.right)\n        struct_ll_left, params_ll_left = calc_tree_ll(\n            left, ops_priors, n_feature, **hyper_params\n        )\n        struct_ll_right, params_ll_right = calc_tree_ll(\n            right, ops_priors, n_feature, **hyper_params\n        )\n        struct_ll += struct_ll_left + struct_ll_right\n        params_ll += params_ll_left + params_ll_right\n\n    op_weight = ops_priors[node.op_name][\"weight\"]\n    # for unary &amp; binary nodes, additionally consider the contribution of splitting\n    if not depth:  # root node\n        struct_ll += np.log(op_weight)\n    else:\n        struct_ll += np.log((1 + depth)) * beta + np.log(op_weight)\n\n    return struct_ll, params_ll\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.calc_y_ll","title":"<code>calc_y_ll(y, outputs, sigma_y)</code>","text":"<p>Calculate the log likelihood f(y|S,Theta,x) where (S,Theta) is represented by the node prior is y ~ N(output,sigma) and output is the matrix of outputs corresponding to different roots.</p> <p>Returns:</p> Name Type Description <code>log_sum</code> <p>the data log likelihood</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>def calc_y_ll(y: np.ndarray, outputs: Union[np.ndarray, pd.DataFrame], sigma_y: float):\n    \"\"\"\n    Calculate the log likelihood f(y|S,Theta,x) where (S,Theta) is represented by the\n    node prior is y ~ N(output,sigma) and output is the matrix of outputs corresponding to\n    different roots.\n\n    Returns:\n        log_sum: the data log likelihood\n    \"\"\"\n    outputs = copy.deepcopy(outputs)\n    scale = np.max(np.abs(outputs))\n    outputs = outputs / scale\n    epsilon = np.eye(outputs.shape[1]) * 1e-6\n    beta = np.linalg.inv(np.matmul(outputs.transpose(), outputs) + epsilon)\n    beta = np.matmul(beta, np.matmul(outputs.transpose(), y))\n    # perform the linear combination\n    output = np.matmul(outputs, beta)\n    # calculate the squared error\n    error = np.sum(np.square(y - output[:, 0]))\n\n    log_sum = error\n    var = 2 * sigma_y * sigma_y\n    log_sum = -log_sum / var\n    log_sum -= 0.5 * len(y) * np.log(np.pi * var)\n    return log_sum\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.check_empty","title":"<code>check_empty(func)</code>","text":"<p>A decorator that, if applied to <code>func</code>, checks whether an argument in <code>func</code> is an un-initialized node (i.e. node.node_type == NodeType.Empty). If so, an error is raised.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>def check_empty(func: Callable):\n    \"\"\"\n    A decorator that, if applied to `func`, checks whether an argument in `func` is an\n    un-initialized node (i.e. node.node_type == NodeType.Empty). If so, an error is raised.\n    \"\"\"\n\n    @wraps(func)\n    def func_wrapper(*args, **kwargs):\n        for arg in args:\n            if isinstance(arg, Node):\n                if arg.node_type == NodeType.EMPTY:\n                    raise TypeError(\n                        \"uninitialized node found in {}\".format(func.__name__)\n                    )\n                break\n        return func(*args, **kwargs)\n\n    return func_wrapper\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.de_transform","title":"<code>de_transform(node)</code>","text":"<p>ACTION 4: De-transform deletes the current <code>node</code> and replaces it with children according to the following rule: if the <code>node</code> is unary, simply replace with its child; if <code>node</code> is binary and root, choose any children that's not leaf; if <code>node</code> is binary and not root, pick any children.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node that gets de-transformed</p> required <p>Returns:</p> Type Description <code>Node</code> <p>first node is the replaced node when <code>node</code> has been de-transformed</p> <code>Optional[Node]</code> <p>second node is the discarded node</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef de_transform(node: Node) -&gt; Tuple[Node, Optional[Node]]:\n    \"\"\"\n    ACTION 4: De-transform deletes the current `node` and replaces it with children\n    according to the following rule: if the `node` is unary, simply replace with its\n    child; if `node` is binary and root, choose any children that's not leaf; if `node`\n    is binary and not root, pick any children.\n\n    Arguments:\n        node: the tree node that gets de-transformed\n\n    Returns:\n        first node is the replaced node when `node` has been de-transformed\n        second node is the discarded node\n    \"\"\"\n    left = cast(Node, node.left)\n    if node.node_type == NodeType.UNARY:\n        return left, None\n\n    r = random.random()\n    right = cast(Node, node.right)\n    # picked node is root\n    if not node.depth:\n        if left.node_type == NodeType.LEAF:\n            return right, left\n        elif right.node_type == NodeType.LEAF:\n            return left, right\n        else:\n            return (left, right) if r &lt; 0.5 else (right, left)\n    elif r &lt; 0.5:\n        return left, right\n    else:\n        return right, left\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.get_all_nodes","title":"<code>get_all_nodes(node)</code>","text":"<p>Get all the nodes below (and including) the given <code>node</code> via pre-order traversal</p> Return <p>a list with all the nodes below (and including) the given <code>node</code></p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef get_all_nodes(node: Node) -&gt; List[Node]:\n    \"\"\"\n    Get all the nodes below (and including) the given `node` via pre-order traversal\n\n    Return:\n        a list with all the nodes below (and including) the given `node`\n    \"\"\"\n    nodes = [node]\n    if node.node_type == NodeType.UNARY:\n        nodes.extend(get_all_nodes(node.left))\n    elif node.node_type == NodeType.BINARY:\n        nodes.extend(get_all_nodes(node.left))\n        nodes.extend(get_all_nodes(node.right))\n    return nodes\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.get_height","title":"<code>get_height(node)</code>","text":"<p>Get the height of a tree starting from <code>node</code> as root. The height of a leaf is defined as 0.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the Node that we hope to calculate <code>height</code> for</p> required <p>Returns:     height: the height of <code>node</code></p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef get_height(node: Node) -&gt; int:\n    \"\"\"\n    Get the height of a tree starting from `node` as root. The height of a leaf is defined as 0.\n\n    Arguments:\n        node: the Node that we hope to calculate `height` for\n    Returns:\n        height: the height of `node`\n    \"\"\"\n    if node.node_type == NodeType.LEAF:\n        return 0\n    elif node.node_type == NodeType.UNARY:\n        return 1 + get_height(node.left)\n    else:  # binary node\n        return 1 + max(get_height(node.left), get_height(node.right))\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.get_num_lt_nodes","title":"<code>get_num_lt_nodes(node)</code>","text":"<p>Get the number of nodes with <code>lt</code> operation in a tree starting from <code>node</code></p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef get_num_lt_nodes(node: Node) -&gt; int:\n    \"\"\"\n    Get the number of nodes with `lt` operation in a tree starting from `node`\n    \"\"\"\n    if node.node_type == NodeType.LEAF:\n        return 0\n    else:\n        base = 1 if node.op_name == \"ln\" else 0\n        if node.node_type == NodeType.UNARY:\n            return base + get_num_lt_nodes(node.left)\n        else:\n            return base + get_num_lt_nodes(node.left) + get_num_lt_nodes(node.right)\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.grow","title":"<code>grow(node, ops_name_lst, ops_weight_lst, ops_priors, n_feature=1, **hyper_params)</code>","text":"<p>ACTION 2: Grow represents the action of growing a subtree from a given <code>node</code></p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node from where the subtree starts to grow</p> required <code>ops_name_lst</code> <code>List[str]</code> <p>list of operation names</p> required <code>ops_weight_lst</code> <code>List[float]</code> <p>list of operation prior weights</p> required <code>ops_priors</code> <code>Dict[str, Dict]</code> <p>the dictionary of operation prior properties</p> required <code>n_feature</code> <code>int</code> <p>the number of features in input data</p> <code>1</code> <code>hyper_params</code> <p>hyperparameters for re-initialization</p> <code>{}</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>def grow(\n    node: Node,\n    ops_name_lst: List[str],\n    ops_weight_lst: List[float],\n    ops_priors: Dict[str, Dict],\n    n_feature: int = 1,\n    **hyper_params,\n):\n    \"\"\"\n    ACTION 2: Grow represents the action of growing a subtree from a given `node`\n\n    Arguments:\n        node: the tree node from where the subtree starts to grow\n        ops_name_lst: list of operation names\n        ops_weight_lst: list of operation prior weights\n        ops_priors: the dictionary of operation prior properties\n        n_feature: the number of features in input data\n        hyper_params: hyperparameters for re-initialization\n    \"\"\"\n    depth = node.depth\n    p = 1 / np.power((1 + depth), -hyper_params.get(\"beta\", -1))\n\n    if depth &gt; 0 and p &lt; random.random():  # create leaf node\n        node.setup(feature=random.randint(0, n_feature - 1))\n    else:\n        ops_name = random.choices(ops_name_lst, ops_weight_lst, k=1)[0]\n        ops_prior = ops_priors[ops_name]\n        node.setup(ops_name, ops_prior, hyper_params=hyper_params)\n\n        # recursively set up downstream nodes\n        grow(\n            cast(Node, node.left),\n            ops_name_lst,\n            ops_weight_lst,\n            ops_priors,\n            n_feature,\n            **hyper_params,\n        )\n        if node.node_type == NodeType.BINARY:\n            grow(\n                cast(Node, node.right),\n                ops_name_lst,\n                ops_weight_lst,\n                ops_priors,\n                n_feature,\n                **hyper_params,\n            )\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.prop","title":"<code>prop(node, ops_name_lst, ops_weight_lst, ops_priors, n_feature=1, **hyper_params)</code>","text":"<p>Propose a new tree from an existing tree with root <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the existing tree node</p> required <code>ops_name_lst</code> <code>List[str]</code> <p>the list of operator names</p> required <code>ops_weight_lst</code> <code>List[float]</code> <p>the list of operator weights</p> required <code>ops_priors</code> <code>Dict[str, Dict]</code> <p>the dictionary of operator prior information</p> required <code>n_feature</code> <code>int</code> <p>the number of features in input data</p> <code>1</code> <code>hyper_params</code> <p>hyperparameters for initialization</p> <code>{}</code> Return <p>new_node: the new node after some action is applied expand_node: whether the node has been expanded shrink_node: whether the node has been shrunk q: quantities for calculating acceptance prob q_inv: quantities for calculating acceptance prob</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef prop(\n    node: Node,\n    ops_name_lst: List[str],\n    ops_weight_lst: List[float],\n    ops_priors: Dict[str, Dict],\n    n_feature: int = 1,\n    **hyper_params,\n):\n    \"\"\"\n    Propose a new tree from an existing tree with root `node`.\n\n    Arguments:\n        node: the existing tree node\n        ops_name_lst: the list of operator names\n        ops_weight_lst: the list of operator weights\n        ops_priors: the dictionary of operator prior information\n        n_feature: the number of features in input data\n        hyper_params: hyperparameters for initialization\n\n    Return:\n        new_node: the new node after some action is applied\n        expand_node: whether the node has been expanded\n        shrink_node: whether the node has been shrunk\n        q: quantities for calculating acceptance prob\n        q_inv: quantities for calculating acceptance prob\n    \"\"\"\n    # PART 1: collect necessary information\n    new_node = copy.deepcopy(node)\n    term_nodes, nterm_nodes, lt_nodes, de_trans_nodes = _get_tree_classified_nodes(\n        new_node\n    )\n\n    # PART 2: sample random action and perform the action\n    # this step also calculates q and q_inv, quantities necessary for calculating\n    # the acceptance probability in MCMC algorithm\n    action, probs = Action.rand_action(\n        len(lt_nodes), len(term_nodes), len(de_trans_nodes)\n    )\n    # flags indicating potential dimensionality change (expand or shrink) in node\n    expand_node, shrink_node = False, False\n\n    # ACTION 1: STAY\n    # q and q_inv simply equal the probability of choosing this action\n    if action == Action.STAY:\n        q = probs[Action.STAY]\n        q_inv = probs[Action.STAY]\n        stay(lt_nodes, **hyper_params)\n    # ACTION 2: GROW\n    # q and q_inv simply equal the probability if the grown node is a leaf node\n    # otherwise, we calculate new information of the `new_node` after the action is applied\n    elif action == Action.GROW:\n        i = random.randint(0, len(term_nodes) - 1)\n        grown_node: Node = term_nodes[i]\n        grow(\n            grown_node,\n            ops_name_lst,\n            ops_weight_lst,\n            ops_priors,\n            n_feature,\n            **hyper_params,\n        )\n        if grown_node.node_type == NodeType.LEAF:\n            q = q_inv = 1\n        else:\n            tree_ll, param_ll = calc_tree_ll(\n                grown_node, ops_priors, n_feature, **hyper_params\n            )\n            # calculate q\n            q = probs[Action.GROW] * np.exp(tree_ll) / len(term_nodes)\n            # calculate q_inv by using updated information of `new_node`\n            (\n                new_term_count,\n                new_nterm_count,\n                new_lt_count,\n                _,\n            ) = _get_tree_classified_counts(new_node)\n            new_prob = (\n                (1 - 0.25 * new_lt_count / (new_lt_count + 3))\n                * (1 - min(1, 4 / (new_nterm_count + 2)))\n                / 3\n            )\n            q_inv = new_prob / max(1, new_nterm_count - 1)  # except the root\n            if new_lt_count &gt; len(lt_nodes):\n                expand_node = True\n    # ACTION 3: PRUNE\n    elif action == Action.PRUNE:\n        i = random.randint(0, len(nterm_nodes) - 1)\n        pruned_node: Node = nterm_nodes[i]\n        prune(pruned_node, n_feature)\n        tree_ll, param_ll = calc_tree_ll(\n            pruned_node, ops_priors, n_feature, **hyper_params\n        )\n\n        new_term_count, new_nterm_count, new_lt_count, _ = _get_tree_classified_counts(\n            new_node\n        )\n        # pruning any tree with `ln` operator will result in shrinkage\n        if new_lt_count &lt; len(lt_nodes):\n            shrink_node = True\n\n        # calculate q\n        q = probs[Action.PRUNE] / ((len(nterm_nodes) - 1) * n_feature)\n        pg = 1 - 0.25 * new_lt_count / (new_lt_count + 3) * 0.75 * min(\n            1, 4 / (new_nterm_count + 2)\n        )\n        # calculate q_inv\n        q_inv = pg * np.exp(tree_ll) / new_term_count\n    # ACTION 4: DE_TRANSFORM\n    elif action == Action.DE_TRANSFORM:\n        num_de_trans = len(de_trans_nodes)\n        i = random.randint(0, num_de_trans - 1)\n        de_trans_node: Node = de_trans_nodes[i]\n        replaced_node, discarded_node = de_transform(de_trans_node)\n        par_node = de_trans_node.parent\n\n        q = probs[Action.DE_TRANSFORM] / num_de_trans\n        if (\n            not par_node\n            and de_trans_node.left\n            and de_trans_node.right\n            and de_trans_node.left.node_type != NodeType.LEAF\n            and de_trans_node.right.node_type != NodeType.LEAF\n        ):\n            q = q / 2\n        elif de_trans_node.node_type == NodeType.BINARY:\n            q = q / 2\n\n        if not par_node:  # de-transformed the root\n            new_node = replaced_node\n            new_node.parent = None\n            update_depth(new_node, 0)\n        elif par_node.left is de_trans_node:\n            par_node.left = replaced_node\n            replaced_node.parent = par_node\n            update_depth(replaced_node, par_node.depth + 1)\n        else:\n            par_node.right = replaced_node\n            replaced_node.parent = par_node\n            update_depth(replaced_node, par_node.depth + 1)\n\n        (\n            new_term_count,\n            new_nterm_count,\n            new_lt_count,\n            new_det_count,\n        ) = _get_tree_classified_counts(new_node)\n\n        if new_lt_count &lt; len(lt_nodes):\n            shrink_node = True\n\n        new_prob = 0.25 * new_lt_count / (new_lt_count + 3)\n        # calculate q_inv\n        new_pdetr = (1 - new_prob) * (1 / 3) * new_det_count / (new_det_count + 3)\n        new_ptr = (1 - new_prob) / 3 - new_pdetr\n        q_inv = (\n            new_ptr\n            * ops_priors[de_trans_node.op_name][\"weight\"]\n            / (new_term_count + new_nterm_count)\n        )\n        if discarded_node:\n            tree_ll, _ = calc_tree_ll(\n                discarded_node, ops_priors, n_feature, **hyper_params\n            )\n            q_inv = q_inv * np.exp(tree_ll)\n    # ACTION 5: TRANSFORM\n    elif action == Action.TRANSFORM:\n        all_nodes = get_all_nodes(new_node)\n        i = random.randint(0, len(all_nodes) - 1)\n        trans_node: Node = all_nodes[i]\n        inserted_node: Node = transform(\n            trans_node,\n            ops_name_lst,\n            ops_weight_lst,\n            ops_priors,\n            n_feature,\n            **hyper_params,\n        )\n\n        if inserted_node.right:\n            ll_right, _ = calc_tree_ll(\n                inserted_node.right, ops_priors, n_feature, **hyper_params\n            )\n        else:\n            ll_right = 0\n        # calculate q\n        q = (\n            probs[Action.TRANSFORM]\n            * ops_priors[inserted_node.op_name][\"weight\"]\n            * np.exp(ll_right)\n            / len(all_nodes)\n        )\n\n        (\n            new_term_count,\n            new_nterm_count,\n            new_lt_count,\n            new_det_count,\n        ) = _get_tree_classified_counts(new_node)\n        if new_lt_count &gt; len(lt_nodes):\n            expand_node = True\n\n        new_prob = 0.25 * new_lt_count / (new_lt_count + 3)\n        # calculate q_inv\n        new_pdetr = (1 - new_prob) * (1 / 3) * new_det_count / (new_det_count + 3)\n        q_inv = new_pdetr / new_det_count\n        if (\n            inserted_node.left\n            and inserted_node.right\n            and inserted_node.left.node_type != NodeType.LEAF\n            and inserted_node.right.node_type != NodeType.LEAF\n        ):\n            q_inv = q_inv / 2\n    # ACTION 6: REASSIGN OPERATION\n    elif action == Action.REASSIGN_OP:\n        i = random.randint(0, len(nterm_nodes) - 1)\n        reassign_node: Node = nterm_nodes[i]\n        old_right = reassign_node.right\n        old_op_name, old_type = reassign_node.op_name, reassign_node.node_type\n        reassign_op(\n            reassign_node,\n            ops_name_lst,\n            ops_weight_lst,\n            ops_priors,\n            n_feature,\n            **hyper_params,\n        )\n        new_type = reassign_node.node_type\n        _, new_nterm_count, new_lt_count, _ = _get_tree_classified_counts(new_node)\n\n        if old_type == new_type:  # binary -&gt; binary &amp; unary -&gt; unary\n            q = ops_priors[reassign_node.op_name][\"weight\"]\n            q_inv = ops_priors[old_op_name][\"weight\"]\n        else:\n            op_weight = ops_priors[reassign_node.op_name][\"weight\"]\n            if old_type == NodeType.UNARY:  # unary -&gt; binary\n                tree_ll, _ = calc_tree_ll(\n                    reassign_node.right, ops_priors, n_feature, **hyper_params\n                )\n                q = (\n                    probs[Action.REASSIGN_OP]\n                    * np.exp(tree_ll)\n                    * op_weight\n                    / len(nterm_nodes)\n                )\n                ll_factor = 1\n            else:  # binary -&gt; unary\n                tree_ll, _ = calc_tree_ll(\n                    old_right, ops_priors, n_feature, **hyper_params\n                )\n                q = probs[Action.REASSIGN_OP] * op_weight / len(nterm_nodes)\n                ll_factor = tree_ll\n            # calculate q_inv\n            new_prob = new_lt_count / (4 * (new_lt_count + 3))\n            q_inv = (\n                0.125\n                * (1 - new_prob)\n                * ll_factor\n                * ops_priors[old_op_name][\"weight\"]\n                / new_nterm_count\n            )\n        if new_lt_count &gt; len(lt_nodes):\n            expand_node = True\n        elif new_lt_count &lt; len(lt_nodes):\n            shrink_node = True\n    # ACTION 7: REASSIGN FEATURE\n    else:\n        i = random.randint(0, len(term_nodes) - 1)\n        reassign_node = term_nodes[i]\n        reassign_feat(reassign_node, n_feature)\n        q = q_inv = 1\n\n    return new_node, expand_node, shrink_node, q, q_inv\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.prop_new","title":"<code>prop_new(roots, index, sigma_y, beta, sigma_a, sigma_b, X, y, ops_name_lst, ops_weight_lst, ops_priors)</code>","text":"<p>Propose new structure, sample new parameters and decide whether to accept the new tree.</p> <p>Parameters:</p> Name Type Description Default <code>roots</code> <code>List[Node]</code> <p>the list of root nodes</p> required <code>index</code> <code>int</code> <p>the index of the root node to update</p> required <code>sigma_y</code> <code>float</code> <p>scale hyperparameter for linear mixture of expression trees</p> required <code>beta</code> <code>float</code> <p>hyperparameter for growing an uninitialized expression tree</p> required <code>sigma_a</code> <code>float</code> <p>hyperparameters for <code>lt</code> operator initialization</p> required <code>sigma_b</code> <code>float</code> <p>hyperparameters for <code>lt</code> operator initialization</p> required <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>input data (independent variable) matrix</p> required <code>y</code> <code>Union[ndarray, DataFrame]</code> <p>dependent variable vector</p> required <code>ops_name_lst</code> <code>List[str]</code> <p>the list of operator names</p> required <code>ops_weight_lst</code> <code>List[float]</code> <p>the list of operator weights</p> required <code>ops_priors</code> <code>Dict[str, Dict]</code> <p>the dictionary of operator prior information</p> required <p>Returns:</p> Name Type Description <code>accept</code> <code>bool</code> <p>whether to accept or reject the new expression tree</p> <code>root</code> <code>Node</code> <p>the old or new expression tree, determined by whether to accept the new tree</p> <code>sigma_y</code> <code>float</code> <p>the old or new sigma_y</p> <code>sigma_a</code> <code>float</code> <p>the old or new sigma_a</p> <code>sigma_b</code> <code>float</code> <p>the old or new sigma_b</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>def prop_new(\n    roots: List[Node],\n    index: int,\n    sigma_y: float,\n    beta: float,\n    sigma_a: float,\n    sigma_b: float,\n    X: Union[np.ndarray, pd.DataFrame],\n    y: Union[np.ndarray, pd.DataFrame],\n    ops_name_lst: List[str],\n    ops_weight_lst: List[float],\n    ops_priors: Dict[str, Dict],\n) -&gt; Tuple[bool, Node, float, float, float]:\n    \"\"\"\n    Propose new structure, sample new parameters and decide whether to accept the new tree.\n\n    Arguments:\n        roots: the list of root nodes\n        index: the index of the root node to update\n        sigma_y: scale hyperparameter for linear mixture of expression trees\n        beta: hyperparameter for growing an uninitialized expression tree\n        sigma_a: hyperparameters for `lt` operator initialization\n        sigma_b: hyperparameters for `lt` operator initialization\n        X: input data (independent variable) matrix\n        y: dependent variable vector\n        ops_name_lst: the list of operator names\n        ops_weight_lst: the list of operator weights\n        ops_priors: the dictionary of operator prior information\n\n    Returns:\n        accept: whether to accept or reject the new expression tree\n        root: the old or new expression tree, determined by whether to accept the new tree\n        sigma_y: the old or new sigma_y\n        sigma_a: the old or new sigma_a\n        sigma_b: the old or new sigma_b\n    \"\"\"\n    # the hyper-param for linear combination, i.e. for `sigma_y`\n    sig = 4\n    K = len(roots)\n    root = roots[index]\n    use_aux_ll = True\n\n    # sample new sigma_a and sigma_b\n    new_sigma_a = invgamma.rvs(1)\n    new_sigma_b = invgamma.rvs(1)\n\n    hyper_params = {\"sigma_a\": sigma_a, \"sigma_b\": sigma_b, \"beta\": beta}\n    new_hyper_params = {\"sigma_a\": new_sigma_a, \"sigma_b\": new_sigma_b, \"beta\": beta}\n    # propose a new tree `node`\n    new_root, expand_node, shrink_node, q, q_inv = prop(\n        root, ops_name_lst, ops_weight_lst, ops_priors, X.shape[1], **new_hyper_params\n    )\n\n    n_feature = X.shape[0]\n    new_outputs = np.zeros((len(y), K))\n    old_outputs = np.zeros((len(y), K))\n\n    for i in np.arange(K):\n        tmp_old = root.evaluate(X)\n        old_outputs[:, i] = tmp_old\n        if i == index:\n            new_outputs[:, i] = new_root.evaluate(X)\n        else:\n            new_outputs[:, i] = tmp_old\n\n    if np.linalg.matrix_rank(new_outputs) &lt; K:  # rejection due to insufficient rank\n        return False, root, sigma_y, sigma_a, sigma_b\n\n    y_ll_old = calc_y_ll(y, old_outputs, sigma_y)\n    # a magic number here as the parameter for generating new sigma_y\n    new_sigma_y = invgamma.rvs(sig)\n    y_ll_new = calc_y_ll(y, new_outputs, new_sigma_y)\n\n    log_y_ratio = y_ll_new - y_ll_old\n    # contribution of f(Theta, S)\n    if shrink_node or expand_node:\n        struct_ll_old = sum(calc_tree_ll(root, ops_priors, n_feature, **hyper_params))\n        struct_ll_new = sum(\n            calc_tree_ll(new_root, ops_priors, n_feature, **hyper_params)\n        )\n        log_struct_ratio = struct_ll_new - struct_ll_old\n    else:\n        log_struct_ratio = calc_tree_ll(\n            new_root, ops_priors, n_feature, **hyper_params\n        )[0] - calc_tree_ll(root, ops_priors, n_feature, **hyper_params)\n\n    # contribution of proposal Q and Qinv\n    log_q_ratio = np.log(max(1e-5, q_inv / q))\n\n    log_r = (\n        log_y_ratio\n        + log_struct_ratio\n        + log_q_ratio\n        + np.log(invgamma.pdf(new_sigma_y, sig))\n        - np.log(invgamma.pdf(sigma_y, sig))\n    )\n\n    if use_aux_ll and (expand_node or shrink_node):\n        old_aux_ll, old_lt_count = calc_aux_ll(root, **hyper_params)\n        new_aux_ll, _ = calc_aux_ll(new_root, **new_hyper_params)\n        log_r += old_aux_ll - new_aux_ll\n        # log for the Jacobian matrix\n        log_r += np.log(max(1e-5, 1 / np.power(2, 2 * old_lt_count)))\n\n    alpha = min(log_r, 0)\n    test = random.random()\n    if np.log(test) &gt;= alpha:  # no accept\n        return False, root, sigma_y, sigma_a, sigma_b\n    else:  # accept\n        return True, new_root, new_sigma_y, new_sigma_a, new_sigma_b\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.prune","title":"<code>prune(node, n_feature=1)</code>","text":"<p>ACTION 3: Prune a non-terminal node into a terminal node and assign it a feature</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node to be pruned</p> required <code>n_feature</code> <code>int</code> <p>the number of features in input data</p> <code>1</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef prune(node: Node, n_feature: int = 1):\n    \"\"\"\n    ACTION 3: Prune a non-terminal node into a terminal node and assign it a feature\n\n    Arguments:\n        node: the tree node to be pruned\n        n_feature: the number of features in input data\n    \"\"\"\n    node.setup(feature=random.randint(0, n_feature - 1))\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.reassign_feat","title":"<code>reassign_feat(node, n_feature=1)</code>","text":"<p>ACTION 7: Re-assign feature randomly picks a feature and assign it to <code>node</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node that gets re-assigned a feature</p> required <code>n_feature</code> <code>int</code> <p>the number of features in input data</p> <code>1</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef reassign_feat(node: Node, n_feature: int = 1):\n    \"\"\"\n    ACTION 7: Re-assign feature randomly picks a feature and assign it to `node`.\n\n    Arguments:\n        node: the tree node that gets re-assigned a feature\n        n_feature: the number of features in input data\n    \"\"\"\n    # make sure we have a leaf node\n    assert node.node_type == NodeType.LEAF\n    node.setup(feature=random.randint(0, n_feature - 1))\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.reassign_op","title":"<code>reassign_op(node, ops_name_lst, ops_weight_lst, ops_priors, n_feature=1, **hyper_params)</code>","text":"<p>ACTION 6: Re-assign action uniformly picks a non-terminal node, and assign a new operator. If the node changes from unary to binary, its original child is taken as the left child, and we grow a new subtree as right child. If the node changes from binary to unary, we preserve the left subtree (this is to make the transition reversible).</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node that gets re-assigned an operator</p> required <code>ops_name_lst</code> <code>List[str]</code> <p>list of operation names</p> required <code>ops_weight_lst</code> <code>List[float]</code> <p>list of operation prior weights</p> required <code>ops_priors</code> <code>Dict[str, Dict]</code> <p>the dictionary of operation prior properties</p> required <code>n_feature</code> <code>int</code> <p>the number of features in input data</p> <code>1</code> <code>hyper_params</code> <code>Dict</code> <p>hyperparameters for re-initialization</p> <code>{}</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef reassign_op(\n    node: Node,\n    ops_name_lst: List[str],\n    ops_weight_lst: List[float],\n    ops_priors: Dict[str, Dict],\n    n_feature: int = 1,\n    **hyper_params: Dict,\n):\n    \"\"\"\n    ACTION 6: Re-assign action uniformly picks a non-terminal node, and assign a new operator.\n    If the node changes from unary to binary, its original child is taken as the left child,\n    and we grow a new subtree as right child. If the node changes from binary to unary, we\n    preserve the left subtree (this is to make the transition reversible).\n\n    Arguments:\n        node: the tree node that gets re-assigned an operator\n        ops_name_lst: list of operation names\n        ops_weight_lst: list of operation prior weights\n        ops_priors: the dictionary of operation prior properties\n        n_feature: the number of features in input data\n        hyper_params: hyperparameters for re-initialization\n    \"\"\"\n    # make sure `node` is non-terminal\n    old_type = node.node_type\n    assert old_type != NodeType.LEAF\n\n    # store the original children and re-setup the `node`\n    old_left, old_right = node.left, node.right\n    new_op = random.choices(ops_name_lst, ops_weight_lst, k=1)[0]\n    node.setup(new_op, ops_priors[new_op], hyper_params=hyper_params)\n\n    new_type = node.node_type\n\n    node.left = old_left\n    if old_type == new_type:  # binary -&gt; binary &amp; unary -&gt; unary\n        node.right = old_right\n    elif new_type == NodeType.BINARY:  # unary -&gt; binary\n        grow(\n            cast(Node, node.right),\n            ops_name_lst,\n            ops_weight_lst,\n            ops_priors,\n            n_feature,\n            **hyper_params,\n        )\n    else:\n        node.right = None\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.stay","title":"<code>stay(lt_nodes, **hyper_params)</code>","text":"<p>ACTION 1: Stay represents the action of doing nothing but to update the parameters for <code>ln</code> operators.</p> <p>Parameters:</p> Name Type Description Default <code>lt_nodes</code> <code>List[Node]</code> <p>the list of nodes with <code>ln</code> operator</p> required <code>hyper_params</code> <code>Dict</code> <p>hyperparameters for re-initialization</p> <code>{}</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>def stay(lt_nodes: List[Node], **hyper_params: Dict):\n    \"\"\"\n    ACTION 1: Stay represents the action of doing nothing but to update the parameters for `ln`\n    operators.\n\n    Arguments:\n        lt_nodes: the list of nodes with `ln` operator\n        hyper_params: hyperparameters for re-initialization\n    \"\"\"\n    for lt_node in lt_nodes:\n        lt_node._init_param(**hyper_params)\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.transform","title":"<code>transform(node, ops_name_lst, ops_weight_lst, ops_priors, n_feature=1, **hyper_params)</code>","text":"<p>ACTION 5: Transform inserts a middle node between the picked <code>node</code> and its parent. Assign an operation to this middle node using the priors. If the middle node is binary, <code>grow</code> its right child. The left child of the middle node is set to <code>node</code> and its parent becomes <code>node.parent</code>.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>the tree node that gets transformed</p> required <code>ops_name_lst</code> <code>List[str]</code> <p>list of operation names</p> required <code>ops_weight_lst</code> <code>List[float]</code> <p>list of operation prior weights</p> required <code>ops_priors</code> <code>Dict[str, Dict]</code> <p>the dictionary of operation prior properties</p> required <code>n_feature</code> <code>int</code> <p>the number of features in input data</p> <code>1</code> <code>hyper_params</code> <code>Dict</code> <p>hyperparameters for re-initialization</p> <code>{}</code> Return <p>the middle node that gets inserted</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef transform(\n    node: Node,\n    ops_name_lst: List[str],\n    ops_weight_lst: List[float],\n    ops_priors: Dict[str, Dict],\n    n_feature: int = 1,\n    **hyper_params: Dict,\n) -&gt; Node:\n    \"\"\"\n    ACTION 5: Transform inserts a middle node between the picked `node` and its\n    parent. Assign an operation to this middle node using the priors. If the middle\n    node is binary, `grow` its right child. The left child of the middle node is\n    set to `node` and its parent becomes `node.parent`.\n\n    Arguments:\n        node: the tree node that gets transformed\n        ops_name_lst: list of operation names\n        ops_weight_lst: list of operation prior weights\n        ops_priors: the dictionary of operation prior properties\n        n_feature: the number of features in input data\n        hyper_params: hyperparameters for re-initialization\n\n    Return:\n        the middle node that gets inserted\n    \"\"\"\n    parent = node.parent\n\n    insert_node = Node(depth=node.depth, parent=parent)\n    insert_op = random.choices(ops_name_lst, ops_weight_lst, k=1)[0]\n    insert_node.setup(insert_op, ops_priors[insert_op], hyper_params=hyper_params)\n\n    if parent:\n        is_left = node is parent.left\n        if is_left:\n            parent.left = insert_node\n        else:\n            parent.right = insert_node\n\n    # set the left child as `node` and grow the right child if needed (binary case)\n    insert_node.left = node\n    node.parent = insert_node\n    if insert_node.node_type == NodeType.BINARY:\n        grow(\n            cast(Node, insert_node.right),\n            ops_name_lst,\n            ops_weight_lst,\n            ops_priors,\n            n_feature,\n            **hyper_params,\n        )\n\n    # make sure the depth property is updated correctly\n    update_depth(node, node.depth + 1)\n    return insert_node\n</code></pre>"},{"location":"reference/autora/theorist/bsr/funcs/#autora.theorist.bsr.funcs.update_depth","title":"<code>update_depth(node, depth)</code>","text":"<p>Update the depth information of all nodes starting from root <code>node</code>, whose depth is set equal to the given <code>depth</code>.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/funcs.py</code> <pre><code>@check_empty\ndef update_depth(node: Node, depth: int):\n    \"\"\"\n    Update the depth information of all nodes starting from root `node`, whose depth\n    is set equal to the given `depth`.\n    \"\"\"\n    node.depth = depth\n    if node.node_type == NodeType.UNARY:\n        update_depth(node.left, depth + 1)\n    elif node.node_type == NodeType.BINARY:\n        update_depth(node.left, depth + 1)\n        update_depth(node.right, depth + 1)\n</code></pre>"},{"location":"reference/autora/theorist/bsr/misc/","title":"autora.theorist.bsr.misc","text":""},{"location":"reference/autora/theorist/bsr/misc/#autora.theorist.bsr.misc.get_ops_expr","title":"<code>get_ops_expr()</code>","text":"<p>Get the literal expression for the operation, the <code>{}</code> placeholder represents an expression that is recursively evaluated from downstream operations. If an operator's expression contains additional parameters (e.g. slope/intercept in linear operator), write the parameter like <code>{param}</code> - the param will be passed in using <code>expr.format(xxx, **params)</code> format.</p> Return <p>A dictionary that maps operator name to its literal expression.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/misc.py</code> <pre><code>def get_ops_expr() -&gt; Dict[str, str]:\n    \"\"\"\n    Get the literal expression for the operation, the `{}` placeholder represents\n    an expression that is recursively evaluated from downstream operations. If an\n    operator's expression contains additional parameters (e.g. slope/intercept in\n    linear operator), write the parameter like `{param}` - the param will be passed\n    in using `expr.format(xxx, **params)` format.\n\n    Return:\n        A dictionary that maps operator name to its literal expression.\n    \"\"\"\n    ops_expr = {\n        \"neg\": \"-({})\",\n        \"sin\": \"sin({})\",\n        \"pow2\": \"({})^2\",\n        \"pow3\": \"({})^3\",\n        \"exp\": \"exp({})\",\n        \"cos\": \"cos({})\",\n        \"+\": \"{}+{}\",\n        \"*\": \"({})*({})\",\n        \"-\": \"{}-{}\",\n        \"inv\": \"1/[{}]\",\n        \"linear\": \"{a}*({})+{b}\",\n    }\n    return ops_expr\n</code></pre>"},{"location":"reference/autora/theorist/bsr/misc/#autora.theorist.bsr.misc.normalize_prior_dict","title":"<code>normalize_prior_dict(prior_dict)</code>","text":"<p>Normalize the prior weights for the operators so that the weights sum to 1 and thus can be directly interpreted/used as probabilities.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/misc.py</code> <pre><code>def normalize_prior_dict(prior_dict: Dict[str, float]):\n    \"\"\"\n    Normalize the prior weights for the operators so that the weights sum to\n    1 and thus can be directly interpreted/used as probabilities.\n    \"\"\"\n    prior_sum = 0.0\n    for k in prior_dict:\n        prior_sum += prior_dict[k]\n    if prior_sum &gt; 0:\n        for k in prior_dict:\n            prior_dict[k] = prior_dict[k] / prior_sum\n    else:\n        for k in prior_dict:\n            prior_dict[k] = 1 / len(prior_dict)\n</code></pre>"},{"location":"reference/autora/theorist/bsr/node/","title":"autora.theorist.bsr.node","text":""},{"location":"reference/autora/theorist/bsr/node/#autora.theorist.bsr.node.Node","title":"<code>Node</code>","text":"Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/node.py</code> <pre><code>class Node:\n    def __init__(\n        self,\n        depth: int = 0,\n        node_type: NodeType = NodeType.EMPTY,\n        left: Optional[\"Node\"] = None,\n        right: Optional[\"Node\"] = None,\n        parent: Optional[\"Node\"] = None,\n        operator: Optional[Callable] = None,\n        op_name: str = \"\",\n        op_arity: int = 0,\n        op_init: Optional[Callable] = None,\n    ):\n        # tree structure attributes\n        self.depth = depth\n        self.node_type = node_type\n        self.left = left\n        self.right = right\n        self.parent = parent\n\n        # a function that does the actual calculation, see definitions below\n        self.operator = operator\n        self.op_name = op_name\n        self.op_arity = op_arity\n        self.op_init = op_init\n\n        # holding temporary calculation result, see `evaluate()`\n        self.result: Optional[np.ndarray] = None\n        # params for additional inputs into `operator`\n        self.params: Dict = {}\n\n    def _init_param(self, **hyper_params):\n        # init is a function randomized by some hyper-params\n        if callable(self.op_init):\n            self.params = self.op_init(**hyper_params)\n        else:  # init is deterministic dict\n            self.params = self.op_init\n\n    def setup(\n        self, op_name: str = \"\", ops_prior: Dict = {}, feature: int = 0, **hyper_params\n    ):\n        \"\"\"\n        Initialize an uninitialized node with given feature, in the case of a leaf node, or some\n        given operator information, in the case of unary or binary node. The type of the node is\n        determined by the feature/operator assigned to it.\n\n        Arguments:\n            op_name: the operator name, if given\n            ops_prior: the prior dictionary of the given operator\n            feature: the index of the assigned feature, if given\n            hyper_params: hyperparameters for initializing the node\n        \"\"\"\n        self.op_name = op_name\n        self.operator = ops_prior.get(\"fn\", None)\n        self.op_arity = ops_prior.get(\"arity\", 0)\n        self.op_init = ops_prior.get(\"init\", {})\n        self._init_param(**hyper_params)\n\n        if self.op_arity == 0:\n            self.params[\"feature\"] = feature\n            self.node_type = NodeType.LEAF\n        elif self.op_arity == 1:\n            self.left = Node(depth=self.depth + 1, parent=self)\n            self.node_type = NodeType.UNARY\n        elif self.op_arity == 2:\n            self.left = Node(depth=self.depth + 1, parent=self)\n            self.right = Node(depth=self.depth + 1, parent=self)\n            self.node_type = NodeType.BINARY\n        else:\n            raise ValueError(\n                \"operation arity should be either 0, 1, 2; get {} instead\".format(\n                    self.op_arity\n                )\n            )\n\n    def evaluate(\n        self, X: Union[np.ndarray, pd.DataFrame], store_result: bool = False\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate the expression, as represented by an expression tree with `self` as the root,\n        using the given data matrix `X`.\n\n        Arguments:\n            X: the data matrix with each row being a data point and each column a feature\n            store_result: whether to store the result of this calculation\n\n        Return:\n            result: the result of this calculation\n        \"\"\"\n        if X is None:\n            raise TypeError(\"input data X is non-existing\")\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n        if self.node_type == NodeType.LEAF:\n            result = np.array(X.iloc[:, self.params[\"feature\"]]).flatten()\n        elif self.node_type == NodeType.UNARY:\n            assert self.left and self.operator\n            result = self.operator(self.left.evaluate(X), **self.params)\n        elif self.node_type == NodeType.BINARY:\n            assert self.left and self.right and self.operator\n            result = self.operator(\n                self.left.evaluate(X), self.right.evaluate(X), **self.params\n            )\n        else:\n            raise NotImplementedError(\"node evaluated before being setup\")\n        if store_result:\n            self.result = result\n        return result\n\n    def get_expression(\n        self,\n        ops_expr: Optional[Dict[str, str]] = None,\n        feature_names: Optional[List[str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Get a literal (string) expression of the expression tree\n\n        Arguments:\n            ops_expr: the dictionary that maps an operation name to its literal format; if not\n                offered, use the default one in `get_ops_expr()`\n            feature_names: the list of names for the data features\n        Return:\n            a literal expression of the tree\n        \"\"\"\n        if not ops_expr:\n            ops_expr = get_ops_expr()\n        if self.node_type == NodeType.LEAF:\n            if feature_names:\n                return feature_names[self.params[\"feature\"]]\n            else:\n                return f\"x{self.params['feature']}\"\n        elif self.node_type == NodeType.UNARY:\n            # if the expr for an operator is not defined, use placeholder\n            # e.g. operator `cosh` -&gt; `cosh(xxx)`\n            assert self.left\n            place_holder = self.op_name + \"({})\"\n            left_expr = self.left.get_expression(ops_expr, feature_names)\n            expr_fmt = ops_expr.get(self.op_name, place_holder)\n            return expr_fmt.format(left_expr, **self.params)\n        elif self.node_type == NodeType.BINARY:\n            assert self.left and self.right\n            place_holder = self.op_name + \"({})\"\n            left_expr = self.left.get_expression(ops_expr, feature_names)\n            right_expr = self.right.get_expression(ops_expr, feature_names)\n            expr_fmt = ops_expr.get(self.op_name, place_holder)\n            return expr_fmt.format(left_expr, right_expr, **self.params)\n        else:  # empty node\n            return \"(empty node)\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Get a literal (string) representation of a tree `node` data structure.\n        See `get_expression` for more information.\n        \"\"\"\n        return self.get_expression()\n</code></pre>"},{"location":"reference/autora/theorist/bsr/node/#autora.theorist.bsr.node.Node.__str__","title":"<code>__str__()</code>","text":"<p>Get a literal (string) representation of a tree <code>node</code> data structure. See <code>get_expression</code> for more information.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/node.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Get a literal (string) representation of a tree `node` data structure.\n    See `get_expression` for more information.\n    \"\"\"\n    return self.get_expression()\n</code></pre>"},{"location":"reference/autora/theorist/bsr/node/#autora.theorist.bsr.node.Node.evaluate","title":"<code>evaluate(X, store_result=False)</code>","text":"<p>Evaluate the expression, as represented by an expression tree with <code>self</code> as the root, using the given data matrix <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>the data matrix with each row being a data point and each column a feature</p> required <code>store_result</code> <code>bool</code> <p>whether to store the result of this calculation</p> <code>False</code> Return <p>result: the result of this calculation</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/node.py</code> <pre><code>def evaluate(\n    self, X: Union[np.ndarray, pd.DataFrame], store_result: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate the expression, as represented by an expression tree with `self` as the root,\n    using the given data matrix `X`.\n\n    Arguments:\n        X: the data matrix with each row being a data point and each column a feature\n        store_result: whether to store the result of this calculation\n\n    Return:\n        result: the result of this calculation\n    \"\"\"\n    if X is None:\n        raise TypeError(\"input data X is non-existing\")\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    if self.node_type == NodeType.LEAF:\n        result = np.array(X.iloc[:, self.params[\"feature\"]]).flatten()\n    elif self.node_type == NodeType.UNARY:\n        assert self.left and self.operator\n        result = self.operator(self.left.evaluate(X), **self.params)\n    elif self.node_type == NodeType.BINARY:\n        assert self.left and self.right and self.operator\n        result = self.operator(\n            self.left.evaluate(X), self.right.evaluate(X), **self.params\n        )\n    else:\n        raise NotImplementedError(\"node evaluated before being setup\")\n    if store_result:\n        self.result = result\n    return result\n</code></pre>"},{"location":"reference/autora/theorist/bsr/node/#autora.theorist.bsr.node.Node.get_expression","title":"<code>get_expression(ops_expr=None, feature_names=None)</code>","text":"<p>Get a literal (string) expression of the expression tree</p> <p>Parameters:</p> Name Type Description Default <code>ops_expr</code> <code>Optional[Dict[str, str]]</code> <p>the dictionary that maps an operation name to its literal format; if not offered, use the default one in <code>get_ops_expr()</code></p> <code>None</code> <code>feature_names</code> <code>Optional[List[str]]</code> <p>the list of names for the data features</p> <code>None</code> <p>Return:     a literal expression of the tree</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/node.py</code> <pre><code>def get_expression(\n    self,\n    ops_expr: Optional[Dict[str, str]] = None,\n    feature_names: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Get a literal (string) expression of the expression tree\n\n    Arguments:\n        ops_expr: the dictionary that maps an operation name to its literal format; if not\n            offered, use the default one in `get_ops_expr()`\n        feature_names: the list of names for the data features\n    Return:\n        a literal expression of the tree\n    \"\"\"\n    if not ops_expr:\n        ops_expr = get_ops_expr()\n    if self.node_type == NodeType.LEAF:\n        if feature_names:\n            return feature_names[self.params[\"feature\"]]\n        else:\n            return f\"x{self.params['feature']}\"\n    elif self.node_type == NodeType.UNARY:\n        # if the expr for an operator is not defined, use placeholder\n        # e.g. operator `cosh` -&gt; `cosh(xxx)`\n        assert self.left\n        place_holder = self.op_name + \"({})\"\n        left_expr = self.left.get_expression(ops_expr, feature_names)\n        expr_fmt = ops_expr.get(self.op_name, place_holder)\n        return expr_fmt.format(left_expr, **self.params)\n    elif self.node_type == NodeType.BINARY:\n        assert self.left and self.right\n        place_holder = self.op_name + \"({})\"\n        left_expr = self.left.get_expression(ops_expr, feature_names)\n        right_expr = self.right.get_expression(ops_expr, feature_names)\n        expr_fmt = ops_expr.get(self.op_name, place_holder)\n        return expr_fmt.format(left_expr, right_expr, **self.params)\n    else:  # empty node\n        return \"(empty node)\"\n</code></pre>"},{"location":"reference/autora/theorist/bsr/node/#autora.theorist.bsr.node.Node.setup","title":"<code>setup(op_name='', ops_prior={}, feature=0, **hyper_params)</code>","text":"<p>Initialize an uninitialized node with given feature, in the case of a leaf node, or some given operator information, in the case of unary or binary node. The type of the node is determined by the feature/operator assigned to it.</p> <p>Parameters:</p> Name Type Description Default <code>op_name</code> <code>str</code> <p>the operator name, if given</p> <code>''</code> <code>ops_prior</code> <code>Dict</code> <p>the prior dictionary of the given operator</p> <code>{}</code> <code>feature</code> <code>int</code> <p>the index of the assigned feature, if given</p> <code>0</code> <code>hyper_params</code> <p>hyperparameters for initializing the node</p> <code>{}</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/node.py</code> <pre><code>def setup(\n    self, op_name: str = \"\", ops_prior: Dict = {}, feature: int = 0, **hyper_params\n):\n    \"\"\"\n    Initialize an uninitialized node with given feature, in the case of a leaf node, or some\n    given operator information, in the case of unary or binary node. The type of the node is\n    determined by the feature/operator assigned to it.\n\n    Arguments:\n        op_name: the operator name, if given\n        ops_prior: the prior dictionary of the given operator\n        feature: the index of the assigned feature, if given\n        hyper_params: hyperparameters for initializing the node\n    \"\"\"\n    self.op_name = op_name\n    self.operator = ops_prior.get(\"fn\", None)\n    self.op_arity = ops_prior.get(\"arity\", 0)\n    self.op_init = ops_prior.get(\"init\", {})\n    self._init_param(**hyper_params)\n\n    if self.op_arity == 0:\n        self.params[\"feature\"] = feature\n        self.node_type = NodeType.LEAF\n    elif self.op_arity == 1:\n        self.left = Node(depth=self.depth + 1, parent=self)\n        self.node_type = NodeType.UNARY\n    elif self.op_arity == 2:\n        self.left = Node(depth=self.depth + 1, parent=self)\n        self.right = Node(depth=self.depth + 1, parent=self)\n        self.node_type = NodeType.BINARY\n    else:\n        raise ValueError(\n            \"operation arity should be either 0, 1, 2; get {} instead\".format(\n                self.op_arity\n            )\n        )\n</code></pre>"},{"location":"reference/autora/theorist/bsr/node/#autora.theorist.bsr.node.NodeType","title":"<code>NodeType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>-1 represents newly grown node (not decided yet) 0 represents no child, as a terminal node 1 represents one child, 2 represents 2 children</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/node.py</code> <pre><code>class NodeType(Enum):\n    \"\"\"\n    -1 represents newly grown node (not decided yet)\n    0 represents no child, as a terminal node\n    1 represents one child,\n    2 represents 2 children\n    \"\"\"\n\n    EMPTY = -1\n    LEAF = 0\n    UNARY = 1\n    BINARY = 2\n</code></pre>"},{"location":"reference/autora/theorist/bsr/operation/","title":"autora.theorist.bsr.operation","text":""},{"location":"reference/autora/theorist/bsr/prior/","title":"autora.theorist.bsr.prior","text":""},{"location":"reference/autora/theorist/bsr/prior/#autora.theorist.bsr.prior.get_prior_dict","title":"<code>get_prior_dict(prior_name='Uniform')</code>","text":"<p>Get the dictionary of prior information as well as several list of key operator properties</p> Argument <p>prior_name: the name of the prior dictionary to use</p> <p>Returns:</p> Name Type Description <code>ops_name_lst</code> <p>the list of operator names</p> <code>ops_weight_lst</code> <p>the list of operator weights</p> <code>prior_dict</code> <p>the dictionary of operator prior information</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/prior.py</code> <pre><code>def get_prior_dict(prior_name=\"Uniform\"):\n    \"\"\"\n    Get the dictionary of prior information as well as several list of key operator properties\n\n    Argument:\n        prior_name: the name of the prior dictionary to use\n\n    Returns:\n        ops_name_lst: the list of operator names\n        ops_weight_lst: the list of operator weights\n        prior_dict: the dictionary of operator prior information\n    \"\"\"\n    ops_prior = _get_prior(prior_name)\n    ops_init = _get_ops_init()\n    ops_fn_and_arity = _get_ops_with_arity()\n\n    ops_name_lst = list(ops_prior.keys())\n    ops_weight_lst = list(ops_prior.values())\n    prior_dict = {\n        k: {\n            \"init\": ops_init.get(k, {}),\n            \"fn\": ops_fn_and_arity[k][0],\n            \"arity\": ops_fn_and_arity[k][1],\n            \"weight\": ops_prior[k],\n        }\n        for k in ops_prior\n    }\n\n    return ops_name_lst, ops_weight_lst, prior_dict\n</code></pre>"},{"location":"reference/autora/theorist/bsr/prior/#autora.theorist.bsr.prior.get_prior_list","title":"<code>get_prior_list(prior_name='Uniform')</code>","text":"<p>Get a dictionary of key prior properties</p> Argument <p>prior_name: the name of the prior dictionary to use</p> <p>Returns:</p> Type Description <p>a dictionary that maps a prior property (e.g. <code>name</code>) to the list of such properties for each operator.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/prior.py</code> <pre><code>def get_prior_list(prior_name=\"Uniform\"):\n    \"\"\"\n    Get a dictionary of key prior properties\n\n    Argument:\n        prior_name: the name of the prior dictionary to use\n\n    Returns:\n        a dictionary that maps a prior property (e.g. `name`) to the list of such properties\n            for each operator.\n    \"\"\"\n    ops_prior = _get_prior(prior_name)\n    ops_init = _get_ops_init()\n    ops_fn_and_arity = _get_ops_with_arity()\n\n    ops_name_lst = list(ops_prior.keys())\n    ops_weight_lst = list(ops_prior.values())\n    ops_init_lst = [ops_init.get(k, None) for k in ops_name_lst]\n    ops_fn_lst = [ops_fn_and_arity[k][0] for k in ops_name_lst]\n    ops_arity_lst = [ops_fn_and_arity[k][1] for k in ops_name_lst]\n    return {\n        \"name\": ops_name_lst,\n        \"weight\": ops_weight_lst,\n        \"init\": ops_init_lst,\n        \"fn\": ops_fn_lst,\n        \"arity\": ops_arity_lst,\n    }\n</code></pre>"},{"location":"reference/autora/theorist/bsr/prior/#autora.theorist.bsr.prior.linear_init","title":"<code>linear_init(**hyper_params)</code>","text":"<p>Initialization function for the linear operator. Two parameters, slope (a) and intercept (b) are initialized.</p> <p>Parameters:</p> Name Type Description Default <code>hyper_params</code> <p>the dictionary for hyperparameters. Specifically, this function requires <code>sigma_a</code> and <code>sigma_b</code> to be present.</p> <code>{}</code> <p>Returns:     a dictionary with initialized <code>a</code> and <code>b</code> parameters.</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/prior.py</code> <pre><code>def linear_init(**hyper_params) -&gt; Dict:\n    \"\"\"\n    Initialization function for the linear operator. Two parameters, slope\n    (a) and intercept (b) are initialized.\n\n    Arguments:\n        hyper_params: the dictionary for hyperparameters. Specifically, this\n            function requires `sigma_a` and `sigma_b` to be present.\n    Returns:\n        a dictionary with initialized `a` and `b` parameters.\n    \"\"\"\n    sigma_a, sigma_b = hyper_params.get(\"sigma_a\", 1), hyper_params.get(\"sigma_b\", 1)\n    return {\n        \"a\": norm.rvs(loc=1, scale=np.sqrt(sigma_a)),\n        \"b\": norm.rvs(loc=0, scale=np.sqrt(sigma_b)),\n    }\n</code></pre>"},{"location":"reference/autora/theorist/bsr/regressor/","title":"autora.theorist.bsr.regressor","text":""},{"location":"reference/autora/theorist/bsr/regressor/#autora.theorist.bsr.regressor.BSRRegressor","title":"<code>BSRRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Bayesian Symbolic Regression (BSR)</p> <p>A MCMC-sampling-based Bayesian approach to symbolic regression -- a machine learning method that bridges <code>X</code> and <code>y</code> by automatically building up mathematical expressions of basic functions. Performance and speed of <code>BSR</code> depends on pre-defined parameters.</p> <p>This class is intended to be compatible with the Scikit-Learn Estimator API.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; num_samples = 1000\n&gt;&gt;&gt; X = np.linspace(start=0, stop=1, num=num_samples).reshape(-1, 1)\n&gt;&gt;&gt; y = np.sqrt(X)\n&gt;&gt;&gt; estimator = BSRRegressor()\n&gt;&gt;&gt; estimator = estimator.fit(X, y)\n&gt;&gt;&gt; estimator.predict([[1.5]])\n</code></pre> <p>Attributes:</p> Name Type Description <code>roots_</code> <code>Optional[List[List[Node]]]</code> <p>the root(s) of the best-fit symbolic regression (SR) tree(s)</p> <code>betas_</code> <code>Optional[List[ndarray]]</code> <p>the beta parameters of the best-fit model</p> <code>train_errs_</code> <code>Optional[List[List[float]]]</code> <p>the training losses associated with the best-fit model</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/regressor.py</code> <pre><code>class BSRRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Bayesian Symbolic Regression (BSR)\n\n    A MCMC-sampling-based Bayesian approach to symbolic regression -- a machine learning method\n    that bridges `X` and `y` by automatically building up mathematical expressions of basic\n    functions. Performance and speed of `BSR` depends on pre-defined parameters.\n\n    This class is intended to be compatible with the\n    [Scikit-Learn Estimator API](https://scikit-learn.org/stable/developers/develop.html).\n\n    Examples:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; num_samples = 1000\n        &gt;&gt;&gt; X = np.linspace(start=0, stop=1, num=num_samples).reshape(-1, 1)\n        &gt;&gt;&gt; y = np.sqrt(X)\n        &gt;&gt;&gt; estimator = BSRRegressor()\n        &gt;&gt;&gt; estimator = estimator.fit(X, y)\n        &gt;&gt;&gt; estimator.predict([[1.5]])\n\n    Attributes:\n        roots_: the root(s) of the best-fit symbolic regression (SR) tree(s)\n        betas_: the beta parameters of the best-fit model\n        train_errs_: the training losses associated with the best-fit model\n    \"\"\"\n\n    def __init__(\n        self,\n        tree_num: int = 3,\n        itr_num: int = 5000,\n        alpha1: float = 0.4,\n        alpha2: float = 0.4,\n        beta: float = -1,\n        show_log: bool = False,\n        val: int = 100,\n        last_idx: int = -1,\n        prior_name: str = \"Uniform\",\n    ):\n        \"\"\"\n        Arguments:\n            tree_num: pre-specified number of SR trees to fit in the model\n            itr_num: number of iterations steps to run for the model fitting process\n            alpha1, alpha2, beta: the hyper-parameters of priors\n            show_log: whether to output certain logging info\n            val: number of validation steps to run for each iteration step\n            last_idx: the index of which latest (most best-fit) model to use\n                (-1 means the latest one)\n        \"\"\"\n        self.tree_num = tree_num\n        self.itr_num = itr_num\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.beta = beta\n        self.show_log = show_log\n        self.val = val\n        self.last_idx = last_idx\n        self.prior_name = prior_name\n\n        # attributes that are not set until `fit`\n        self.roots_: Optional[List[List[Node]]] = None\n        self.betas_: Optional[List[np.ndarray]] = None\n        self.train_errs_: Optional[List[List[float]]] = None\n\n        self.X_: Optional[Union[np.ndarray, pd.DataFrame]] = None\n        self.y_: Optional[Union[np.ndarray, pd.DataFrame]] = None\n\n    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        \"\"\"\n        Applies the fitted model to a set of independent variables `X`,\n        to give predictions for the dependent variable `y`.\n\n        Arguments:\n            X: independent variables in an n-dimensional array\n        Returns:\n            y: predicted dependent variable values\n        \"\"\"\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n\n        check_is_fitted(self, attributes=[\"roots_\"])\n\n        k = self.tree_num\n        n_test = X.shape[0]\n        tree_outs = np.zeros((n_test, k))\n\n        assert self.roots_ and self.betas_\n        for i in np.arange(k):\n            tree_out = self.roots_[-self.last_idx][i].evaluate(X)\n            tree_out.shape = tree_out.shape[0]\n            tree_outs[:, i] = tree_out\n\n        ones = np.ones((n_test, 1))\n        tree_outs = np.concatenate((ones, tree_outs), axis=1)\n        _beta = self.betas_[-self.last_idx]\n        output = np.matmul(tree_outs, _beta)\n\n        return output\n\n    def fit(\n        self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.DataFrame]\n    ):\n        \"\"\"\n        Runs the optimization for a given set of `X`s and `y`s.\n\n        Arguments:\n            X: independent variables in an n-dimensional array\n            y: dependent variables in an n-dimensional array\n        Returns:\n            self (BSR): the fitted estimator\n        \"\"\"\n        # train_data must be a dataframe\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n        train_errs: List[List[float]] = []\n        roots: List[List[Node]] = []\n        betas: List[np.ndarray] = []\n        itr_num = self.itr_num\n        k = self.tree_num\n        beta = self.beta\n\n        if self.show_log:\n            _logger.info(\"Starting training\")\n        while len(train_errs) &lt; itr_num:\n            n_feature = X.shape[1]\n            n_train = X.shape[0]\n\n            ops_name_lst, ops_weight_lst, ops_priors = get_prior_dict(\n                prior_name=self.prior_name\n            )\n\n            # List of tree samples\n            root_lists: List[List[Node]] = [[] for _ in range(k)]\n\n            sigma_a_list = []  # List of sigma_a, for each component tree\n            sigma_b_list = []  # List of sigma_b, for each component tree\n\n            sigma_y = invgamma.rvs(1)  # for output y\n\n            # Initialization\n            for count in np.arange(k):\n                # create a new root node\n                root = Node(0)\n                sigma_a = invgamma.rvs(1)\n                sigma_b = invgamma.rvs(1)\n\n                # grow a tree from the root node\n                if self.show_log:\n                    _logger.info(\"Grow a tree from the root node\")\n\n                grow(\n                    root,\n                    ops_name_lst,\n                    ops_weight_lst,\n                    ops_priors,\n                    n_feature,\n                    sigma_a=sigma_a,\n                    sigma_b=sigma_b,\n                )\n\n                # put the root into list\n                root_lists[count].append(root)\n                sigma_a_list.append(sigma_a)\n                sigma_b_list.append(sigma_b)\n\n            # calculate beta\n            if self.show_log:\n                _logger.info(\"Calculate beta\")\n            # added a constant in the regression by fwl\n            tree_outputs = np.zeros((n_train, k))\n\n            for count in np.arange(k):\n                temp = root_lists[count][-1].evaluate(X)\n                temp.shape = temp.shape[0]\n                tree_outputs[:, count] = temp\n\n            constant = np.ones((n_train, 1))  # added a constant\n            tree_outputs = np.concatenate((constant, tree_outputs), axis=1)\n            scale = np.max(np.abs(tree_outputs))\n            tree_outputs = tree_outputs / scale\n            epsilon = (\n                np.eye(tree_outputs.shape[1]) * 1e-6\n            )  # add to the matrix to prevent singular matrrix\n            yy = np.array(y)\n            yy.shape = (yy.shape[0], 1)\n            _beta = np.linalg.inv(\n                np.matmul(tree_outputs.transpose(), tree_outputs) + epsilon\n            )\n            _beta = np.matmul(_beta, np.matmul(tree_outputs.transpose(), yy))\n            output = np.matmul(tree_outputs, _beta)\n            # rescale the beta, above we scale tree_outputs for calculation by fwl\n            _beta /= scale\n\n            total = 0\n            accepted = 0\n            errs = []\n            total_list = []\n\n            tic = time.time()\n\n            if self.show_log:\n                _logger.info(\"While total &lt; \", self.val)\n            while total &lt; self.val:\n                switch_label = False\n                for count in range(k):\n                    curr_roots = []  # list of current components\n                    for i in np.arange(k):\n                        curr_roots.append(root_lists[i][-1])\n                    # pick the root to be changed\n                    sigma_a = sigma_a_list[count]\n                    sigma_b = sigma_b_list[count]\n\n                    # the returned root is a new copy\n                    if self.show_log:\n                        _logger.info(\"new_prop...\")\n                    res, root, sigma_y, sigma_a, sigma_b = prop_new(\n                        curr_roots,\n                        count,\n                        sigma_y,\n                        beta,\n                        sigma_a,\n                        sigma_b,\n                        X,\n                        y,\n                        ops_name_lst,\n                        ops_weight_lst,\n                        ops_priors,\n                    )\n                    if self.show_log:\n                        _logger.info(\"res:\", res)\n                        print(root)\n\n                    total += 1\n                    # update sigma_a and sigma_b\n                    sigma_a_list[count] = sigma_a\n                    sigma_b_list[count] = sigma_b\n\n                    if res:\n                        # flag = False\n                        accepted += 1\n                        # record newly accepted root\n                        root_lists[count].append(copy.deepcopy(root))\n\n                        tree_outputs = np.zeros((n_train, k))\n\n                        for i in np.arange(k):\n                            temp = root_lists[count][-1].evaluate(X)\n                            temp.shape = temp.shape[0]\n                            tree_outputs[:, i] = temp\n\n                        constant = np.ones((n_train, 1))\n                        tree_outputs = np.concatenate((constant, tree_outputs), axis=1)\n                        scale = np.max(np.abs(tree_outputs))\n                        tree_outputs = tree_outputs / scale\n                        epsilon = (\n                            np.eye(tree_outputs.shape[1]) * 1e-6\n                        )  # add to prevent singular matrix\n                        yy = np.array(y)\n                        yy.shape = (yy.shape[0], 1)\n                        _beta = np.linalg.inv(\n                            np.matmul(tree_outputs.transpose(), tree_outputs) + epsilon\n                        )\n                        _beta = np.matmul(\n                            _beta, np.matmul(tree_outputs.transpose(), yy)\n                        )\n\n                        output = np.matmul(tree_outputs, _beta)\n                        # rescale the beta, above we scale tree_outputs for calculation\n                        _beta /= scale\n\n                        error = 0\n                        for i in np.arange(n_train):\n                            error += (output[i, 0] - y[i]) * (output[i, 0] - y[i])\n\n                        rmse = np.sqrt(error / n_train)\n                        errs.append(rmse)\n\n                        total_list.append(total)\n                        total = 0\n\n                    if len(errs) &gt; 100:\n                        lapses = min(10, len(errs))\n                        converge_ratio = 1 - np.min(errs[-lapses:]) / np.mean(\n                            errs[-lapses:]\n                        )\n                        if converge_ratio &lt; 0.05:\n                            # converged\n                            switch_label = True\n                            break\n                if switch_label:\n                    break\n\n            if self.show_log:\n                for i in np.arange(0, len(y)):\n                    _logger.info(output[i, 0], y[i])\n\n            toc = time.time()\n            tictoc = toc - tic\n            if self.show_log:\n                _logger.info(\"Run time: {:.2f}s\".format(tictoc))\n\n                _logger.info(\"------\")\n                _logger.info(\n                    \"Mean rmse of last 5 accepts: {}\".format(np.mean(errs[-6:-1]))\n                )\n\n            train_errs.append(errs)\n            roots.append(curr_roots)\n            betas.append(_beta)\n\n        self.roots_ = roots\n        self.train_errs_ = train_errs\n        self.betas_ = betas\n        self.X_, self.y_ = X, y\n        return self\n\n    def _model(self, last_ind: int = 1) -&gt; List[str]:\n        \"\"\"\n        Return the models in the last-i-th iteration, default `last_ind = 1` refers to the\n        last (final) iteration.\n        \"\"\"\n        models = []\n        assert self.roots_\n        for i in range(self.tree_num):\n            models.append(self.roots_[-last_ind][i].get_expression())\n        return models\n\n    def _complexity(self) -&gt; int:\n        \"\"\"\n        Return the complexity of the final models, which equals to the sum of nodes in all\n        expression trees.\n        \"\"\"\n        cp = 0\n        assert self.roots_\n        for i in range(self.tree_num):\n            root_node = self.roots_[-1][i]\n            num = len(get_all_nodes(root_node))\n            cp = cp + num\n        return cp\n</code></pre>"},{"location":"reference/autora/theorist/bsr/regressor/#autora.theorist.bsr.regressor.BSRRegressor.__init__","title":"<code>__init__(tree_num=3, itr_num=5000, alpha1=0.4, alpha2=0.4, beta=-1, show_log=False, val=100, last_idx=-1, prior_name='Uniform')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tree_num</code> <code>int</code> <p>pre-specified number of SR trees to fit in the model</p> <code>3</code> <code>itr_num</code> <code>int</code> <p>number of iterations steps to run for the model fitting process</p> <code>5000</code> <code>alpha1,</code> <code>(alpha2, beta)</code> <p>the hyper-parameters of priors</p> required <code>show_log</code> <code>bool</code> <p>whether to output certain logging info</p> <code>False</code> <code>val</code> <code>int</code> <p>number of validation steps to run for each iteration step</p> <code>100</code> <code>last_idx</code> <code>int</code> <p>the index of which latest (most best-fit) model to use (-1 means the latest one)</p> <code>-1</code> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/regressor.py</code> <pre><code>def __init__(\n    self,\n    tree_num: int = 3,\n    itr_num: int = 5000,\n    alpha1: float = 0.4,\n    alpha2: float = 0.4,\n    beta: float = -1,\n    show_log: bool = False,\n    val: int = 100,\n    last_idx: int = -1,\n    prior_name: str = \"Uniform\",\n):\n    \"\"\"\n    Arguments:\n        tree_num: pre-specified number of SR trees to fit in the model\n        itr_num: number of iterations steps to run for the model fitting process\n        alpha1, alpha2, beta: the hyper-parameters of priors\n        show_log: whether to output certain logging info\n        val: number of validation steps to run for each iteration step\n        last_idx: the index of which latest (most best-fit) model to use\n            (-1 means the latest one)\n    \"\"\"\n    self.tree_num = tree_num\n    self.itr_num = itr_num\n    self.alpha1 = alpha1\n    self.alpha2 = alpha2\n    self.beta = beta\n    self.show_log = show_log\n    self.val = val\n    self.last_idx = last_idx\n    self.prior_name = prior_name\n\n    # attributes that are not set until `fit`\n    self.roots_: Optional[List[List[Node]]] = None\n    self.betas_: Optional[List[np.ndarray]] = None\n    self.train_errs_: Optional[List[List[float]]] = None\n\n    self.X_: Optional[Union[np.ndarray, pd.DataFrame]] = None\n    self.y_: Optional[Union[np.ndarray, pd.DataFrame]] = None\n</code></pre>"},{"location":"reference/autora/theorist/bsr/regressor/#autora.theorist.bsr.regressor.BSRRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Runs the optimization for a given set of <code>X</code>s and <code>y</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>independent variables in an n-dimensional array</p> required <code>y</code> <code>Union[ndarray, DataFrame]</code> <p>dependent variables in an n-dimensional array</p> required <p>Returns:     self (BSR): the fitted estimator</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/regressor.py</code> <pre><code>def fit(\n    self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.DataFrame]\n):\n    \"\"\"\n    Runs the optimization for a given set of `X`s and `y`s.\n\n    Arguments:\n        X: independent variables in an n-dimensional array\n        y: dependent variables in an n-dimensional array\n    Returns:\n        self (BSR): the fitted estimator\n    \"\"\"\n    # train_data must be a dataframe\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n    train_errs: List[List[float]] = []\n    roots: List[List[Node]] = []\n    betas: List[np.ndarray] = []\n    itr_num = self.itr_num\n    k = self.tree_num\n    beta = self.beta\n\n    if self.show_log:\n        _logger.info(\"Starting training\")\n    while len(train_errs) &lt; itr_num:\n        n_feature = X.shape[1]\n        n_train = X.shape[0]\n\n        ops_name_lst, ops_weight_lst, ops_priors = get_prior_dict(\n            prior_name=self.prior_name\n        )\n\n        # List of tree samples\n        root_lists: List[List[Node]] = [[] for _ in range(k)]\n\n        sigma_a_list = []  # List of sigma_a, for each component tree\n        sigma_b_list = []  # List of sigma_b, for each component tree\n\n        sigma_y = invgamma.rvs(1)  # for output y\n\n        # Initialization\n        for count in np.arange(k):\n            # create a new root node\n            root = Node(0)\n            sigma_a = invgamma.rvs(1)\n            sigma_b = invgamma.rvs(1)\n\n            # grow a tree from the root node\n            if self.show_log:\n                _logger.info(\"Grow a tree from the root node\")\n\n            grow(\n                root,\n                ops_name_lst,\n                ops_weight_lst,\n                ops_priors,\n                n_feature,\n                sigma_a=sigma_a,\n                sigma_b=sigma_b,\n            )\n\n            # put the root into list\n            root_lists[count].append(root)\n            sigma_a_list.append(sigma_a)\n            sigma_b_list.append(sigma_b)\n\n        # calculate beta\n        if self.show_log:\n            _logger.info(\"Calculate beta\")\n        # added a constant in the regression by fwl\n        tree_outputs = np.zeros((n_train, k))\n\n        for count in np.arange(k):\n            temp = root_lists[count][-1].evaluate(X)\n            temp.shape = temp.shape[0]\n            tree_outputs[:, count] = temp\n\n        constant = np.ones((n_train, 1))  # added a constant\n        tree_outputs = np.concatenate((constant, tree_outputs), axis=1)\n        scale = np.max(np.abs(tree_outputs))\n        tree_outputs = tree_outputs / scale\n        epsilon = (\n            np.eye(tree_outputs.shape[1]) * 1e-6\n        )  # add to the matrix to prevent singular matrrix\n        yy = np.array(y)\n        yy.shape = (yy.shape[0], 1)\n        _beta = np.linalg.inv(\n            np.matmul(tree_outputs.transpose(), tree_outputs) + epsilon\n        )\n        _beta = np.matmul(_beta, np.matmul(tree_outputs.transpose(), yy))\n        output = np.matmul(tree_outputs, _beta)\n        # rescale the beta, above we scale tree_outputs for calculation by fwl\n        _beta /= scale\n\n        total = 0\n        accepted = 0\n        errs = []\n        total_list = []\n\n        tic = time.time()\n\n        if self.show_log:\n            _logger.info(\"While total &lt; \", self.val)\n        while total &lt; self.val:\n            switch_label = False\n            for count in range(k):\n                curr_roots = []  # list of current components\n                for i in np.arange(k):\n                    curr_roots.append(root_lists[i][-1])\n                # pick the root to be changed\n                sigma_a = sigma_a_list[count]\n                sigma_b = sigma_b_list[count]\n\n                # the returned root is a new copy\n                if self.show_log:\n                    _logger.info(\"new_prop...\")\n                res, root, sigma_y, sigma_a, sigma_b = prop_new(\n                    curr_roots,\n                    count,\n                    sigma_y,\n                    beta,\n                    sigma_a,\n                    sigma_b,\n                    X,\n                    y,\n                    ops_name_lst,\n                    ops_weight_lst,\n                    ops_priors,\n                )\n                if self.show_log:\n                    _logger.info(\"res:\", res)\n                    print(root)\n\n                total += 1\n                # update sigma_a and sigma_b\n                sigma_a_list[count] = sigma_a\n                sigma_b_list[count] = sigma_b\n\n                if res:\n                    # flag = False\n                    accepted += 1\n                    # record newly accepted root\n                    root_lists[count].append(copy.deepcopy(root))\n\n                    tree_outputs = np.zeros((n_train, k))\n\n                    for i in np.arange(k):\n                        temp = root_lists[count][-1].evaluate(X)\n                        temp.shape = temp.shape[0]\n                        tree_outputs[:, i] = temp\n\n                    constant = np.ones((n_train, 1))\n                    tree_outputs = np.concatenate((constant, tree_outputs), axis=1)\n                    scale = np.max(np.abs(tree_outputs))\n                    tree_outputs = tree_outputs / scale\n                    epsilon = (\n                        np.eye(tree_outputs.shape[1]) * 1e-6\n                    )  # add to prevent singular matrix\n                    yy = np.array(y)\n                    yy.shape = (yy.shape[0], 1)\n                    _beta = np.linalg.inv(\n                        np.matmul(tree_outputs.transpose(), tree_outputs) + epsilon\n                    )\n                    _beta = np.matmul(\n                        _beta, np.matmul(tree_outputs.transpose(), yy)\n                    )\n\n                    output = np.matmul(tree_outputs, _beta)\n                    # rescale the beta, above we scale tree_outputs for calculation\n                    _beta /= scale\n\n                    error = 0\n                    for i in np.arange(n_train):\n                        error += (output[i, 0] - y[i]) * (output[i, 0] - y[i])\n\n                    rmse = np.sqrt(error / n_train)\n                    errs.append(rmse)\n\n                    total_list.append(total)\n                    total = 0\n\n                if len(errs) &gt; 100:\n                    lapses = min(10, len(errs))\n                    converge_ratio = 1 - np.min(errs[-lapses:]) / np.mean(\n                        errs[-lapses:]\n                    )\n                    if converge_ratio &lt; 0.05:\n                        # converged\n                        switch_label = True\n                        break\n            if switch_label:\n                break\n\n        if self.show_log:\n            for i in np.arange(0, len(y)):\n                _logger.info(output[i, 0], y[i])\n\n        toc = time.time()\n        tictoc = toc - tic\n        if self.show_log:\n            _logger.info(\"Run time: {:.2f}s\".format(tictoc))\n\n            _logger.info(\"------\")\n            _logger.info(\n                \"Mean rmse of last 5 accepts: {}\".format(np.mean(errs[-6:-1]))\n            )\n\n        train_errs.append(errs)\n        roots.append(curr_roots)\n        betas.append(_beta)\n\n    self.roots_ = roots\n    self.train_errs_ = train_errs\n    self.betas_ = betas\n    self.X_, self.y_ = X, y\n    return self\n</code></pre>"},{"location":"reference/autora/theorist/bsr/regressor/#autora.theorist.bsr.regressor.BSRRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Applies the fitted model to a set of independent variables <code>X</code>, to give predictions for the dependent variable <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>independent variables in an n-dimensional array</p> required <p>Returns:     y: predicted dependent variable values</p> Source code in <code>temp_dir/bsr/src/autora/theorist/bsr/regressor.py</code> <pre><code>def predict(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"\n    Applies the fitted model to a set of independent variables `X`,\n    to give predictions for the dependent variable `y`.\n\n    Arguments:\n        X: independent variables in an n-dimensional array\n    Returns:\n        y: predicted dependent variable values\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X)\n\n    check_is_fitted(self, attributes=[\"roots_\"])\n\n    k = self.tree_num\n    n_test = X.shape[0]\n    tree_outs = np.zeros((n_test, k))\n\n    assert self.roots_ and self.betas_\n    for i in np.arange(k):\n        tree_out = self.roots_[-self.last_idx][i].evaluate(X)\n        tree_out.shape = tree_out.shape[0]\n        tree_outs[:, i] = tree_out\n\n    ones = np.ones((n_test, 1))\n    tree_outs = np.concatenate((ones, tree_outs), axis=1)\n    _beta = self.betas_[-self.last_idx]\n    output = np.matmul(tree_outs, _beta)\n\n    return output\n</code></pre>"},{"location":"reference/autora/theorist/darts/","title":"autora.theorist.darts","text":""},{"location":"reference/autora/theorist/darts/architect/","title":"autora.theorist.darts.architect","text":""},{"location":"reference/autora/theorist/darts/architect/#autora.theorist.darts.architect.Architect","title":"<code>Architect</code>","text":"<p>               Bases: <code>object</code></p> <p>A learner operating on the architecture weights of a DARTS model. This learner handles training the weights associated with mixture operations (architecture weights).</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/architect.py</code> <pre><code>class Architect(object):\n    \"\"\"\n    A learner operating on the architecture weights of a DARTS model.\n    This learner handles training the weights associated with mixture operations\n    (architecture weights).\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Network,\n        arch_learning_rate_max: float,\n        arch_momentum: float,\n        arch_weight_decay: float,\n        arch_weight_decay_df: float = 0,\n        arch_weight_decay_base: float = 0,\n        fair_darts_loss_weight: float = 1,\n    ):\n        \"\"\"\n        Initializes the architecture learner.\n\n        Arguments:\n            model: a network model implementing the full DARTS model.\n            arch_learning_rate_max: learning rate for the architecture weights\n            arch_momentum: arch_momentum used in the Adam optimizer for architecture weights\n            arch_weight_decay: general weight decay for the architecture weights\n            arch_weight_decay_df: (weight decay applied to architecture weights in proportion\n                to the number of parameters of an operation)\n            arch_weight_decay_base: (a constant weight decay applied to architecture weights)\n            fair_darts_loss_weight: (a regularizer that pushes architecture weights more toward\n                zero or one in the fair DARTS variant)\n        \"\"\"\n        # set parameters for architecture learning\n        self.network_arch_momentum = arch_momentum\n        self.network_weight_decay = arch_weight_decay\n        self.network_weight_decay_df = arch_weight_decay_df\n        self.arch_weight_decay_base = arch_weight_decay_base * model._steps\n        self.fair_darts_loss_weight = fair_darts_loss_weight\n\n        self.model = model\n        self.lr = arch_learning_rate_max\n        # architecture is optimized using Adam\n        self.optimizer = torch.optim.Adam(\n            self.model.arch_parameters(),\n            lr=arch_learning_rate_max,\n            betas=(0.5, 0.999),\n            weight_decay=arch_weight_decay,\n        )\n\n        # initialize weight decay matrix\n        self._init_decay_weights()\n\n        # initialize the logged loss\n        self.current_loss = 0\n\n    def _init_decay_weights(self):\n        \"\"\"\n        This function initializes the weight decay matrix. The weight decay matrix\n        is subtracted from the architecture weight matrix on every learning step. The matrix\n        specifies a weight decay which is proportional to the number of parameters used in an\n        operation.\n        \"\"\"\n        n_params = list()\n        for operation in self.model.cells._ops[0]._ops:\n            if isiterable(operation):\n                n_params_total = (\n                    1  # any non-zero operation is counted as an additional parameter\n                )\n                for subop in operation:\n                    for parameter in subop.parameters():\n                        if parameter.requires_grad is True:\n                            n_params_total += parameter.data.numel()\n            else:\n                n_params_total = 0  # no operation gets zero parameters\n            n_params.append(n_params_total)\n\n        self.decay_weights = Variable(\n            torch.zeros(self.model.arch_parameters()[0].data.shape)\n        )\n        for idx, param in enumerate(n_params):\n            if param &gt; 0:\n                self.decay_weights[:, idx] = (\n                    param * self.network_weight_decay_df + self.arch_weight_decay_base\n                )\n            else:\n                self.decay_weights[:, idx] = param\n        self.decay_weights = self.decay_weights\n        self.decay_weights = self.decay_weights.data\n\n    def _compute_unrolled_model(\n        self,\n        input: torch.Tensor,\n        target: torch.Tensor,\n        eta: float,\n        network_optimizer: torch.optim.Optimizer,\n    ):\n        \"\"\"\n        Helper function used to compute the approximate architecture gradient.\n\n        Arguments:\n            input: input patterns\n            target: target patterns\n            eta: learning rate\n            network_optimizer: optimizer used to updating the architecture weights\n\n        Returns:\n            unrolled_model: the unrolled architecture\n        \"\"\"\n        loss = self.model._loss(input, target)\n        theta = _concat(self.model.parameters()).data\n        try:\n            moment = _concat(\n                network_optimizer.state[v][\"momentum_buffer\"]\n                for v in self.model.parameters()\n            ).mul_(self.network_arch_momentum)\n        except Exception:\n            moment = torch.zeros_like(theta)\n        dtheta = (\n            _concat(torch.autograd.grad(loss, self.model.parameters())).data\n            + self.network_weight_decay * theta\n        )\n        unrolled_model = self._construct_model_from_theta(\n            theta.sub(eta, moment + dtheta)\n        )\n        return unrolled_model\n\n    def step(\n        self,\n        input_valid: torch.Tensor,\n        target_valid: torch.Tensor,\n        network_optimizer: torch.optim.Optimizer,\n        unrolled: bool,\n        input_train: Optional[torch.Tensor] = None,\n        target_train: Optional[torch.Tensor] = None,\n        eta: float = 1,\n    ):\n        \"\"\"\n        Updates the architecture parameters for one training iteration\n\n        Arguments:\n            input_valid: input patterns for validation set\n            target_valid: target patterns for validation set\n            network_optimizer: optimizer used to updating the architecture weights\n            unrolled: whether to use the unrolled architecture or not (i.e., whether to use\n                the approximate architecture gradient or not)\n            input_train: input patterns for training set\n            target_train: target patterns for training set\n            eta: learning rate for the architecture weights\n        \"\"\"\n\n        # input_train, target_train only needed for approximation (unrolled=True)\n        # of architecture gradient\n        # when performing a single weigh update\n\n        # initialize gradients to be zero\n        self.optimizer.zero_grad()\n        # use different backward step depending on whether to use\n        # 2nd order approximation for gradient update\n        if unrolled:  # probably using eta of parameter update here\n            self._backward_step_unrolled(\n                input_train,\n                target_train,\n                input_valid,\n                target_valid,\n                eta,\n                network_optimizer,\n            )\n        else:\n            self._backward_step(input_valid, target_valid)\n        # move Adam one step\n        self.optimizer.step()\n\n    # backward step (using non-approximate architecture gradient, i.e., full training)\n    def _backward_step(self, input_valid: torch.Tensor, target_valid: torch.Tensor):\n        \"\"\"\n        Computes the loss and updates the architecture weights assuming full optimization\n        of coefficients for the current architecture.\n\n        Arguments:\n            input_valid: input patterns for validation set\n            target_valid: target patterns for validation set\n        \"\"\"\n        if self.model.DARTS_type == DARTSType.ORIGINAL:\n            loss = self.model._loss(input_valid, target_valid)\n        elif self.model.DARTS_type == DARTSType.FAIR:\n            loss1 = self.model._loss(input_valid, target_valid)\n            loss2 = -F.mse_loss(\n                torch.sigmoid(self.model.alphas_normal),\n                0.5 * torch.ones(self.model.alphas_normal.shape, requires_grad=False),\n            )  # torch.tensor(0.5, requires_grad=False)\n            loss = loss1 + self.fair_darts_loss_weight * loss2\n        else:\n            raise Exception(\n                \"DARTS Type \" + str(self.model.DARTS_type) + \" not implemented\"\n            )\n\n        loss.backward()\n        self.current_loss = loss.item()\n\n        # weight decay proportional to degrees of freedom\n        for p in self.model.arch_parameters():\n            p.data.sub_((self.decay_weights * self.lr))  # weight decay\n\n    # backward pass using second order approximation\n    def _backward_step_unrolled(\n        self,\n        input_train: torch.Tensor,\n        target_train: torch.Tensor,\n        input_valid: torch.Tensor,\n        target_valid: torch.Tensor,\n        eta: float,\n        network_optimizer: torch.optim.Optimizer,\n    ):\n        \"\"\"\n        Computes the loss and updates the architecture weights using the approximate architecture\n        gradient.\n\n        Arguments:\n            input_train: input patterns for training set\n            target_train: target patterns for training set\n            input_valid: input patterns for validation set\n            target_valid: target patterns for validation set\n            eta: learning rate\n            network_optimizer: optimizer used to updating the architecture weights\n\n        \"\"\"\n\n        # gets the model\n        unrolled_model = self._compute_unrolled_model(\n            input_train, target_train, eta, network_optimizer\n        )\n\n        if self.model.DARTS_type == DARTSType.ORIGINAL:\n            unrolled_loss = unrolled_model._loss(input_valid, target_valid)\n        elif self.model.DARTS_type == DARTSType.FAIR:\n            loss1 = self.model._loss(input_valid, target_valid)\n            loss2 = -F.mse_loss(\n                torch.sigmoid(self.model.alphas_normal),\n                torch.tensor(0.5, requires_grad=False),\n            )\n            unrolled_loss = loss1 + self.fair_darts_loss_weight * loss2\n        else:\n            raise Exception(\n                \"DARTS Type \" + str(self.model.DARTS_type) + \" not implemented\"\n            )\n\n        unrolled_loss.backward()\n        dalpha = [v.grad for v in unrolled_model.arch_parameters()]\n        vector = [v.grad.data for v in unrolled_model.parameters()]\n        implicit_grads = self._hessian_vector_product(vector, input_train, target_train)\n\n        for g, ig in zip(dalpha, implicit_grads):\n            g.data.sub_(eta, ig.data)\n\n        for v, g in zip(self.model.arch_parameters(), dalpha):\n            if v.grad is None:\n                v.grad = Variable(g.data)\n            else:\n                v.grad.data.copy_(g.data)\n\n    def _construct_model_from_theta(self, theta: torch.Tensor):\n        \"\"\"\n        Helper function used to compute the approximate gradient update\n        for the architecture weights.\n\n        Arguments:\n            theta: term used to compute approximate gradient update\n\n        \"\"\"\n        model_new = self.model.new()\n        model_dict = self.model.state_dict()\n\n        params, offset = {}, 0\n        for k, v in self.model.named_parameters():\n            v_length = np.prod(v.size())\n            params[k] = theta[offset : (offset + v_length)].view(v.size())\n            offset += v_length\n\n        assert offset == len(theta)\n        model_dict.update(params)\n        model_new.load_state_dict(model_dict)\n        return model_new  # .cuda() # Edit SM 10/26/19: uncommented for cuda\n\n    # second order approximation of architecture gradient (see Eqn. 8 from Liu et al, 2019)\n    def _hessian_vector_product(\n        self, vector: torch.Tensor, input: torch.Tensor, target: torch.Tensor, r=1e-2\n    ):\n        \"\"\"\n        Helper function used to compute the approximate gradient update\n        for the architecture weights. It computes the hessian vector product outlined in Eqn. 8\n        from Liu et al, 2019.\n\n        Arguments:\n            vector: input vector\n            input: input patterns\n            target: target patterns\n            r: coefficient used to compute the hessian vector product\n\n        \"\"\"\n        R = r / _concat(vector).norm()\n        for p, v in zip(self.model.parameters(), vector):\n            p.data.add_(R, v)\n        loss = self.model._loss(input, target)\n        grads_p = torch.autograd.grad(loss, self.model.arch_parameters())\n\n        for p, v in zip(self.model.parameters(), vector):\n            p.data.sub_(2 * R, v)\n        loss = self.model._loss(input, target)\n        grads_n = torch.autograd.grad(loss, self.model.arch_parameters())\n\n        for p, v in zip(self.model.parameters(), vector):\n            p.data.add_(R, v)\n\n        # this implements Eqn. 8 from Liu et al. (2019)\n        return [(x - y).div_(2 * R) for x, y in zip(grads_p, grads_n)]\n</code></pre>"},{"location":"reference/autora/theorist/darts/architect/#autora.theorist.darts.architect.Architect.__init__","title":"<code>__init__(model, arch_learning_rate_max, arch_momentum, arch_weight_decay, arch_weight_decay_df=0, arch_weight_decay_base=0, fair_darts_loss_weight=1)</code>","text":"<p>Initializes the architecture learner.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Network</code> <p>a network model implementing the full DARTS model.</p> required <code>arch_learning_rate_max</code> <code>float</code> <p>learning rate for the architecture weights</p> required <code>arch_momentum</code> <code>float</code> <p>arch_momentum used in the Adam optimizer for architecture weights</p> required <code>arch_weight_decay</code> <code>float</code> <p>general weight decay for the architecture weights</p> required <code>arch_weight_decay_df</code> <code>float</code> <p>(weight decay applied to architecture weights in proportion to the number of parameters of an operation)</p> <code>0</code> <code>arch_weight_decay_base</code> <code>float</code> <p>(a constant weight decay applied to architecture weights)</p> <code>0</code> <code>fair_darts_loss_weight</code> <code>float</code> <p>(a regularizer that pushes architecture weights more toward zero or one in the fair DARTS variant)</p> <code>1</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/architect.py</code> <pre><code>def __init__(\n    self,\n    model: Network,\n    arch_learning_rate_max: float,\n    arch_momentum: float,\n    arch_weight_decay: float,\n    arch_weight_decay_df: float = 0,\n    arch_weight_decay_base: float = 0,\n    fair_darts_loss_weight: float = 1,\n):\n    \"\"\"\n    Initializes the architecture learner.\n\n    Arguments:\n        model: a network model implementing the full DARTS model.\n        arch_learning_rate_max: learning rate for the architecture weights\n        arch_momentum: arch_momentum used in the Adam optimizer for architecture weights\n        arch_weight_decay: general weight decay for the architecture weights\n        arch_weight_decay_df: (weight decay applied to architecture weights in proportion\n            to the number of parameters of an operation)\n        arch_weight_decay_base: (a constant weight decay applied to architecture weights)\n        fair_darts_loss_weight: (a regularizer that pushes architecture weights more toward\n            zero or one in the fair DARTS variant)\n    \"\"\"\n    # set parameters for architecture learning\n    self.network_arch_momentum = arch_momentum\n    self.network_weight_decay = arch_weight_decay\n    self.network_weight_decay_df = arch_weight_decay_df\n    self.arch_weight_decay_base = arch_weight_decay_base * model._steps\n    self.fair_darts_loss_weight = fair_darts_loss_weight\n\n    self.model = model\n    self.lr = arch_learning_rate_max\n    # architecture is optimized using Adam\n    self.optimizer = torch.optim.Adam(\n        self.model.arch_parameters(),\n        lr=arch_learning_rate_max,\n        betas=(0.5, 0.999),\n        weight_decay=arch_weight_decay,\n    )\n\n    # initialize weight decay matrix\n    self._init_decay_weights()\n\n    # initialize the logged loss\n    self.current_loss = 0\n</code></pre>"},{"location":"reference/autora/theorist/darts/architect/#autora.theorist.darts.architect.Architect.step","title":"<code>step(input_valid, target_valid, network_optimizer, unrolled, input_train=None, target_train=None, eta=1)</code>","text":"<p>Updates the architecture parameters for one training iteration</p> <p>Parameters:</p> Name Type Description Default <code>input_valid</code> <code>Tensor</code> <p>input patterns for validation set</p> required <code>target_valid</code> <code>Tensor</code> <p>target patterns for validation set</p> required <code>network_optimizer</code> <code>Optimizer</code> <p>optimizer used to updating the architecture weights</p> required <code>unrolled</code> <code>bool</code> <p>whether to use the unrolled architecture or not (i.e., whether to use the approximate architecture gradient or not)</p> required <code>input_train</code> <code>Optional[Tensor]</code> <p>input patterns for training set</p> <code>None</code> <code>target_train</code> <code>Optional[Tensor]</code> <p>target patterns for training set</p> <code>None</code> <code>eta</code> <code>float</code> <p>learning rate for the architecture weights</p> <code>1</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/architect.py</code> <pre><code>def step(\n    self,\n    input_valid: torch.Tensor,\n    target_valid: torch.Tensor,\n    network_optimizer: torch.optim.Optimizer,\n    unrolled: bool,\n    input_train: Optional[torch.Tensor] = None,\n    target_train: Optional[torch.Tensor] = None,\n    eta: float = 1,\n):\n    \"\"\"\n    Updates the architecture parameters for one training iteration\n\n    Arguments:\n        input_valid: input patterns for validation set\n        target_valid: target patterns for validation set\n        network_optimizer: optimizer used to updating the architecture weights\n        unrolled: whether to use the unrolled architecture or not (i.e., whether to use\n            the approximate architecture gradient or not)\n        input_train: input patterns for training set\n        target_train: target patterns for training set\n        eta: learning rate for the architecture weights\n    \"\"\"\n\n    # input_train, target_train only needed for approximation (unrolled=True)\n    # of architecture gradient\n    # when performing a single weigh update\n\n    # initialize gradients to be zero\n    self.optimizer.zero_grad()\n    # use different backward step depending on whether to use\n    # 2nd order approximation for gradient update\n    if unrolled:  # probably using eta of parameter update here\n        self._backward_step_unrolled(\n            input_train,\n            target_train,\n            input_valid,\n            target_valid,\n            eta,\n            network_optimizer,\n        )\n    else:\n        self._backward_step(input_valid, target_valid)\n    # move Adam one step\n    self.optimizer.step()\n</code></pre>"},{"location":"reference/autora/theorist/darts/dataset/","title":"autora.theorist.darts.dataset","text":""},{"location":"reference/autora/theorist/darts/dataset/#autora.theorist.darts.dataset.DARTSDataset","title":"<code>DARTSDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset for the DARTS algorithm.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/dataset.py</code> <pre><code>class DARTSDataset(Dataset):\n    \"\"\"\n    A dataset for the DARTS algorithm.\n    \"\"\"\n\n    def __init__(self, input_data: torch.tensor, output_data: torch.tensor):\n        \"\"\"\n        Initializes the dataset.\n\n        Arguments:\n            input_data: The input data to the dataset.\n            output_data: The output data to the dataset.\n        \"\"\"\n        assert input_data.shape[0] == output_data.shape[0]\n        self.input_data = input_data\n        self.output_data = output_data\n\n    def __len__(self, experiment_id: Optional[int] = None) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset.\n\n        Arguments:\n            experiment_id:\n\n        Returns:\n            The length of the dataset.\n        \"\"\"\n        return self.input_data.shape[0]\n\n    def __getitem__(self, idx: int) -&gt; Tuple[torch.tensor, torch.tensor]:\n        \"\"\"\n        Returns the item at the given index.\n\n        Arguments:\n            idx: The index of the item to return.\n\n        Returns:\n            The item at the given index.\n\n        \"\"\"\n        input_tensor = self.input_data[idx]\n        output_tensor = self.output_data[idx]\n        return input_tensor, output_tensor\n</code></pre>"},{"location":"reference/autora/theorist/darts/dataset/#autora.theorist.darts.dataset.DARTSDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the item at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the item to return.</p> required <p>Returns:</p> Type Description <code>Tuple[tensor, tensor]</code> <p>The item at the given index.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[torch.tensor, torch.tensor]:\n    \"\"\"\n    Returns the item at the given index.\n\n    Arguments:\n        idx: The index of the item to return.\n\n    Returns:\n        The item at the given index.\n\n    \"\"\"\n    input_tensor = self.input_data[idx]\n    output_tensor = self.output_data[idx]\n    return input_tensor, output_tensor\n</code></pre>"},{"location":"reference/autora/theorist/darts/dataset/#autora.theorist.darts.dataset.DARTSDataset.__init__","title":"<code>__init__(input_data, output_data)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>tensor</code> <p>The input data to the dataset.</p> required <code>output_data</code> <code>tensor</code> <p>The output data to the dataset.</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/dataset.py</code> <pre><code>def __init__(self, input_data: torch.tensor, output_data: torch.tensor):\n    \"\"\"\n    Initializes the dataset.\n\n    Arguments:\n        input_data: The input data to the dataset.\n        output_data: The output data to the dataset.\n    \"\"\"\n    assert input_data.shape[0] == output_data.shape[0]\n    self.input_data = input_data\n    self.output_data = output_data\n</code></pre>"},{"location":"reference/autora/theorist/darts/dataset/#autora.theorist.darts.dataset.DARTSDataset.__len__","title":"<code>__len__(experiment_id=None)</code>","text":"<p>Returns the length of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_id</code> <code>Optional[int]</code> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The length of the dataset.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/dataset.py</code> <pre><code>def __len__(self, experiment_id: Optional[int] = None) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset.\n\n    Arguments:\n        experiment_id:\n\n    Returns:\n        The length of the dataset.\n    \"\"\"\n    return self.input_data.shape[0]\n</code></pre>"},{"location":"reference/autora/theorist/darts/dataset/#autora.theorist.darts.dataset.darts_dataset_from_ndarray","title":"<code>darts_dataset_from_ndarray(input_data, output_data)</code>","text":"<p>A function to create a dataset from numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ndarray</code> <p>The input data to the dataset.</p> required <code>output_data</code> <code>ndarray</code> <p>The output data to the dataset.</p> required <p>Returns:</p> Type Description <code>DARTSDataset</code> <p>The dataset.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/dataset.py</code> <pre><code>def darts_dataset_from_ndarray(\n    input_data: np.ndarray, output_data: np.ndarray\n) -&gt; DARTSDataset:\n    \"\"\"\n    A function to create a dataset from numpy arrays.\n\n    Arguments:\n        input_data: The input data to the dataset.\n        output_data: The output data to the dataset.\n\n    Returns:\n        The dataset.\n\n    \"\"\"\n\n    obj = DARTSDataset(\n        torch.tensor(input_data, dtype=torch.get_default_dtype()),\n        torch.tensor(output_data, dtype=torch.get_default_dtype()),\n    )\n    return obj\n</code></pre>"},{"location":"reference/autora/theorist/darts/fan_out/","title":"autora.theorist.darts.fan_out","text":""},{"location":"reference/autora/theorist/darts/fan_out/#autora.theorist.darts.fan_out.Fan_Out","title":"<code>Fan_Out</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural network class that splits a given input vector into separate nodes. Each element of the original input vector is allocated a separate node in a computation graph.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/fan_out.py</code> <pre><code>class Fan_Out(nn.Module):\n    \"\"\"\n    A neural network class that splits a given input vector into separate nodes. Each element of\n    the original input vector is allocated a separate node in a computation graph.\n    \"\"\"\n\n    def __init__(self, num_inputs: int):\n        \"\"\"\n        Initialize the Fan Out operation.\n\n        Arguments:\n                num_inputs (int): The number of distinct input nodes to generate\n        \"\"\"\n        super(Fan_Out, self).__init__()\n\n        self._num_inputs = num_inputs\n\n        self.input_output = list()\n        for i in range(num_inputs):\n            linearConnection = nn.Linear(num_inputs, 1, bias=False)\n            linearConnection.weight.data = torch.zeros(1, num_inputs)\n            linearConnection.weight.data[0, i] = 1\n            linearConnection.weight.requires_grad = False\n            self.input_output.append(linearConnection)\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the Fan Out operation.\n\n        Arguments:\n            input: input vector whose elements are split into separate input nodes\n        \"\"\"\n\n        output = list()\n        for i in range(self._num_inputs):\n            output.append(self.input_output[i](input))\n\n        return output\n</code></pre>"},{"location":"reference/autora/theorist/darts/fan_out/#autora.theorist.darts.fan_out.Fan_Out.__init__","title":"<code>__init__(num_inputs)</code>","text":"<p>Initialize the Fan Out operation.</p> <p>Parameters:</p> Name Type Description Default <code>num_inputs</code> <code>int</code> <p>The number of distinct input nodes to generate</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/fan_out.py</code> <pre><code>def __init__(self, num_inputs: int):\n    \"\"\"\n    Initialize the Fan Out operation.\n\n    Arguments:\n            num_inputs (int): The number of distinct input nodes to generate\n    \"\"\"\n    super(Fan_Out, self).__init__()\n\n    self._num_inputs = num_inputs\n\n    self.input_output = list()\n    for i in range(num_inputs):\n        linearConnection = nn.Linear(num_inputs, 1, bias=False)\n        linearConnection.weight.data = torch.zeros(1, num_inputs)\n        linearConnection.weight.data[0, i] = 1\n        linearConnection.weight.requires_grad = False\n        self.input_output.append(linearConnection)\n</code></pre>"},{"location":"reference/autora/theorist/darts/fan_out/#autora.theorist.darts.fan_out.Fan_Out.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass of the Fan Out operation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input vector whose elements are split into separate input nodes</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/fan_out.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the Fan Out operation.\n\n    Arguments:\n        input: input vector whose elements are split into separate input nodes\n    \"\"\"\n\n    output = list()\n    for i in range(self._num_inputs):\n        output.append(self.input_output[i](input))\n\n    return output\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/","title":"autora.theorist.darts.model_search","text":""},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Cell","title":"<code>Cell</code>","text":"<p>               Bases: <code>Module</code></p> <p>A cell as defined in differentiable architecture search. A single cell corresponds to a computation graph with the number of input nodes defined by n_input_states and the number of hidden nodes defined by steps. Input nodes only project to hidden nodes and hidden nodes project to each other with an acyclic connectivity pattern. The output of a cell corresponds to the concatenation of all hidden nodes. Hidden nodes are computed by integrating transformed outputs from sending nodes. Outputs from sending nodes correspond to mixture operations, i.e. a weighted combination of pre-specified operations applied to the variable specified by the sending node (see MixedOp).</p> <p>Attributes:</p> Name Type Description <code>_steps</code> <p>number of hidden nodes</p> <code>_n_input_states</code> <p>number of input nodes</p> <code>_ops</code> <p>list of mixture operations (amounts to the list of edges in the cell)</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>class Cell(nn.Module):\n    \"\"\"\n    A cell as defined in differentiable architecture search. A single cell corresponds\n    to a computation graph with the number of input nodes defined by n_input_states and\n    the number of hidden nodes defined by steps. Input nodes only project to hidden nodes and hidden\n    nodes project to each other with an acyclic connectivity pattern. The output of a cell\n    corresponds to the concatenation of all hidden nodes. Hidden nodes are computed by integrating\n    transformed outputs from sending nodes. Outputs from sending nodes correspond to\n    mixture operations, i.e. a weighted combination of pre-specified operations applied to the\n    variable specified by the sending node (see MixedOp).\n\n    Attributes:\n        _steps: number of hidden nodes\n        _n_input_states: number of input nodes\n        _ops: list of mixture operations (amounts to the list of edges in the cell)\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int = 2,\n        n_input_states: int = 1,\n        primitives: Sequence[str] = PRIMITIVES,\n    ):\n        \"\"\"\n        Initializes a cell based on the number of hidden nodes (steps)\n        and the number of input nodes (n_input_states).\n\n        Arguments:\n            steps: number of hidden nodes\n            n_input_states: number of input nodes\n        \"\"\"\n        # The first and second nodes of cell k are set equal to the outputs of\n        # cell k \u2212 2 and cell k \u2212 1, respectively, and 1 \u00d7 1 convolutions\n        # (ReLUConvBN) are inserted as necessary\n        super(Cell, self).__init__()\n\n        # set parameters\n        self._steps = steps  # hidden nodes\n        self._n_input_states = n_input_states  # input nodes\n\n        # EDIT 11/04/19 SM: adapting to new SimpleNet data (changed from\n        # multiplier to steps)\n        self._multiplier = steps\n\n        # set operations according to number of modules (empty)\n        self._ops = nn.ModuleList()\n        # iterate over edges: edges between each hidden node and input nodes +\n        # prev hidden nodes\n        for i in range(self._steps):  # hidden nodes\n            for j in range(self._n_input_states + i):  # 2 refers to the 2 input nodes\n                # defines the stride for link between cells\n                # adds a mixed operation (derived from architecture parameters alpha)\n                # for 4 intermediate nodes, a total of 14 connections\n                # (MixedOps) is added\n                op = MixedOp(primitives)\n                # appends cell with mixed operation\n                self._ops.append(op)\n\n    def forward(self, input_states: List, weights: torch.Tensor):\n        \"\"\"\n        Computes the output of a cell given a list of input states\n        (variables represented in input nodes) and a weight matrix specifying the weights of each\n        operation for each edge.\n\n        Arguments:\n            input_states: list of input nodes\n            weights: matrix specifying architecture weights, i.e. the weights associated\n                with each operation for each edge\n        \"\"\"\n        # initialize states (activities of each node in the cell)\n        states = list()\n\n        # add each input node to the number of states\n        for input in input_states:\n            states.append(input)\n\n        offset = 0\n        # this computes the states from intermediate nodes and adds them to the list of states\n        # (values of nodes)\n        # for each hidden node, compute edge between existing states (input\n        # nodes / previous hidden) nodes and current node\n        for i in range(\n            self._steps\n        ):  # compute the state for each hidden node, first hidden node is\n            # sum of input nodes, second is sum of input and first hidden\n            s = sum(\n                self._ops[offset + j](h, weights[offset + j])\n                for j, h in enumerate(states)\n            )\n            offset += len(states)\n            states.append(s)\n\n        # concatenates the states of the last n (self._multiplier) intermediate\n        # nodes to get the output of a cell\n        result = torch.cat(states[-self._multiplier :], dim=1)\n        return result\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Cell.__init__","title":"<code>__init__(steps=2, n_input_states=1, primitives=PRIMITIVES)</code>","text":"<p>Initializes a cell based on the number of hidden nodes (steps) and the number of input nodes (n_input_states).</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>number of hidden nodes</p> <code>2</code> <code>n_input_states</code> <code>int</code> <p>number of input nodes</p> <code>1</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def __init__(\n    self,\n    steps: int = 2,\n    n_input_states: int = 1,\n    primitives: Sequence[str] = PRIMITIVES,\n):\n    \"\"\"\n    Initializes a cell based on the number of hidden nodes (steps)\n    and the number of input nodes (n_input_states).\n\n    Arguments:\n        steps: number of hidden nodes\n        n_input_states: number of input nodes\n    \"\"\"\n    # The first and second nodes of cell k are set equal to the outputs of\n    # cell k \u2212 2 and cell k \u2212 1, respectively, and 1 \u00d7 1 convolutions\n    # (ReLUConvBN) are inserted as necessary\n    super(Cell, self).__init__()\n\n    # set parameters\n    self._steps = steps  # hidden nodes\n    self._n_input_states = n_input_states  # input nodes\n\n    # EDIT 11/04/19 SM: adapting to new SimpleNet data (changed from\n    # multiplier to steps)\n    self._multiplier = steps\n\n    # set operations according to number of modules (empty)\n    self._ops = nn.ModuleList()\n    # iterate over edges: edges between each hidden node and input nodes +\n    # prev hidden nodes\n    for i in range(self._steps):  # hidden nodes\n        for j in range(self._n_input_states + i):  # 2 refers to the 2 input nodes\n            # defines the stride for link between cells\n            # adds a mixed operation (derived from architecture parameters alpha)\n            # for 4 intermediate nodes, a total of 14 connections\n            # (MixedOps) is added\n            op = MixedOp(primitives)\n            # appends cell with mixed operation\n            self._ops.append(op)\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Cell.forward","title":"<code>forward(input_states, weights)</code>","text":"<p>Computes the output of a cell given a list of input states (variables represented in input nodes) and a weight matrix specifying the weights of each operation for each edge.</p> <p>Parameters:</p> Name Type Description Default <code>input_states</code> <code>List</code> <p>list of input nodes</p> required <code>weights</code> <code>Tensor</code> <p>matrix specifying architecture weights, i.e. the weights associated with each operation for each edge</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def forward(self, input_states: List, weights: torch.Tensor):\n    \"\"\"\n    Computes the output of a cell given a list of input states\n    (variables represented in input nodes) and a weight matrix specifying the weights of each\n    operation for each edge.\n\n    Arguments:\n        input_states: list of input nodes\n        weights: matrix specifying architecture weights, i.e. the weights associated\n            with each operation for each edge\n    \"\"\"\n    # initialize states (activities of each node in the cell)\n    states = list()\n\n    # add each input node to the number of states\n    for input in input_states:\n        states.append(input)\n\n    offset = 0\n    # this computes the states from intermediate nodes and adds them to the list of states\n    # (values of nodes)\n    # for each hidden node, compute edge between existing states (input\n    # nodes / previous hidden) nodes and current node\n    for i in range(\n        self._steps\n    ):  # compute the state for each hidden node, first hidden node is\n        # sum of input nodes, second is sum of input and first hidden\n        s = sum(\n            self._ops[offset + j](h, weights[offset + j])\n            for j, h in enumerate(states)\n        )\n        offset += len(states)\n        states.append(s)\n\n    # concatenates the states of the last n (self._multiplier) intermediate\n    # nodes to get the output of a cell\n    result = torch.cat(states[-self._multiplier :], dim=1)\n    return result\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.DARTSType","title":"<code>DARTSType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumerator that indexes different variants of DARTS.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>class DARTSType(str, Enum):\n    \"\"\"\n    Enumerator that indexes different variants of DARTS.\n    \"\"\"\n\n    # Liu, Simonyan &amp; Yang (2018). Darts: Differentiable architecture search\n    ORIGINAL = \"original\"\n\n    # Chu, Zhou, Zhang &amp; Li (2020). Fair darts: Eliminating unfair advantages\n    # in differentiable architecture search\n    FAIR = \"fair\"\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.MixedOp","title":"<code>MixedOp</code>","text":"<p>               Bases: <code>Module</code></p> <p>Mixture operation as applied in Differentiable Architecture Search (DARTS). A mixture operation amounts to a weighted mixture of a pre-defined set of operations that is applied to an input variable.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>class MixedOp(nn.Module):\n    \"\"\"\n    Mixture operation as applied in Differentiable Architecture Search (DARTS).\n    A mixture operation amounts to a weighted mixture of a pre-defined set of operations\n    that is applied to an input variable.\n    \"\"\"\n\n    def __init__(self, primitives: Sequence[str] = PRIMITIVES):\n        \"\"\"\n        Initializes a mixture operation based on a pre-specified set of primitive operations.\n\n        Arguments:\n            primitives: list of primitives to be used in the mixture operation\n        \"\"\"\n        super(MixedOp, self).__init__()\n        self._ops = nn.ModuleList()\n        # loop through all the 8 primitive operations\n        for primitive in primitives:\n            # OPS returns an nn module for a given primitive (defines as a string)\n            op = operation_factory(primitive)\n\n            # add the operation\n            self._ops.append(op)\n\n    def forward(self, x: torch.Tensor, weights: torch.Tensor) -&gt; float:\n        \"\"\"\n        Computes a mixture operation as a weighted sum of all primitive operations.\n\n        Arguments:\n            x: input to the mixture operations\n            weights: weight vector containing the weights associated with each operation\n\n        Returns:\n            y: result of the weighted mixture operation\n        \"\"\"\n        # there are 8 weights for all the eight primitives. then it returns the\n        # weighted sum of all operations performed on a given input\n        return sum(w * op(x) for w, op in zip(weights, self._ops))\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.MixedOp.__init__","title":"<code>__init__(primitives=PRIMITIVES)</code>","text":"<p>Initializes a mixture operation based on a pre-specified set of primitive operations.</p> <p>Parameters:</p> Name Type Description Default <code>primitives</code> <code>Sequence[str]</code> <p>list of primitives to be used in the mixture operation</p> <code>PRIMITIVES</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def __init__(self, primitives: Sequence[str] = PRIMITIVES):\n    \"\"\"\n    Initializes a mixture operation based on a pre-specified set of primitive operations.\n\n    Arguments:\n        primitives: list of primitives to be used in the mixture operation\n    \"\"\"\n    super(MixedOp, self).__init__()\n    self._ops = nn.ModuleList()\n    # loop through all the 8 primitive operations\n    for primitive in primitives:\n        # OPS returns an nn module for a given primitive (defines as a string)\n        op = operation_factory(primitive)\n\n        # add the operation\n        self._ops.append(op)\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.MixedOp.forward","title":"<code>forward(x, weights)</code>","text":"<p>Computes a mixture operation as a weighted sum of all primitive operations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input to the mixture operations</p> required <code>weights</code> <code>Tensor</code> <p>weight vector containing the weights associated with each operation</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>float</code> <p>result of the weighted mixture operation</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def forward(self, x: torch.Tensor, weights: torch.Tensor) -&gt; float:\n    \"\"\"\n    Computes a mixture operation as a weighted sum of all primitive operations.\n\n    Arguments:\n        x: input to the mixture operations\n        weights: weight vector containing the weights associated with each operation\n\n    Returns:\n        y: result of the weighted mixture operation\n    \"\"\"\n    # there are 8 weights for all the eight primitives. then it returns the\n    # weighted sum of all operations performed on a given input\n    return sum(w * op(x) for w, op in zip(weights, self._ops))\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network","title":"<code>Network</code>","text":"<p>               Bases: <code>Module</code></p> <p>A PyTorch computation graph according to DARTS. It consists of a single computation cell which transforms an input vector (containing all input variable) into an output vector, by applying a set of mixture operations which are defined by the architecture weights (labeled \"alphas\" of the network).</p> <p>The network flow looks as follows: An input vector (with _n_input_states elements) is split into _n_input_states separate input nodes (one node per element). The input nodes are then passed through a computation cell with _steps hidden nodes (see Cell). The output of the computation cell corresponds to the concatenation of its hidden nodes (a single vector). The final output corresponds to a (trained) affine transformation of this concatenation (labeled \"classifier\").</p> <p>Attributes:</p> Name Type Description <code>_n_input_states</code> <p>length of input vector (translates to number of input nodes)</p> <code>_num_classes</code> <p>length of output vector</p> <code>_criterion</code> <p>optimization criterion used to define the loss</p> <code>_steps</code> <p>number of hidden nodes in the cell</p> <code>_architecture_fixed</code> <p>specifies whether the architecture weights shall remain fixed (not trained)</p> <code>_classifier_weight_decay</code> <p>a weight decay applied to the classifier</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>class Network(nn.Module):\n    \"\"\"\n    A PyTorch computation graph according to DARTS.\n    It consists of a single computation cell which transforms an\n    input vector (containing all input variable) into an output vector, by applying a set of\n    mixture operations which are defined by the architecture weights (labeled \"alphas\" of the\n    network).\n\n    The network flow looks as follows: An input vector (with _n_input_states elements) is split into\n    _n_input_states separate input nodes (one node per element). The input nodes are then passed\n    through a computation cell with _steps hidden nodes (see Cell). The output of the computation\n    cell corresponds to the concatenation of its hidden nodes (a single vector). The final output\n    corresponds to a (trained) affine transformation of this concatenation (labeled \"classifier\").\n\n    Attributes:\n        _n_input_states: length of input vector (translates to number of input nodes)\n        _num_classes: length of output vector\n        _criterion: optimization criterion used to define the loss\n        _steps: number of hidden nodes in the cell\n        _architecture_fixed: specifies whether the architecture weights shall remain fixed\n            (not trained)\n        _classifier_weight_decay: a weight decay applied to the classifier\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int,\n        criterion: Callable,\n        steps: int = 2,\n        n_input_states: int = 2,\n        architecture_fixed: bool = False,\n        train_classifier_coefficients: bool = False,\n        train_classifier_bias: bool = False,\n        classifier_weight_decay: float = 0,\n        darts_type: DARTSType = DARTSType.ORIGINAL,\n        primitives: Sequence[str] = PRIMITIVES,\n    ):\n        \"\"\"\n        Initializes the network.\n\n        Arguments:\n            num_classes: length of output vector\n            criterion: optimization criterion used to define the loss\n            steps: number of hidden nodes in the cell\n            n_input_states: length of input vector (translates to number of input nodes)\n            architecture_fixed: specifies whether the architecture weights shall remain fixed\n            train_classifier_coefficients: specifies whether the classifier coefficients shall be\n                trained\n            train_classifier_bias: specifies whether the classifier bias shall be trained\n            classifier_weight_decay: a weight decay applied to the classifier\n            darts_type: variant of DARTS (regular or fair) that is applied for training\n        \"\"\"\n        super(Network, self).__init__()\n\n        # set parameters\n        self._num_classes = num_classes  # number of output classes\n        self._criterion = criterion  # optimization criterion (e.g., softmax)\n        self._steps = steps  # the number of intermediate nodes (e.g., 2)\n        self._n_input_states = n_input_states  # number of input nodes\n        self.DARTS_type = darts_type  # darts variant\n        self._multiplier = (\n            1  # the number of internal nodes that get concatenated to the output\n        )\n        self.primitives = primitives\n\n        # set parameters\n        self._dim_output = self._steps\n        self._architecture_fixed = architecture_fixed\n        self._classifier_weight_decay = classifier_weight_decay\n\n        # input nodes\n        self.stem = nn.Sequential(Fan_Out(self._n_input_states))\n\n        self.cells = (\n            nn.ModuleList()\n        )  # get list of all current modules (should be empty)\n\n        # generate a cell that undergoes architecture search\n        self.cells = Cell(steps, self._n_input_states, self.primitives)\n\n        # last layer is a linear classifier (e.g. with 10 CIFAR classes)\n        self.classifier = nn.Linear(\n            self._dim_output, num_classes\n        )  # make this the number of input states\n\n        # initialize classifier weights\n        if train_classifier_coefficients is False:\n            self.classifier.weight.data.fill_(1)\n            self.classifier.weight.requires_grad = False\n\n        if train_classifier_bias is False:\n            self.classifier.bias.data.fill_(0)\n            self.classifier.bias.requires_grad = False\n\n        # initializes weights of the architecture\n        self._initialize_alphas()\n\n    # function for copying the network\n    def new(self) -&gt; nn.Module:\n        \"\"\"\n        Returns a copy of the network.\n\n        Returns:\n            a copy of the network\n\n        \"\"\"\n\n        model_new = Network(\n            # self._C, self._num_classes, self._criterion, steps=self._steps\n            num_classes=self._num_classes,\n            criterion=self._criterion,\n            steps=self._steps,\n            n_input_states=self._n_input_states,\n            architecture_fixed=self._architecture_fixed,\n            classifier_weight_decay=self._classifier_weight_decay,\n            darts_type=self.DARTS_type,\n            primitives=self.primitives,\n        )\n\n        for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\n            x.data.copy_(y.data)\n        return model_new\n\n    # computes forward pass for full network\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Computes output of the network.\n\n        Arguments:\n            x: input to the network\n        \"\"\"\n\n        # compute stem first\n        input_states = self.stem(x)\n\n        # get architecture weights\n        if self._architecture_fixed:\n            weights = self.alphas_normal\n        else:\n            if self.DARTS_type == DARTSType.ORIGINAL:\n                weights = F.softmax(self.alphas_normal, dim=-1)\n            elif self.DARTS_type == DARTSType.FAIR:\n                weights = torch.sigmoid(self.alphas_normal)\n            else:\n                raise Exception(\n                    \"DARTS Type \" + str(self.DARTS_type) + \" not implemented\"\n                )\n\n        # then apply cell with weights\n        cell_output = self.cells(input_states, weights)\n\n        # compute logits\n        logits = self.classifier(cell_output.view(cell_output.size(0), -1))\n        # just gets output to have only 2 dimensions (batch_size x num units in\n        # output layer)\n\n        return logits\n\n    def _loss(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the loss of the network for the specified criterion.\n\n        Arguments:\n            input: input patterns\n            target: target patterns\n\n        Returns:\n            loss\n        \"\"\"\n        logits = self(input)\n        return self._criterion(logits, target)  # returns cross entropy by default\n\n    # regularization\n    def apply_weight_decay_to_classifier(self, lr: float):\n        \"\"\"\n        Applies a weight decay to the weights projecting from the cell to the final output layer.\n\n        Arguments:\n            lr: learning rate\n        \"\"\"\n        # weight decay proportional to degrees of freedom\n        for p in self.classifier.parameters():\n            if p.requires_grad is False:\n                continue\n            p.data.sub_(\n                self._classifier_weight_decay\n                * lr\n                * torch.sign(p.data)\n                * (torch.abs(p.data))\n            )  # weight decay\n\n    def _initialize_alphas(self):\n        \"\"\"\n        Initializes the architecture weights.\n        \"\"\"\n        # compute the number of possible connections between nodes\n        k = sum(1 for i in range(self._steps) for n in range(self._n_input_states + i))\n        # number of available primitive operations (8 different types for a\n        # conv net)\n        num_ops = len(self.primitives)\n\n        # e.g., generate 14 (number of available edges) by 8 (operations)\n        # weight matrix for normal alphas of the architecture\n        self.alphas_normal = Variable(\n            1e-3 * torch.randn(k, num_ops), requires_grad=True\n        )\n        # those are all the parameters of the architecture\n        self._arch_parameters = [self.alphas_normal]\n\n    # provide back the architecture as a parameter\n    def arch_parameters(self) -&gt; List:\n        \"\"\"\n        Returns architecture weights.\n\n        Returns:\n            _arch_parameters: architecture weights.\n        \"\"\"\n        return self._arch_parameters\n\n    # fixes architecture\n    def fix_architecture(\n        self, switch: bool, new_weights: Optional[torch.Tensor] = None\n    ):\n        \"\"\"\n        Freezes or unfreezes the architecture weights.\n\n        Arguments:\n            switch: set true to freeze architecture weights or false unfreeze\n            new_weights: new set of architecture weights\n        \"\"\"\n        self._architecture_fixed = switch\n        if new_weights is not None:\n            self.alphas_normal = new_weights\n        return\n\n    def sample_alphas_normal(\n        self, sample_amp: float = 1, fair_darts_weight_threshold: float = 0\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Samples an architecture from the mixed operations from a probability distribution that is\n        defined by the (softmaxed) architecture weights.\n        This amounts to selecting one operation per edge (i.e., setting the architecture\n        weight of that operation to one while setting the others to zero).\n\n        Arguments:\n            sample_amp: temperature that is applied before passing the weights through a softmax\n            fair_darts_weight_threshold: used in fair DARTS. If an architecture weight is below\n                this value then it is set to zero.\n\n        Returns:\n            alphas_normal_sample: sampled architecture weights.\n        \"\"\"\n\n        alphas_normal = self.alphas_normal.clone()\n        alphas_normal_sample = Variable(torch.zeros(alphas_normal.data.shape))\n\n        for edge in range(alphas_normal.data.shape[0]):\n            if self.DARTS_type == DARTSType.ORIGINAL:\n                W_soft = F.softmax(alphas_normal[edge] * sample_amp, dim=0)\n            elif self.DARTS_type == DARTSType.FAIR:\n                transformed_alphas_normal = alphas_normal[edge]\n                above_threshold = False\n                for idx in range(len(transformed_alphas_normal.data)):\n                    if (\n                        torch.sigmoid(transformed_alphas_normal).data[idx]\n                        &gt; fair_darts_weight_threshold\n                    ):\n                        above_threshold = True\n                        break\n                if above_threshold:\n                    W_soft = F.softmax(transformed_alphas_normal * sample_amp, dim=0)\n                else:\n                    W_soft = Variable(torch.zeros(alphas_normal[edge].shape))\n                    W_soft[self.primitives.index(\"none\")] = 1\n\n            else:\n                raise Exception(\n                    \"DARTS Type \" + str(self.DARTS_type) + \" not implemented\"\n                )\n\n            if torch.any(W_soft != W_soft):\n                warnings.warn(\n                    \"Cannot properly sample from architecture weights due to nan entries.\"\n                )\n                k_sample = random.randrange(len(W_soft))\n            else:\n                k_sample = np.random.choice(range(len(W_soft)), p=W_soft.data.numpy())\n            alphas_normal_sample[edge, k_sample] = 1\n\n        return alphas_normal_sample\n\n    def max_alphas_normal(self) -&gt; torch.Tensor:\n        \"\"\"\n        Samples an architecture from the mixed operations by selecting, for each edge,\n        the operation with the largest architecture weight.\n\n        Returns:\n            alphas_normal_sample: sampled architecture weights.\n        \"\"\"\n        alphas_normal = self.alphas_normal.clone()\n        alphas_normal_sample = Variable(torch.zeros(alphas_normal.data.shape))\n\n        for edge in range(alphas_normal.data.shape[0]):\n            row = alphas_normal[edge]\n            max_idx = np.argmax(row.data)\n            alphas_normal_sample[edge, max_idx] = 1\n\n        return alphas_normal_sample\n\n    # returns the genotype of the model\n    def genotype(self, sample: bool = False) -&gt; Genotype:\n        \"\"\"\n        Computes a genotype of the model which specifies the current computation graph based on\n        the largest architecture weight for each edge, or based on a sample.\n        The genotype can be used for parsing or plotting the computation graph.\n\n        Arguments:\n            sample: if set to true, the architecture will be determined by sampling\n                from a probability distribution that is determined by the\n                softmaxed architecture weights. If set to false (default), the architecture will be\n                determined based on the largest architecture weight per edge.\n\n        Returns:\n            genotype: genotype describing the current (sampled) architecture\n        \"\"\"\n\n        # this function uses the architecture weights to retrieve the\n        # operations with the highest weights\n        def _parse(weights):\n            gene = []\n            n = (\n                self._n_input_states\n            )  # 2 ... changed this to adapt to number of input states\n            start = 0\n            for i in range(self._steps):\n                end = start + n\n                W = weights[start:end].copy()\n                # first get all the edges for a given node, edges are sorted according to their\n                # highest (non-none) weight, starting from the edge with the smallest heighest\n                # weight\n\n                if \"none\" in self.primitives:\n                    none_index = self.primitives.index(\"none\")\n                else:\n                    none_index = -1\n\n                edges = sorted(\n                    range(n),\n                    key=lambda x: -max(\n                        W[x][k] for k in range(len(W[x])) if k != none_index\n                    ),\n                )\n                # for each edge, figure out which is the primitive with the\n                # highest\n                for (\n                    j\n                ) in edges:  # looping through all the edges for the current node (i)\n                    if sample:\n                        W_soft = F.softmax(Variable(torch.from_numpy(W[j])))\n                        k_best = np.random.choice(\n                            range(len(W[j])), p=W_soft.data.numpy()\n                        )\n                    else:\n                        k_best = None\n                        # looping through all the primitives\n                        for k in range(len(W[j])):\n                            # choose the primitive with the highest weight\n                            # if k != self.primitives.index('none'):\n                            # EDIT SM 01/13: commented to include \"none\"\n                            # weights in genotype\n                            if k_best is None or W[j][k] &gt; W[j][k_best]:\n                                k_best = k\n                        # add gene (primitive, edge number)\n                    gene.append((self.primitives[k_best], j))\n                start = end\n                n += 1\n            return gene\n\n        if self._architecture_fixed:\n            gene_normal = _parse(self.alphas_normal.data.cpu().numpy())\n        else:\n            gene_normal = _parse(\n                F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy()\n            )\n\n        concat = range(2 + self._steps - self._multiplier, self._steps + 2)\n        genotype = Genotype(\n            normal=gene_normal,\n            normal_concat=concat,\n        )\n        return genotype\n\n    def count_parameters(self, print_parameters: bool = False) -&gt; Tuple[int, int, list]:\n        \"\"\"\n        Counts and returns the parameters (coefficients) of the architecture defined by the\n        highest architecture weights.\n\n        Arguments:\n            print_parameters: if set to true, the function will print all parameters.\n\n        Returns:\n            n_params_total: total number of parameters\n            n_params_base: number of parameters determined by the classifier\n            param_list: list of parameters specifying the corresponding edge (operation)\n                and value\n        \"\"\"\n\n        # counts only parameters of operations with the highest architecture weight\n        n_params_total = 0\n\n        # count classifier\n        for parameter in self.classifier.parameters():\n            if parameter.requires_grad is True:\n                n_params_total += parameter.data.numel()\n\n        # count stem\n        for parameter in self.stem.parameters():\n            if parameter.requires_grad is True:\n                n_params_total += parameter.data.numel()\n\n        n_params_base = (\n            n_params_total  # number of parameters, excluding individual cells\n        )\n\n        param_list = list()\n        # now count number of parameters for cells that have highest\n        # probability\n        for idx, op in enumerate(self.cells._ops):\n            # pick most operation with highest likelihood\n            values = self.alphas_normal[idx, :].data.numpy()\n            maxIdx = np.where(values == max(values))\n\n            tmp_param_list = list()\n            if isiterable(op._ops[maxIdx[0].item(0)]):  # Zero is not iterable\n                for subop in op._ops[maxIdx[0].item(0)]:\n                    for parameter in subop.parameters():\n                        tmp_param_list.append(parameter.data.numpy().squeeze())\n                        if parameter.requires_grad is True:\n                            n_params_total += parameter.data.numel()\n\n            if print_parameters:\n                print(\n                    \"Edge (\"\n                    + str(idx)\n                    + \"): \"\n                    + get_operation_label(\n                        self.primitives[maxIdx[0].item(0)], tmp_param_list\n                    )\n                )\n            param_list.append(tmp_param_list)\n\n        # # get parameters from final linear classifier\n        # tmp_param_list = list()\n        # for parameter in self.classifier.parameters():\n        #   for subparameter in parameter:\n        #     tmp_param_list.append(subparameter.data.numpy().squeeze())\n\n        # get parameters from final linear for each edge\n        for edge in range(self._steps):\n            tmp_param_list = list()\n            # add weight\n            tmp_param_list.append(\n                self.classifier._parameters[\"weight\"].data[:, edge].numpy()\n            )\n            # add partial bias (bias of classifier units will be divided by\n            # number of edges)\n            if \"bias\" in self.classifier._parameters.keys() and edge == 0:\n                tmp_param_list.append(self.classifier._parameters[\"bias\"].data.numpy())\n            param_list.append(tmp_param_list)\n\n            if print_parameters:\n                print(\n                    \"Classifier from Node \"\n                    + str(edge)\n                    + \": \"\n                    + get_operation_label(\"classifier_concat\", tmp_param_list)\n                )\n\n        return (n_params_total, n_params_base, param_list)\n\n    def architecture_to_str_list(\n        self,\n        input_labels: Sequence[str],\n        output_labels: Sequence[str],\n        output_function_label: str = \"\",\n        decimals_to_display: int = 2,\n        output_format: Literal[\"latex\", \"console\"] = \"console\",\n    ) -&gt; List:\n        \"\"\"\n        Returns a list of strings representing the model.\n\n        Arguments:\n            input_labels: list of strings representing the input states.\n            output_labels: list of strings representing the output states.\n            output_function_label: string representing the output function.\n            decimals_to_display: number of decimals to display.\n            output_format: if set to `\"console\"`, returns equations formatted for the command line,\n                if set to `\"latex\"`, returns equations in latex format\n\n\n        Returns:\n            list of strings representing the model\n        \"\"\"\n        (n_params_total, n_params_base, param_list) = self.count_parameters(\n            print_parameters=False\n        )\n        genotype = self.genotype().normal\n        steps = self._steps\n        edge_list = list()\n\n        n = len(input_labels)\n        start = 0\n        for i in range(steps):  # for every node\n            end = start + n\n            # for k in [2*i, 2*i + 1]:\n\n            edge_operations_list = list()\n            op_list = list()\n\n            for k in range(start, end):\n                if (\n                    output_format == \"latex\"\n                ):  # for every edge projecting to current node\n                    v = \"k_\" + str(i + 1)\n                else:\n                    v = \"k\" + str(i + 1)\n                op, j = genotype[k]\n                if j &lt; len(input_labels):\n                    u = input_labels[j]\n                else:\n                    if output_format == \"latex\":\n                        u = \"k_\" + str(j - len(input_labels) + 1)\n                    else:\n                        u = \"k\" + str(j - len(input_labels) + 1)\n                if op != \"none\":\n                    op_label = op\n                    params = param_list[\n                        start + j\n                    ]  # note: genotype order and param list order don't align\n                    op_label = get_operation_label(\n                        op,\n                        params,\n                        decimals=decimals_to_display,\n                        input_var=u,\n                        output_format=output_format,\n                    )\n                    op_list.append(op)\n                    edge_operations_list.append(op_label)\n\n            if len(edge_operations_list) == 0:\n                edge_str = v + \" = 0\"\n            else:\n                edge_str = \"\"\n            for i, edge_operation in enumerate(edge_operations_list):\n                if i == 0:\n                    edge_str += v + \" = \" + edge_operation\n                if i &gt; 0:\n                    if (\n                        op_list[i] != \"add\"\n                        and op_list[i] != \"subtract\"\n                        and op_list[i] != \"none\"\n                    ):\n                        edge_str += \" +\"\n                    edge_str += \" \" + edge_operation\n\n            edge_list.append(edge_str)\n            start = end\n            n += 1\n\n        # TODO: extend to multiple outputs\n        if output_format == \"latex\":\n            classifier_str = output_labels[0] + \" = \" + output_function_label\n            if output_function_label != \"\":\n                classifier_str += \"\\\\left(\"\n        else:\n            classifier_str = output_labels[0] + \" = \" + output_function_label\n            if output_function_label != \"\":\n                classifier_str += \"(\"\n\n        bias = None\n        for i in range(steps):\n            param_idx = len(param_list) - steps + i\n            tmp_param_list = param_list[param_idx]\n            if i == 0 and len(tmp_param_list) == 2:\n                bias = tmp_param_list[1]\n            if i &gt; 0:\n                classifier_str += \" + \"\n\n            if output_format == \"latex\":\n                input_var = \"k_\" + str(i + 1)\n            else:\n                input_var = \"k\" + str(i + 1)\n\n            classifier_str += get_operation_label(\n                \"classifier\",\n                tmp_param_list[0],\n                decimals=decimals_to_display,\n                input_var=input_var,\n            )\n\n            if i == steps - 1 and bias is not None:\n                classifier_str += \" + \" + str(bias[0])\n\n            if i == steps - 1:\n                if output_function_label != \"\":\n                    if output_format == \"latex\":\n                        classifier_str += \"\\\\right)\"\n                    else:\n                        classifier_str += \")\"\n\n        edge_list.append(classifier_str)\n\n        return edge_list\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.__init__","title":"<code>__init__(num_classes, criterion, steps=2, n_input_states=2, architecture_fixed=False, train_classifier_coefficients=False, train_classifier_bias=False, classifier_weight_decay=0, darts_type=DARTSType.ORIGINAL, primitives=PRIMITIVES)</code>","text":"<p>Initializes the network.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>length of output vector</p> required <code>criterion</code> <code>Callable</code> <p>optimization criterion used to define the loss</p> required <code>steps</code> <code>int</code> <p>number of hidden nodes in the cell</p> <code>2</code> <code>n_input_states</code> <code>int</code> <p>length of input vector (translates to number of input nodes)</p> <code>2</code> <code>architecture_fixed</code> <code>bool</code> <p>specifies whether the architecture weights shall remain fixed</p> <code>False</code> <code>train_classifier_coefficients</code> <code>bool</code> <p>specifies whether the classifier coefficients shall be trained</p> <code>False</code> <code>train_classifier_bias</code> <code>bool</code> <p>specifies whether the classifier bias shall be trained</p> <code>False</code> <code>classifier_weight_decay</code> <code>float</code> <p>a weight decay applied to the classifier</p> <code>0</code> <code>darts_type</code> <code>DARTSType</code> <p>variant of DARTS (regular or fair) that is applied for training</p> <code>ORIGINAL</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int,\n    criterion: Callable,\n    steps: int = 2,\n    n_input_states: int = 2,\n    architecture_fixed: bool = False,\n    train_classifier_coefficients: bool = False,\n    train_classifier_bias: bool = False,\n    classifier_weight_decay: float = 0,\n    darts_type: DARTSType = DARTSType.ORIGINAL,\n    primitives: Sequence[str] = PRIMITIVES,\n):\n    \"\"\"\n    Initializes the network.\n\n    Arguments:\n        num_classes: length of output vector\n        criterion: optimization criterion used to define the loss\n        steps: number of hidden nodes in the cell\n        n_input_states: length of input vector (translates to number of input nodes)\n        architecture_fixed: specifies whether the architecture weights shall remain fixed\n        train_classifier_coefficients: specifies whether the classifier coefficients shall be\n            trained\n        train_classifier_bias: specifies whether the classifier bias shall be trained\n        classifier_weight_decay: a weight decay applied to the classifier\n        darts_type: variant of DARTS (regular or fair) that is applied for training\n    \"\"\"\n    super(Network, self).__init__()\n\n    # set parameters\n    self._num_classes = num_classes  # number of output classes\n    self._criterion = criterion  # optimization criterion (e.g., softmax)\n    self._steps = steps  # the number of intermediate nodes (e.g., 2)\n    self._n_input_states = n_input_states  # number of input nodes\n    self.DARTS_type = darts_type  # darts variant\n    self._multiplier = (\n        1  # the number of internal nodes that get concatenated to the output\n    )\n    self.primitives = primitives\n\n    # set parameters\n    self._dim_output = self._steps\n    self._architecture_fixed = architecture_fixed\n    self._classifier_weight_decay = classifier_weight_decay\n\n    # input nodes\n    self.stem = nn.Sequential(Fan_Out(self._n_input_states))\n\n    self.cells = (\n        nn.ModuleList()\n    )  # get list of all current modules (should be empty)\n\n    # generate a cell that undergoes architecture search\n    self.cells = Cell(steps, self._n_input_states, self.primitives)\n\n    # last layer is a linear classifier (e.g. with 10 CIFAR classes)\n    self.classifier = nn.Linear(\n        self._dim_output, num_classes\n    )  # make this the number of input states\n\n    # initialize classifier weights\n    if train_classifier_coefficients is False:\n        self.classifier.weight.data.fill_(1)\n        self.classifier.weight.requires_grad = False\n\n    if train_classifier_bias is False:\n        self.classifier.bias.data.fill_(0)\n        self.classifier.bias.requires_grad = False\n\n    # initializes weights of the architecture\n    self._initialize_alphas()\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.apply_weight_decay_to_classifier","title":"<code>apply_weight_decay_to_classifier(lr)</code>","text":"<p>Applies a weight decay to the weights projecting from the cell to the final output layer.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>learning rate</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def apply_weight_decay_to_classifier(self, lr: float):\n    \"\"\"\n    Applies a weight decay to the weights projecting from the cell to the final output layer.\n\n    Arguments:\n        lr: learning rate\n    \"\"\"\n    # weight decay proportional to degrees of freedom\n    for p in self.classifier.parameters():\n        if p.requires_grad is False:\n            continue\n        p.data.sub_(\n            self._classifier_weight_decay\n            * lr\n            * torch.sign(p.data)\n            * (torch.abs(p.data))\n        )  # weight decay\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.arch_parameters","title":"<code>arch_parameters()</code>","text":"<p>Returns architecture weights.</p> <p>Returns:</p> Name Type Description <code>_arch_parameters</code> <code>List</code> <p>architecture weights.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def arch_parameters(self) -&gt; List:\n    \"\"\"\n    Returns architecture weights.\n\n    Returns:\n        _arch_parameters: architecture weights.\n    \"\"\"\n    return self._arch_parameters\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.architecture_to_str_list","title":"<code>architecture_to_str_list(input_labels, output_labels, output_function_label='', decimals_to_display=2, output_format='console')</code>","text":"<p>Returns a list of strings representing the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_labels</code> <code>Sequence[str]</code> <p>list of strings representing the input states.</p> required <code>output_labels</code> <code>Sequence[str]</code> <p>list of strings representing the output states.</p> required <code>output_function_label</code> <code>str</code> <p>string representing the output function.</p> <code>''</code> <code>decimals_to_display</code> <code>int</code> <p>number of decimals to display.</p> <code>2</code> <code>output_format</code> <code>Literal['latex', 'console']</code> <p>if set to <code>\"console\"</code>, returns equations formatted for the command line, if set to <code>\"latex\"</code>, returns equations in latex format</p> <code>'console'</code> <p>Returns:</p> Type Description <code>List</code> <p>list of strings representing the model</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def architecture_to_str_list(\n    self,\n    input_labels: Sequence[str],\n    output_labels: Sequence[str],\n    output_function_label: str = \"\",\n    decimals_to_display: int = 2,\n    output_format: Literal[\"latex\", \"console\"] = \"console\",\n) -&gt; List:\n    \"\"\"\n    Returns a list of strings representing the model.\n\n    Arguments:\n        input_labels: list of strings representing the input states.\n        output_labels: list of strings representing the output states.\n        output_function_label: string representing the output function.\n        decimals_to_display: number of decimals to display.\n        output_format: if set to `\"console\"`, returns equations formatted for the command line,\n            if set to `\"latex\"`, returns equations in latex format\n\n\n    Returns:\n        list of strings representing the model\n    \"\"\"\n    (n_params_total, n_params_base, param_list) = self.count_parameters(\n        print_parameters=False\n    )\n    genotype = self.genotype().normal\n    steps = self._steps\n    edge_list = list()\n\n    n = len(input_labels)\n    start = 0\n    for i in range(steps):  # for every node\n        end = start + n\n        # for k in [2*i, 2*i + 1]:\n\n        edge_operations_list = list()\n        op_list = list()\n\n        for k in range(start, end):\n            if (\n                output_format == \"latex\"\n            ):  # for every edge projecting to current node\n                v = \"k_\" + str(i + 1)\n            else:\n                v = \"k\" + str(i + 1)\n            op, j = genotype[k]\n            if j &lt; len(input_labels):\n                u = input_labels[j]\n            else:\n                if output_format == \"latex\":\n                    u = \"k_\" + str(j - len(input_labels) + 1)\n                else:\n                    u = \"k\" + str(j - len(input_labels) + 1)\n            if op != \"none\":\n                op_label = op\n                params = param_list[\n                    start + j\n                ]  # note: genotype order and param list order don't align\n                op_label = get_operation_label(\n                    op,\n                    params,\n                    decimals=decimals_to_display,\n                    input_var=u,\n                    output_format=output_format,\n                )\n                op_list.append(op)\n                edge_operations_list.append(op_label)\n\n        if len(edge_operations_list) == 0:\n            edge_str = v + \" = 0\"\n        else:\n            edge_str = \"\"\n        for i, edge_operation in enumerate(edge_operations_list):\n            if i == 0:\n                edge_str += v + \" = \" + edge_operation\n            if i &gt; 0:\n                if (\n                    op_list[i] != \"add\"\n                    and op_list[i] != \"subtract\"\n                    and op_list[i] != \"none\"\n                ):\n                    edge_str += \" +\"\n                edge_str += \" \" + edge_operation\n\n        edge_list.append(edge_str)\n        start = end\n        n += 1\n\n    # TODO: extend to multiple outputs\n    if output_format == \"latex\":\n        classifier_str = output_labels[0] + \" = \" + output_function_label\n        if output_function_label != \"\":\n            classifier_str += \"\\\\left(\"\n    else:\n        classifier_str = output_labels[0] + \" = \" + output_function_label\n        if output_function_label != \"\":\n            classifier_str += \"(\"\n\n    bias = None\n    for i in range(steps):\n        param_idx = len(param_list) - steps + i\n        tmp_param_list = param_list[param_idx]\n        if i == 0 and len(tmp_param_list) == 2:\n            bias = tmp_param_list[1]\n        if i &gt; 0:\n            classifier_str += \" + \"\n\n        if output_format == \"latex\":\n            input_var = \"k_\" + str(i + 1)\n        else:\n            input_var = \"k\" + str(i + 1)\n\n        classifier_str += get_operation_label(\n            \"classifier\",\n            tmp_param_list[0],\n            decimals=decimals_to_display,\n            input_var=input_var,\n        )\n\n        if i == steps - 1 and bias is not None:\n            classifier_str += \" + \" + str(bias[0])\n\n        if i == steps - 1:\n            if output_function_label != \"\":\n                if output_format == \"latex\":\n                    classifier_str += \"\\\\right)\"\n                else:\n                    classifier_str += \")\"\n\n    edge_list.append(classifier_str)\n\n    return edge_list\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.count_parameters","title":"<code>count_parameters(print_parameters=False)</code>","text":"<p>Counts and returns the parameters (coefficients) of the architecture defined by the highest architecture weights.</p> <p>Parameters:</p> Name Type Description Default <code>print_parameters</code> <code>bool</code> <p>if set to true, the function will print all parameters.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>n_params_total</code> <code>int</code> <p>total number of parameters</p> <code>n_params_base</code> <code>int</code> <p>number of parameters determined by the classifier</p> <code>param_list</code> <code>list</code> <p>list of parameters specifying the corresponding edge (operation) and value</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def count_parameters(self, print_parameters: bool = False) -&gt; Tuple[int, int, list]:\n    \"\"\"\n    Counts and returns the parameters (coefficients) of the architecture defined by the\n    highest architecture weights.\n\n    Arguments:\n        print_parameters: if set to true, the function will print all parameters.\n\n    Returns:\n        n_params_total: total number of parameters\n        n_params_base: number of parameters determined by the classifier\n        param_list: list of parameters specifying the corresponding edge (operation)\n            and value\n    \"\"\"\n\n    # counts only parameters of operations with the highest architecture weight\n    n_params_total = 0\n\n    # count classifier\n    for parameter in self.classifier.parameters():\n        if parameter.requires_grad is True:\n            n_params_total += parameter.data.numel()\n\n    # count stem\n    for parameter in self.stem.parameters():\n        if parameter.requires_grad is True:\n            n_params_total += parameter.data.numel()\n\n    n_params_base = (\n        n_params_total  # number of parameters, excluding individual cells\n    )\n\n    param_list = list()\n    # now count number of parameters for cells that have highest\n    # probability\n    for idx, op in enumerate(self.cells._ops):\n        # pick most operation with highest likelihood\n        values = self.alphas_normal[idx, :].data.numpy()\n        maxIdx = np.where(values == max(values))\n\n        tmp_param_list = list()\n        if isiterable(op._ops[maxIdx[0].item(0)]):  # Zero is not iterable\n            for subop in op._ops[maxIdx[0].item(0)]:\n                for parameter in subop.parameters():\n                    tmp_param_list.append(parameter.data.numpy().squeeze())\n                    if parameter.requires_grad is True:\n                        n_params_total += parameter.data.numel()\n\n        if print_parameters:\n            print(\n                \"Edge (\"\n                + str(idx)\n                + \"): \"\n                + get_operation_label(\n                    self.primitives[maxIdx[0].item(0)], tmp_param_list\n                )\n            )\n        param_list.append(tmp_param_list)\n\n    # # get parameters from final linear classifier\n    # tmp_param_list = list()\n    # for parameter in self.classifier.parameters():\n    #   for subparameter in parameter:\n    #     tmp_param_list.append(subparameter.data.numpy().squeeze())\n\n    # get parameters from final linear for each edge\n    for edge in range(self._steps):\n        tmp_param_list = list()\n        # add weight\n        tmp_param_list.append(\n            self.classifier._parameters[\"weight\"].data[:, edge].numpy()\n        )\n        # add partial bias (bias of classifier units will be divided by\n        # number of edges)\n        if \"bias\" in self.classifier._parameters.keys() and edge == 0:\n            tmp_param_list.append(self.classifier._parameters[\"bias\"].data.numpy())\n        param_list.append(tmp_param_list)\n\n        if print_parameters:\n            print(\n                \"Classifier from Node \"\n                + str(edge)\n                + \": \"\n                + get_operation_label(\"classifier_concat\", tmp_param_list)\n            )\n\n    return (n_params_total, n_params_base, param_list)\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.fix_architecture","title":"<code>fix_architecture(switch, new_weights=None)</code>","text":"<p>Freezes or unfreezes the architecture weights.</p> <p>Parameters:</p> Name Type Description Default <code>switch</code> <code>bool</code> <p>set true to freeze architecture weights or false unfreeze</p> required <code>new_weights</code> <code>Optional[Tensor]</code> <p>new set of architecture weights</p> <code>None</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def fix_architecture(\n    self, switch: bool, new_weights: Optional[torch.Tensor] = None\n):\n    \"\"\"\n    Freezes or unfreezes the architecture weights.\n\n    Arguments:\n        switch: set true to freeze architecture weights or false unfreeze\n        new_weights: new set of architecture weights\n    \"\"\"\n    self._architecture_fixed = switch\n    if new_weights is not None:\n        self.alphas_normal = new_weights\n    return\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.forward","title":"<code>forward(x)</code>","text":"<p>Computes output of the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input to the network</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"\n    Computes output of the network.\n\n    Arguments:\n        x: input to the network\n    \"\"\"\n\n    # compute stem first\n    input_states = self.stem(x)\n\n    # get architecture weights\n    if self._architecture_fixed:\n        weights = self.alphas_normal\n    else:\n        if self.DARTS_type == DARTSType.ORIGINAL:\n            weights = F.softmax(self.alphas_normal, dim=-1)\n        elif self.DARTS_type == DARTSType.FAIR:\n            weights = torch.sigmoid(self.alphas_normal)\n        else:\n            raise Exception(\n                \"DARTS Type \" + str(self.DARTS_type) + \" not implemented\"\n            )\n\n    # then apply cell with weights\n    cell_output = self.cells(input_states, weights)\n\n    # compute logits\n    logits = self.classifier(cell_output.view(cell_output.size(0), -1))\n    # just gets output to have only 2 dimensions (batch_size x num units in\n    # output layer)\n\n    return logits\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.genotype","title":"<code>genotype(sample=False)</code>","text":"<p>Computes a genotype of the model which specifies the current computation graph based on the largest architecture weight for each edge, or based on a sample. The genotype can be used for parsing or plotting the computation graph.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>bool</code> <p>if set to true, the architecture will be determined by sampling from a probability distribution that is determined by the softmaxed architecture weights. If set to false (default), the architecture will be determined based on the largest architecture weight per edge.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>genotype</code> <code>Genotype</code> <p>genotype describing the current (sampled) architecture</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def genotype(self, sample: bool = False) -&gt; Genotype:\n    \"\"\"\n    Computes a genotype of the model which specifies the current computation graph based on\n    the largest architecture weight for each edge, or based on a sample.\n    The genotype can be used for parsing or plotting the computation graph.\n\n    Arguments:\n        sample: if set to true, the architecture will be determined by sampling\n            from a probability distribution that is determined by the\n            softmaxed architecture weights. If set to false (default), the architecture will be\n            determined based on the largest architecture weight per edge.\n\n    Returns:\n        genotype: genotype describing the current (sampled) architecture\n    \"\"\"\n\n    # this function uses the architecture weights to retrieve the\n    # operations with the highest weights\n    def _parse(weights):\n        gene = []\n        n = (\n            self._n_input_states\n        )  # 2 ... changed this to adapt to number of input states\n        start = 0\n        for i in range(self._steps):\n            end = start + n\n            W = weights[start:end].copy()\n            # first get all the edges for a given node, edges are sorted according to their\n            # highest (non-none) weight, starting from the edge with the smallest heighest\n            # weight\n\n            if \"none\" in self.primitives:\n                none_index = self.primitives.index(\"none\")\n            else:\n                none_index = -1\n\n            edges = sorted(\n                range(n),\n                key=lambda x: -max(\n                    W[x][k] for k in range(len(W[x])) if k != none_index\n                ),\n            )\n            # for each edge, figure out which is the primitive with the\n            # highest\n            for (\n                j\n            ) in edges:  # looping through all the edges for the current node (i)\n                if sample:\n                    W_soft = F.softmax(Variable(torch.from_numpy(W[j])))\n                    k_best = np.random.choice(\n                        range(len(W[j])), p=W_soft.data.numpy()\n                    )\n                else:\n                    k_best = None\n                    # looping through all the primitives\n                    for k in range(len(W[j])):\n                        # choose the primitive with the highest weight\n                        # if k != self.primitives.index('none'):\n                        # EDIT SM 01/13: commented to include \"none\"\n                        # weights in genotype\n                        if k_best is None or W[j][k] &gt; W[j][k_best]:\n                            k_best = k\n                    # add gene (primitive, edge number)\n                gene.append((self.primitives[k_best], j))\n            start = end\n            n += 1\n        return gene\n\n    if self._architecture_fixed:\n        gene_normal = _parse(self.alphas_normal.data.cpu().numpy())\n    else:\n        gene_normal = _parse(\n            F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy()\n        )\n\n    concat = range(2 + self._steps - self._multiplier, self._steps + 2)\n    genotype = Genotype(\n        normal=gene_normal,\n        normal_concat=concat,\n    )\n    return genotype\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.max_alphas_normal","title":"<code>max_alphas_normal()</code>","text":"<p>Samples an architecture from the mixed operations by selecting, for each edge, the operation with the largest architecture weight.</p> <p>Returns:</p> Name Type Description <code>alphas_normal_sample</code> <code>Tensor</code> <p>sampled architecture weights.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def max_alphas_normal(self) -&gt; torch.Tensor:\n    \"\"\"\n    Samples an architecture from the mixed operations by selecting, for each edge,\n    the operation with the largest architecture weight.\n\n    Returns:\n        alphas_normal_sample: sampled architecture weights.\n    \"\"\"\n    alphas_normal = self.alphas_normal.clone()\n    alphas_normal_sample = Variable(torch.zeros(alphas_normal.data.shape))\n\n    for edge in range(alphas_normal.data.shape[0]):\n        row = alphas_normal[edge]\n        max_idx = np.argmax(row.data)\n        alphas_normal_sample[edge, max_idx] = 1\n\n    return alphas_normal_sample\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.new","title":"<code>new()</code>","text":"<p>Returns a copy of the network.</p> <p>Returns:</p> Type Description <code>Module</code> <p>a copy of the network</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def new(self) -&gt; nn.Module:\n    \"\"\"\n    Returns a copy of the network.\n\n    Returns:\n        a copy of the network\n\n    \"\"\"\n\n    model_new = Network(\n        # self._C, self._num_classes, self._criterion, steps=self._steps\n        num_classes=self._num_classes,\n        criterion=self._criterion,\n        steps=self._steps,\n        n_input_states=self._n_input_states,\n        architecture_fixed=self._architecture_fixed,\n        classifier_weight_decay=self._classifier_weight_decay,\n        darts_type=self.DARTS_type,\n        primitives=self.primitives,\n    )\n\n    for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\n        x.data.copy_(y.data)\n    return model_new\n</code></pre>"},{"location":"reference/autora/theorist/darts/model_search/#autora.theorist.darts.model_search.Network.sample_alphas_normal","title":"<code>sample_alphas_normal(sample_amp=1, fair_darts_weight_threshold=0)</code>","text":"<p>Samples an architecture from the mixed operations from a probability distribution that is defined by the (softmaxed) architecture weights. This amounts to selecting one operation per edge (i.e., setting the architecture weight of that operation to one while setting the others to zero).</p> <p>Parameters:</p> Name Type Description Default <code>sample_amp</code> <code>float</code> <p>temperature that is applied before passing the weights through a softmax</p> <code>1</code> <code>fair_darts_weight_threshold</code> <code>float</code> <p>used in fair DARTS. If an architecture weight is below this value then it is set to zero.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>alphas_normal_sample</code> <code>Tensor</code> <p>sampled architecture weights.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/model_search.py</code> <pre><code>def sample_alphas_normal(\n    self, sample_amp: float = 1, fair_darts_weight_threshold: float = 0\n) -&gt; torch.Tensor:\n    \"\"\"\n    Samples an architecture from the mixed operations from a probability distribution that is\n    defined by the (softmaxed) architecture weights.\n    This amounts to selecting one operation per edge (i.e., setting the architecture\n    weight of that operation to one while setting the others to zero).\n\n    Arguments:\n        sample_amp: temperature that is applied before passing the weights through a softmax\n        fair_darts_weight_threshold: used in fair DARTS. If an architecture weight is below\n            this value then it is set to zero.\n\n    Returns:\n        alphas_normal_sample: sampled architecture weights.\n    \"\"\"\n\n    alphas_normal = self.alphas_normal.clone()\n    alphas_normal_sample = Variable(torch.zeros(alphas_normal.data.shape))\n\n    for edge in range(alphas_normal.data.shape[0]):\n        if self.DARTS_type == DARTSType.ORIGINAL:\n            W_soft = F.softmax(alphas_normal[edge] * sample_amp, dim=0)\n        elif self.DARTS_type == DARTSType.FAIR:\n            transformed_alphas_normal = alphas_normal[edge]\n            above_threshold = False\n            for idx in range(len(transformed_alphas_normal.data)):\n                if (\n                    torch.sigmoid(transformed_alphas_normal).data[idx]\n                    &gt; fair_darts_weight_threshold\n                ):\n                    above_threshold = True\n                    break\n            if above_threshold:\n                W_soft = F.softmax(transformed_alphas_normal * sample_amp, dim=0)\n            else:\n                W_soft = Variable(torch.zeros(alphas_normal[edge].shape))\n                W_soft[self.primitives.index(\"none\")] = 1\n\n        else:\n            raise Exception(\n                \"DARTS Type \" + str(self.DARTS_type) + \" not implemented\"\n            )\n\n        if torch.any(W_soft != W_soft):\n            warnings.warn(\n                \"Cannot properly sample from architecture weights due to nan entries.\"\n            )\n            k_sample = random.randrange(len(W_soft))\n        else:\n            k_sample = np.random.choice(range(len(W_soft)), p=W_soft.data.numpy())\n        alphas_normal_sample[edge, k_sample] = 1\n\n    return alphas_normal_sample\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/","title":"autora.theorist.darts.operations","text":""},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Cosine","title":"<code>Cosine</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the cosine function.</p> \\[ x = \\cos(x) \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Cosine(nn.Module):\n    r\"\"\"\n    A pytorch module implementing the cosine function.\n\n    $$\n    x = \\cos(x)\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the cosine function.\n        \"\"\"\n        super(Cosine, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the cosine function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return torch.cos(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Cosine.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the cosine function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the cosine function.\n    \"\"\"\n    super(Cosine, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Cosine.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the cosine function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the cosine function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return torch.cos(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Exponential","title":"<code>Exponential</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the exponential function.</p> \\[ x = e^x \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Exponential(nn.Module):\n    \"\"\"\n    A pytorch module implementing the exponential function.\n\n    $$\n    x = e^x\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the exponential function.\n        \"\"\"\n        super(Exponential, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the exponential function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return torch.exp(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Exponential.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the exponential function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the exponential function.\n    \"\"\"\n    super(Exponential, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Exponential.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the exponential function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the exponential function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return torch.exp(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Identity","title":"<code>Identity</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the identity function.</p> \\[ x = x \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Identity(nn.Module):\n    \"\"\"\n    A pytorch module implementing the identity function.\n\n    $$\n    x = x\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the identify function.\n        \"\"\"\n        super(Identity, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the identity function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return x\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Identity.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the identify function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the identify function.\n    \"\"\"\n    super(Identity, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Identity.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the identity function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the identity function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return x\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.MultInverse","title":"<code>MultInverse</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the multiplicative inverse.</p> \\[ x = \\frac{1}{x} \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class MultInverse(nn.Module):\n    r\"\"\"\n    A pytorch module implementing the multiplicative inverse.\n\n    $$\n    x = \\frac{1}{x}\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the multiplicative inverse.\n        \"\"\"\n        super(MultInverse, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the multiplicative inverse.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return torch.pow(x, -1)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.MultInverse.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the multiplicative inverse.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the multiplicative inverse.\n    \"\"\"\n    super(MultInverse, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.MultInverse.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the multiplicative inverse.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the multiplicative inverse.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return torch.pow(x, -1)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.NatLogarithm","title":"<code>NatLogarithm</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the natural logarithm function.</p> \\[ x = \\ln(x) \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class NatLogarithm(nn.Module):\n    r\"\"\"\n    A pytorch module implementing the natural logarithm function.\n\n    $$\n    x = \\ln(x)\n    $$\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the natural logarithm function.\n        \"\"\"\n        super(NatLogarithm, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the natural logarithm function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        # make sure x is in domain of natural logarithm\n        mask = x.clone()\n        mask[(x &lt;= 0.0).detach()] = 0\n        mask[(x &gt; 0.0).detach()] = 1\n\n        epsilon = 1e-10\n        result = torch.log(nn.functional.relu(x) + epsilon) * mask\n\n        return result\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.NatLogarithm.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the natural logarithm function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the natural logarithm function.\n    \"\"\"\n    super(NatLogarithm, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.NatLogarithm.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the natural logarithm function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the natural logarithm function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    # make sure x is in domain of natural logarithm\n    mask = x.clone()\n    mask[(x &lt;= 0.0).detach()] = 0\n    mask[(x &gt; 0.0).detach()] = 1\n\n    epsilon = 1e-10\n    result = torch.log(nn.functional.relu(x) + epsilon) * mask\n\n    return result\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.NegIdentity","title":"<code>NegIdentity</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the inverse of an identity function.</p> \\[ x = -x \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class NegIdentity(nn.Module):\n    \"\"\"\n    A pytorch module implementing the inverse of an identity function.\n\n    $$\n    x = -x\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the inverse of an identity function.\n        \"\"\"\n        super(NegIdentity, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the inverse of an identity function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return -x\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.NegIdentity.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the inverse of an identity function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the inverse of an identity function.\n    \"\"\"\n    super(NegIdentity, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.NegIdentity.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the inverse of an identity function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the inverse of an identity function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return -x\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Sine","title":"<code>Sine</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the sine function.</p> \\[ x = \\sin(x) \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Sine(nn.Module):\n    r\"\"\"\n    A pytorch module implementing the sine function.\n\n    $$\n    x = \\sin(x)\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the sine function.\n        \"\"\"\n        super(Sine, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the sine function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return torch.sin(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Sine.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the sine function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the sine function.\n    \"\"\"\n    super(Sine, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Sine.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the sine function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the sine function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return torch.sin(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Softminus","title":"<code>Softminus</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the softminus function:</p> \\[ \\operatorname{Softminus}(x) = x - \\operatorname{log} \\left( 1 + e^{\u03b2 x} \\right) \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Softminus(nn.Module):\n    \"\"\"\n    A pytorch module implementing the softminus function:\n\n    $$\n    \\\\operatorname{Softminus}(x) = x - \\\\operatorname{log} \\\\left( 1 + e^{\u03b2 x} \\\\right)\n    $$\n    \"\"\"\n\n    # This docstring is a normal string, so backslashes need to be escaped\n\n    def __init__(self):\n        \"\"\"\n        Initializes the softminus function.\n        \"\"\"\n        super(Softminus, self).__init__()\n        # self.beta = nn.Linear(1, 1, bias=False)\n        self.beta = nn.Parameter(torch.ones(1))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the softminus function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        y = x - torch.log(1 + torch.exp(self.beta * x)) / self.beta\n        return y\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Softminus.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the softminus function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the softminus function.\n    \"\"\"\n    super(Softminus, self).__init__()\n    # self.beta = nn.Linear(1, 1, bias=False)\n    self.beta = nn.Parameter(torch.ones(1))\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Softminus.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the softminus function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the softminus function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    y = x - torch.log(1 + torch.exp(self.beta * x)) / self.beta\n    return y\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Softplus","title":"<code>Softplus</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the softplus function:</p> \\[ \\operatorname{Softplus}(x) = \\frac{1}{\u03b2} \\operatorname{log} \\left( 1 + e^{\u03b2 x} \\right) \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Softplus(nn.Module):\n    r\"\"\"\n    A pytorch module implementing the softplus function:\n\n    $$\n    \\operatorname{Softplus}(x) = \\frac{1}{\u03b2} \\operatorname{log} \\left( 1 + e^{\u03b2 x} \\right)\n    $$\n    \"\"\"\n\n    # This docstring is a raw-string (it starts `r\"\"\"` rather than `\"\"\"`)\n    # so backslashes need not be escaped\n\n    def __init__(self):\n        \"\"\"\n        Initializes the softplus function.\n        \"\"\"\n        super(Softplus, self).__init__()\n        # self.beta = nn.Linear(1, 1, bias=False)\n        self.beta = nn.Parameter(torch.ones(1))\n        # elf.softplus = nn.Softplus(beta=self.beta)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the softplus function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        y = torch.log(1 + torch.exp(self.beta * x)) / self.beta\n        # y = self.softplus(x)\n        return y\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Softplus.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the softplus function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the softplus function.\n    \"\"\"\n    super(Softplus, self).__init__()\n    # self.beta = nn.Linear(1, 1, bias=False)\n    self.beta = nn.Parameter(torch.ones(1))\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Softplus.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the softplus function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the softplus function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    y = torch.log(1 + torch.exp(self.beta * x)) / self.beta\n    # y = self.softplus(x)\n    return y\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Tangens_Hyperbolicus","title":"<code>Tangens_Hyperbolicus</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the tangens hyperbolicus function.</p> \\[ x = \\tanh(x) \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Tangens_Hyperbolicus(nn.Module):\n    r\"\"\"\n    A pytorch module implementing the tangens hyperbolicus function.\n\n    $$\n    x = \\tanh(x)\n    $$\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the tangens hyperbolicus function.\n        \"\"\"\n        super(Tangens_Hyperbolicus, self).__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the tangens hyperbolicus function.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        return torch.tanh(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Tangens_Hyperbolicus.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the tangens hyperbolicus function.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the tangens hyperbolicus function.\n    \"\"\"\n    super(Tangens_Hyperbolicus, self).__init__()\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Tangens_Hyperbolicus.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the tangens hyperbolicus function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the tangens hyperbolicus function.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    return torch.tanh(x)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Zero","title":"<code>Zero</code>","text":"<p>               Bases: <code>Module</code></p> <p>A pytorch module implementing the zero operation (i.e., a null operation). A zero operation presumes that there is no relationship between the input and output.</p> \\[ x = 0 \\] Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>class Zero(nn.Module):\n    \"\"\"\n    A pytorch module implementing the zero operation (i.e., a null operation). A zero operation\n    presumes that there is no relationship between the input and output.\n\n    $$\n    x = 0\n    $$\n    \"\"\"\n\n    def __init__(self, stride):\n        \"\"\"\n        Initializes the zero operation.\n        \"\"\"\n        super(Zero, self).__init__()\n        self.stride = stride\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the zero operation.\n\n        Arguments:\n            x: input tensor\n        \"\"\"\n        if self.stride == 1:\n            return x.mul(0.0)\n        return x[:, :, :: self.stride, :: self.stride].mul(0.0)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Zero.__init__","title":"<code>__init__(stride)</code>","text":"<p>Initializes the zero operation.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def __init__(self, stride):\n    \"\"\"\n    Initializes the zero operation.\n    \"\"\"\n    super(Zero, self).__init__()\n    self.stride = stride\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.Zero.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the zero operation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the zero operation.\n\n    Arguments:\n        x: input tensor\n    \"\"\"\n    if self.stride == 1:\n        return x.mul(0.0)\n    return x[:, :, :: self.stride, :: self.stride].mul(0.0)\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.get_operation_label","title":"<code>get_operation_label(op_name, params_org, decimals=4, input_var='x', output_format='console')</code>","text":"<p>Returns a complete string describing a DARTS operation.</p> <p>Parameters:</p> Name Type Description Default <code>op_name</code> <code>str</code> <p>name of the operation</p> required <code>params_org</code> <code>List</code> <p>original parameters of the operation</p> required <code>decimals</code> <code>int</code> <p>number of decimals to be used for converting the parameters into string format</p> <code>4</code> <code>input_var</code> <code>str</code> <p>name of the input variable</p> <code>'x'</code> <code>output_format</code> <code>Literal['latex', 'console']</code> <p>format of the output string (either \"latex\" or \"console\")</p> <code>'console'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_operation_label(\"classifier\", [1], decimals=2)\n'1.00 * x'\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; print(get_operation_label(\"classifier_concat\", np.array([1, 2, 3]),\n...     decimals=2, output_format=\"latex\"))\nx \\circ \\left(1.00\\right) + \\left(2.00\\right) + \\left(3.00\\right)\n&gt;&gt;&gt; get_operation_label(\"classifier_concat\", np.array([1, 2, 3]),\n...     decimals=2, output_format=\"console\")\n'x .* (1.00) .+ (2.00) .+ (3.00)'\n&gt;&gt;&gt; get_operation_label(\"linear_exp\", [1,2], decimals=2)\n'exp(1.00 * x + 2.00)'\n&gt;&gt;&gt; get_operation_label(\"none\", [])\n''\n&gt;&gt;&gt; get_operation_label(\"reciprocal\", [1], decimals=0)\n'1 / x'\n&gt;&gt;&gt; get_operation_label(\"linear_reciprocal\", [1, 2], decimals=0)\n'1 / (1 * x + 2)'\n&gt;&gt;&gt; get_operation_label(\"linear_relu\", [1], decimals=0)\n'ReLU(1 * x)'\n&gt;&gt;&gt; print(get_operation_label(\"linear_relu\", [1], decimals=0, output_format=\"latex\"))\n\\operatorname{ReLU}\\left(1x\\right)\n&gt;&gt;&gt; get_operation_label(\"linear\", [1, 2], decimals=0)\n'1 * x + 2'\n&gt;&gt;&gt; get_operation_label(\"linear\", [1, 2], decimals=0, output_format=\"latex\")\n'1 x + 2'\n&gt;&gt;&gt; get_operation_label(\"linrelu\", [1], decimals=0)  # Mistyped operation name\nTraceback (most recent call last):\n...\nNotImplementedError: operation 'linrelu' is not defined for output_format 'console'\n</code></pre> Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def get_operation_label(\n    op_name: str,\n    params_org: typing.List,\n    decimals: int = 4,\n    input_var: str = \"x\",\n    output_format: typing.Literal[\"latex\", \"console\"] = \"console\",\n) -&gt; str:\n    r\"\"\"\n    Returns a complete string describing a DARTS operation.\n\n    Arguments:\n        op_name: name of the operation\n        params_org: original parameters of the operation\n        decimals: number of decimals to be used for converting the parameters into string format\n        input_var: name of the input variable\n        output_format: format of the output string (either \"latex\" or \"console\")\n\n    Examples:\n        &gt;&gt;&gt; get_operation_label(\"classifier\", [1], decimals=2)\n        '1.00 * x'\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; print(get_operation_label(\"classifier_concat\", np.array([1, 2, 3]),\n        ...     decimals=2, output_format=\"latex\"))\n        x \\circ \\left(1.00\\right) + \\left(2.00\\right) + \\left(3.00\\right)\n        &gt;&gt;&gt; get_operation_label(\"classifier_concat\", np.array([1, 2, 3]),\n        ...     decimals=2, output_format=\"console\")\n        'x .* (1.00) .+ (2.00) .+ (3.00)'\n        &gt;&gt;&gt; get_operation_label(\"linear_exp\", [1,2], decimals=2)\n        'exp(1.00 * x + 2.00)'\n        &gt;&gt;&gt; get_operation_label(\"none\", [])\n        ''\n        &gt;&gt;&gt; get_operation_label(\"reciprocal\", [1], decimals=0)\n        '1 / x'\n        &gt;&gt;&gt; get_operation_label(\"linear_reciprocal\", [1, 2], decimals=0)\n        '1 / (1 * x + 2)'\n        &gt;&gt;&gt; get_operation_label(\"linear_relu\", [1], decimals=0)\n        'ReLU(1 * x)'\n        &gt;&gt;&gt; print(get_operation_label(\"linear_relu\", [1], decimals=0, output_format=\"latex\"))\n        \\operatorname{ReLU}\\left(1x\\right)\n        &gt;&gt;&gt; get_operation_label(\"linear\", [1, 2], decimals=0)\n        '1 * x + 2'\n        &gt;&gt;&gt; get_operation_label(\"linear\", [1, 2], decimals=0, output_format=\"latex\")\n        '1 x + 2'\n        &gt;&gt;&gt; get_operation_label(\"linrelu\", [1], decimals=0)  # Mistyped operation name\n        Traceback (most recent call last):\n        ...\n        NotImplementedError: operation 'linrelu' is not defined for output_format 'console'\n    \"\"\"\n    if output_format != \"latex\" and output_format != \"console\":\n        raise ValueError(\"output_format must be either 'latex' or 'console'\")\n\n    params = params_org.copy()\n\n    format_string = \"{:.\" + \"{:.0f}\".format(decimals) + \"f}\"\n\n    classifier_str = \"\"\n    if op_name == \"classifier\":\n        value = params[0]\n        classifier_str = f\"{format_string.format(value)} * {input_var}\"\n        return classifier_str\n\n    if op_name == \"classifier_concat\":\n        if output_format == \"latex\":\n            classifier_str = input_var + \" \\\\circ \\\\left(\"\n        else:\n            classifier_str = input_var + \" .* (\"\n        for param_idx, param in enumerate(params):\n            if param_idx &gt; 0:\n                if output_format == \"latex\":\n                    classifier_str += \" + \\\\left(\"\n                else:\n                    classifier_str += \" .+ (\"\n\n            if isiterable(param.tolist()):\n                param_formatted = list()\n                for value in param.tolist():\n                    param_formatted.append(format_string.format(value))\n\n                for value_idx, value in enumerate(param_formatted):\n                    if value_idx &lt; len(param) - 1:\n                        classifier_str += value + \" + \"\n                    else:\n                        if output_format == \"latex\":\n                            classifier_str += value + \"\\\\right)\"\n                        else:\n                            classifier_str += value + \")\"\n\n            else:\n                value = format_string.format(param)\n\n                if output_format == \"latex\":\n                    classifier_str += value + \"\\\\right)\"\n                else:\n                    classifier_str += value + \")\"\n\n        return classifier_str\n\n    num_params = len(params)\n\n    c = [str(format_string.format(p)) for p in params_org]\n    c.extend([\"\", \"\", \"\"])\n\n    if num_params == 1:  # without bias\n        if output_format == \"console\":\n            labels = {\n                \"none\": \"\",\n                \"add\": f\"+ {input_var}\",\n                \"subtract\": f\"- {input_var}\",\n                \"mult\": f\"{c[0]} * {input_var}\",\n                \"linear\": f\"{c[0]} * {input_var}\",\n                \"relu\": f\"ReLU({input_var})\",\n                \"linear_relu\": f\"ReLU({c[0]} * {input_var})\",\n                \"logistic\": f\"logistic({input_var})\",\n                \"linear_logistic\": f\"logistic({c[0]} * {input_var})\",\n                \"exp\": f\"exp({input_var})\",\n                \"linear_exp\": f\"exp({c[0]} * {input_var})\",\n                \"reciprocal\": f\"1 / {input_var}\",\n                \"linear_reciprocal\": f\"1 / ({c[0]} * {input_var})\",\n                \"ln\": f\"ln({input_var})\",\n                \"linear_ln\": f\"ln({c[0]} * {input_var})\",\n                \"cos\": f\"cos({input_var})\",\n                \"linear_cos\": f\"cos({c[0]} * {input_var})\",\n                \"sin\": f\"sin({input_var})\",\n                \"linear_sin\": f\"sin({c[0]} * {input_var})\",\n                \"tanh\": f\"tanh({input_var})\",\n                \"linear_tanh\": f\"tanh({c[0]} * {input_var})\",\n                \"classifier\": classifier_str,\n            }\n        elif output_format == \"latex\":\n            labels = {\n                \"none\": \"\",\n                \"add\": f\"+ {input_var}\",\n                \"subtract\": f\"- {input_var}\",\n                \"mult\": f\"{c[0]} {input_var}\",\n                \"linear\": c[0] + \"\" + input_var,\n                \"relu\": f\"\\\\operatorname{{ReLU}}\\\\left({input_var}\\\\right)\",\n                \"linear_relu\": f\"\\\\operatorname{{ReLU}}\\\\left({c[0]}{input_var}\\\\right)\",\n                \"logistic\": f\"\\\\sigma\\\\left({input_var}\\\\right)\",\n                \"linear_logistic\": f\"\\\\sigma\\\\left({c[0]} {input_var} \\\\right)\",\n                \"exp\": f\"+ e^{input_var}\",\n                \"linear_exp\": f\"e^{{{c[0]} {input_var} }}\",\n                \"reciprocal\": f\"\\\\frac{{1}}{{{input_var}}}\",\n                \"linear_reciprocal\": f\"\\\\frac{{1}}{{{c[0]} {input_var} }}\",\n                \"ln\": f\"\\\\ln\\\\left({input_var}\\\\right)\",\n                \"linear_ln\": f\"\\\\ln\\\\left({c[0]} {input_var} \\\\right)\",\n                \"cos\": f\"\\\\cos\\\\left({input_var}\\\\right)\",\n                \"linear_cos\": f\"\\\\cos\\\\left({c[0]} {input_var} \\\\right)\",\n                \"sin\": f\"\\\\sin\\\\left({input_var}\\\\right)\",\n                \"linear_sin\": f\"\\\\sin\\\\left({c[0]} {input_var} \\\\right)\",\n                \"tanh\": f\"\\\\tanh\\\\left({input_var}\\\\right)\",\n                \"linear_tanh\": f\"\\\\tanh\\\\left({c[0]} {input_var} \\\\right)\",\n                \"classifier\": classifier_str,\n            }\n    else:  # with bias\n        if output_format == \"console\":\n            labels = {\n                \"none\": \"\",\n                \"add\": f\"+ {input_var}\",\n                \"subtract\": f\"- {input_var}\",\n                \"mult\": f\"{c[0]} * {input_var}\",\n                \"linear\": f\"{c[0]} * {input_var} + {c[1]}\",\n                \"relu\": f\"ReLU({input_var})\",\n                \"linear_relu\": f\"ReLU({c[0]} * {input_var} + {c[1]} )\",\n                \"logistic\": f\"logistic({input_var})\",\n                \"linear_logistic\": f\"logistic({c[0]} * {input_var} + {c[1]})\",\n                \"exp\": f\"exp({input_var})\",\n                \"linear_exp\": f\"exp({c[0]} * {input_var} + {c[1]})\",\n                \"reciprocal\": f\"1 / {input_var}\",\n                \"linear_reciprocal\": f\"1 / ({c[0]} * {input_var} + {c[1]})\",\n                \"ln\": f\"ln({input_var})\",\n                \"linear_ln\": f\"ln({c[0]} * {input_var} + {c[1]})\",\n                \"cos\": f\"cos({input_var})\",\n                \"linear_cos\": f\"cos({c[0]} * {input_var} + {c[1]})\",\n                \"sin\": f\"sin({input_var})\",\n                \"linear_sin\": f\"sin({c[0]} * {input_var} + {c[1]})\",\n                \"tanh\": f\"tanh({input_var})\",\n                \"linear_tanh\": f\"tanh({c[0]} * {input_var} + {c[1]})\",\n                \"classifier\": classifier_str,\n            }\n        elif output_format == \"latex\":\n            labels = {\n                \"none\": \"\",\n                \"add\": f\"+ {input_var}\",\n                \"subtract\": f\"- {input_var}\",\n                \"mult\": f\"{c[0]} * {input_var}\",\n                \"linear\": f\"{c[0]} {input_var} + {c[1]}\",\n                \"relu\": f\"\\\\operatorname{{ReLU}}\\\\left( {input_var}\\\\right)\",\n                \"linear_relu\": f\"\\\\operatorname{{ReLU}}\\\\left({c[0]}{input_var} + {c[1]} \\\\right)\",\n                \"logistic\": f\"\\\\sigma\\\\left( {input_var} \\\\right)\",\n                \"linear_logistic\": f\"\\\\sigma\\\\left( {c[0]} {input_var} + {c[1]} \\\\right)\",\n                \"exp\": f\"e^{input_var}\",\n                \"linear_exp\": f\"e^{{ {c[0]} {input_var} + {c[1]} }}\",\n                \"reciprocal\": f\"\\\\frac{{1}}{{{input_var}}}\",\n                \"linear_reciprocal\": f\"\\\\frac{{1}} {{ {c[0]}{input_var} + {c[1]} }}\",\n                \"ln\": f\"\\\\ln\\\\left({input_var}\\\\right)\",\n                \"linear_ln\": f\"\\\\ln\\\\left({c[0]} {input_var} + {c[1]} \\\\right)\",\n                \"cos\": f\"\\\\cos\\\\left({input_var}\\\\right)\",\n                \"linear_cos\": f\"\\\\cos\\\\left({c[0]} {input_var} + {c[1]} \\\\right)\",\n                \"sin\": f\"\\\\sin\\\\left({input_var}\\\\right)\",\n                \"linear_sin\": f\"\\\\sin\\\\left({c[0]} {input_var} + {c[1]} \\\\right)\",\n                \"tanh\": f\"\\\\tanh\\\\left({input_var}\\\\right)\",\n                \"linear_tanh\": f\"\\\\tanh\\\\left({c[0]} {input_var} + {c[1]} \\\\right)\",\n                \"classifier\": classifier_str,\n            }\n\n    if op_name not in labels:\n        raise NotImplementedError(\n            f\"operation '{op_name}' is not defined for output_format '{output_format}'\"\n        )\n\n    return labels[op_name]\n</code></pre>"},{"location":"reference/autora/theorist/darts/operations/#autora.theorist.darts.operations.isiterable","title":"<code>isiterable(p_object)</code>","text":"<p>Checks if an object is iterable.</p> <p>Parameters:</p> Name Type Description Default <code>p_object</code> <code>Any</code> <p>object to be checked</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/operations.py</code> <pre><code>def isiterable(p_object: typing.Any) -&gt; bool:\n    \"\"\"\n    Checks if an object is iterable.\n\n    Arguments:\n        p_object: object to be checked\n    \"\"\"\n    try:\n        iter(p_object)\n    except TypeError:\n        return False\n    return True\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/","title":"autora.theorist.darts.regressor","text":""},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSExecutionMonitor","title":"<code>DARTSExecutionMonitor</code>","text":"<p>A monitor of the execution of the DARTS algorithm.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>class DARTSExecutionMonitor:\n    \"\"\"\n    A monitor of the execution of the DARTS algorithm.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the execution monitor.\n        \"\"\"\n        self.arch_weight_history = list()\n        self.loss_history = list()\n        self.epoch_history = list()\n        self.primitives = list()\n\n    def execution_monitor(\n        self,\n        network: Network,\n        architect: Architect,\n        epoch: int,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        A function to monitor the execution of the DARTS algorithm.\n\n        Arguments:\n            network: The DARTS network containing the weights each operation\n                in the mixture architecture\n            architect: The architect object used to construct the mixture architecture.\n            epoch: The current epoch of the training.\n            **kwargs: other parameters which may be passed from the DARTS optimizer\n        \"\"\"\n\n        # collect data for visualization\n        self.epoch_history.append(epoch)\n        self.arch_weight_history.append(\n            network.arch_parameters()[0].detach().numpy().copy()[np.newaxis, :]\n        )\n        self.loss_history.append(architect.current_loss)\n        self.primitives = network.primitives\n\n    def display(self):\n        \"\"\"\n        A function to display the execution monitor. This function will generate two plots:\n        (1) A plot of the training loss vs. epoch,\n        (2) a plot of the architecture weights vs. epoch, divided into subplots by each edge\n        in the mixture architecture.\n        \"\"\"\n\n        loss_fig, loss_ax = plt.subplots(1, 1)\n        loss_ax.plot(self.loss_history)\n\n        loss_ax.set_ylabel(\"Loss\", fontsize=14)\n        loss_ax.set_xlabel(\"Epoch\", fontsize=14)\n        loss_ax.set_title(\"Training Loss\")\n\n        arch_weight_history_array = np.vstack(self.arch_weight_history)\n        num_epochs, num_edges, num_primitives = arch_weight_history_array.shape\n\n        subplots_per_side = int(np.ceil(np.sqrt(num_edges)))\n\n        arch_fig, arch_axes = plt.subplots(\n            subplots_per_side,\n            subplots_per_side,\n            sharex=True,\n            sharey=True,\n            figsize=(10, 10),\n            squeeze=False,\n        )\n\n        arch_fig.suptitle(\"Architecture Weights\", fontsize=10)\n\n        for edge_i, ax in zip(range(num_edges), arch_axes.flat):\n            for primitive_i in range(num_primitives):\n                print(f\"{edge_i}, {primitive_i}, {ax}\")\n                ax.plot(\n                    arch_weight_history_array[:, edge_i, primitive_i],\n                    label=f\"{self.primitives[primitive_i]}\",\n                )\n\n            ax.set_title(\"k{}\".format(edge_i), fontsize=8)\n\n            # there is no need to have the legend for each subplot\n            if edge_i == 0:\n                ax.legend(loc=\"upper center\")\n                ax.set_ylabel(\"Edge Weights\", fontsize=8)\n                ax.set_xlabel(\"Epoch\", fontsize=8)\n\n        return SimpleNamespace(\n            loss_fig=loss_fig,\n            loss_ax=loss_ax,\n            arch_fig=arch_fig,\n            arch_axes=arch_axes,\n        )\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSExecutionMonitor.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the execution monitor.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the execution monitor.\n    \"\"\"\n    self.arch_weight_history = list()\n    self.loss_history = list()\n    self.epoch_history = list()\n    self.primitives = list()\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSExecutionMonitor.display","title":"<code>display()</code>","text":"<p>A function to display the execution monitor. This function will generate two plots: (1) A plot of the training loss vs. epoch, (2) a plot of the architecture weights vs. epoch, divided into subplots by each edge in the mixture architecture.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def display(self):\n    \"\"\"\n    A function to display the execution monitor. This function will generate two plots:\n    (1) A plot of the training loss vs. epoch,\n    (2) a plot of the architecture weights vs. epoch, divided into subplots by each edge\n    in the mixture architecture.\n    \"\"\"\n\n    loss_fig, loss_ax = plt.subplots(1, 1)\n    loss_ax.plot(self.loss_history)\n\n    loss_ax.set_ylabel(\"Loss\", fontsize=14)\n    loss_ax.set_xlabel(\"Epoch\", fontsize=14)\n    loss_ax.set_title(\"Training Loss\")\n\n    arch_weight_history_array = np.vstack(self.arch_weight_history)\n    num_epochs, num_edges, num_primitives = arch_weight_history_array.shape\n\n    subplots_per_side = int(np.ceil(np.sqrt(num_edges)))\n\n    arch_fig, arch_axes = plt.subplots(\n        subplots_per_side,\n        subplots_per_side,\n        sharex=True,\n        sharey=True,\n        figsize=(10, 10),\n        squeeze=False,\n    )\n\n    arch_fig.suptitle(\"Architecture Weights\", fontsize=10)\n\n    for edge_i, ax in zip(range(num_edges), arch_axes.flat):\n        for primitive_i in range(num_primitives):\n            print(f\"{edge_i}, {primitive_i}, {ax}\")\n            ax.plot(\n                arch_weight_history_array[:, edge_i, primitive_i],\n                label=f\"{self.primitives[primitive_i]}\",\n            )\n\n        ax.set_title(\"k{}\".format(edge_i), fontsize=8)\n\n        # there is no need to have the legend for each subplot\n        if edge_i == 0:\n            ax.legend(loc=\"upper center\")\n            ax.set_ylabel(\"Edge Weights\", fontsize=8)\n            ax.set_xlabel(\"Epoch\", fontsize=8)\n\n    return SimpleNamespace(\n        loss_fig=loss_fig,\n        loss_ax=loss_ax,\n        arch_fig=arch_fig,\n        arch_axes=arch_axes,\n    )\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSExecutionMonitor.execution_monitor","title":"<code>execution_monitor(network, architect, epoch, **kwargs)</code>","text":"<p>A function to monitor the execution of the DARTS algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The DARTS network containing the weights each operation in the mixture architecture</p> required <code>architect</code> <code>Architect</code> <p>The architect object used to construct the mixture architecture.</p> required <code>epoch</code> <code>int</code> <p>The current epoch of the training.</p> required <code>**kwargs</code> <code>Any</code> <p>other parameters which may be passed from the DARTS optimizer</p> <code>{}</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def execution_monitor(\n    self,\n    network: Network,\n    architect: Architect,\n    epoch: int,\n    **kwargs: Any,\n):\n    \"\"\"\n    A function to monitor the execution of the DARTS algorithm.\n\n    Arguments:\n        network: The DARTS network containing the weights each operation\n            in the mixture architecture\n        architect: The architect object used to construct the mixture architecture.\n        epoch: The current epoch of the training.\n        **kwargs: other parameters which may be passed from the DARTS optimizer\n    \"\"\"\n\n    # collect data for visualization\n    self.epoch_history.append(epoch)\n    self.arch_weight_history.append(\n        network.arch_parameters()[0].detach().numpy().copy()[np.newaxis, :]\n    )\n    self.loss_history.append(architect.current_loss)\n    self.primitives = network.primitives\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSRegressor","title":"<code>DARTSRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Differentiable ARchiTecture Search Regressor.</p> <p>DARTS finds a composition of functions and coefficients to minimize a loss function suitable for the dependent variable.</p> <p>This class is intended to be compatible with the Scikit-Learn Estimator API.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; num_samples = 1000\n&gt;&gt;&gt; X = np.linspace(start=0, stop=1, num=num_samples).reshape(-1, 1)\n&gt;&gt;&gt; y = 15. * np.ones(num_samples)\n&gt;&gt;&gt; estimator = DARTSRegressor(num_graph_nodes=1)\n&gt;&gt;&gt; estimator = estimator.fit(X, y)\n&gt;&gt;&gt; estimator.predict([[0.5]])\narray([[15.051043]], dtype=float32)\n</code></pre> <p>Attributes:</p> Name Type Description <code>network_</code> <code>Optional[Network]</code> <p>represents the optimized network for the architecture search, without the output function</p> <code>model_</code> <code>Optional[Network]</code> <p>represents the best-fit model including the output function after sampling of the network to pick a single computation graph. By default, this is the computation graph with the maximum weights, but can be set to a graph based on a sample on the edge weights by running the <code>resample_model(sample_strategy=\"sample\")</code> method. It can be reset by running the <code>resample_model(sample_strategy=\"max\")</code> method.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>class DARTSRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"\n    Differentiable ARchiTecture Search Regressor.\n\n    DARTS finds a composition of functions and coefficients to minimize a loss function suitable for\n    the dependent variable.\n\n    This class is intended to be compatible with the\n    [Scikit-Learn Estimator API](https://scikit-learn.org/stable/developers/develop.html).\n\n    Examples:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; num_samples = 1000\n        &gt;&gt;&gt; X = np.linspace(start=0, stop=1, num=num_samples).reshape(-1, 1)\n        &gt;&gt;&gt; y = 15. * np.ones(num_samples)\n        &gt;&gt;&gt; estimator = DARTSRegressor(num_graph_nodes=1)\n        &gt;&gt;&gt; estimator = estimator.fit(X, y)\n        &gt;&gt;&gt; estimator.predict([[0.5]])\n        array([[15.051043]], dtype=float32)\n\n\n    Attributes:\n        network_: represents the optimized network for the architecture search, without the\n            output function\n        model_: represents the best-fit model including the output function\n            after sampling of the network to pick a single computation graph.\n            By default, this is the computation graph with the maximum weights,\n            but can be set to a graph based on a sample on the edge weights\n            by running the `resample_model(sample_strategy=\"sample\")` method.\n            It can be reset by running the `resample_model(sample_strategy=\"max\")` method.\n\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 64,\n        num_graph_nodes: int = 2,\n        output_type: IMPLEMENTED_OUTPUT_TYPES = \"real\",\n        classifier_weight_decay: float = 1e-2,\n        darts_type: IMPLEMENTED_DARTS_TYPES = \"original\",\n        init_weights_function: Optional[Callable] = None,\n        param_updates_per_epoch: int = 10,\n        param_updates_for_sampled_model: int = 100,\n        param_learning_rate_max: float = 2.5e-2,\n        param_learning_rate_min: float = 0.01,\n        param_momentum: float = 9e-1,\n        param_weight_decay: float = 3e-4,\n        arch_updates_per_epoch: int = 1,\n        arch_learning_rate_max: float = 3e-3,\n        arch_weight_decay: float = 1e-4,\n        arch_weight_decay_df: float = 3e-4,\n        arch_weight_decay_base: float = 0.0,\n        arch_momentum: float = 9e-1,\n        fair_darts_loss_weight: int = 1,\n        max_epochs: int = 10,\n        grad_clip: float = 5,\n        primitives: Sequence[str] = PRIMITIVES,\n        train_classifier_coefficients: bool = False,\n        train_classifier_bias: bool = False,\n        execution_monitor: Callable = (lambda *args, **kwargs: None),\n        sampling_strategy: SAMPLING_STRATEGIES = \"max\",\n    ) -&gt; None:\n        \"\"\"\n        Initializes the DARTSRegressor.\n\n        Arguments:\n            batch_size: Batch size for the data loader.\n            num_graph_nodes: Number of nodes in the desired computation graph.\n            output_type: Type of output function to use. This function is applied to transform\n                the output of the mixture architecture.\n            classifier_weight_decay: Weight decay for the classifier.\n            darts_type: Type of DARTS to use ('original' or 'fair').\n            init_weights_function: Function to initialize the parameters of each operation.\n            param_updates_per_epoch: Number of updates to perform per epoch.\n                for the operation parameters.\n            param_learning_rate_max: Initial (maximum) learning rate for the operation parameters.\n            param_learning_rate_min: Final (minimum) learning rate for the operation parameters.\n            param_momentum: Momentum for the operation parameters.\n            param_weight_decay: Weight decay for the operation parameters.\n            arch_updates_per_epoch: Number of architecture weight updates to perform per epoch.\n            arch_learning_rate_max: Initial (maximum) learning rate for the architecture.\n            arch_weight_decay: Weight decay for the architecture weights.\n            arch_weight_decay_df: An additional weight decay that scales with the number of\n                parameters (degrees of freedom) in the operation. The higher this weight decay,\n                the more DARTS will prefer simple operations.\n            arch_weight_decay_base: A base weight decay that is added to the scaled weight decay.\n                arch_momentum: Momentum for the architecture weights.\n            fair_darts_loss_weight: Weight of the loss in fair darts which forces architecture\n                weights to become either 0 or 1.\n            max_epochs: Maximum number of epochs to train for.\n            grad_clip: Gradient clipping value for updating the parameters of the operations.\n            primitives: List of primitives (operations) to use.\n            train_classifier_coefficients: Whether to train the coefficients of the classifier.\n            train_classifier_bias: Whether to train the bias of the classifier.\n            execution_monitor: Function to monitor the execution of the model.\n            primitives: list of primitive operations used in the DARTS network,\n                e.g., 'add', 'subtract', 'none'. For details, see\n                [`autora.theorist.darts.operations`][autora.theorist.darts.operations]\n        \"\"\"\n\n        self.batch_size = batch_size\n\n        self.num_graph_nodes = num_graph_nodes\n        self.classifier_weight_decay = classifier_weight_decay\n        self.darts_type = darts_type\n        self.init_weights_function = init_weights_function\n\n        self.param_updates_per_epoch = param_updates_per_epoch\n        self.param_updates_for_sampled_model = param_updates_for_sampled_model\n\n        self.param_learning_rate_max = param_learning_rate_max\n        self.param_learning_rate_min = param_learning_rate_min\n        self.param_momentum = param_momentum\n        self.arch_momentum = arch_momentum\n        self.param_weight_decay = param_weight_decay\n\n        self.arch_updates_per_epoch = arch_updates_per_epoch\n        self.arch_weight_decay = arch_weight_decay\n        self.arch_weight_decay_df = arch_weight_decay_df\n        self.arch_weight_decay_base = arch_weight_decay_base\n        self.arch_learning_rate_max = arch_learning_rate_max\n        self.fair_darts_loss_weight = fair_darts_loss_weight\n\n        self.max_epochs = max_epochs\n        self.grad_clip = grad_clip\n\n        self.primitives = primitives\n\n        self.output_type = output_type\n        self.darts_type = darts_type\n\n        self.X_: Optional[np.ndarray] = None\n        self.y_: Optional[np.ndarray] = None\n        self.network_: Optional[Network] = None\n        self.model_: Optional[Network] = None\n\n        self.train_classifier_coefficients = train_classifier_coefficients\n        self.train_classifier_bias = train_classifier_bias\n\n        self.execution_monitor = execution_monitor\n\n        self.sampling_strategy = sampling_strategy\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Runs the optimization for a given set of `X`s and `y`s.\n\n        Arguments:\n            X: independent variables in an n-dimensional array\n            y: dependent variables in an n-dimensional array\n\n        Returns:\n            self (DARTSRegressor): the fitted estimator\n        \"\"\"\n\n        if self.output_type == \"class\":\n            raise NotImplementedError(\n                \"Classification not implemented for DARTSRegressor.\"\n            )\n\n        params = self.get_params()\n\n        fit_results = _general_darts(X=X, y=y, network=self.network_, **params)\n        self.X_ = X\n        self.y_ = y\n        self.network_ = fit_results.network\n        self.model_ = fit_results.model\n        return self\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Applies the fitted model to a set of independent variables `X`,\n        to give predictions for the dependent variable `y`.\n\n        Arguments:\n            X: independent variables in an n-dimensional array\n\n        Returns:\n            y: predicted dependent variable values\n        \"\"\"\n        X_ = check_array(X)\n\n        # First run the checks using the scikit-learn API, listing the key parameters\n        check_is_fitted(self, attributes=[\"model_\"])\n\n        # Since self.model_ is initialized as None, mypy throws an error if we\n        # just call self.model_(X) in the predict method, as it could still be none.\n        # MyPy doesn't understand that the sklearn check_is_fitted function\n        # ensures the self.model_ parameter is initialized and otherwise throws an error,\n        # so we check that explicitly here and pass the model which can't be None.\n        assert self.model_ is not None\n\n        y_ = self.model_(torch.as_tensor(X_).float())\n        y = y_.detach().numpy()\n\n        return y\n\n    def visualize_model(\n        self,\n        input_labels: Optional[Sequence[str]] = None,\n    ):\n        \"\"\"\n        Visualizes the model architecture as a graph.\n\n        Arguments:\n            input_labels: labels for the input nodes\n\n        \"\"\"\n\n        check_is_fitted(self, attributes=[\"model_\"])\n        assert self.model_ is not None\n        fitted_sampled_network = self.model_[0]\n\n        genotype = Network.genotype(fitted_sampled_network).normal\n        (\n            _,\n            _,\n            param_list,\n        ) = fitted_sampled_network.count_parameters()\n\n        if input_labels is not None:\n            input_labels_ = tuple(input_labels)\n        else:\n            input_labels_ = self._get_input_labels()\n\n        assert self.y_ is not None\n        out_dim = 1 if self.y_.ndim == 1 else self.y_.shape[1]\n\n        out_func = get_output_str(ValueType(self.output_type))\n\n        # call to plot function\n        graph = darts_model_plot(\n            genotype=genotype,\n            input_labels=input_labels_,\n            param_list=param_list,\n            full_label=True,\n            out_dim=out_dim,\n            out_fnc=out_func,\n        )\n\n        return graph\n\n    def _get_input_labels(self):\n        \"\"\"\n        Returns the input labels for the model.\n\n        Returns:\n            input_labels: labels for the input nodes\n\n        \"\"\"\n        return self._get_labels(self.X_, \"x\")\n\n    def _get_output_labels(self):\n        \"\"\"\n        Returns the output labels for the model.\n\n        Returns:\n            output_labels: labels for the output nodes\n\n        \"\"\"\n        return self._get_labels(self.y_, \"y\")\n\n    def _get_labels(\n        self, data: Optional[np.ndarray], default_label: str\n    ) -&gt; Sequence[str]:\n        \"\"\"\n        Returns the labels for the model.\n\n        Arguments:\n            data: data to get labels for\n            default_label: default label to use if no labels are provided\n\n        Returns:\n            labels: labels for the model\n\n        \"\"\"\n        assert data is not None\n\n        if hasattr(data, \"columns\"):  # it's a dataframe with column names\n            labels_ = tuple(data.columns)\n        elif (\n            hasattr(data, \"name\") and len(data.shape) == 1\n        ):  # it's a single series with a single name\n            labels_ = (data.name,)\n\n        else:\n            dim = 1 if data.ndim == 1 else data.shape[1]\n            labels_ = tuple(f\"{default_label}{i+1}\" for i in range(dim))\n        return labels_\n\n    def model_repr(\n        self,\n        input_labels: Optional[Sequence[str]] = None,\n        output_labels: Optional[Sequence[str]] = None,\n        output_function_label: str = \"\",\n        decimals_to_display: int = 2,\n        output_format: Literal[\"latex\", \"console\"] = \"console\",\n    ) -&gt; str:\n        \"\"\"\n        Prints the equations of the model architecture.\n\n        Args:\n            input_labels: which names to use for the independent variables (X)\n            output_labels: which names to use for the dependent variables (y)\n            output_function_label: name to use for the output transformation\n            decimals_to_display: amount of rounding for the coefficient values\n            output_format: whether the output should be formatted for\n                the command line (`console`) or as equations in a latex file (`latex`)\n\n        Returns:\n            The equations of the model architecture\n\n        \"\"\"\n        assert self.model_ is not None\n        fitted_sampled_network: Network = self.model_[0]\n\n        if input_labels is None:\n            input_labels_ = self._get_input_labels()\n        else:\n            input_labels_ = input_labels\n\n        if output_labels is None:\n            output_labels_ = self._get_output_labels()\n        else:\n            output_labels_ = output_labels\n\n        edge_list = fitted_sampled_network.architecture_to_str_list(\n            input_labels=input_labels_,\n            output_labels=output_labels_,\n            output_function_label=output_function_label,\n            decimals_to_display=decimals_to_display,\n            output_format=output_format,\n        )\n\n        model_repr_ = \"\\n\".join([\"Model:\"] + edge_list)\n        return model_repr_\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSRegressor.__init__","title":"<code>__init__(batch_size=64, num_graph_nodes=2, output_type='real', classifier_weight_decay=0.01, darts_type='original', init_weights_function=None, param_updates_per_epoch=10, param_updates_for_sampled_model=100, param_learning_rate_max=0.025, param_learning_rate_min=0.01, param_momentum=0.9, param_weight_decay=0.0003, arch_updates_per_epoch=1, arch_learning_rate_max=0.003, arch_weight_decay=0.0001, arch_weight_decay_df=0.0003, arch_weight_decay_base=0.0, arch_momentum=0.9, fair_darts_loss_weight=1, max_epochs=10, grad_clip=5, primitives=PRIMITIVES, train_classifier_coefficients=False, train_classifier_bias=False, execution_monitor=lambda *args, **kwargs: None, sampling_strategy='max')</code>","text":"<p>Initializes the DARTSRegressor.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for the data loader.</p> <code>64</code> <code>num_graph_nodes</code> <code>int</code> <p>Number of nodes in the desired computation graph.</p> <code>2</code> <code>output_type</code> <code>IMPLEMENTED_OUTPUT_TYPES</code> <p>Type of output function to use. This function is applied to transform the output of the mixture architecture.</p> <code>'real'</code> <code>classifier_weight_decay</code> <code>float</code> <p>Weight decay for the classifier.</p> <code>0.01</code> <code>darts_type</code> <code>IMPLEMENTED_DARTS_TYPES</code> <p>Type of DARTS to use ('original' or 'fair').</p> <code>'original'</code> <code>init_weights_function</code> <code>Optional[Callable]</code> <p>Function to initialize the parameters of each operation.</p> <code>None</code> <code>param_updates_per_epoch</code> <code>int</code> <p>Number of updates to perform per epoch. for the operation parameters.</p> <code>10</code> <code>param_learning_rate_max</code> <code>float</code> <p>Initial (maximum) learning rate for the operation parameters.</p> <code>0.025</code> <code>param_learning_rate_min</code> <code>float</code> <p>Final (minimum) learning rate for the operation parameters.</p> <code>0.01</code> <code>param_momentum</code> <code>float</code> <p>Momentum for the operation parameters.</p> <code>0.9</code> <code>param_weight_decay</code> <code>float</code> <p>Weight decay for the operation parameters.</p> <code>0.0003</code> <code>arch_updates_per_epoch</code> <code>int</code> <p>Number of architecture weight updates to perform per epoch.</p> <code>1</code> <code>arch_learning_rate_max</code> <code>float</code> <p>Initial (maximum) learning rate for the architecture.</p> <code>0.003</code> <code>arch_weight_decay</code> <code>float</code> <p>Weight decay for the architecture weights.</p> <code>0.0001</code> <code>arch_weight_decay_df</code> <code>float</code> <p>An additional weight decay that scales with the number of parameters (degrees of freedom) in the operation. The higher this weight decay, the more DARTS will prefer simple operations.</p> <code>0.0003</code> <code>arch_weight_decay_base</code> <code>float</code> <p>A base weight decay that is added to the scaled weight decay. arch_momentum: Momentum for the architecture weights.</p> <code>0.0</code> <code>fair_darts_loss_weight</code> <code>int</code> <p>Weight of the loss in fair darts which forces architecture weights to become either 0 or 1.</p> <code>1</code> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to train for.</p> <code>10</code> <code>grad_clip</code> <code>float</code> <p>Gradient clipping value for updating the parameters of the operations.</p> <code>5</code> <code>primitives</code> <code>Sequence[str]</code> <p>List of primitives (operations) to use.</p> <code>PRIMITIVES</code> <code>train_classifier_coefficients</code> <code>bool</code> <p>Whether to train the coefficients of the classifier.</p> <code>False</code> <code>train_classifier_bias</code> <code>bool</code> <p>Whether to train the bias of the classifier.</p> <code>False</code> <code>execution_monitor</code> <code>Callable</code> <p>Function to monitor the execution of the model.</p> <code>lambda *args, **kwargs: None</code> <code>primitives</code> <code>Sequence[str]</code> <p>list of primitive operations used in the DARTS network, e.g., 'add', 'subtract', 'none'. For details, see <code>autora.theorist.darts.operations</code></p> <code>PRIMITIVES</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 64,\n    num_graph_nodes: int = 2,\n    output_type: IMPLEMENTED_OUTPUT_TYPES = \"real\",\n    classifier_weight_decay: float = 1e-2,\n    darts_type: IMPLEMENTED_DARTS_TYPES = \"original\",\n    init_weights_function: Optional[Callable] = None,\n    param_updates_per_epoch: int = 10,\n    param_updates_for_sampled_model: int = 100,\n    param_learning_rate_max: float = 2.5e-2,\n    param_learning_rate_min: float = 0.01,\n    param_momentum: float = 9e-1,\n    param_weight_decay: float = 3e-4,\n    arch_updates_per_epoch: int = 1,\n    arch_learning_rate_max: float = 3e-3,\n    arch_weight_decay: float = 1e-4,\n    arch_weight_decay_df: float = 3e-4,\n    arch_weight_decay_base: float = 0.0,\n    arch_momentum: float = 9e-1,\n    fair_darts_loss_weight: int = 1,\n    max_epochs: int = 10,\n    grad_clip: float = 5,\n    primitives: Sequence[str] = PRIMITIVES,\n    train_classifier_coefficients: bool = False,\n    train_classifier_bias: bool = False,\n    execution_monitor: Callable = (lambda *args, **kwargs: None),\n    sampling_strategy: SAMPLING_STRATEGIES = \"max\",\n) -&gt; None:\n    \"\"\"\n    Initializes the DARTSRegressor.\n\n    Arguments:\n        batch_size: Batch size for the data loader.\n        num_graph_nodes: Number of nodes in the desired computation graph.\n        output_type: Type of output function to use. This function is applied to transform\n            the output of the mixture architecture.\n        classifier_weight_decay: Weight decay for the classifier.\n        darts_type: Type of DARTS to use ('original' or 'fair').\n        init_weights_function: Function to initialize the parameters of each operation.\n        param_updates_per_epoch: Number of updates to perform per epoch.\n            for the operation parameters.\n        param_learning_rate_max: Initial (maximum) learning rate for the operation parameters.\n        param_learning_rate_min: Final (minimum) learning rate for the operation parameters.\n        param_momentum: Momentum for the operation parameters.\n        param_weight_decay: Weight decay for the operation parameters.\n        arch_updates_per_epoch: Number of architecture weight updates to perform per epoch.\n        arch_learning_rate_max: Initial (maximum) learning rate for the architecture.\n        arch_weight_decay: Weight decay for the architecture weights.\n        arch_weight_decay_df: An additional weight decay that scales with the number of\n            parameters (degrees of freedom) in the operation. The higher this weight decay,\n            the more DARTS will prefer simple operations.\n        arch_weight_decay_base: A base weight decay that is added to the scaled weight decay.\n            arch_momentum: Momentum for the architecture weights.\n        fair_darts_loss_weight: Weight of the loss in fair darts which forces architecture\n            weights to become either 0 or 1.\n        max_epochs: Maximum number of epochs to train for.\n        grad_clip: Gradient clipping value for updating the parameters of the operations.\n        primitives: List of primitives (operations) to use.\n        train_classifier_coefficients: Whether to train the coefficients of the classifier.\n        train_classifier_bias: Whether to train the bias of the classifier.\n        execution_monitor: Function to monitor the execution of the model.\n        primitives: list of primitive operations used in the DARTS network,\n            e.g., 'add', 'subtract', 'none'. For details, see\n            [`autora.theorist.darts.operations`][autora.theorist.darts.operations]\n    \"\"\"\n\n    self.batch_size = batch_size\n\n    self.num_graph_nodes = num_graph_nodes\n    self.classifier_weight_decay = classifier_weight_decay\n    self.darts_type = darts_type\n    self.init_weights_function = init_weights_function\n\n    self.param_updates_per_epoch = param_updates_per_epoch\n    self.param_updates_for_sampled_model = param_updates_for_sampled_model\n\n    self.param_learning_rate_max = param_learning_rate_max\n    self.param_learning_rate_min = param_learning_rate_min\n    self.param_momentum = param_momentum\n    self.arch_momentum = arch_momentum\n    self.param_weight_decay = param_weight_decay\n\n    self.arch_updates_per_epoch = arch_updates_per_epoch\n    self.arch_weight_decay = arch_weight_decay\n    self.arch_weight_decay_df = arch_weight_decay_df\n    self.arch_weight_decay_base = arch_weight_decay_base\n    self.arch_learning_rate_max = arch_learning_rate_max\n    self.fair_darts_loss_weight = fair_darts_loss_weight\n\n    self.max_epochs = max_epochs\n    self.grad_clip = grad_clip\n\n    self.primitives = primitives\n\n    self.output_type = output_type\n    self.darts_type = darts_type\n\n    self.X_: Optional[np.ndarray] = None\n    self.y_: Optional[np.ndarray] = None\n    self.network_: Optional[Network] = None\n    self.model_: Optional[Network] = None\n\n    self.train_classifier_coefficients = train_classifier_coefficients\n    self.train_classifier_bias = train_classifier_bias\n\n    self.execution_monitor = execution_monitor\n\n    self.sampling_strategy = sampling_strategy\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Runs the optimization for a given set of <code>X</code>s and <code>y</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>independent variables in an n-dimensional array</p> required <code>y</code> <code>ndarray</code> <p>dependent variables in an n-dimensional array</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>DARTSRegressor</code> <p>the fitted estimator</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Runs the optimization for a given set of `X`s and `y`s.\n\n    Arguments:\n        X: independent variables in an n-dimensional array\n        y: dependent variables in an n-dimensional array\n\n    Returns:\n        self (DARTSRegressor): the fitted estimator\n    \"\"\"\n\n    if self.output_type == \"class\":\n        raise NotImplementedError(\n            \"Classification not implemented for DARTSRegressor.\"\n        )\n\n    params = self.get_params()\n\n    fit_results = _general_darts(X=X, y=y, network=self.network_, **params)\n    self.X_ = X\n    self.y_ = y\n    self.network_ = fit_results.network\n    self.model_ = fit_results.model\n    return self\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSRegressor.model_repr","title":"<code>model_repr(input_labels=None, output_labels=None, output_function_label='', decimals_to_display=2, output_format='console')</code>","text":"<p>Prints the equations of the model architecture.</p> <p>Parameters:</p> Name Type Description Default <code>input_labels</code> <code>Optional[Sequence[str]]</code> <p>which names to use for the independent variables (X)</p> <code>None</code> <code>output_labels</code> <code>Optional[Sequence[str]]</code> <p>which names to use for the dependent variables (y)</p> <code>None</code> <code>output_function_label</code> <code>str</code> <p>name to use for the output transformation</p> <code>''</code> <code>decimals_to_display</code> <code>int</code> <p>amount of rounding for the coefficient values</p> <code>2</code> <code>output_format</code> <code>Literal['latex', 'console']</code> <p>whether the output should be formatted for the command line (<code>console</code>) or as equations in a latex file (<code>latex</code>)</p> <code>'console'</code> <p>Returns:</p> Type Description <code>str</code> <p>The equations of the model architecture</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def model_repr(\n    self,\n    input_labels: Optional[Sequence[str]] = None,\n    output_labels: Optional[Sequence[str]] = None,\n    output_function_label: str = \"\",\n    decimals_to_display: int = 2,\n    output_format: Literal[\"latex\", \"console\"] = \"console\",\n) -&gt; str:\n    \"\"\"\n    Prints the equations of the model architecture.\n\n    Args:\n        input_labels: which names to use for the independent variables (X)\n        output_labels: which names to use for the dependent variables (y)\n        output_function_label: name to use for the output transformation\n        decimals_to_display: amount of rounding for the coefficient values\n        output_format: whether the output should be formatted for\n            the command line (`console`) or as equations in a latex file (`latex`)\n\n    Returns:\n        The equations of the model architecture\n\n    \"\"\"\n    assert self.model_ is not None\n    fitted_sampled_network: Network = self.model_[0]\n\n    if input_labels is None:\n        input_labels_ = self._get_input_labels()\n    else:\n        input_labels_ = input_labels\n\n    if output_labels is None:\n        output_labels_ = self._get_output_labels()\n    else:\n        output_labels_ = output_labels\n\n    edge_list = fitted_sampled_network.architecture_to_str_list(\n        input_labels=input_labels_,\n        output_labels=output_labels_,\n        output_function_label=output_function_label,\n        decimals_to_display=decimals_to_display,\n        output_format=output_format,\n    )\n\n    model_repr_ = \"\\n\".join([\"Model:\"] + edge_list)\n    return model_repr_\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Applies the fitted model to a set of independent variables <code>X</code>, to give predictions for the dependent variable <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>independent variables in an n-dimensional array</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>predicted dependent variable values</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies the fitted model to a set of independent variables `X`,\n    to give predictions for the dependent variable `y`.\n\n    Arguments:\n        X: independent variables in an n-dimensional array\n\n    Returns:\n        y: predicted dependent variable values\n    \"\"\"\n    X_ = check_array(X)\n\n    # First run the checks using the scikit-learn API, listing the key parameters\n    check_is_fitted(self, attributes=[\"model_\"])\n\n    # Since self.model_ is initialized as None, mypy throws an error if we\n    # just call self.model_(X) in the predict method, as it could still be none.\n    # MyPy doesn't understand that the sklearn check_is_fitted function\n    # ensures the self.model_ parameter is initialized and otherwise throws an error,\n    # so we check that explicitly here and pass the model which can't be None.\n    assert self.model_ is not None\n\n    y_ = self.model_(torch.as_tensor(X_).float())\n    y = y_.detach().numpy()\n\n    return y\n</code></pre>"},{"location":"reference/autora/theorist/darts/regressor/#autora.theorist.darts.regressor.DARTSRegressor.visualize_model","title":"<code>visualize_model(input_labels=None)</code>","text":"<p>Visualizes the model architecture as a graph.</p> <p>Parameters:</p> Name Type Description Default <code>input_labels</code> <code>Optional[Sequence[str]]</code> <p>labels for the input nodes</p> <code>None</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/regressor.py</code> <pre><code>def visualize_model(\n    self,\n    input_labels: Optional[Sequence[str]] = None,\n):\n    \"\"\"\n    Visualizes the model architecture as a graph.\n\n    Arguments:\n        input_labels: labels for the input nodes\n\n    \"\"\"\n\n    check_is_fitted(self, attributes=[\"model_\"])\n    assert self.model_ is not None\n    fitted_sampled_network = self.model_[0]\n\n    genotype = Network.genotype(fitted_sampled_network).normal\n    (\n        _,\n        _,\n        param_list,\n    ) = fitted_sampled_network.count_parameters()\n\n    if input_labels is not None:\n        input_labels_ = tuple(input_labels)\n    else:\n        input_labels_ = self._get_input_labels()\n\n    assert self.y_ is not None\n    out_dim = 1 if self.y_.ndim == 1 else self.y_.shape[1]\n\n    out_func = get_output_str(ValueType(self.output_type))\n\n    # call to plot function\n    graph = darts_model_plot(\n        genotype=genotype,\n        input_labels=input_labels_,\n        param_list=param_list,\n        full_label=True,\n        out_dim=out_dim,\n        out_fnc=out_func,\n    )\n\n    return graph\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/","title":"autora.theorist.darts.utils","text":""},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.AvgrageMeter","title":"<code>AvgrageMeter</code>","text":"<p>               Bases: <code>object</code></p> <p>Computes and stores the average and current value.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>class AvgrageMeter(object):\n    \"\"\"\n    Computes and stores the average and current value.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the average meter.\n        \"\"\"\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Resets the average meter.\n        \"\"\"\n        self.avg = 0\n        self.sum = 0\n        self.cnt = 0\n\n    def update(self, val: float, n: int = 1):\n        \"\"\"\n        Updates the average meter.\n\n        Arguments:\n            val: value to update the average meter with\n            n: number of times to update the average meter\n        \"\"\"\n        self.sum += val * n\n        self.cnt += n\n        self.avg = self.sum / self.cnt\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.AvgrageMeter.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the average meter.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the average meter.\n    \"\"\"\n    self.reset()\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.AvgrageMeter.reset","title":"<code>reset()</code>","text":"<p>Resets the average meter.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the average meter.\n    \"\"\"\n    self.avg = 0\n    self.sum = 0\n    self.cnt = 0\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.AvgrageMeter.update","title":"<code>update(val, n=1)</code>","text":"<p>Updates the average meter.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>float</code> <p>value to update the average meter with</p> required <code>n</code> <code>int</code> <p>number of times to update the average meter</p> <code>1</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def update(self, val: float, n: int = 1):\n    \"\"\"\n    Updates the average meter.\n\n    Arguments:\n        val: value to update the average meter with\n        n: number of times to update the average meter\n    \"\"\"\n    self.sum += val * n\n    self.cnt += n\n    self.avg = self.sum / self.cnt\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.accuracy","title":"<code>accuracy(output, target, topk=(1,))</code>","text":"<p>Computes the accuracy over the k top predictions for the specified values of k.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Tensor</code> <p>output of the model</p> required <code>target</code> <code>Tensor</code> <p>target of the model</p> required <code>topk</code> <code>Tuple</code> <p>values of k to compute the accuracy at</p> <code>(1,)</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple = (1,)) -&gt; List:\n    \"\"\"\n    Computes the accuracy over the k top predictions for the specified values of k.\n\n    Arguments:\n        output: output of the model\n        target: target of the model\n        topk: values of k to compute the accuracy at\n    \"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.assign_slurm_instance","title":"<code>assign_slurm_instance(slurm_id, arch_weight_decay_list, num_node_list, seed_list)</code>","text":"<p>Determines the meta-search parameters based on the slum job id.</p> <p>Parameters:</p> Name Type Description Default <code>slurm_id</code> <code>int</code> <p>slurm job id</p> required <code>arch_weight_decay_list</code> <code>List</code> <p>list of weight decay values</p> required <code>num_node_list</code> <code>List</code> <p>list of number of nodes</p> required <code>seed_list</code> <code>List</code> <p>list of seeds</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def assign_slurm_instance(\n    slurm_id: int,\n    arch_weight_decay_list: List,\n    num_node_list: List,\n    seed_list: List,\n) -&gt; Tuple:\n    \"\"\"\n    Determines the meta-search parameters based on the slum job id.\n\n    Arguments:\n        slurm_id: slurm job id\n        arch_weight_decay_list: list of weight decay values\n        num_node_list: list of number of nodes\n        seed_list: list of seeds\n    \"\"\"\n\n    seed_id = np.floor(\n        slurm_id / (len(num_node_list) * len(arch_weight_decay_list))\n    ) % len(seed_list)\n    k_id = np.floor(slurm_id / (len(arch_weight_decay_list))) % len(num_node_list)\n    weight_decay_id = slurm_id % len(arch_weight_decay_list)\n\n    return (\n        arch_weight_decay_list[int(weight_decay_id)],\n        int(num_node_list[int(k_id)]),\n        int(seed_list[int(seed_id)]),\n    )\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.compute_BIC","title":"<code>compute_BIC(output_type, model, input, target)</code>","text":"<p>Returns the Bayesian information criterion for a DARTS model.</p> <p>Parameters:</p> Name Type Description Default <code>output_type</code> <code>ValueType</code> <p>output type of the dependent variable</p> required <code>model</code> <code>Module</code> <p>model to compute the BIC for</p> required <code>input</code> <code>Tensor</code> <p>input of the model</p> required <code>target</code> <code>Tensor</code> <p>target of the model</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def compute_BIC(\n    output_type: ValueType,\n    model: torch.nn.Module,\n    input: torch.Tensor,\n    target: torch.Tensor,\n) -&gt; float:\n    \"\"\"\n    Returns the Bayesian information criterion for a DARTS model.\n\n    Arguments:\n        output_type: output type of the dependent variable\n        model: model to compute the BIC for\n        input: input of the model\n        target: target of the model\n    \"\"\"\n\n    # compute raw model output\n    classifier_output = model(input)\n\n    # compute associated probability\n    m = get_output_format(output_type)\n    prediction = m(classifier_output).detach()\n\n    k, _, _ = model.countParameters()  # for most likely architecture\n\n    if output_type == ValueType.CLASS:\n        target_flattened = torch.flatten(target.long())\n        llik = 0\n        for idx in range(len(target_flattened)):\n            lik = prediction[idx, target_flattened[idx]]\n            llik += np.log(lik)\n        n = len(target_flattened)  # number of data points\n\n        BIC = np.log(n) * k - 2 * llik\n        BIC = BIC\n\n    elif output_type == ValueType.PROBABILITY_SAMPLE:\n        llik = 0\n        for idx in range(len(target)):\n            # fail safe if model doesn't produce probabilities\n            if prediction[idx] &gt; 1:\n                prediction[idx] = 1\n            elif prediction[idx] &lt; 0:\n                prediction[idx] = 0\n\n            if target[idx] == 1:\n                lik = prediction[idx]\n            elif target[idx] == 0:\n                lik = 1 - prediction[idx]\n            else:\n                raise Exception(\"Target must contain either zeros or ones.\")\n            llik += np.log(lik)\n        n = len(target)  # number of data points\n\n        BIC = np.log(n) * k - 2 * llik\n        BIC = BIC[0]\n\n    else:\n        raise Exception(\n            \"BIC computation not implemented for output type \"\n            + str(ValueType.PROBABILITY)\n            + \".\"\n        )\n\n    return BIC\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.compute_BIC_AIC","title":"<code>compute_BIC_AIC(soft_targets, soft_prediction, model)</code>","text":"<p>Returns the Bayesian information criterion (BIC) as well as the Aikaike information criterion (AIC) for a DARTS model.</p> <p>Parameters:</p> Name Type Description Default <code>soft_targets</code> <code>ndarray</code> <p>soft target of the model</p> required <code>soft_prediction</code> <code>ndarray</code> <p>soft prediction of the model</p> required <code>model</code> <code>Network</code> <p>model to compute the BIC and AIC for</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def compute_BIC_AIC(\n    soft_targets: np.ndarray, soft_prediction: np.ndarray, model: Network\n) -&gt; Tuple:\n    \"\"\"\n    Returns the Bayesian information criterion (BIC) as well as the\n    Aikaike information criterion (AIC) for a DARTS model.\n\n    Arguments:\n        soft_targets: soft target of the model\n        soft_prediction: soft prediction of the model\n        model: model to compute the BIC and AIC for\n    \"\"\"\n\n    lik = np.sum(\n        np.multiply(soft_prediction, soft_targets), axis=1\n    )  # likelihood of data given model\n    llik = np.sum(np.log(lik))  # log likelihood\n    n = len(lik)  # number of data points\n    k, _, _ = model.count_parameters()  # for most likely architecture\n\n    BIC = np.log(n) * k - 2 * llik\n\n    AIC = 2 * k - 2 * llik\n\n    return BIC, AIC\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.count_parameters_in_MB","title":"<code>count_parameters_in_MB(model)</code>","text":"<p>Returns the number of parameters for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Network</code> <p>model to count the parameters for</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def count_parameters_in_MB(model: Network) -&gt; int:\n    \"\"\"\n    Returns the number of parameters for a model.\n\n    Arguments:\n        model: model to count the parameters for\n    \"\"\"\n    for name, v in model.named_parameters():\n        if \"auxiliary\" not in name:\n            count_var = np.prod(v.size())\n    return np.sum(count_var) / 1e6\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.create_exp_dir","title":"<code>create_exp_dir(path, scripts_to_save=None, parent_folder='exps', results_folder=None)</code>","text":"<p>Creates an experiment directory and saves all necessary scripts and files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to save the experiment directory to</p> required <code>scripts_to_save</code> <code>Optional[List]</code> <p>list of scripts to save</p> <code>None</code> <code>parent_folder</code> <code>str</code> <p>parent folder for the experiment directory</p> <code>'exps'</code> <code>results_folder</code> <code>Optional[str]</code> <p>folder for the results of the experiment</p> <code>None</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def create_exp_dir(\n    path: str,\n    scripts_to_save: Optional[List] = None,\n    parent_folder: str = \"exps\",\n    results_folder: Optional[str] = None,\n):\n    \"\"\"\n    Creates an experiment directory and saves all necessary scripts and files.\n\n    Arguments:\n        path: path to save the experiment directory to\n        scripts_to_save: list of scripts to save\n        parent_folder: parent folder for the experiment directory\n        results_folder: folder for the results of the experiment\n    \"\"\"\n    os.chdir(parent_folder)  # Edit SM 10/23/19: use local experiment directory\n    if not os.path.exists(path):\n        os.mkdir(path)\n    print(\"Experiment dir : {}\".format(path))\n\n    if results_folder is not None:\n        try:\n            os.mkdir(os.path.join(path, results_folder))\n        except OSError:\n            pass\n\n    if scripts_to_save is not None:\n        try:\n            os.mkdir(os.path.join(path, \"scripts\"))\n        except OSError:\n            pass\n        os.chdir(\"..\")  # Edit SM 10/23/19: use local experiment directory\n        for script in scripts_to_save:\n            dst_file = os.path.join(\n                parent_folder, path, \"scripts\", os.path.basename(script)\n            )\n            shutil.copyfile(script, dst_file)\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.create_output_file_name","title":"<code>create_output_file_name(file_prefix, log_version=None, weight_decay=None, k=None, seed=None, theorist=None)</code>","text":"<p>Creates a file name for the output file of a theorist study.</p> <p>Parameters:</p> Name Type Description Default <code>file_prefix</code> <code>str</code> <p>prefix of the file name</p> required <code>log_version</code> <code>Optional[int]</code> <p>log version of the theorist run</p> <code>None</code> <code>weight_decay</code> <code>Optional[float]</code> <p>weight decay of the model</p> <code>None</code> <code>k</code> <code>Optional[int]</code> <p>number of nodes in the model</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>seed of the model</p> <code>None</code> <code>theorist</code> <code>Optional[str]</code> <p>name of the DARTS variant</p> <code>None</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def create_output_file_name(\n    file_prefix: str,\n    log_version: Optional[int] = None,\n    weight_decay: Optional[float] = None,\n    k: Optional[int] = None,\n    seed: Optional[int] = None,\n    theorist: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Creates a file name for the output file of a theorist study.\n\n    Arguments:\n        file_prefix: prefix of the file name\n        log_version: log version of the theorist run\n        weight_decay: weight decay of the model\n        k: number of nodes in the model\n        seed: seed of the model\n        theorist: name of the DARTS variant\n    \"\"\"\n\n    output_str = file_prefix\n\n    if theorist is not None:\n        output_str += \"_\" + str(theorist)\n\n    if log_version is not None:\n        output_str += \"_v_\" + str(log_version)\n\n    if weight_decay is not None:\n        output_str += \"_wd_\" + str(weight_decay)\n\n    if k is not None:\n        output_str += \"_k_\" + str(k)\n\n    if k is not None:\n        output_str += \"_s_\" + str(seed)\n\n    return output_str\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.cross_entropy","title":"<code>cross_entropy(pred, soft_targets)</code>","text":"<p>Returns the cross entropy loss for a soft target.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>prediction of the model</p> required <code>soft_targets</code> <code>Tensor</code> <p>soft target of the model</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def cross_entropy(pred: torch.Tensor, soft_targets: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Returns the cross entropy loss for a soft target.\n\n    Arguments:\n        pred: prediction of the model\n        soft_targets: soft target of the model\n    \"\"\"\n    # assuming pred and soft_targets are both Variables with shape (batchsize, num_of_classes),\n    # each row of pred is predicted logits and each row of soft_targets is a discrete distribution.\n    logsoftmax = nn.LogSoftmax(dim=1)\n    return torch.mean(torch.sum(-soft_targets * logsoftmax(pred), 1))\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.format_input_target","title":"<code>format_input_target(input, target, criterion)</code>","text":"<p>Formats the input and target for the model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>tensor</code> <p>input to the model</p> required <code>target</code> <code>tensor</code> <p>target of the model</p> required <code>criterion</code> <code>Callable</code> <p>criterion to use for the model</p> required <p>Returns:</p> Name Type Description <code>input</code> <code>Tuple[tensor, tensor]</code> <p>formatted input and target for the model</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def format_input_target(\n    input: torch.tensor, target: torch.tensor, criterion: Callable\n) -&gt; Tuple[torch.tensor, torch.tensor]:\n    \"\"\"\n    Formats the input and target for the model.\n\n    Args:\n        input: input to the model\n        target: target of the model\n        criterion: criterion to use for the model\n\n    Returns:\n        input: formatted input and target for the model\n\n    \"\"\"\n\n    if isinstance(criterion, nn.CrossEntropyLoss):\n        target = target.squeeze()\n\n    return (input, target)\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.get_best_fitting_models","title":"<code>get_best_fitting_models(model_name_list, loss_list, BIC_list, topk)</code>","text":"<p>Returns the topk best fitting models.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_list</code> <code>List</code> <p>list of model names</p> required <code>loss_list</code> <code>List</code> <p>list of loss values</p> required <code>BIC_list</code> <code>List</code> <p>list of BIC values</p> required <code>topk</code> <code>int</code> <p>number of topk models to return</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def get_best_fitting_models(\n    model_name_list: List,\n    loss_list: List,\n    BIC_list: List,\n    topk: int,\n) -&gt; Tuple:\n    \"\"\"\n    Returns the topk best fitting models.\n\n    Arguments:\n        model_name_list: list of model names\n        loss_list: list of loss values\n        BIC_list: list of BIC values\n        topk: number of topk models to return\n    \"\"\"\n\n    topk_losses = sorted(zip(loss_list, model_name_list), reverse=False)[:topk]\n    res = list(zip(*topk_losses))\n    topk_losses_names = res[1]\n\n    topk_BICs = sorted(zip(BIC_list, model_name_list), reverse=False)[:topk]\n    res = list(zip(*topk_BICs))\n    topk_BICs_names = res[1]\n\n    return (topk_losses_names, topk_BICs_names)\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.get_loss_function","title":"<code>get_loss_function(outputType)</code>","text":"<p>Returns the loss function for the given output type of a dependent variable.</p> <p>Parameters:</p> Name Type Description Default <code>outputType</code> <code>ValueType</code> <p>output type of the dependent variable</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def get_loss_function(outputType: ValueType):\n    \"\"\"\n    Returns the loss function for the given output type of a dependent variable.\n\n    Arguments:\n        outputType: output type of the dependent variable\n    \"\"\"\n\n    return LOSS_FUNCTION_MAPPING.get(outputType, nn.MSELoss())\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.get_output_format","title":"<code>get_output_format(outputType)</code>","text":"<p>Returns the output format (activation function of the final output layer) for the given output type of a dependent variable.</p> <p>Parameters:</p> Name Type Description Default <code>outputType</code> <code>ValueType</code> <p>output type of the dependent variable</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def get_output_format(outputType: ValueType):\n    \"\"\"\n    Returns the output format (activation function of the final output layer)\n    for the given output type of a dependent variable.\n\n    Arguments:\n        outputType: output type of the dependent variable\n    \"\"\"\n\n    return OUTPUT_FORMAT_MAPPING.get(outputType, nn.MSELoss())\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.get_output_str","title":"<code>get_output_str(outputType)</code>","text":"<p>Returns the output string for the given output type of a dependent variable.</p> <p>Parameters:</p> Name Type Description Default <code>outputType</code> <code>ValueType</code> <p>output type of the dependent variable</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def get_output_str(outputType: ValueType) -&gt; str:\n    \"\"\"\n    Returns the output string for the given output type of a dependent variable.\n\n    Arguments:\n        outputType: output type of the dependent variable\n    \"\"\"\n\n    return OUTPUT_STR_MAPPING.get(outputType, \"\")\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.load","title":"<code>load(model, model_path)</code>","text":"<p>Loads a model from a file.</p> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def load(model: torch.nn.Module, model_path: str):\n    \"\"\"\n    Loads a model from a file.\n    \"\"\"\n    model.load_state_dict(torch.load(model_path))\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.read_log_files","title":"<code>read_log_files(results_path, winning_architecture_only=False)</code>","text":"<p>Reads the log files from an experiment directory and returns the results.</p> <p>Parameters:</p> Name Type Description Default <code>results_path</code> <code>str</code> <p>path to the experiment results directory</p> required <code>winning_architecture_only</code> <code>bool</code> <p>if True, only the winning architecture is returned</p> <code>False</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def read_log_files(results_path: str, winning_architecture_only: bool = False) -&gt; Tuple:\n    \"\"\"\n    Reads the log files from an experiment directory and returns the results.\n\n    Arguments:\n        results_path: path to the experiment results directory\n        winning_architecture_only: if True, only the winning architecture is returned\n    \"\"\"\n\n    current_wd = os.getcwd()\n\n    os.chdir(results_path)\n    filelist = glob.glob(\"*.{}\".format(\"csv\"))\n\n    model_name_list = list()\n    loss_list = list()\n    BIC_list = list()\n    AIC_list = list()\n\n    # READ LOG FILES\n\n    print(\"Reading log files... \")\n    for file in filelist:\n        with open(file) as csvfile:\n            readCSV = csv.reader(csvfile, delimiter=\",\")\n            for row in readCSV:\n                if winning_architecture_only is False or \"sample0\" in row[0]:\n                    model_name_list.append(row[0])\n                    loss_list.append(float(row[1]))\n                    BIC_list.append(float(row[2].replace(\"[\", \"\").replace(\"]\", \"\")))\n                    AIC_list.append(float(row[3].replace(\"[\", \"\").replace(\"]\", \"\")))\n\n    os.chdir(current_wd)\n\n    return (model_name_list, loss_list, BIC_list, AIC_list)\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.save","title":"<code>save(model, model_path, exp_folder=None)</code>","text":"<p>Saves a model to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>model to save</p> required <code>model_path</code> <code>str</code> <p>path to save the model to</p> required <code>exp_folder</code> <code>Optional[str]</code> <p>general experiment directory to save the model to</p> <code>None</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def save(model: torch.nn.Module, model_path: str, exp_folder: Optional[str] = None):\n    \"\"\"\n    Saves a model to a file.\n\n    Arguments:\n        model: model to save\n        model_path: path to save the model to\n        exp_folder: general experiment directory to save the model to\n    \"\"\"\n    if exp_folder is not None:\n        os.chdir(\"exps\")  # Edit SM 10/23/19: use local experiment directory\n    torch.save(model.state_dict(), model_path)\n    if exp_folder is not None:\n        os.chdir(\"..\")  # Edit SM 10/23/19: use local experiment directory\n</code></pre>"},{"location":"reference/autora/theorist/darts/utils/#autora.theorist.darts.utils.sigmid_mse","title":"<code>sigmid_mse(output, target)</code>","text":"<p>Returns the MSE loss for a sigmoid output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Tensor</code> <p>output of the model</p> required <code>target</code> <code>Tensor</code> <p>target of the model</p> required Source code in <code>temp_dir/darts/src/autora/theorist/darts/utils.py</code> <pre><code>def sigmid_mse(output: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Returns the MSE loss for a sigmoid output.\n\n    Arguments:\n        output: output of the model\n        target: target of the model\n    \"\"\"\n    m = nn.Sigmoid()\n    output = m(output)\n    loss = torch.mean((output - target) ** 2)\n    return loss\n</code></pre>"},{"location":"reference/autora/theorist/darts/visualize/","title":"autora.theorist.darts.visualize","text":""},{"location":"reference/autora/theorist/darts/visualize/#autora.theorist.darts.visualize.darts_model_plot","title":"<code>darts_model_plot(genotype, full_label=False, param_list=(), input_labels=(), out_dim=None, out_fnc=None, decimals_to_display=2)</code>","text":"<p>Generates a graphviz plot for a DARTS model based on the genotype of the model.</p> <p>Parameters:</p> Name Type Description Default <code>genotype</code> <code>Genotype</code> <p>the genotype of the model</p> required <code>full_label</code> <code>bool</code> <p>if True, the labels of the nodes will be the full name of the operation (including the coefficients)</p> <code>False</code> <code>param_list</code> <code>Sequence</code> <p>a list of parameters to be included in the labels of the nodes</p> <code>()</code> <code>input_labels</code> <code>Sequence</code> <p>a list of labels to be included in the input nodes</p> <code>()</code> <code>out_dim</code> <code>Optional[int]</code> <p>the number of output nodes of the model</p> <code>None</code> <code>out_fnc</code> <code>Optional[str]</code> <p>the (activation) function to be used for the output nodes</p> <code>None</code> <code>decimals_to_display</code> <code>int</code> <p>number of decimals to include in parameter values on plot</p> <code>2</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/visualize.py</code> <pre><code>def darts_model_plot(\n    genotype: Genotype,\n    full_label: bool = False,\n    param_list: typing.Sequence = (),\n    input_labels: typing.Sequence = (),\n    out_dim: Optional[int] = None,\n    out_fnc: Optional[str] = None,\n    decimals_to_display: int = 2,\n) -&gt; Digraph:\n    \"\"\"\n    Generates a graphviz plot for a DARTS model based on the genotype of the model.\n\n    Arguments:\n        genotype: the genotype of the model\n        full_label: if True, the labels of the nodes will be the full name of the operation\n            (including the coefficients)\n        param_list: a list of parameters to be included in the labels of the nodes\n        input_labels: a list of labels to be included in the input nodes\n        out_dim: the number of output nodes of the model\n        out_fnc: the (activation) function to be used for the output nodes\n        decimals_to_display: number of decimals to include in parameter values on plot\n    \"\"\"\n\n    format_string = \"{:.\" + \"{:.0f}\".format(decimals_to_display) + \"f}\"\n\n    graph = Digraph(\n        edge_attr=dict(fontsize=\"20\", fontname=\"times\"),\n        node_attr=dict(\n            style=\"filled\",\n            shape=\"rect\",\n            align=\"center\",\n            fontsize=\"20\",\n            height=\"0.5\",\n            width=\"0.5\",\n            penwidth=\"2\",\n            fontname=\"times\",\n        ),\n        engine=\"dot\",\n    )\n    graph.body.extend([\"rankdir=LR\"])\n\n    for input_node in input_labels:\n        graph.node(input_node, fillcolor=\"#F1EDB9\")  # fillcolor='darkseagreen2'\n    # assert len(genotype) % 2 == 0\n\n    # determine number of steps (intermediate nodes)\n    steps = 0\n    for op, j in genotype:\n        if j == 0:\n            steps += 1\n\n    for i in range(steps):\n        graph.node(\"k\" + str(i + 1), fillcolor=\"#BBCCF9\")  # fillcolor='lightblue'\n\n    params_counter = 0\n    n = len(input_labels)\n    start = 0\n    for i in range(steps):\n        end = start + n\n        _logger.debug(start, end)\n        # for k in [2*i, 2*i + 1]:\n        for k in range(\n            start, end\n        ):  # adapted this iteration from get_genotype() in model_search.py\n            _logger.debug(genotype, k)\n            op, j = genotype[k]\n            if j &lt; len(input_labels):\n                u = input_labels[j]\n            else:\n                u = \"k\" + str(j - len(input_labels) + 1)\n            v = \"k\" + str(i + 1)\n            params_counter = k\n            if op != \"none\":\n                op_label = op\n                if full_label:\n                    params = param_list[\n                        start + j\n                    ]  # note: genotype order and param list order don't align\n                    op_label = get_operation_label(\n                        op, params, decimals=decimals_to_display\n                    )\n                    graph.edge(u, v, label=op_label, fillcolor=\"gray\")\n                else:\n                    graph.edge(\n                        u,\n                        v,\n                        label=\"(\" + str(j + start) + \") \" + op_label,\n                        fillcolor=\"gray\",\n                    )  # '(' + str(k) + ') '\n        start = end\n        n += 1\n\n    # determine output nodes\n\n    out_nodes = list()\n    if out_dim is None:\n        out_nodes.append(\"out\")\n    else:\n        biases = None\n        if full_label:\n            params = param_list[params_counter + 1]\n            if len(params) &gt; 1:\n                biases = params[1]  # first node contains biases\n\n        for idx in range(out_dim):\n            out_str = \"\"\n            # specify node ID\n            if out_fnc is not None:\n                out_str = out_str + out_fnc + \"(r_\" + str(idx)\n            else:\n                out_str = \"(r_\" + str(idx)\n\n            if out_dim == 1:\n                if out_fnc is not None:\n                    out_str = \"P(detected) = \" + out_fnc + \"(x\"\n                else:\n                    # out_str = 'dx_1 = (x'\n                    out_str = \"P_n = (x\"\n\n            # if available, add bias\n            if biases is not None:\n                out_str = out_str + \" + \" + format_string.format(biases[idx]) + \")\"\n            else:\n                out_str = out_str + \")\"\n\n            # add node\n            graph.node(out_str, fillcolor=\"#CBE7C7\")  # fillcolor='palegoldenrod'\n            out_nodes.append(out_str)\n\n    for i in range(steps):\n        u = \"k\" + str(i + 1)\n        if full_label:\n            params_org = param_list[params_counter + 1 + i]  # count from k\n            for out_idx, out_str in enumerate(out_nodes):\n                params = list()\n                params.append(params_org[0][out_idx])\n                op_label = get_operation_label(\n                    \"classifier\", params, decimals=decimals_to_display\n                )\n                graph.edge(u, out_str, label=op_label, fillcolor=\"gray\")\n        else:\n            for out_idx, out_str in enumerate(out_nodes):\n                graph.edge(u, out_str, label=\"linear\", fillcolor=\"gray\")\n\n    return graph\n</code></pre>"},{"location":"reference/autora/theorist/darts/visualize/#autora.theorist.darts.visualize.plot","title":"<code>plot(genotype, filename, file_format='pdf', view_file=None, full_label=False, param_list=(), input_labels=(), out_dim=None, out_fnc=None)</code>","text":"<p>Generates a graphviz plot for a DARTS model based on the genotype of the model.</p> <p>Parameters:</p> Name Type Description Default <code>genotype</code> <code>Genotype</code> <p>the genotype of the model</p> required <code>filename</code> <code>str</code> <p>the filename of the output file</p> required <code>file_format</code> <code>str</code> <p>the format of the output file</p> <code>'pdf'</code> <code>view_file</code> <code>Optional[bool]</code> <p>if True, the plot will be displayed in a window</p> <code>None</code> <code>full_label</code> <code>bool</code> <p>if True, the labels of the nodes will be the full name of the operation (including the coefficients)</p> <code>False</code> <code>param_list</code> <code>Tuple</code> <p>a list of parameters to be included in the labels of the nodes</p> <code>()</code> <code>input_labels</code> <code>Tuple</code> <p>a list of labels to be included in the input nodes</p> <code>()</code> <code>out_dim</code> <code>Optional[int]</code> <p>the number of output nodes of the model</p> <code>None</code> <code>out_fnc</code> <code>Optional[str]</code> <p>the (activation) function to be used for the output nodes</p> <code>None</code> Source code in <code>temp_dir/darts/src/autora/theorist/darts/visualize.py</code> <pre><code>def plot(\n    genotype: Genotype,\n    filename: str,\n    file_format: str = \"pdf\",\n    view_file: Optional[bool] = None,\n    full_label: bool = False,\n    param_list: typing.Tuple = (),\n    input_labels: typing.Tuple = (),\n    out_dim: Optional[int] = None,\n    out_fnc: Optional[str] = None,\n):\n    \"\"\"\n    Generates a graphviz plot for a DARTS model based on the genotype of the model.\n\n    Arguments:\n        genotype: the genotype of the model\n        filename: the filename of the output file\n        file_format: the format of the output file\n        view_file: if True, the plot will be displayed in a window\n        full_label: if True, the labels of the nodes will be the full name of the operation\n            (including the coefficients)\n        param_list: a list of parameters to be included in the labels of the nodes\n        input_labels: a list of labels to be included in the input nodes\n        out_dim: the number of output nodes of the model\n        out_fnc: the (activation) function to be used for the output nodes\n    \"\"\"\n\n    g = darts_model_plot(\n        genotype=genotype,\n        full_label=full_label,\n        param_list=param_list,\n        input_labels=input_labels,\n        out_dim=out_dim,\n        out_fnc=out_fnc,\n    )\n\n    if view_file is None:\n        if file_format == \"pdf\":\n            view_file = True\n        else:\n            view_file = False\n\n    g.render(filename, view=view_file, format=file_format)\n</code></pre>"},{"location":"reference/autora/utils/deprecation/","title":"autora.utils.deprecation","text":""},{"location":"reference/autora/utils/deprecation/#autora.utils.deprecation.deprecate","title":"<code>deprecate(f, message, callback=_logger.warning)</code>","text":"<p>Wrapper to make function aliases which print a warning that a name is an alias.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>the function to be aliased</p> required <code>message</code> <code>str</code> <p>the message to be emitted when the deprecated code is used</p> required <code>callback</code> <code>Callable</code> <p>a function to call to handle the warning message</p> <code>warning</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def original():\n...     return 1\n&gt;&gt;&gt; deprecated = deprecate(original, \"`original` is deprecated.\")\n</code></pre> <p>The original function is unaffected:</p> <pre><code>&gt;&gt;&gt; original()\n1\n</code></pre> <p>The aliased function works the same way, but also emits a warning.</p> <pre><code>&gt;&gt;&gt; deprecated()\n`original` is deprecated.\n1\n</code></pre> <p>You can also set a custom callback instead of the default \"warning\":</p> <pre><code>&gt;&gt;&gt; a0 = deprecate(original, \"`original` is deprecated.\", callback=print)\n&gt;&gt;&gt; a0()\n`original` is deprecated.\n1\n</code></pre> Source code in <code>autora/utils/deprecation.py</code> <pre><code>def deprecate(\n    f: Callable,\n    message: str,\n    callback: Callable = _logger.warning,\n):\n    \"\"\"\n    Wrapper to make function aliases which print a warning that a name is an alias.\n\n    Args:\n        f: the function to be aliased\n        message: the message to be emitted when the deprecated code is used\n        callback: a function to call to handle the warning message\n\n    Examples:\n        &gt;&gt;&gt; def original():\n        ...     return 1\n        &gt;&gt;&gt; deprecated = deprecate(original, \"`original` is deprecated.\")\n\n        The original function is unaffected:\n        &gt;&gt;&gt; original()\n        1\n\n        The aliased function works the same way, but also emits a warning.\n        &gt;&gt;&gt; deprecated()  # doctest: +SKIP\n        `original` is deprecated.\n        1\n\n        You can also set a custom callback instead of the default \"warning\":\n        &gt;&gt;&gt; a0 = deprecate(original, \"`original` is deprecated.\", callback=print)\n        &gt;&gt;&gt; a0()\n        `original` is deprecated.\n        1\n    \"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwds):\n        callback(message)\n        return f(*args, **kwds)\n\n    return wrapper\n</code></pre>"},{"location":"reference/autora/utils/deprecation/#autora.utils.deprecation.deprecated_alias","title":"<code>deprecated_alias(f, alias_name, callback=_logger.warning)</code>","text":"<p>Wrapper to make function aliases which print a warning that a name is an alias.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>the function to be aliased</p> required <code>alias_name</code> <code>str</code> <p>the name under which the function is aliased, like `foo = deprecated_alias(bar, \"foo\")</p> required <code>callback</code> <code>Callable</code> <p>a function to call to handle the warning message</p> <code>warning</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def original():\n...     return 1\n&gt;&gt;&gt; alias = deprecated_alias(original, \"alias\")\n</code></pre> <p>The original function is unaffected:</p> <pre><code>&gt;&gt;&gt; original()\n1\n</code></pre> <p>The aliased function works the same way, but also emits a warning.</p> <pre><code>&gt;&gt;&gt; alias()\nUse `original` instead. `alias` is deprecated.\n1\n</code></pre> <p>You can also set a custom callback instead of the default \"warning\":</p> <pre><code>&gt;&gt;&gt; a0 = deprecated_alias(original, \"a0\", callback=print)\n&gt;&gt;&gt; a0()\nUse `original` instead. `a0` is deprecated.\n1\n</code></pre> <p>The callback is given a single argument, the warning string. You can replace it if you like:</p> <pre><code>&gt;&gt;&gt; a0 = deprecated_alias(original, \"a0\", callback=lambda _: print(\"alternative message\"))\n&gt;&gt;&gt; a0()\nalternative message\n1\n</code></pre> Source code in <code>autora/utils/deprecation.py</code> <pre><code>def deprecated_alias(\n    f: Callable, alias_name: str, callback: Callable = _logger.warning\n):\n    \"\"\"\n    Wrapper to make function aliases which print a warning that a name is an alias.\n\n    Args:\n        f: the function to be aliased\n        alias_name: the name under which the function is aliased,\n            like `foo = deprecated_alias(bar, \"foo\")\n        callback: a function to call to handle the warning message\n\n    Examples:\n        &gt;&gt;&gt; def original():\n        ...     return 1\n        &gt;&gt;&gt; alias = deprecated_alias(original, \"alias\")\n\n        The original function is unaffected:\n        &gt;&gt;&gt; original()\n        1\n\n        The aliased function works the same way, but also emits a warning.\n        &gt;&gt;&gt; alias()  # doctest: +SKIP\n        Use `original` instead. `alias` is deprecated.\n        1\n\n        You can also set a custom callback instead of the default \"warning\":\n        &gt;&gt;&gt; a0 = deprecated_alias(original, \"a0\", callback=print)\n        &gt;&gt;&gt; a0()\n        Use `original` instead. `a0` is deprecated.\n        1\n\n        The callback is given a single argument, the warning string.\n        You can replace it if you like:\n        &gt;&gt;&gt; a0 = deprecated_alias(original, \"a0\", callback=lambda _: print(\"alternative message\"))\n        &gt;&gt;&gt; a0()\n        alternative message\n        1\n    \"\"\"\n    message = \"Use `%s` instead. `%s` is deprecated.\" % (f.__name__, alias_name)\n    wrapped = deprecate(f=f, message=message, callback=callback)\n    return wrapped\n</code></pre>"},{"location":"reference/autora/utils/dictionary/","title":"autora.utils.dictionary","text":""},{"location":"reference/autora/utils/dictionary/#autora.utils.dictionary.LazyDict","title":"<code>LazyDict</code>","text":"<p>               Bases: <code>Mapping</code></p> <p>Inspired by https://gist.github.com/gyli/9b50bb8537069b4e154fec41a4b5995a</p> Source code in <code>autora/utils/dictionary.py</code> <pre><code>class LazyDict(Mapping):\n    \"\"\"Inspired by https://gist.github.com/gyli/9b50bb8537069b4e154fec41a4b5995a\"\"\"\n\n    def __init__(self, *args, **kw):\n        self._raw_dict = dict(*args, **kw)\n\n    def __getitem__(self, key):\n        func = self._raw_dict.__getitem__(key)\n        return func()\n\n    def __iter__(self):\n        return iter(self._raw_dict)\n\n    def __len__(self):\n        return len(self._raw_dict)\n</code></pre>"},{"location":"reference/autora/variable/","title":"autora.variable","text":""},{"location":"reference/autora/variable/#autora.variable.DV","title":"<code>DV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Variable</code></p> <p>Dependent variable.</p> Source code in <code>temp_dir/core/src/autora/variable/__init__.py</code> <pre><code>@dataclass\nclass DV(Variable):\n    \"\"\"Dependent variable.\"\"\"\n\n    name: str = \"DV\"\n    variable_label: str = \"Dependent Variable\"\n</code></pre>"},{"location":"reference/autora/variable/#autora.variable.IV","title":"<code>IV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Variable</code></p> <p>Independent variable.</p> Source code in <code>temp_dir/core/src/autora/variable/__init__.py</code> <pre><code>@dataclass\nclass IV(Variable):\n    \"\"\"Independent variable.\"\"\"\n\n    name: str = \"IV\"\n    variable_label: str = \"Independent Variable\"\n</code></pre>"},{"location":"reference/autora/variable/#autora.variable.IVTrial","title":"<code>IVTrial</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IV</code></p> <p>Experiment trial as independent variable.</p> Source code in <code>temp_dir/core/src/autora/variable/__init__.py</code> <pre><code>@dataclass\nclass IVTrial(IV):\n    \"\"\"\n    Experiment trial as independent variable.\n    \"\"\"\n\n    name: str = \"trial\"\n    UID: str = \"\"\n    variable_label: str = \"Trial\"\n    units: str = \"trials\"\n    priority: int = 0\n    value_range: Tuple[Any, Any] = (0, 10000000)\n    value: float = 0\n</code></pre>"},{"location":"reference/autora/variable/#autora.variable.ValueType","title":"<code>ValueType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Specifies supported value types supported by Variables.</p> Source code in <code>temp_dir/core/src/autora/variable/__init__.py</code> <pre><code>class ValueType(str, Enum):\n    \"\"\"Specifies supported value types supported by Variables.\"\"\"\n\n    BOOLEAN = \"boolean\"\n    INTEGER = \"integer\"\n    REAL = \"real\"\n    SIGMOID = \"sigmoid\"\n    PROBABILITY = \"probability\"  # single probability\n    PROBABILITY_SAMPLE = \"probability_sample\"  # sample from single probability\n    PROBABILITY_DISTRIBUTION = (\n        \"probability_distribution\"  # probability distribution over classes\n    )\n    CLASS = \"class\"  # sample from probability distribution over classes\n</code></pre>"},{"location":"reference/autora/variable/#autora.variable.Variable","title":"<code>Variable</code>  <code>dataclass</code>","text":"<p>Describes an experimental variable: name, type, range, units, and value of a variable.</p> Source code in <code>temp_dir/core/src/autora/variable/__init__.py</code> <pre><code>@dataclass\nclass Variable:\n    \"\"\"Describes an experimental variable: name, type, range, units, and value of a variable.\"\"\"\n\n    name: str = \"\"\n    value_range: Optional[Tuple[Any, Any]] = None\n    allowed_values: Optional[Sequence] = None\n    units: str = \"\"\n    type: ValueType = ValueType.REAL\n    variable_label: str = \"\"\n    rescale: float = 1\n    is_covariate: bool = False\n</code></pre>"},{"location":"reference/autora/variable/#autora.variable.VariableCollection","title":"<code>VariableCollection</code>  <code>dataclass</code>","text":"<p>Immutable metadata about dependent / independent variables and covariates.</p> Source code in <code>temp_dir/core/src/autora/variable/__init__.py</code> <pre><code>@dataclass(frozen=True)\nclass VariableCollection:\n    \"\"\"Immutable metadata about dependent / independent variables and covariates.\"\"\"\n\n    independent_variables: Sequence[Variable] = field(default_factory=list)\n    dependent_variables: Sequence[Variable] = field(default_factory=list)\n    covariates: Sequence[Variable] = field(default_factory=list)\n</code></pre>"},{"location":"reference/autora/variable/time/","title":"autora.variable.time","text":""},{"location":"reference/autora/variable/time/#autora.variable.time.DVTime","title":"<code>DVTime</code>","text":"<p>               Bases: <code>DV</code>, <code>VTime</code></p> <p>A class representing time as a dependent variable.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>class DVTime(DV, VTime):\n    \"\"\"\n    A class representing time as a dependent variable.\n    \"\"\"\n\n    _name = \"time_DV\"\n    _UID = \"\"\n    _variable_label = \"Time\"\n    _units = \"s\"\n    _priority = 0\n    _value_range = (0, 604800)  # don't record more than a week\n    _value = 0\n\n    _is_covariate = True\n\n    # Initializes reference time.\n    # The reference time usually denotes the beginning of an experiment trial.\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initializes the time as dependent variable. The reference time usually denotes\n        the beginning of an experiment trial.\n\n        For arguments, see [autora.variable.Variable][autora.variable.Variable.__init__]\n        \"\"\"\n        print(self._variable_label)\n        super(DVTime, self).__init__(*args, **kwargs)\n        print(self._variable_label)\n\n    # Measure number of seconds relative to reference time\n    def measure(self):\n        \"\"\"\n        Measures the time in seconds relative to the reference time.\n        \"\"\"\n        value = time.time() - self._t0\n        self.set_value(value)\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.DVTime.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the time as dependent variable. The reference time usually denotes the beginning of an experiment trial.</p> <p>For arguments, see [autora.variable.Variable][autora.variable.Variable.__init__]</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initializes the time as dependent variable. The reference time usually denotes\n    the beginning of an experiment trial.\n\n    For arguments, see [autora.variable.Variable][autora.variable.Variable.__init__]\n    \"\"\"\n    print(self._variable_label)\n    super(DVTime, self).__init__(*args, **kwargs)\n    print(self._variable_label)\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.DVTime.measure","title":"<code>measure()</code>","text":"<p>Measures the time in seconds relative to the reference time.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def measure(self):\n    \"\"\"\n    Measures the time in seconds relative to the reference time.\n    \"\"\"\n    value = time.time() - self._t0\n    self.set_value(value)\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.IVTime","title":"<code>IVTime</code>","text":"<p>               Bases: <code>IV</code>, <code>VTime</code></p> <p>A class representing time as an independent variable.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>class IVTime(IV, VTime):\n    \"\"\"\n    A class representing time as an independent variable.\n    \"\"\"\n\n    _name = \"time_IV\"\n    _UID = \"\"\n    _variable_label = \"Time\"\n    _units = \"s\"\n    _priority = 0\n    _value_range = (0, 3600)\n    _value = 0\n\n    # Initializes reference time.\n    # The reference time usually denotes the beginning of an experiment trial.\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initializes the time as independent variable.\n\n        For arguments, see [autora.variable.Variable][autora.variable.Variable.__init__]\n        \"\"\"\n        super(IVTime, self).__init__(*args, **kwargs)\n\n    # Waits until specified time has passed relative to reference time\n    def manipulate(self):\n        \"\"\"\n        Waits for the specified time to pass.\n        \"\"\"\n\n        t_wait = self.get_value() - (time.time() - self._t0)\n        if t_wait &lt;= 0:\n            return\n        else:\n            time.sleep(t_wait)\n\n    def disconnect(self):\n        \"\"\"\n        Disconnects the time.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.IVTime.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the time as independent variable.</p> <p>For arguments, see [autora.variable.Variable][autora.variable.Variable.__init__]</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initializes the time as independent variable.\n\n    For arguments, see [autora.variable.Variable][autora.variable.Variable.__init__]\n    \"\"\"\n    super(IVTime, self).__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.IVTime.disconnect","title":"<code>disconnect()</code>","text":"<p>Disconnects the time.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def disconnect(self):\n    \"\"\"\n    Disconnects the time.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.IVTime.manipulate","title":"<code>manipulate()</code>","text":"<p>Waits for the specified time to pass.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def manipulate(self):\n    \"\"\"\n    Waits for the specified time to pass.\n    \"\"\"\n\n    t_wait = self.get_value() - (time.time() - self._t0)\n    if t_wait &lt;= 0:\n        return\n    else:\n        time.sleep(t_wait)\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.VTime","title":"<code>VTime</code>","text":"<p>A class representing time as a general experimental variable.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>class VTime:\n    \"\"\"\n    A class representing time as a general experimental variable.\n    \"\"\"\n\n    _t0 = 0\n\n    def __init__(self):\n        \"\"\"\n        Initializes the time.\n        \"\"\"\n        self._t0 = time.time()\n\n    # Resets reference time.\n    def reset(self):\n        \"\"\"\n        Resets the time.\n        \"\"\"\n        self._t0 = time.time()\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.VTime.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the time.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the time.\n    \"\"\"\n    self._t0 = time.time()\n</code></pre>"},{"location":"reference/autora/variable/time/#autora.variable.time.VTime.reset","title":"<code>reset()</code>","text":"<p>Resets the time.</p> Source code in <code>temp_dir/core/src/autora/variable/time.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the time.\n    \"\"\"\n    self._t0 = time.time()\n</code></pre>"},{"location":"reference/autora/workflow/__main__/","title":"autora.workflow.__main__","text":""},{"location":"reference/autora/workflow/__main__/#autora.workflow.__main__.main","title":"<code>main(fully_qualified_function_name, in_path=None, in_loader=default_serializer, out_path=None, out_dumper=default_serializer, verbose=False, debug=False)</code>","text":"<p>Run an arbitrary function on an optional input State object and save the output.</p> Source code in <code>autora/workflow/__main__.py</code> <pre><code>def main(\n    fully_qualified_function_name: Annotated[\n        str,\n        typer.Argument(\n            help=\"Fully qualified name of the function to load, like `module.function`\"\n        ),\n    ],\n    in_path: Annotated[\n        Optional[pathlib.Path],\n        typer.Option(help=\"Path to a file with the initial state\"),\n    ] = None,\n    in_loader: Annotated[\n        SupportedSerializer,\n        typer.Option(\n            help=\"(de)serializer to load the data\",\n        ),\n    ] = default_serializer,\n    out_path: Annotated[\n        Optional[pathlib.Path],\n        typer.Option(help=\"Path to output the final state\"),\n    ] = None,\n    out_dumper: Annotated[\n        SupportedSerializer,\n        typer.Option(\n            help=\"serializer to save the data\",\n        ),\n    ] = default_serializer,\n    verbose: Annotated[bool, typer.Option(help=\"Turns on info logging level.\")] = False,\n    debug: Annotated[bool, typer.Option(help=\"Turns on debug logging level.\")] = False,\n):\n    \"\"\"Run an arbitrary function on an optional input State object and save the output.\"\"\"\n    _configure_logger(debug, verbose)\n    starting_state = load_state(in_path, in_loader)\n    _logger.info(f\"Starting State: {starting_state}\")\n    function = _load_function(fully_qualified_function_name)\n    ending_state = function(starting_state)\n    _logger.info(f\"Ending State: {ending_state}\")\n    dump_state(ending_state, out_path, out_dumper)\n\n    return\n</code></pre>"},{"location":"synthetic/docs/","title":"Synthetic Data","text":"<p>Synthetic experiment data for testing AutoRA theorists and experimentalists. </p>"},{"location":"synthetic/docs/#overview","title":"Overview","text":"Name Category Links Linear mixed model Abstract Ref Expected Value Theory Economics Ref Prospect theory Economics Ref Task Switching Neuroscience Ref Exponential Learning Psychology Ref Luce-Choice-Ratio Psychology Ref Q-Learning Psychology Ref Stevens' Power Law Psychophysics Ref Weber-Fechner Law Psychophysics Ref"},{"location":"synthetic/docs/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Most synthetic experiment runners are part of autora-core:</p> <pre><code>pip install -U \"autora\"\n</code></pre> <p>Success</p> <p>It is recommended to use a <code>python</code> environment manager like <code>virtualenv</code>.</p> <p>Print a description of the prospect theory model by Kahneman and Tversky by running: <pre><code>python -c \"\nfrom autora.experiment_runner.synthetic.economics.prospect_theory import prospect_theory\nstudy = prospect_theory()\nprint(study.description)\n\"\n</code></pre></p>"},{"location":"synthetic/docs/Examples/Abstract/LMM/","title":"LMM","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The linear mixed model experiment has to be initialized with a specific formula and effects.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.abstract.lmm import lmm_experiment\n# Formula\nformula = 'rt ~ 1 + x1'\n# Initialize effects:\nfixed_effects = {'Intercept': 0., 'x1': 2.}\n\ns = lmm_experiment(formula=formula, fixed_effects=fixed_effects)\n</pre> import numpy as np from autora.experiment_runner.synthetic.abstract.lmm import lmm_experiment # Formula formula = 'rt ~ 1 + x1' # Initialize effects: fixed_effects = {'Intercept': 0., 'x1': 2.}  s = lmm_experiment(formula=formula, fixed_effects=fixed_effects) <p>Check the docstring to get information about the model</p> In\u00a0[3]: Copied! <pre>help(lmm_experiment)\n</pre> help(lmm_experiment) <pre>Help on function lmm_experiment in module autora.experiment_runner.synthetic.abstract.lmm:\n\nlmm_experiment(formula: str, fixed_effects: Optional[dict] = None, random_effects: Optional[dict] = None, X: Optional[Sequence[autora.variable.IV]] = None, random_state: Optional[int] = None, name: str = 'Linear Mixed Model Experiment')\n    A linear mixed model synthetic experiments.\n    \n    Parameters:\n        name: name of the experiment\n        formula: formula of the linear mixed model (similar to lmer package in R)\n        fixed_effects: dictionary describing the fixed effects (Intercept and slopes)\n        random_effects: nested dictionary describing the random effects of slopes and intercept.\n            These are standard deviasions in a normal distribution with a mean of zero.\n        X: Independent variable descriptions. Used to add allowed values\n\n</pre> <p>... or use the describe function:</p> In\u00a0[4]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    A linear mixed model synthetic experiments.\n\n    Parameters:\n        name: name of the experiment\n        formula: formula of the linear mixed model (similar to lmer package in R)\n        fixed_effects: dictionary describing the fixed effects (Intercept and slopes)\n        random_effects: nested dictionary describing the random effects of slopes and intercept.\n            These are standard deviasions in a normal distribution with a mean of zero.\n        X: Independent variable descriptions. Used to add allowed values\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[5]: Copied! <pre>s.name\n</pre> s.name Out[5]: <pre>'Linear Mixed Model Experiment'</pre> <p>... a valid variables description:</p> In\u00a0[6]: Copied! <pre>s.variables\n</pre> s.variables Out[6]: <pre>VariableCollection(independent_variables=[IV(name='x1', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Independent Variable', rescale=1, is_covariate=False)], dependent_variables=[DV(name='rt', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Dependent Variable', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... this variable description depends on the formula:</p> In\u00a0[7]: Copied! <pre>formula = 'probability ~ 1 + intensity'\n# Initialize effects:\nfixed_effects = {'Intercept': 0., 'intensity': 2.}\n\ns = lmm_experiment(formula=formula, fixed_effects=fixed_effects)\ns.variables\n</pre> formula = 'probability ~ 1 + intensity' # Initialize effects: fixed_effects = {'Intercept': 0., 'intensity': 2.}  s = lmm_experiment(formula=formula, fixed_effects=fixed_effects) s.variables Out[7]: <pre>VariableCollection(independent_variables=[IV(name='intensity', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Independent Variable', rescale=1, is_covariate=False)], dependent_variables=[DV(name='probability', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Dependent Variable', rescale=1, is_covariate=False)], covariates=[])</pre> <p>Since we did set the variables via a formula, there is no domain:</p> In\u00a0[8]: Copied! <pre>s.domain()\n</pre> s.domain() <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 s.domain()\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/src/autora/experiment_runner/synthetic/abstract/lmm.py:300, in lmm_experiment.&lt;locals&gt;.domain()\n    298 def domain():\n    299     \"\"\"A function which returns all possible independent variable values as a 2D array.\"\"\"\n--&gt; 300     x = variables.independent_variables[0].allowed_values.reshape(-1, 1)\n    301     return x\n\nAttributeError: 'NoneType' object has no attribute 'reshape'</pre> <p>We can set the variables manually in the init function:</p> In\u00a0[9]: Copied! <pre>from autora.variable import IV\nformula = 'rt ~ 1 + x1'\n# Initialize effects:\nfixed_effects = {'Intercept': 0., 'x1': 2.}\nindependent_variables = [IV(name='x1', allowed_values=np.linspace(0, 10, 11), value_range=(0, 100))]\n#\ns = lmm_experiment(formula=formula, fixed_effects=fixed_effects, X=independent_variables)\n</pre> from autora.variable import IV formula = 'rt ~ 1 + x1' # Initialize effects: fixed_effects = {'Intercept': 0., 'x1': 2.} independent_variables = [IV(name='x1', allowed_values=np.linspace(0, 10, 11), value_range=(0, 100))] # s = lmm_experiment(formula=formula, fixed_effects=fixed_effects, X=independent_variables) <p>... the given variable names have to match the formula:</p> In\u00a0[10]: Copied! <pre>from autora.variable import IV\nformula_ = 'rt ~ 1 + x1'\n# Initialize effects:\nfixed_effects_ = {'Intercept': 0., 'x1': 2.}\nindependent_variables_ = [IV(name='x', allowed_values=np.linspace(0, 10, 11), value_range=(0, 100))]\n#\ns_ = lmm_experiment(formula=formula, fixed_effects=fixed_effects, X=independent_variables)\n</pre> from autora.variable import IV formula_ = 'rt ~ 1 + x1' # Initialize effects: fixed_effects_ = {'Intercept': 0., 'x1': 2.} independent_variables_ = [IV(name='x', allowed_values=np.linspace(0, 10, 11), value_range=(0, 100))] # s_ = lmm_experiment(formula=formula, fixed_effects=fixed_effects, X=independent_variables) <p>... now we can generate the full domain of the data</p> In\u00a0[11]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x Out[11]: <pre>array([[ 0.],\n       [ 1.],\n       [ 2.],\n       [ 3.],\n       [ 4.],\n       [ 5.],\n       [ 6.],\n       [ 7.],\n       [ 8.],\n       [ 9.],\n       [10.]])</pre> <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[12]: Copied! <pre>experiment_data = s.run(x)\nexperiment_data\n</pre> experiment_data = s.run(x) experiment_data Out[12]: x1 rt 0 0.0 0.006061 1 1.0 1.988994 2 2.0 4.007712 3 3.0 5.980667 4 4.0 7.997771 5 5.0 9.997181 6 6.0 12.018475 7 7.0 13.990661 8 8.0 16.001257 9 9.0 17.998309 10 10.0 19.989205 <p>... we could also run without noise:</p> In\u00a0[22]: Copied! <pre>s.run(x, added_noise=0)\n</pre> s.run(x, added_noise=0) Out[22]: x1 rt 0 0.0 0.0 1 1.0 2.0 2 2.0 4.0 3 3.0 6.0 4 4.0 8.0 5 5.0 10.0 6 6.0 12.0 7 7.0 14.0 8 8.0 16.0 9 9.0 18.0 10 10.0 20.0 <p>... a function to plot the ground truth (no noise):</p> In\u00a0[23]: Copied! <pre>s.plotter()\n</pre> s.plotter() <p>... against a fitted model if it exists:</p> In\u00a0[24]: Copied! <pre>from sklearn.linear_model import LinearRegression\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = experiment_data[ivs]\ny = experiment_data[dvs]\nmodel = LinearRegression().fit(X, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = experiment_data[ivs] y = experiment_data[dvs] model = LinearRegression().fit(X, y) s.plotter(model) <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[25]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[26]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=2) print(state.conditions) <pre>    x1\n4  4.0\n1  1.0\n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[27]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[27]: x1 rt 4 4.0 8.014508 1 1.0 2.019479 <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[28]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"rt = \"\n      f\"{model.coef_[0][0]:.2f}*S0 \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"rt = \"       f\"{model.coef_[0][0]:.2f}*S0 \"       f\"{model.intercept_[0]:+.2f} \") <pre>rt = 2.00*S0 +0.02 \n</pre>"},{"location":"synthetic/docs/Examples/Abstract/LMM/#linear-mixed-model","title":"Linear Mixed Model\u00b6","text":""},{"location":"synthetic/docs/Examples/Economics/Expected-Value-Theory/","title":"Expected Value Theory","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The expected value theory experiment has to be initialized with a specific formula and effects.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.economics.expected_value_theory import expected_value_theory\n\ns = expected_value_theory()\n</pre> import numpy as np from autora.experiment_runner.synthetic.economics.expected_value_theory import expected_value_theory  s = expected_value_theory() <p>Check the docstring to get information about the model</p> In\u00a0[3]: Copied! <pre>help(expected_value_theory)\n</pre> help(expected_value_theory) <pre>Help on function expected_value_theory in module autora.experiment_runner.synthetic.economics.expected_value_theory:\n\nexpected_value_theory(name='Expected Value Theory', choice_temperature: float = 0.1, value_lambda: float = 0.5, resolution=10, minimum_value=-1, maximum_value=1)\n    Expected Value Theory\n    \n    Parameters:\n        name:\n        choice_temperature:\n        value_lambda:\n        resolution:\n        minimum_value:\n        maximum_value:\n        Examples:\n            &gt;&gt;&gt; s = expected_value_theory()\n            &gt;&gt;&gt; s.run(np.array([[1,2,.1,.9]]), random_state=42)\n               V_A  P_A  V_B  P_B  choose_A\n            0  1.0  2.0  0.1  0.9  0.999938\n\n</pre> <p>... or use the describe function:</p> In\u00a0[4]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    Expected Value Theory\n\n    Parameters:\n        name:\n        choice_temperature:\n        value_lambda:\n        resolution:\n        minimum_value:\n        maximum_value:\n        Examples:\n            &gt;&gt;&gt; s = expected_value_theory()\n            &gt;&gt;&gt; s.run(np.array([[1,2,.1,.9]]), random_state=42)\n               V_A  P_A  V_B  P_B  choose_A\n            0  1.0  2.0  0.1  0.9  0.999938\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[5]: Copied! <pre>s.name\n</pre> s.name Out[5]: <pre>'Expected Value Theory'</pre> <p>... a valid variables description:</p> In\u00a0[6]: Copied! <pre>s.variables\n</pre> s.variables Out[6]: <pre>VariableCollection(independent_variables=[IV(name='V_A', value_range=(-1, 1), allowed_values=array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]), units='dollar', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Value of Option A', rescale=1, is_covariate=False), IV(name='P_A', value_range=(0, 1), allowed_values=array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), units='probability', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Probability of Option A', rescale=1, is_covariate=False), IV(name='V_B', value_range=(-1, 1), allowed_values=array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]), units='dollar', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Value of Option B', rescale=1, is_covariate=False), IV(name='P_B', value_range=(0, 1), allowed_values=array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), units='probability', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Probability of Option B', rescale=1, is_covariate=False)], dependent_variables=[DV(name='choose_A', value_range=(0, 1), allowed_values=None, units='probability', type=&lt;ValueType.PROBABILITY: 'probability'&gt;, variable_label='Probability of Choosing Option A', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... now we can generate the full domain of the data</p> In\u00a0[7]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x Out[7]: <pre>array([[-1.        , -0.77777778, -0.55555556, -0.33333333],\n       [-0.11111111,  0.11111111,  0.33333333,  0.55555556],\n       [ 0.77777778,  1.        ,  0.        ,  0.11111111],\n       [ 0.22222222,  0.33333333,  0.44444444,  0.55555556],\n       [ 0.66666667,  0.77777778,  0.88888889,  1.        ],\n       [-1.        , -0.77777778, -0.55555556, -0.33333333],\n       [-0.11111111,  0.11111111,  0.33333333,  0.55555556],\n       [ 0.77777778,  1.        ,  0.        ,  0.11111111],\n       [ 0.22222222,  0.33333333,  0.44444444,  0.55555556],\n       [ 0.66666667,  0.77777778,  0.88888889,  1.        ]])</pre> <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[8]: Copied! <pre>experiment_data = s.run(x)\nexperiment_data\n</pre> experiment_data = s.run(x) experiment_data Out[8]: V_A P_A V_B P_B choose_A 0 -1.000000 -0.777778 -0.555556 -0.333333 0.942923 1 -0.111111 0.111111 0.333333 0.555556 0.225084 2 0.777778 1.000000 0.000000 0.111111 0.980658 3 0.222222 0.333333 0.444444 0.555556 0.311833 4 0.666667 0.777778 0.888889 1.000000 0.120028 5 -1.000000 -0.777778 -0.555556 -0.333333 0.945268 6 -0.111111 0.111111 0.333333 0.555556 0.267370 7 0.777778 1.000000 0.000000 0.111111 0.975918 8 0.222222 0.333333 0.444444 0.555556 0.232454 9 0.666667 0.777778 0.888889 1.000000 0.113917 <p>... a function to plot the ground truth (no noise):</p> In\u00a0[9]: Copied! <pre>s.plotter()\n</pre> s.plotter() <p>... against a fitted model if it exists:</p> In\u00a0[10]: Copied! <pre>from sklearn.linear_model import LinearRegression\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = experiment_data[ivs]\ny = experiment_data[dvs]\nmodel = LinearRegression().fit(X, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = experiment_data[ivs] y = experiment_data[dvs] model = LinearRegression().fit(X, y) s.plotter(model) <pre>/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[11]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[12]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=2) print(state.conditions) <pre>           V_A       P_A       V_B       P_B\n5093  0.111111  0.000000  1.000000  0.333333\n4839 -0.111111  0.888889 -0.333333  1.000000\n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[13]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[13]: V_A P_A V_B P_B choose_A 239 -1.000000 0.222222 -0.333333 1.0 0.625000 7580 0.555556 0.555556 0.777778 0.0 0.780558 <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[14]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"rt = \"\n      f\"{model.coef_[0][0]:.2f}*V_A \"\n      f\"{model.coef_[0][1]:.2f}*P_A \"\n      f\"{model.coef_[0][2]:.2f}*V_B \"\n      f\"{model.coef_[0][3]:.2f}*P_B \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"rt = \"       f\"{model.coef_[0][0]:.2f}*V_A \"       f\"{model.coef_[0][1]:.2f}*P_A \"       f\"{model.coef_[0][2]:.2f}*V_B \"       f\"{model.coef_[0][3]:.2f}*P_B \"       f\"{model.intercept_[0]:+.2f} \") <pre>rt = 0.05*V_A 0.01*P_A 0.04*V_B -0.03*P_B +0.72 \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"synthetic/docs/Examples/Economics/Expected-Value-Theory/#expected-value-theory","title":"Expected Value Theory\u00b6","text":""},{"location":"synthetic/docs/Examples/Economics/Prospect-Theory/","title":"Prospect Theory","text":"In\u00a0[3]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The prospect theory experiment has to be initialized with a specific formula and effects.</p> In\u00a0[4]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.economics.prospect_theory import prospect_theory\n\ns = prospect_theory()\n</pre> import numpy as np from autora.experiment_runner.synthetic.economics.prospect_theory import prospect_theory  s = prospect_theory() <p>Check the docstring to get information about the model</p> In\u00a0[5]: Copied! <pre>help(prospect_theory)\n</pre> help(prospect_theory) <pre>Help on function prospect_theory in module autora.experiment_runner.synthetic.economics.prospect_theory:\n\nprospect_theory(name='Prospect Theory', choice_temperature=0.1, value_alpha=0.88, value_beta=0.88, value_lambda=2.25, probability_alpha=0.61, probability_beta=0.69, resolution=10, minimum_value=-1, maximum_value=1)\n    Parameters from\n    D. Kahneman, A. Tversky, Prospect theory: An analysis of decision under risk.\n    Econometrica 47, 263\u2013292 (1979). doi:10.2307/1914185\n    \n    Power value function according to:\n        - A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n          uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n    \n        - I. Gilboa, Expected utility with purely subjective non-additive probabilities.\n          J. Math. Econ. 16, 65\u201388 (1987). doi:10.1016/0304-4068(87)90022-X\n    \n        - D. Schmeidler, Subjective probability and expected utility without additivity.\n          Econometrica 57, 571 (1989). doi:10.2307/1911053\n    \n    Probability function according to:\n        A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n        uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n    Examples:\n        &gt;&gt;&gt; s = prospect_theory()\n        &gt;&gt;&gt; s.run(np.array([[.9,.1,.1,.9]]), random_state=42)\n           V_A  P_A  V_B  P_B  choose_A\n        0  0.9  0.1  0.1  0.9  0.709777\n\n</pre> <p>... or use the describe function:</p> In\u00a0[6]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    Parameters from\n    D. Kahneman, A. Tversky, Prospect theory: An analysis of decision under risk.\n    Econometrica 47, 263\u2013292 (1979). doi:10.2307/1914185\n\n    Power value function according to:\n        - A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n          uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n\n        - I. Gilboa, Expected utility with purely subjective non-additive probabilities.\n          J. Math. Econ. 16, 65\u201388 (1987). doi:10.1016/0304-4068(87)90022-X\n\n        - D. Schmeidler, Subjective probability and expected utility without additivity.\n          Econometrica 57, 571 (1989). doi:10.2307/1911053\n\n    Probability function according to:\n        A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of\n        uncertainty. J. Risk Uncertain. 5, 297\u2013323 (1992). doi:10.1007/BF00122574\n    Examples:\n        &gt;&gt;&gt; s = prospect_theory()\n        &gt;&gt;&gt; s.run(np.array([[.9,.1,.1,.9]]), random_state=42)\n           V_A  P_A  V_B  P_B  choose_A\n        0  0.9  0.1  0.1  0.9  0.709777\n\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[7]: Copied! <pre>s.name\n</pre> s.name Out[7]: <pre>'Prospect Theory'</pre> <p>... a valid variables description:</p> In\u00a0[8]: Copied! <pre>s.variables\n</pre> s.variables Out[8]: <pre>VariableCollection(independent_variables=[IV(name='V_A', value_range=(-1, 1), allowed_values=array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]), units='dollar', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Value of Option A', rescale=1, is_covariate=False), IV(name='P_A', value_range=(0, 1), allowed_values=array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), units='probability', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Probability of Option A', rescale=1, is_covariate=False), IV(name='V_B', value_range=(-1, 1), allowed_values=array([-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n        0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]), units='dollar', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Value of Option B', rescale=1, is_covariate=False), IV(name='P_B', value_range=(0, 1), allowed_values=array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), units='probability', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Probability of Option B', rescale=1, is_covariate=False)], dependent_variables=[DV(name='choose_A', value_range=(0, 1), allowed_values=None, units='probability', type=&lt;ValueType.PROBABILITY: 'probability'&gt;, variable_label='Probability of Choosing Option A', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... now we can generate the full domain of the data</p> In\u00a0[9]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x Out[9]: <pre>array([[-1.        ,  0.        , -1.        ,  0.        ],\n       [-1.        ,  0.11111111, -1.        ,  0.        ],\n       [-1.        ,  0.22222222, -1.        ,  0.        ],\n       ...,\n       [ 1.        ,  0.77777778,  1.        ,  1.        ],\n       [ 1.        ,  0.88888889,  1.        ,  1.        ],\n       [ 1.        ,  1.        ,  1.        ,  1.        ]])</pre> <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[10]: Copied! <pre>experiment_data = s.run(x)\nexperiment_data\n</pre> experiment_data = s.run(x) experiment_data Out[10]: V_A P_A V_B P_B choose_A 0 -1.0 0.000000 -1.0 0.0 0.557904 1 -1.0 0.111111 -1.0 0.0 0.015968 2 -1.0 0.222222 -1.0 0.0 0.002003 3 -1.0 0.333333 -1.0 0.0 0.000365 4 -1.0 0.444444 -1.0 0.0 0.000073 ... ... ... ... ... ... 9995 1.0 0.555556 1.0 1.0 0.003957 9996 1.0 0.666667 1.0 1.0 0.007600 9997 1.0 0.777778 1.0 1.0 0.017028 9998 1.0 0.888889 1.0 1.0 0.049545 9999 1.0 1.000000 1.0 1.0 0.431814 <p>10000 rows \u00d7 5 columns</p> <p>... a function to plot the ground truth (no noise):</p> In\u00a0[11]: Copied! <pre>s.plotter()\n</pre> s.plotter() <p>... against a fitted model if it exists:</p> In\u00a0[12]: Copied! <pre>from sklearn.linear_model import LinearRegression\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = experiment_data[ivs]\ny = experiment_data[dvs]\nmodel = LinearRegression().fit(X, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = experiment_data[ivs] y = experiment_data[dvs] model = LinearRegression().fit(X, y) s.plotter(model) <pre>/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[13]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[14]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=2) print(state.conditions) <pre>           V_A       P_A       V_B       P_B\n7382  0.555556  0.333333  0.777778  0.222222\n5411  0.111111  0.444444 -0.777778  0.111111\n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[15]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[15]: V_A P_A V_B P_B choose_A 7382 0.555556 0.333333 0.777778 0.222222 0.477033 5411 0.111111 0.444444 -0.777778 0.111111 0.979092 <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[16]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"rt = \"\n      f\"{model.coef_[0][0]:.2f}*V_A \"\n      f\"{model.coef_[0][1]:.2f}*P_A \"\n      f\"{model.coef_[0][2]:.2f}*V_B \"\n      f\"{model.coef_[0][3]:.2f}*P_B \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"rt = \"       f\"{model.coef_[0][0]:.2f}*V_A \"       f\"{model.coef_[0][1]:.2f}*P_A \"       f\"{model.coef_[0][2]:.2f}*V_B \"       f\"{model.coef_[0][3]:.2f}*P_B \"       f\"{model.intercept_[0]:+.2f} \") <pre>rt = -0.08*V_A 0.02*P_A -0.30*V_B -0.02*P_B +0.75 \n</pre>"},{"location":"synthetic/docs/Examples/Economics/Prospect-Theory/#prospect-theory","title":"Prospect Theory\u00b6","text":""},{"location":"synthetic/docs/Examples/Neuroscience/Task-Switching/","title":"Task Switching","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The task switching experiment has to be initialized with a specific formula and effects.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.neuroscience.task_switching import task_switching\n\ns = task_switching()\n</pre> import numpy as np from autora.experiment_runner.synthetic.neuroscience.task_switching import task_switching  s = task_switching() <p>Check the docstring to get information about the model</p> In\u00a0[3]: Copied! <pre>help(task_switching)\n</pre> help(task_switching) <pre>Help on function task_switching in module autora.experiment_runner.synthetic.neuroscience.task_switching:\n\ntask_switching(name='Task Switching', resolution=50, priming_default=0.3, temperature=0.2, minimum_task_control=0.15, constant=1.5)\n    Task Switching\n    \n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus\n        priming_default: default for task priming\n        temperature: temperature for softmax when computing performance of current task\n        constant: constant for task activation\n        minimum_task_control: minimum task control\n    Examples:\n        &gt;&gt;&gt; s = task_switching()\n        &gt;&gt;&gt; s.run(np.array([[.5,.7,0]]), random_state=42)\n           cur_task_strength  alt_task_strength  is_switch  cur_task_performance\n        0                0.5                0.7        0.0              0.685351\n\n</pre> <p>... or use the describe function:</p> In\u00a0[4]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    Task Switching\n\n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus\n        priming_default: default for task priming\n        temperature: temperature for softmax when computing performance of current task\n        constant: constant for task activation\n        minimum_task_control: minimum task control\n    Examples:\n        &gt;&gt;&gt; s = task_switching()\n        &gt;&gt;&gt; s.run(np.array([[.5,.7,0]]), random_state=42)\n           cur_task_strength  alt_task_strength  is_switch  cur_task_performance\n        0                0.5                0.7        0.0              0.685351\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[5]: Copied! <pre>s.name\n</pre> s.name Out[5]: <pre>'Task Switching'</pre> <p>... a valid variables description:</p> In\u00a0[6]: Copied! <pre>s.variables\n</pre> s.variables Out[6]: <pre>VariableCollection(independent_variables=[IV(name='cur_task_strength', value_range=(0, 1), allowed_values=array([0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 , 0.22,\n       0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42, 0.44,\n       0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64, 0.66,\n       0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86, 0.88,\n       0.9 , 0.92, 0.94, 0.96, 0.98, 1.  ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Strength of Current Task', rescale=1, is_covariate=False), IV(name='alt_task_strength', value_range=(0, 1), allowed_values=array([0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 , 0.22,\n       0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42, 0.44,\n       0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64, 0.66,\n       0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86, 0.88,\n       0.9 , 0.92, 0.94, 0.96, 0.98, 1.  ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Strength of Alternative Task', rescale=1, is_covariate=False), IV(name='is_switch', value_range=(0, 1), allowed_values=[0, 1], units='indicator', type=&lt;ValueType.PROBABILITY_SAMPLE: 'probability_sample'&gt;, variable_label='Is Switch', rescale=1, is_covariate=False)], dependent_variables=[DV(name='cur_task_performance', value_range=(0, 1), allowed_values=None, units='performance', type=&lt;ValueType.PROBABILITY: 'probability'&gt;, variable_label='Accuracy of Current Task', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... now we can generate the full domain of the data</p> In\u00a0[7]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x Out[7]: <pre>array([[0.02, 0.02, 0.  ],\n       [0.02, 0.04, 0.  ],\n       [0.02, 0.06, 0.  ],\n       ...,\n       [1.  , 0.96, 1.  ],\n       [1.  , 0.98, 1.  ],\n       [1.  , 1.  , 1.  ]])</pre> <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[8]: Copied! <pre>experiment_data = s.run(x)\nexperiment_data\n</pre> experiment_data = s.run(x) experiment_data Out[8]: cur_task_strength alt_task_strength is_switch cur_task_performance 0 0.02 0.02 0.0 0.922331 1 0.02 0.04 0.0 0.901138 2 0.02 0.06 0.0 0.887451 3 0.02 0.08 0.0 0.869731 4 0.02 0.10 0.0 0.860710 ... ... ... ... ... 4995 1.00 0.92 1.0 0.536209 4996 1.00 0.94 1.0 0.531290 4997 1.00 0.96 1.0 0.533204 4998 1.00 0.98 1.0 0.519257 4999 1.00 1.00 1.0 0.516433 <p>5000 rows \u00d7 4 columns</p> <p>... a function to plot the ground truth (no noise):</p> In\u00a0[9]: Copied! <pre>s.plotter()\n</pre> s.plotter() <p>... against a fitted model if it exists:</p> In\u00a0[10]: Copied! <pre>from sklearn.linear_model import LinearRegression\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = experiment_data[ivs]\ny = experiment_data[dvs]\nmodel = LinearRegression().fit(X, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = experiment_data[ivs] y = experiment_data[dvs] model = LinearRegression().fit(X, y) s.plotter(model) <pre>/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[11]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[12]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=2) print(state.conditions) <pre>      cur_task_strength  alt_task_strength  is_switch\n4326               0.88               0.28          0\n621                0.14               0.22          1\n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[13]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[13]: cur_task_strength alt_task_strength is_switch cur_task_performance 4326 0.88 0.28 0 0.931829 621 0.14 0.22 1 0.714810 <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[16]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"rt = \"\n      f\"{model.coef_[0][0]:.2f}*cur_task_strength \"\n      f\"{model.coef_[0][1]:.2f}*alt_task_strength \"\n      f\"{model.coef_[0][2]:.2f}*is_switch \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"rt = \"       f\"{model.coef_[0][0]:.2f}*cur_task_strength \"       f\"{model.coef_[0][1]:.2f}*alt_task_strength \"       f\"{model.coef_[0][2]:.2f}*is_switch \"       f\"{model.intercept_[0]:+.2f} \") <pre>rt = 0.10*cur_task_strength 0.01*alt_task_strength -0.14*is_switch +0.84 \n</pre>"},{"location":"synthetic/docs/Examples/Neuroscience/Task-Switching/#task-switching","title":"Task Switching\u00b6","text":""},{"location":"synthetic/docs/Examples/Psychology/Exponential-Learning/","title":"Exponential Learning","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The exponential learning experiment has to be initialized with a specific formula and effects.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.psychology.exp_learning import exp_learning\n\ns = exp_learning()\n</pre> import numpy as np from autora.experiment_runner.synthetic.psychology.exp_learning import exp_learning  s = exp_learning() <p>Check the docstring to get information about the model</p> In\u00a0[3]: Copied! <pre>help(exp_learning)\n</pre> help(exp_learning) <pre>Help on SyntheticExperimentCollection in module autora.experiment_runner.synthetic.utilities object:\n\nclass SyntheticExperimentCollection(builtins.object)\n |  SyntheticExperimentCollection(name: 'Optional[str]' = None, description: 'Optional[str]' = None, params: 'Optional[Dict]' = None, variables: 'Optional[VariableCollection]' = None, domain: 'Optional[Callable]' = None, run: 'Optional[Callable]' = None, ground_truth: 'Optional[Callable]' = None, plotter: 'Optional[Callable[[Optional[_SupportsPredict]], None]]' = None, factory_function: 'Optional[_SyntheticExperimentFactory]' = None) -&gt; None\n |  \n |  Represents a synthetic experiment.\n |  \n |  Attributes:\n |      name: the name of the theory\n |      params: a dictionary with the settable parameters of the model and their respective values\n |      variables: a VariableCollection describing the variables of the model\n |      domain: a function which returns all the available X values for the model\n |      run: a function which takes X values and returns simulated y values **with\n |          statistical noise**\n |      ground_truth: a function which takes X values and returns simulated y values **without any\n |          statistical noise**\n |      plotter: a function which plots the ground truth and, optionally, a model with a\n |          `predict` method (e.g. scikit-learn estimators)\n |  \n |  Methods defined here:\n |  \n |  __delattr__(self, name)\n |      Implement delattr(self, name).\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  __hash__(self)\n |      Return hash(self).\n |  \n |  __init__(self, name: 'Optional[str]' = None, description: 'Optional[str]' = None, params: 'Optional[Dict]' = None, variables: 'Optional[VariableCollection]' = None, domain: 'Optional[Callable]' = None, run: 'Optional[Callable]' = None, ground_truth: 'Optional[Callable]' = None, plotter: 'Optional[Callable[[Optional[_SupportsPredict]], None]]' = None, factory_function: 'Optional[_SyntheticExperimentFactory]' = None) -&gt; None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  __setattr__(self, name, value)\n |      Implement setattr(self, name, value).\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __annotations__ = {'description': 'Optional[str]', 'domain': 'Optional...\n |  \n |  __dataclass_fields__ = {'description': Field(name='description',type='...\n |  \n |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n |  \n |  __match_args__ = ('name', 'description', 'params', 'variables', 'domai...\n |  \n |  description = None\n |  \n |  domain = None\n |  \n |  factory_function = None\n |  \n |  ground_truth = None\n |  \n |  name = None\n |  \n |  params = None\n |  \n |  plotter = None\n |  \n |  run = None\n |  \n |  variables = None\n\n</pre> <p>... or use the describe function:</p> In\u00a0[4]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    Exponential Learning\n\n    Args:\n        p_asymptotic: additive bias on constant multiplier\n        lr: learning rate\n        maximum_initial_value: upper bound for initial p value\n        minimum_initial_value: lower bound for initial p value\n        minimum_trial: upper bound for exponential constant\n        name: name of the experiment\n        resolution: number of allowed values for stimulus\n        Examples:\n        &gt;&gt;&gt; s = exp_learning()\n        &gt;&gt;&gt; s.run(np.array([[.2,.1]]), random_state=42)\n           P_asymptotic  trial  performance\n        0           0.2    0.1     0.205444\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[5]: Copied! <pre>s.name\n</pre> s.name Out[5]: <pre>'Exponential Learning'</pre> <p>... a valid variables description:</p> In\u00a0[6]: Copied! <pre>s.variables\n</pre> s.variables Out[6]: <pre>VariableCollection(independent_variables=[IV(name='P_asymptotic', value_range=(0, 0.5), allowed_values=array([0.        , 0.00505051, 0.01010101, 0.01515152, 0.02020202,\n       0.02525253, 0.03030303, 0.03535354, 0.04040404, 0.04545455,\n       0.05050505, 0.05555556, 0.06060606, 0.06565657, 0.07070707,\n       0.07575758, 0.08080808, 0.08585859, 0.09090909, 0.0959596 ,\n       0.1010101 , 0.10606061, 0.11111111, 0.11616162, 0.12121212,\n       0.12626263, 0.13131313, 0.13636364, 0.14141414, 0.14646465,\n       0.15151515, 0.15656566, 0.16161616, 0.16666667, 0.17171717,\n       0.17676768, 0.18181818, 0.18686869, 0.19191919, 0.1969697 ,\n       0.2020202 , 0.20707071, 0.21212121, 0.21717172, 0.22222222,\n       0.22727273, 0.23232323, 0.23737374, 0.24242424, 0.24747475,\n       0.25252525, 0.25757576, 0.26262626, 0.26767677, 0.27272727,\n       0.27777778, 0.28282828, 0.28787879, 0.29292929, 0.2979798 ,\n       0.3030303 , 0.30808081, 0.31313131, 0.31818182, 0.32323232,\n       0.32828283, 0.33333333, 0.33838384, 0.34343434, 0.34848485,\n       0.35353535, 0.35858586, 0.36363636, 0.36868687, 0.37373737,\n       0.37878788, 0.38383838, 0.38888889, 0.39393939, 0.3989899 ,\n       0.4040404 , 0.40909091, 0.41414141, 0.41919192, 0.42424242,\n       0.42929293, 0.43434343, 0.43939394, 0.44444444, 0.44949495,\n       0.45454545, 0.45959596, 0.46464646, 0.46969697, 0.47474747,\n       0.47979798, 0.48484848, 0.48989899, 0.49494949, 0.5       ]), units='performance', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Asymptotic Performance', rescale=1, is_covariate=False), IV(name='trial', value_range=(1, 100), allowed_values=array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n        12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,\n        23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,\n        34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,\n        45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,\n        56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n        67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,\n        89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.,\n       100.]), units='trials', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Trials', rescale=1, is_covariate=False)], dependent_variables=[DV(name='performance', value_range=(0, 1.0), allowed_values=None, units='performance', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Performance', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... now we can generate the full domain of the data</p> In\u00a0[7]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x Out[7]: <pre>array([[  0. ,   1. ],\n       [  0. ,   2. ],\n       [  0. ,   3. ],\n       ...,\n       [  0.5,  98. ],\n       [  0.5,  99. ],\n       [  0.5, 100. ]])</pre> <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[8]: Copied! <pre>experiment_data = s.run(x)\nexperiment_data\n</pre> experiment_data = s.run(x) experiment_data Out[8]: P_asymptotic trial performance 0 0.0 1.0 0.036016 1 0.0 2.0 0.068082 2 0.0 3.0 0.085388 3 0.0 4.0 0.112860 4 0.0 5.0 0.130249 ... ... ... ... 9995 0.5 96.0 0.990840 9996 0.5 97.0 0.976043 9997 0.5 98.0 0.972922 9998 0.5 99.0 0.969821 9999 0.5 100.0 0.976879 <p>10000 rows \u00d7 3 columns</p> <p>... a function to plot the ground truth (no noise):</p> In\u00a0[9]: Copied! <pre>s.plotter()\n</pre> s.plotter() <p>... against a fitted model if it exists:</p> In\u00a0[10]: Copied! <pre>from sklearn.linear_model import LinearRegression\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = experiment_data[ivs]\ny = experiment_data[dvs]\nmodel = LinearRegression().fit(X, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = experiment_data[ivs] y = experiment_data[dvs] model = LinearRegression().fit(X, y) s.plotter(model) <pre>/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[11]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[12]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=2) print(state.conditions) <pre>      P_asymptotic  trial\n8380      0.419192   81.0\n2670      0.131313   71.0\n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[13]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[13]: P_asymptotic trial performance 8380 0.419192 81.0 0.955751 2670 0.131313 71.0 0.892922 <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[17]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"performance = \"\n      f\"{model.coef_[0][0]:.2f}*P_asymptotic \"\n      f\"{model.coef_[0][1]:.2f}*trial \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"performance = \"       f\"{model.coef_[0][0]:.2f}*P_asymptotic \"       f\"{model.coef_[0][1]:.2f}*trial \"       f\"{model.intercept_[0]:+.2f} \") <pre>performance = 0.00*P_asymptotic 0.01*trial +0.45 \n</pre>"},{"location":"synthetic/docs/Examples/Psychology/Exponential-Learning/#exponential-learning","title":"Exponential Learning\u00b6","text":""},{"location":"synthetic/docs/Examples/Psychology/Luce-Choice-Ratio/","title":"Luce Choice Ratio","text":"In\u00a0[3]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The luce choice ratio experiment has to be initialized with a specific formula and effects.</p> In\u00a0[4]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.psychology.luce_choice_ratio import  luce_choice_ratio\n\ns = luce_choice_ratio()\n</pre> import numpy as np from autora.experiment_runner.synthetic.psychology.luce_choice_ratio import  luce_choice_ratio  s = luce_choice_ratio() <p>Check the docstring to get information about the model</p> In\u00a0[5]: Copied! <pre>help(luce_choice_ratio)\n</pre> help(luce_choice_ratio) <pre>Help on function luce_choice_ratio in module autora.experiment_runner.synthetic.psychology.luce_choice_ratio:\n\nluce_choice_ratio(name='Luce-Choice-Ratio', resolution=8, maximum_similarity=10, focus=0.8)\n    Luce-Choice-Ratio\n    \n    Args:\n        name: name of the experiment\n        added_noise: standard deviation of normally distributed noise added to y-values\n        resolution: number of allowed values for stimulus DVs\n        maximum_similarity: upperbound for DVs\n        focus: parameter measuring participant focus\n        random_state: integer used to seed the random number generator\n    \n    Shepard-Luce Choice Rule according to:\n        - Equation (4) in Logan, G. D., &amp; Gordon, R. D. (2001).\n        - and in Executive control of visual attention in dual-task situations.\n            Psychological review, 108(2), 393.\n        - Equation (5) in Luce, R. D. (1963). Detection and recognition.\n    \n    Examples:\n        We can instantiate a Shepard-Cue Choice Experiment. We use a seed to get replicable results:\n        &gt;&gt;&gt; l_s_experiment = luce_choice_ratio()\n    \n        We can look at the name of the experiment:\n        &gt;&gt;&gt; l_s_experiment.name\n        'Luce-Choice-Ratio'\n    \n        To call the ground truth, we can use an attribute of the experiment:\n        &gt;&gt;&gt; l_s_experiment.ground_truth(np.array([[1,2,3,4]]))\n           similarity_category_A1  ...  choose_A1\n        0                       1  ...   0.210526\n        &lt;BLANKLINE&gt;\n        [1 rows x 5 columns]\n    \n        We can also run an experiment:\n        &gt;&gt;&gt; l_s_experiment.run(np.array([[1,2,3,4]]), random_state=42)\n           similarity_category_A1  ...  choose_A1\n        0                       1  ...   0.211328\n        &lt;BLANKLINE&gt;\n        [1 rows x 5 columns]\n    \n        To plot the experiment use:\n        &gt;&gt;&gt; l_s_experiment.plotter()\n        &gt;&gt;&gt; plt.show()  # doctest: +SKIP\n\n</pre> <p>... or use the describe function:</p> In\u00a0[6]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    Luce-Choice-Ratio\n\n    Args:\n        name: name of the experiment\n        added_noise: standard deviation of normally distributed noise added to y-values\n        resolution: number of allowed values for stimulus DVs\n        maximum_similarity: upperbound for DVs\n        focus: parameter measuring participant focus\n        random_state: integer used to seed the random number generator\n\n    Shepard-Luce Choice Rule according to:\n        - Equation (4) in Logan, G. D., &amp; Gordon, R. D. (2001).\n        - and in Executive control of visual attention in dual-task situations.\n            Psychological review, 108(2), 393.\n        - Equation (5) in Luce, R. D. (1963). Detection and recognition.\n\n    Examples:\n        We can instantiate a Shepard-Cue Choice Experiment. We use a seed to get replicable results:\n        &gt;&gt;&gt; l_s_experiment = luce_choice_ratio()\n\n        We can look at the name of the experiment:\n        &gt;&gt;&gt; l_s_experiment.name\n        'Luce-Choice-Ratio'\n\n        To call the ground truth, we can use an attribute of the experiment:\n        &gt;&gt;&gt; l_s_experiment.ground_truth(np.array([[1,2,3,4]]))\n           similarity_category_A1  ...  choose_A1\n        0                       1  ...   0.210526\n        &lt;BLANKLINE&gt;\n        [1 rows x 5 columns]\n\n        We can also run an experiment:\n        &gt;&gt;&gt; l_s_experiment.run(np.array([[1,2,3,4]]), random_state=42)\n           similarity_category_A1  ...  choose_A1\n        0                       1  ...   0.211328\n        &lt;BLANKLINE&gt;\n        [1 rows x 5 columns]\n\n        To plot the experiment use:\n        &gt;&gt;&gt; l_s_experiment.plotter()\n        &gt;&gt;&gt; plt.show()  # doctest: +SKIP\n\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[7]: Copied! <pre>s.name\n</pre> s.name Out[7]: <pre>'Luce-Choice-Ratio'</pre> <p>... a valid variables description:</p> In\u00a0[8]: Copied! <pre>s.variables\n</pre> s.variables Out[8]: <pre>VariableCollection(independent_variables=[IV(name='similarity_category_A1', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category A1', rescale=1, is_covariate=False), IV(name='similarity_category_A2', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category A2', rescale=1, is_covariate=False), IV(name='similarity_category_B1', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category B1', rescale=1, is_covariate=False), IV(name='similarity_category_B2', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category B2', rescale=1, is_covariate=False)], dependent_variables=[DV(name='choose_A1', value_range=(0, 1), allowed_values=None, units='probability', type=&lt;ValueType.PROBABILITY: 'probability'&gt;, variable_label='Probability of Choosing A1', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... now we can generate the full domain of the data</p> In\u00a0[9]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x Out[9]: <pre>array([[ 0.1       ,  0.1       ,  0.1       ,  0.1       ],\n       [ 0.1       ,  1.51428571,  0.1       ,  0.1       ],\n       [ 0.1       ,  2.92857143,  0.1       ,  0.1       ],\n       ...,\n       [10.        ,  7.17142857, 10.        , 10.        ],\n       [10.        ,  8.58571429, 10.        , 10.        ],\n       [10.        , 10.        , 10.        , 10.        ]])</pre> <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[10]: Copied! <pre>experiment_data = s.run(x)\nexperiment_data\n</pre> experiment_data = s.run(x) experiment_data Out[10]: similarity_category_A1 similarity_category_A2 similarity_category_B1 similarity_category_B2 choose_A1 0 0.1 0.100000 0.1 0.1 0.377072 1 0.1 1.514286 0.1 0.1 0.062468 2 0.1 2.928571 0.1 0.1 0.033302 3 0.1 4.342857 0.1 0.1 0.018954 4 0.1 5.757143 0.1 0.1 0.015625 ... ... ... ... ... ... 4091 10.0 4.342857 10.0 10.0 0.518347 4092 10.0 5.757143 10.0 10.0 0.481514 4093 10.0 7.171429 10.0 10.0 0.451186 4094 10.0 8.585714 10.0 10.0 0.423662 4095 10.0 10.000000 10.0 10.0 0.400149 <p>4096 rows \u00d7 5 columns</p> <p>... a function to plot the ground truth (no noise):</p> In\u00a0[11]: Copied! <pre>s.plotter()\n</pre> s.plotter() <p>... against a fitted model if it exists:</p> In\u00a0[12]: Copied! <pre>from sklearn.linear_model import LinearRegression\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = experiment_data[ivs]\ny = experiment_data[dvs]\nmodel = LinearRegression().fit(X, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = experiment_data[ivs] y = experiment_data[dvs] model = LinearRegression().fit(X, y) s.plotter(model) <pre>/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n/Users/younesstrittmatter/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[13]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[14]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=2) print(state.conditions) <pre>      similarity_category_A1  similarity_category_A2  similarity_category_B1  \\\n277                 0.100000                5.757143                2.928571   \n1335                2.928571                5.757143                8.585714   \n\n      similarity_category_B2  \n277                 7.171429  \n1335               10.000000  \n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[15]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[15]: similarity_category_A1 similarity_category_A2 similarity_category_B1 similarity_category_B2 choose_A1 277 0.100000 5.757143 2.928571 7.171429 0.010618 1335 2.928571 5.757143 8.585714 10.000000 0.219450 <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[16]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"choose_A1 = \"\n      f\"{model.coef_[0][0]:.2f}*similarity_category_A1 \"\n      f\"{model.coef_[0][1]:.2f}*similarity_category_A2 \"\n      f\"{model.coef_[0][2]:.2f}*similarity_category_B1 \"\n      f\"{model.coef_[0][3]:.2f}*similarity_category_B2 \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"choose_A1 = \"       f\"{model.coef_[0][0]:.2f}*similarity_category_A1 \"       f\"{model.coef_[0][1]:.2f}*similarity_category_A2 \"       f\"{model.coef_[0][2]:.2f}*similarity_category_B1 \"       f\"{model.coef_[0][3]:.2f}*similarity_category_B2 \"       f\"{model.intercept_[0]:+.2f} \") <pre>choose_A1 = 0.01*similarity_category_A1 0.00*similarity_category_A2 0.02*similarity_category_B1 0.01*similarity_category_B2 -0.15 \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"synthetic/docs/Examples/Psychology/Luce-Choice-Ratio/#luce-choice-ratio","title":"Luce-Choice-Ratio\u00b6","text":""},{"location":"synthetic/docs/Examples/Psychology/Q-Learning/","title":"Q Learning","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>The q-learning experiment has to be initialized with a specific formula and effects.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.psychology.q_learning import  q_learning\n\ns = q_learning()\n</pre> import numpy as np from autora.experiment_runner.synthetic.psychology.q_learning import  q_learning  s = q_learning() <p>Check the docstring to get information about the model</p> In\u00a0[3]: Copied! <pre>help(q_learning)\n</pre> help(q_learning) <pre>Help on function q_learning in module autora.experiment_runner.synthetic.psychology.q_learning:\n\nq_learning(name='Q-Learning', learning_rate: float = 0.2, decision_noise: float = 3.0, n_actions: int = 2, forget_rate: float = 0.0, perseverance_bias: float = 0.0, correlated_reward: bool = False)\n    An agent that runs simple Q-learning for an n-armed bandits tasks.\n    \n    Args:\n        name: name of the experiment\n        trials: number of trials\n        learning_rate: learning rate for Q-learning\n        decision_noise: softmax parameter for decision noise\n        n_actions: number of actions\n        forget_rate: rate of forgetting\n        perseverance_bias: bias towards choosing the previously chosen action\n        correlated_reward: whether rewards are correlated\n    \n    Examples:\n        &gt;&gt;&gt; experiment = q_learning()\n    \n        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n        # always be a list of numpy arrays. Each array corresponds to the choices made by the agent\n        # for each trial in the input. Thus, arrays have shape (n_trials, n_actions).\n        &gt;&gt;&gt; experiment.run(np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]),\n        ...                random_state=42)\n        [array([[1., 0.],\n               [0., 1.],\n               [0., 1.],\n               [0., 1.],\n               [1., 0.],\n               [1., 0.]])]\n    \n        # The runner can accept pandas DataFrames. Each cell of the DataFrame should contain a\n        # numpy array with shape (n_trials, n_actions). The return value will be a list of numpy\n        # arrays, each corresponding to the choices made by the agent for each trial in the input.\n        &gt;&gt;&gt; experiment.run(\n        ...     pd.DataFrame(\n        ...         {'reward array': [np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]])]}),\n        ...     random_state = 42)\n        [array([[1., 0.],\n               [0., 1.],\n               [0., 1.],\n               [0., 1.],\n               [1., 0.],\n               [1., 0.]])]\n\n</pre> <p>... or use the describe function:</p> In\u00a0[4]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    An agent that runs simple Q-learning for an n-armed bandits tasks.\n\n    Args:\n        name: name of the experiment\n        trials: number of trials\n        learning_rate: learning rate for Q-learning\n        decision_noise: softmax parameter for decision noise\n        n_actions: number of actions\n        forget_rate: rate of forgetting\n        perseverance_bias: bias towards choosing the previously chosen action\n        correlated_reward: whether rewards are correlated\n\n    Examples:\n        &gt;&gt;&gt; experiment = q_learning()\n\n        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n        # always be a list of numpy arrays. Each array corresponds to the choices made by the agent\n        # for each trial in the input. Thus, arrays have shape (n_trials, n_actions).\n        &gt;&gt;&gt; experiment.run(np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]]),\n        ...                random_state=42)\n        [array([[1., 0.],\n               [0., 1.],\n               [0., 1.],\n               [0., 1.],\n               [1., 0.],\n               [1., 0.]])]\n\n        # The runner can accept pandas DataFrames. Each cell of the DataFrame should contain a\n        # numpy array with shape (n_trials, n_actions). The return value will be a list of numpy\n        # arrays, each corresponding to the choices made by the agent for each trial in the input.\n        &gt;&gt;&gt; experiment.run(\n        ...     pd.DataFrame(\n        ...         {'reward array': [np.array([[0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0]])]}),\n        ...     random_state = 42)\n        [array([[1., 0.],\n               [0., 1.],\n               [0., 1.],\n               [0., 1.],\n               [1., 0.],\n               [1., 0.]])]\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[5]: Copied! <pre>s.name\n</pre> s.name Out[5]: <pre>'Q-Learning'</pre> <p>... a valid variables description:</p> In\u00a0[6]: Copied! <pre>s.variables\n</pre> s.variables Out[6]: <pre>VariableCollection(independent_variables=[IV(name='reward array', value_range=None, allowed_values=None, units='reward', type=&lt;ValueType.BOOLEAN: 'boolean'&gt;, variable_label='Reward Sequence', rescale=1, is_covariate=False)], dependent_variables=[DV(name='choice array', value_range=None, allowed_values=None, units='actions', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Action Sequence', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... the conditions for this experiment are reward sequences. This is a variable type not yet fully integrated in AutoRA. Therefore ther is no domain yet:</p> In\u00a0[7]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x <p>... the plotter is not implemented yet:</p> In\u00a0[8]: Copied! <pre>s.plotter()\n</pre> s.plotter() <pre>\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 s.plotter()\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/src/autora/experiment_runner/synthetic/psychology/q_learning.py:257, in q_learning.&lt;locals&gt;.plotter()\n    256 def plotter():\n--&gt; 257     raise NotImplementedError\n\nNotImplementedError: </pre> <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[9]: Copied! <pre>from autora.state import StandardState, on_state, Delta, experiment_runner_on_state, estimator_on_state\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, Delta, experiment_runner_on_state, estimator_on_state # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Here, we use a special experimentalist that can generate random trial sequences and wrap it with the <code>on_state</code> function to use them on state:</p> In\u00a0[10]: Copied! <pre>%%capture\n!pip install autora-experimentalist-bandit-random\n</pre> %%capture !pip install autora-experimentalist-bandit-random In\u00a0[11]: Copied! <pre>from autora.experimentalist.bandit_random import bandit_random_pool\n# Wrap the functions to use on state\n# Experimentalists:\n\n@on_state()\ndef pool_on_state(num_samples):\n      return Delta(conditions=bandit_random_pool(num_rewards=2, sequence_length=20, num_samples=num_samples))\n\n\nstate = pool_on_state(state, num_samples=2)\nprint(state.conditions)\n</pre> from autora.experimentalist.bandit_random import bandit_random_pool # Wrap the functions to use on state # Experimentalists:  @on_state() def pool_on_state(num_samples):       return Delta(conditions=bandit_random_pool(num_rewards=2, sequence_length=20, num_samples=num_samples))   state = pool_on_state(state, num_samples=2) print(state.conditions) <pre>                                        reward array\n0  [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...\n1  [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1...\n</pre> <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[12]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data Out[12]: reward array choice array 0 [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1... [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0... 1 [[0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1... [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0... <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[17]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"choose_A1 = \"\n      f\"{model.coef_[0][0]:.2f}*similarity_category_A1 \"\n      f\"{model.coef_[0][1]:.2f}*similarity_category_A2 \"\n      f\"{model.coef_[0][2]:.2f}*similarity_category_B1 \"\n      f\"{model.coef_[0][3]:.2f}*similarity_category_B2 \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"choose_A1 = \"       f\"{model.coef_[0][0]:.2f}*similarity_category_A1 \"       f\"{model.coef_[0][1]:.2f}*similarity_category_A2 \"       f\"{model.coef_[0][2]:.2f}*similarity_category_B1 \"       f\"{model.coef_[0][3]:.2f}*similarity_category_B2 \"       f\"{model.intercept_[0]:+.2f} \") In\u00a0[\u00a0]: Copied! <pre>\n</pre> <pre>Epoch 86/100 --- Loss: 0.5586882; Time: 0.0762s; Convergence value: 1.78e-01\nEpoch 87/100 --- Loss: 0.7901477; Time: 0.0767s; Convergence value: 1.82e-01\nEpoch 88/100 --- Loss: 0.5265486; Time: 0.0751s; Convergence value: 1.92e-01\nEpoch 89/100 --- Loss: 0.4401408; Time: 0.0743s; Convergence value: 1.86e-01\nEpoch 90/100 --- Loss: 0.3039415; Time: 0.0756s; Convergence value: 1.82e-01\nEpoch 91/100 --- Loss: 0.3906522; Time: 0.0771s; Convergence value: 1.73e-01\nEpoch 92/100 --- Loss: 0.5437022; Time: 0.0769s; Convergence value: 1.65e-01\nEpoch 93/100 --- Loss: 0.4635772; Time: 0.0737s; Convergence value: 1.54e-01\nEpoch 94/100 --- Loss: 0.4845441; Time: 0.0743s; Convergence value: 1.48e-01\nEpoch 95/100 --- Loss: 0.2648371; Time: 0.0770s; Convergence value: 1.56e-01\nEpoch 96/100 --- Loss: 0.3382604; Time: 0.0748s; Convergence value: 1.37e-01\nEpoch 97/100 --- Loss: 0.2581106; Time: 0.0742s; Convergence value: 1.25e-01\nEpoch 98/100 --- Loss: 0.6365235; Time: 0.0737s; Convergence value: 1.47e-01\nEpoch 99/100 --- Loss: 0.2228255; Time: 0.0741s; Convergence value: 1.67e-01\nEpoch 100/100 --- Loss: 0.3986339; Time: 0.0745s; Convergence value: 1.73e-01\nMaximum number of training epochs reached.\nModel did not converge yet.\nTest the trained RNN on a test dataset...\nEpoch 1/1 --- Loss: 0.4823526; Time: 0.0079s; Convergence value: nan\nMaximum number of training epochs reached.\nModel did not converge yet.\nRNN training took 7.47 seconds.\n</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[18], line 12\n      9     y = np.stack(experiment_data[dv].tolist())\n     10     return Delta(models=[theorist_a.fit(x, y)])\n---&gt; 12 state = theorist_on_state(state)\n     13 # Access the last model:\n     14 model = state.models[-1]\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/state.py:939, in delta_to_state.&lt;locals&gt;._f(state_, **kwargs)\n    937 @wraps(f)\n    938 def _f(state_: S, **kwargs) -&gt; S:\n--&gt; 939     delta = f(state_, **kwargs)\n    940     assert isinstance(delta, Mapping), (\n    941         \"Output of %s must be a `Delta`, `UserDict`, \" \"or `dict`.\" % f\n    942     )\n    943     new_state = state_ + delta\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/state.py:675, in inputs_from_state.&lt;locals&gt;._f(state_, **kwargs)\n    673     arguments_from_state[\"state\"] = state_\n    674 arguments = dict(arguments_from_state, **kwargs)\n--&gt; 675 result = f(**arguments)\n    676 return result\n\nCell In[18], line 10, in theorist_on_state(experiment_data)\n      8 x = np.stack(experiment_data[iv].tolist())\n      9 y = np.stack(experiment_data[dv].tolist())\n---&gt; 10 return Delta(models=[theorist_a.fit(x, y)])\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/theorist/rnn_sindy_rl/__init__.py:159, in RNNSindy.fit(self, conditions, observations, epochs, **kwargs)\n    156 if epochs is None:\n    157     epochs = self.epochs\n--&gt; 159 self.rnn = rnn_main(\n    160     xs=conditions,\n    161     ys=observations,\n    162     model=self.rnn,\n    163     epochs=epochs,\n    164     **kwargs,\n    165 )\n    167 self.sindy = sindy_main(\n    168     conditions,\n    169     observations,\n   (...)\n    180     **kwargs,\n    181 )\n    183 return self\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/autora/theorist/rnn_sindy_rl/rnn_main.py:103, in main(xs, ys, model, epochs, n_steps_per_call, batch_size, learning_rate, convergence_threshold, analysis, save_name, checkpoint, **kwargs)\n     98 # save trained parameters  \n     99 state_dict = {\n    100   'model': model.state_dict() if isinstance(model, torch.nn.Module) else [model_i.state_dict() for model_i in model],\n    101   'optimizer': optimizer_rnn.state_dict() if isinstance(optimizer_rnn, torch.optim.Adam) else [optim_i.state_dict() for optim_i in optimizer_rnn],\n    102 }\n--&gt; 103 torch.save(state_dict, save_name)\n    105 print(f'Saved RNN parameters to file {save_name}.')\n    107 # Analysis\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/torch/serialization.py:651, in save(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\n    648 _check_save_filelike(f)\n    650 if _use_new_zipfile_serialization:\n--&gt; 651     with _open_zipfile_writer(f) as opened_zipfile:\n    652         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n    653         return\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/torch/serialization.py:525, in _open_zipfile_writer(name_or_buffer)\n    523 else:\n    524     container = _open_zipfile_writer_buffer\n--&gt; 525 return container(name_or_buffer)\n\nFile ~/Documents/GitHub/AutoRA/autora-synthetic/venv/lib/python3.11/site-packages/torch/serialization.py:496, in _open_zipfile_writer_file.__init__(self, name)\n    494     super().__init__(torch._C.PyTorchFileWriter(self.file_stream))\n    495 else:\n--&gt; 496     super().__init__(torch._C.PyTorchFileWriter(self.name))\n\nRuntimeError: Parent directory trained_models does not exist.</pre>"},{"location":"synthetic/docs/Examples/Psychology/Q-Learning/#q-learning","title":"Q-Learning\u00b6","text":""},{"location":"synthetic/docs/Examples/Psychophysics/Weber-Fechner-Law/","title":"Weber Fechner Law","text":"In\u00a0[11]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora\" <p>Load the Weber-Fechner Law:</p> In\u00a0[12]: Copied! <pre>import numpy as np\nfrom autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\ns = weber_fechner_law()\n</pre> import numpy as np from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law s = weber_fechner_law() <p>Check the docstring to get information about the model</p> In\u00a0[13]: Copied! <pre>help(weber_fechner_law)\n</pre> help(weber_fechner_law) <pre>Help on function weber_fechner_law in module autora.experiment_runner.synthetic.psychophysics.weber_fechner_law:\n\nweber_fechner_law(name='Weber-Fechner Law', resolution=100, constant=1.0, maximum_stimulus_intensity=5.0)\n    Weber-Fechner Law\n    \n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus 1 and 2\n        constant: constant multiplier\n        maximum_stimulus_intensity: maximum value for stimulus 1 and 2\n    \n    Examples:\n        &gt;&gt;&gt; experiment = weber_fechner_law()\n    \n        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n        # always be a pandas DataFrame.\n        &gt;&gt;&gt; experiment.run(np.array([[.1,.2]]), random_state=42)\n            S1   S2  difference_detected\n        0  0.1  0.2             0.696194\n    \n        &gt;&gt;&gt; experiment.run(pd.DataFrame({'S1': [0.1], 'S2': [0.2]}), random_state=42)\n            S1   S2  difference_detected\n        0  0.1  0.2             0.696194\n\n</pre> <p>... or use the describe function:</p> In\u00a0[14]: Copied! <pre>from autora.experiment_runner.synthetic.utilities import describe\n\nprint(describe(s))\n</pre> from autora.experiment_runner.synthetic.utilities import describe  print(describe(s)) <pre>\n    Weber-Fechner Law\n\n    Args:\n        name: name of the experiment\n        resolution: number of allowed values for stimulus 1 and 2\n        constant: constant multiplier\n        maximum_stimulus_intensity: maximum value for stimulus 1 and 2\n\n    Examples:\n        &gt;&gt;&gt; experiment = weber_fechner_law()\n\n        # The runner can accept numpy arrays or pandas DataFrames, but the return value will\n        # always be a pandas DataFrame.\n        &gt;&gt;&gt; experiment.run(np.array([[.1,.2]]), random_state=42)\n            S1   S2  difference_detected\n        0  0.1  0.2             0.696194\n\n        &gt;&gt;&gt; experiment.run(pd.DataFrame({'S1': [0.1], 'S2': [0.2]}), random_state=42)\n            S1   S2  difference_detected\n        0  0.1  0.2             0.696194\n\n    \n</pre> <p>The synthetic experiement <code>s</code> has properties like the name of the experiment:</p> In\u00a0[15]: Copied! <pre>s.name\n</pre> s.name Out[15]: <pre>'Weber-Fechner Law'</pre> <p>... a valid metadata description:</p> In\u00a0[16]: Copied! <pre>s.variables\n</pre> s.variables Out[16]: <pre>VariableCollection(independent_variables=[IV(name='S1', value_range=(0.01, 5.0), allowed_values=array([0.01      , 0.06040404, 0.11080808, 0.16121212, 0.21161616,\n       0.2620202 , 0.31242424, 0.36282828, 0.41323232, 0.46363636,\n       0.5140404 , 0.56444444, 0.61484848, 0.66525253, 0.71565657,\n       0.76606061, 0.81646465, 0.86686869, 0.91727273, 0.96767677,\n       1.01808081, 1.06848485, 1.11888889, 1.16929293, 1.21969697,\n       1.27010101, 1.32050505, 1.37090909, 1.42131313, 1.47171717,\n       1.52212121, 1.57252525, 1.62292929, 1.67333333, 1.72373737,\n       1.77414141, 1.82454545, 1.87494949, 1.92535354, 1.97575758,\n       2.02616162, 2.07656566, 2.1269697 , 2.17737374, 2.22777778,\n       2.27818182, 2.32858586, 2.3789899 , 2.42939394, 2.47979798,\n       2.53020202, 2.58060606, 2.6310101 , 2.68141414, 2.73181818,\n       2.78222222, 2.83262626, 2.8830303 , 2.93343434, 2.98383838,\n       3.03424242, 3.08464646, 3.13505051, 3.18545455, 3.23585859,\n       3.28626263, 3.33666667, 3.38707071, 3.43747475, 3.48787879,\n       3.53828283, 3.58868687, 3.63909091, 3.68949495, 3.73989899,\n       3.79030303, 3.84070707, 3.89111111, 3.94151515, 3.99191919,\n       4.04232323, 4.09272727, 4.14313131, 4.19353535, 4.24393939,\n       4.29434343, 4.34474747, 4.39515152, 4.44555556, 4.4959596 ,\n       4.54636364, 4.59676768, 4.64717172, 4.69757576, 4.7479798 ,\n       4.79838384, 4.84878788, 4.89919192, 4.94959596, 5.        ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Stimulus 1 Intensity', rescale=1, is_covariate=False), IV(name='S2', value_range=(0.01, 5.0), allowed_values=array([0.01      , 0.06040404, 0.11080808, 0.16121212, 0.21161616,\n       0.2620202 , 0.31242424, 0.36282828, 0.41323232, 0.46363636,\n       0.5140404 , 0.56444444, 0.61484848, 0.66525253, 0.71565657,\n       0.76606061, 0.81646465, 0.86686869, 0.91727273, 0.96767677,\n       1.01808081, 1.06848485, 1.11888889, 1.16929293, 1.21969697,\n       1.27010101, 1.32050505, 1.37090909, 1.42131313, 1.47171717,\n       1.52212121, 1.57252525, 1.62292929, 1.67333333, 1.72373737,\n       1.77414141, 1.82454545, 1.87494949, 1.92535354, 1.97575758,\n       2.02616162, 2.07656566, 2.1269697 , 2.17737374, 2.22777778,\n       2.27818182, 2.32858586, 2.3789899 , 2.42939394, 2.47979798,\n       2.53020202, 2.58060606, 2.6310101 , 2.68141414, 2.73181818,\n       2.78222222, 2.83262626, 2.8830303 , 2.93343434, 2.98383838,\n       3.03424242, 3.08464646, 3.13505051, 3.18545455, 3.23585859,\n       3.28626263, 3.33666667, 3.38707071, 3.43747475, 3.48787879,\n       3.53828283, 3.58868687, 3.63909091, 3.68949495, 3.73989899,\n       3.79030303, 3.84070707, 3.89111111, 3.94151515, 3.99191919,\n       4.04232323, 4.09272727, 4.14313131, 4.19353535, 4.24393939,\n       4.29434343, 4.34474747, 4.39515152, 4.44555556, 4.4959596 ,\n       4.54636364, 4.59676768, 4.64717172, 4.69757576, 4.7479798 ,\n       4.79838384, 4.84878788, 4.89919192, 4.94959596, 5.        ]), units='intensity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Stimulus 2 Intensity', rescale=1, is_covariate=False)], dependent_variables=[DV(name='difference_detected', value_range=(0, 5.0), allowed_values=None, units='sensation', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Sensation', rescale=1, is_covariate=False)], covariates=[])</pre> <p>... a function to generate the full domain of the data (if possible)</p> In\u00a0[17]: Copied! <pre>x = s.domain()\nx\n</pre> x = s.domain() x <p>... the experiment_runner which can be called to generate experimental results:</p> In\u00a0[18]: Copied! <pre>import numpy as np\ny = s.run(x)  # doctest: +ELLIPSIS\ny\n</pre> import numpy as np y = s.run(x)  # doctest: +ELLIPSIS y Out[18]: <pre>array([[0.01      , 0.01      ],\n       [0.01      , 0.06040404],\n       [0.01      , 0.11080808],\n       ...,\n       [4.94959596, 4.94959596],\n       [4.94959596, 5.        ],\n       [5.        , 5.        ]])</pre> <p>... a function to plot the ground truth:</p> In\u00a0[19]: Copied! <pre>s.plotter()\n</pre> s.plotter() Out[19]: S1 S2 difference_detected 0 0.010000 0.010000 -0.005815 1 0.010000 0.060404 1.782883 2 0.010000 0.110808 2.398774 3 0.010000 0.161212 2.780492 4 0.010000 0.211616 3.046934 ... ... ... ... 5045 4.899192 4.949596 0.011407 5046 4.899192 5.000000 0.008136 5047 4.949596 4.949596 -0.002980 5048 4.949596 5.000000 -0.000581 5049 5.000000 5.000000 -0.010569 <p>5050 rows \u00d7 3 columns</p> <p>... against a fitted model if it exists:</p> In\u00a0[20]: Copied! <pre>from sklearn.linear_model import LinearRegression\nmodel = LinearRegression().fit(x, y)\ns.plotter(model)\n</pre> from sklearn.linear_model import LinearRegression model = LinearRegression().fit(x, y) s.plotter(model) <p>We can wrap this functions to use with the state logic of AutoRA: First, we create the state with the variables:</p> In\u00a0[21]: Copied! <pre>from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state\nfrom autora.experimentalist.grid import grid_pool\nfrom autora.experimentalist.random import random_sample\nfrom functools import partial\nimport random\n\n# We can get the variables from the runner\nvariables = s.variables\n\n# With the variables, we initialize a StandardState\nstate = StandardState(variables)\n</pre> from autora.state import StandardState, on_state, experiment_runner_on_state, estimator_on_state from autora.experimentalist.grid import grid_pool from autora.experimentalist.random import random_sample from functools import partial import random  # We can get the variables from the runner variables = s.variables  # With the variables, we initialize a StandardState state = StandardState(variables) <p>Wrap the experimentalists in <code>on_state</code> function to use them on state:</p> In\u00a0[23]: Copied! <pre># Wrap the functions to use on state\n# Experimentalists:\npool_on_state = on_state(grid_pool, output=['conditions'])\nsample_on_state = on_state(random_sample, output=['conditions'])\n\nstate = pool_on_state(state)\nstate = sample_on_state(state, num_samples=20)\nprint(state.conditions)\n</pre> # Wrap the functions to use on state # Experimentalists: pool_on_state = on_state(grid_pool, output=['conditions']) sample_on_state = on_state(random_sample, output=['conditions'])  state = pool_on_state(state) state = sample_on_state(state, num_samples=20) print(state.conditions) <p>Wrap the runner with the <code>experiment_runner_on_state</code> wrapper to use it on state:</p> In\u00a0[24]: Copied! <pre># Runner:\nrun_on_state = experiment_runner_on_state(s.run)\nstate = run_on_state(state)\n\nstate.experiment_data\n</pre> # Runner: run_on_state = experiment_runner_on_state(s.run) state = run_on_state(state)  state.experiment_data <pre>            S1        S2\n1267  0.614848  3.387071\n544   0.262020  2.227778\n5401  2.731818  0.060404\n1120  0.564444  1.018081\n5469  2.731818  3.487879\n8337  4.193535  1.874949\n6560  3.286263  3.034242\n5432  2.731818  1.622929\n644   0.312424  2.227778\n8874  4.445556  3.739899\n163   0.060404  3.185455\n6114  3.084646  0.715657\n3789  1.874949  4.495960\n5197  2.580606  4.899192\n3468  1.723737  3.437475\n2678  1.320505  3.941515\n3275  1.622929  3.790303\n8184  4.092727  4.243939\n772   0.362828  3.639091\n8410  4.243939  0.514040\n</pre> <p>Wrap the regressor with the <code>estimator_on_state</code> wrapper:</p> In\u00a0[25]: Copied! <pre>theorist = LinearRegression()\ntheorist_on_state = estimator_on_state(theorist)\n\nstate = theorist_on_state(state)\n# Access the last model:\nmodel = state.models[-1]\n\n\nprint(f\"I = \"\n      f\"{model.coef_[0][0]:.2f}*S0 \"\n      f\"{model.coef_[0][1]:+.2f}*S1 \"\n      f\"{model.intercept_[0]:+.2f} \")\n</pre> theorist = LinearRegression() theorist_on_state = estimator_on_state(theorist)  state = theorist_on_state(state) # Access the last model: model = state.models[-1]   print(f\"I = \"       f\"{model.coef_[0][0]:.2f}*S0 \"       f\"{model.coef_[0][1]:+.2f}*S1 \"       f\"{model.intercept_[0]:+.2f} \") Out[25]: S1 S2 difference_detected 1267 0.614848 3.387071 1.695802 544 0.262020 2.227778 2.146260 5401 2.731818 0.060404 -3.822774 1120 0.564444 1.018081 0.596201 5469 2.731818 3.487879 0.234643 8337 4.193535 1.874949 -0.811834 6560 3.286263 3.034242 -0.071182 5432 2.731818 1.622929 -0.524767 644 0.312424 2.227778 1.984785 8874 4.445556 3.739899 -0.157984 163 0.060404 3.185455 3.967021 6114 3.084646 0.715657 -1.452406 3789 1.874949 4.495960 0.883423 5197 2.580606 4.899192 0.644102 3468 1.723737 3.437475 0.703138 2678 1.320505 3.941515 1.101408 3275 1.622929 3.790303 0.850367 8184 4.092727 4.243939 0.034932 772 0.362828 3.639091 2.304981 8410 4.243939 0.514040 -2.108381"},{"location":"synthetic/docs/Examples/Psychophysics/Weber-Fechner-Law/#synthetic-data-examples","title":"Synthetic Data Examples\u00b6","text":""},{"location":"theorist/","title":"Theorist Overview","text":"<p>AutoRA consists of a set of techniques designed to automate the construction of interpretable models from data. To approach this problem, we can consider computational models as small, interpretable computation graphs (see also Musslick, 2021). A computation graph can take experiment parameters as input (e.g. the brightness of a visual stimulus) and can transform this input through a combination of functions to produce observable dependent measures as output (e.g. the probability that a participant can detect the stimulus).</p> <p></p> <p>Theorist use information about experimental conditions that have already been probed \\(\\vec{x}' \\in X'\\) and  respective dependent measures \\(\\vec{y}' \\in Y'\\). The following table includes the theorists currently implemented in AutoRA.</p> Name Links Description Arguments Bayesian Machine Scientist (BMS) Package, Docs A theorist that uses one algorithmic Bayesian approach to symbolic regression, with the aim of discovering interpretable expressions which capture relationships within data. \\(X', Y'\\) Bayesian Symbolic Regression (BSR) Package, Docs A theorist that uses another algorithmic Bayesian approach to symbolic regression, with the aim of discovering interpretable expressions which capture relationships within data. \\(X', Y'\\) Differentiable Architecture Search (DARTS) Package, Docs A theorist that automates the discovery of neural network architectures by making architecture search amenable to gradient descent. \\(X', Y'\\)"},{"location":"tutorials/","title":"Basic Tutorials","text":"<p>AutoRA consists of modules specific to different stages of empirical research as well as supporting utilities and ancillary research procedures. The following tutorials demonstrate how to use these various components, and how to integrate them into closed-loop scientific discovery.</p> <p>Once you understand the basics of AutoRA, you may be curious how to apply AutoRA to real-world discovery problems. You can find use case examples of AutoRA in the Use Case Tutorials section. </p> <p>If you have questions about these tutorials or would otherwise like to discuss use of AutoRA, feel free to start or join a discussion in the Using AutoRA section of the AutoRA forum.</p>"},{"location":"tutorials/Theorist/","title":"Theorist","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[theorist-bms]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[theorist-bms]\" <p>After importing the necessary modules,</p> In\u00a0[\u00a0]: Copied! <pre>from autora.theorist.bms import BMSRegressor\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> from autora.theorist.bms import BMSRegressor import numpy as np import matplotlib.pyplot as plt <p>we begin by generating data with a ground-truth equation, $y = \\sin(x) + x^3$.</p> In\u00a0[\u00a0]: Copied! <pre>x = np.expand_dims(np.linspace(start=-1, stop=1, num=500), 1)\ny = np.power(x, 3) + np.sin(x)\n</pre> x = np.expand_dims(np.linspace(start=-1, stop=1, num=500), 1) y = np.power(x, 3) + np.sin(x) <p>Then we set up the BMS regressor with our chosen meta parameters. In this case, we will specify the number of <code>epochs</code> as well as temperatures (<code>ts</code>). Note, BMS also allows users to specify unique priors over the operations considered in the search space (<code>prior_par</code>), but in this simple example we will stick with those priors implemented by the original authors, Guimer\u00e0 et al. (2020).</p> In\u00a0[\u00a0]: Copied! <pre>temperatures = [1.0] + [1.04**k for k in range(1, 20)]\n\nbms_estimator = BMSRegressor(\n    epochs=500,\n    ts=temperatures,\n)\n</pre> temperatures = [1.0] + [1.04**k for k in range(1, 20)]  bms_estimator = BMSRegressor(     epochs=500,     ts=temperatures, ) <p>With our regressor initialized, we can call the <code>fit</code> method to discover an equation for our data and then use the <code>predict</code> method to generate predictions using our discovered equation.</p> In\u00a0[\u00a0]: Copied! <pre>bms_estimator.fit(x,y)\ny_pred = bms_estimator.predict(x)\n</pre> bms_estimator.fit(x,y) y_pred = bms_estimator.predict(x) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [04:47&lt;00:00,  1.74it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <p>Finally, we can plot the results.</p> In\u00a0[\u00a0]: Copied! <pre># plot out the ground truth versus predicted responses\nplt.figure()\nplt.plot(x, y, \"o\")\nplt.plot(x, y_pred, \"-\")\nplt.show()\n</pre> # plot out the ground truth versus predicted responses plt.figure() plt.plot(x, y, \"o\") plt.plot(x, y_pred, \"-\") plt.show() <p>In this simple case, the algorithm finds an equation with a perfect fit.</p>"},{"location":"tutorials/Theorist/#theorist-tutorial","title":"Theorist Tutorial\u00b6","text":"<p>Theorists are classes designed to automate the construction of interpretable models from data. AutoRA theorists are implemented as sklearn regressors and can be used with the <code>fit</code> and <code>predict</code> methods.</p> <p></p> <p>In order to use a theorist, you must first install the corresponding theorist package. Some theorists are installed by default when you install <code>autora</code>. Once a theorist is installed, you can instantiate it and use it as you would any other sklearn regressor. That is, you can call the <code>fit</code> function of the theorist by passing in experimental conditions and corresponding observations, and then call the <code>predict</code> function to generate predicted observations for novel experimental conditions using the discovered model.</p> <p>The following tutorial demonstrates how to use the <code>BMSRegressor</code> (Guimer\u00e0 et al., 2020, in Sci. Adv.)\u2013a theorist that can discover an interpretable equation relating the independent variables of an experiment (experiment conditions) to predicted dependent variables (observations).</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/","title":"Overview","text":"<p>AutoRA (Automated Research Assistant) is an open-source framework designed to automate various stages of empirical research, including model discovery, experimental design, and data collection.</p> <p>This notebook is the first of four notebooks within the basic tutorials of <code>autora</code>. We suggest that you go through these notebooks in order as each builds upon the last. However, each notebook is self-contained and so there is no need to run the content of the last notebook for your current notebook.</p> <p>These notebooks provide a comprehensive introduction to the capabilities of <code>autora</code>. It demonstrates the fundamental components of <code>autora</code>, and how they can be combined to facilitate automated (closed-loop) empirical research through synthetic experiments.</p> <p>How to use this notebook You can progress through the notebook section by section or directly navigate to specific sections. If you choose the latter, it is recommended to execute all cells in the notebook initially, allowing you to easily rerun the cells in each section later without issues.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q \"autora[experimentalist-falsification]\"\n!pip install -q \"autora[experimentalist-novelty]\"\n!pip install -q \"autora[theorist-bms]\"\n</pre> !pip install -q \"autora[experimentalist-falsification]\" !pip install -q \"autora[experimentalist-novelty]\" !pip install -q \"autora[theorist-bms]\" <pre>WARNING: typer 0.12.3 does not provide the extra 'all'\nWARNING: typer 0.12.3 does not provide the extra 'all'\nWARNING: typer 0.12.3 does not provide the extra 'all'\n</pre> <p>To make all simulations in this notebook replicable, we will set some seeds.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n</pre> import numpy as np import torch  np.random.seed(42) torch.manual_seed(42) Out[\u00a0]: <pre>&lt;torch._C.Generator at 0x1f2aa767710&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>#Setup: Import modules\nimport matplotlib.pyplot as plt\nfrom autora.theorist.bms import BMSRegressor\nfrom autora.experimentalist.random import random_sample #Note that this sampler is embedded within the autora-core module and so does not need to be explicitly installed\n\n#Step 0: Defining variables\nground_truth = lambda x: np.sin(x) #Define a ground truth model that we will attempt to recover - here a sine wave\ninitial_X = np.linspace(0, 4 * np.pi, 200) #Define initial data\n\n#Step 1: EXPERIMENTALIST: Sample using the experimentalist\nnew_conditions = random_sample(initial_X, num_samples=30)\nnew_conditions = np.array(new_conditions).reshape(-1,1) #Turn variable into a 2D array\n\n#Step 2: EXPERIMENT RUNNER: Define and then obtain observations using the experiment runner\nrun_experiment = lambda x: ground_truth(x) + np.random.normal(0, 0.1, size=x.shape) #Define the runner, which here is simply the ground truth with noise\nnew_observations = run_experiment(new_conditions) #Obtain observations from the runner for the conditions proposed by the experimentalist\nnew_observations = new_observations.reshape(-1,1) #Turn variable into a 2D array\n\n#Step 3: THEORIST: Initiate and fit a model using the theorist\ntheorist_bms = BMSRegressor(epochs=100) #Initiate the BMS theorist\ntheorist_bms.fit(new_conditions, new_observations) #Fit a model to the data\n\n#Wrap-Up: Plot data and model\nsort_index = np.argsort(new_conditions, axis=0)[:,0] #We will first sort our data\nnew_conditions = new_conditions[sort_index,:]\nnew_observations = new_observations[sort_index,:]\n\nplt.plot(initial_X, ground_truth(initial_X), label='Ground Truth')\nplt.plot(new_conditions, new_observations, 'o', label='Sampled Conditions')\nplt.plot(initial_X, theorist_bms.predict(initial_X.reshape(-1,1)), label=f'Bayesian Machine Scientist ({theorist_bms.repr()})')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.legend()\n</pre> #Setup: Import modules import matplotlib.pyplot as plt from autora.theorist.bms import BMSRegressor from autora.experimentalist.random import random_sample #Note that this sampler is embedded within the autora-core module and so does not need to be explicitly installed  #Step 0: Defining variables ground_truth = lambda x: np.sin(x) #Define a ground truth model that we will attempt to recover - here a sine wave initial_X = np.linspace(0, 4 * np.pi, 200) #Define initial data  #Step 1: EXPERIMENTALIST: Sample using the experimentalist new_conditions = random_sample(initial_X, num_samples=30) new_conditions = np.array(new_conditions).reshape(-1,1) #Turn variable into a 2D array  #Step 2: EXPERIMENT RUNNER: Define and then obtain observations using the experiment runner run_experiment = lambda x: ground_truth(x) + np.random.normal(0, 0.1, size=x.shape) #Define the runner, which here is simply the ground truth with noise new_observations = run_experiment(new_conditions) #Obtain observations from the runner for the conditions proposed by the experimentalist new_observations = new_observations.reshape(-1,1) #Turn variable into a 2D array  #Step 3: THEORIST: Initiate and fit a model using the theorist theorist_bms = BMSRegressor(epochs=100) #Initiate the BMS theorist theorist_bms.fit(new_conditions, new_observations) #Fit a model to the data  #Wrap-Up: Plot data and model sort_index = np.argsort(new_conditions, axis=0)[:,0] #We will first sort our data new_conditions = new_conditions[sort_index,:] new_observations = new_observations[sort_index,:]  plt.plot(initial_X, ground_truth(initial_X), label='Ground Truth') plt.plot(new_conditions, new_observations, 'o', label='Sampled Conditions') plt.plot(initial_X, theorist_bms.predict(initial_X.reshape(-1,1)), label=f'Bayesian Machine Scientist ({theorist_bms.repr()})') plt.xlabel('x') plt.ylabel('y') plt.title('Sine Function') plt.legend() <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 24.88it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x1f2d6a4a1a0&gt;</pre> <p>Do not stop with this toy example! At this point, it may be tempting to start working on your own project, but we urge you to continue through the tutorials. <code>autora</code> has a lot of embedded functionality that you are going to want to use, and this toy example has stripped those away. So, keep going and see how much <code>autora</code> has to offer!</p> In\u00a0[\u00a0]: Copied! <pre>ground_truth = lambda x: np.sin(x)\nrun_experiment = lambda x: ground_truth(x) + np.random.normal(0, 0.1, size=x.shape)\n</pre> ground_truth = lambda x: np.sin(x) run_experiment = lambda x: ground_truth(x) + np.random.normal(0, 0.1, size=x.shape) <p>Next, we generate a pool of all possible experimental conditions from the domain $[0, 2\\pi]$.</p> In\u00a0[\u00a0]: Copied! <pre>condition_pool = np.linspace(0, 2 * np.pi, 100)\n</pre> condition_pool = np.linspace(0, 2 * np.pi, 100) <p>In order to run a simple synthetic experiment, we can first sample from the pool of possible experiment conditions (without replacement), and then pass these conditions to the synthetic experiment runner:</p> In\u00a0[\u00a0]: Copied! <pre>initial_conditions = np.random.choice(condition_pool, size=10, replace=False)\ninitial_observations = run_experiment(initial_conditions)\n\n# plot sampled conditions against ground-truth\nplt.plot(condition_pool, ground_truth(condition_pool), label='Ground Truth')\nplt.plot(initial_conditions, initial_observations, 'o', label='Sampled Conditions')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.legend()\n</pre> initial_conditions = np.random.choice(condition_pool, size=10, replace=False) initial_observations = run_experiment(initial_conditions)  # plot sampled conditions against ground-truth plt.plot(condition_pool, ground_truth(condition_pool), label='Ground Truth') plt.plot(initial_conditions, initial_observations, 'o', label='Sampled Conditions') plt.xlabel('x') plt.ylabel('y') plt.title('Sine Function') plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x1f28b43a200&gt;</pre> <p>Certain theorists and experimentalists may need to have knowledge about the experimental variables, such as the domain from which new experiment conditions are sampled. To provide this information, we can utilize a <code>VariableCollection</code> object, which contains immutable metadata about dependent variables, independent variables, and covariates. In the context of our synthetic experiment, we have a single independent variable (<code>iv</code>) denoted as $x$, and a single dependent variable (<code>dv</code>) denoted as $y$.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.variable import Variable, ValueType, VariableCollection\n\n# Specify independent variable\niv = Variable(\n    name=\"x\",                       # name of the independent variable\n    value_range=(0, 2 * np.pi),     # specify the domain\n    allowed_values=condition_pool,  # alternatively, we can specify the pool of allowed conditions directly\n)\n\n# specify dependent variable\ndv = Variable(\n    name=\"y\",                       # name of the dependent variable\n    type=ValueType.REAL,            # specify the variable type (some theorists require this to optimize)\n)\n\n# Variable collection with ivs and dvs\nvariables = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n</pre> from autora.variable import Variable, ValueType, VariableCollection  # Specify independent variable iv = Variable(     name=\"x\",                       # name of the independent variable     value_range=(0, 2 * np.pi),     # specify the domain     allowed_values=condition_pool,  # alternatively, we can specify the pool of allowed conditions directly )  # specify dependent variable dv = Variable(     name=\"y\",                       # name of the dependent variable     type=ValueType.REAL,            # specify the variable type (some theorists require this to optimize) )  # Variable collection with ivs and dvs variables = VariableCollection(     independent_variables=[iv],     dependent_variables=[dv], ) <p>Note: For expository reasons, we focus in this tutorial on simple synthetic experiments. In general, <code>autora</code> provides functionality for automating more complex synthetic experiments, as well as real-world experiments, such as behavioral data collection via web-based experiments, experiments with electrical circuits via Tinkerforge, and other automated experimentation platforms.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\nfrom autora.theorist.bms import BMSRegressor\n\ntheorist_lr = LinearRegression()\ntheorist_bms = BMSRegressor(epochs=100)\n</pre> from sklearn.linear_model import LinearRegression from autora.theorist.bms import BMSRegressor  theorist_lr = LinearRegression() theorist_bms = BMSRegressor(epochs=100) <p>Once instantiated, we can fit the theorist to link experimental conditions with observations. However, before doing so, we should convert both inputs into 2-dimensional numpy arrays. Theorists should return a single model directly or multiple models within a list.</p> In\u00a0[\u00a0]: Copied! <pre># convert data to 2-dimensional numpy array\ninitial_conditions = initial_conditions.reshape(-1, 1)\ninitial_observations = initial_observations.reshape(-1, 1)\nprint(f\"Size of the initial conditions: {initial_conditions.shape},\\nSize of the initial observations: {initial_observations.shape}\\n\")\n\n# fit theorists\ntheorist_lr.fit(initial_conditions, initial_observations)\ntheorist_bms.fit(initial_conditions, initial_observations)\n</pre> # convert data to 2-dimensional numpy array initial_conditions = initial_conditions.reshape(-1, 1) initial_observations = initial_observations.reshape(-1, 1) print(f\"Size of the initial conditions: {initial_conditions.shape},\\nSize of the initial observations: {initial_observations.shape}\\n\")  # fit theorists theorist_lr.fit(initial_conditions, initial_observations) theorist_bms.fit(initial_conditions, initial_observations) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>Size of the initial conditions: (10, 1),\nSize of the initial observations: (10, 1)\n\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 22.11it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> Out[\u00a0]: <pre>sin(X0)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0BMSRegressoriFitted<pre>sin(X0)</pre> <p>For some theorists, we can inspect the resulting model architecture. For instance, in the BMS theorist, we can obtain the model formula via <code>theorist_bms.repr()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Model of BMS theorist: {theorist_bms.repr()}\")\n</pre> print(f\"Model of BMS theorist: {theorist_bms.repr()}\") <pre>Model of BMS theorist: sin(X0)\n</pre> <p>We may now obtain predictions from both theorists for the entire pool of experiment conditions.</p> In\u00a0[\u00a0]: Copied! <pre># convert condition pool into 2-dimensional numpy array before generating respective predictions\ncondition_pool = condition_pool.reshape(-1, 1)\n\n# obtain predictions\npredicted_observations_lr = theorist_lr.predict(condition_pool)\npredicted_observations_bms = theorist_bms.predict(condition_pool)\n</pre> # convert condition pool into 2-dimensional numpy array before generating respective predictions condition_pool = condition_pool.reshape(-1, 1)  # obtain predictions predicted_observations_lr = theorist_lr.predict(condition_pool) predicted_observations_bms = theorist_bms.predict(condition_pool) <p>In the next code segment, we plot the theorists' predictions against the ground truth. For the BMS theorist, we can obtain a latex expression of the model architecture using <code>theorist_bms.latex()</code>.</p> In\u00a0[\u00a0]: Copied! <pre># obtain latex expression of BMS theorist\nbms_model = theorist_bms.latex()\n\n# plot model predictions against ground-truth\nimport matplotlib.pyplot as plt\nplt.plot(condition_pool, ground_truth(condition_pool), label='Ground Truth')\nplt.plot(initial_conditions, initial_observations, 'o', label='Data Used for Model Identification')\nplt.plot(condition_pool, predicted_observations_lr, label='Linear Regression')\nplt.plot(condition_pool, predicted_observations_bms, label='Bayesian Machine Scientist: $' + bms_model + '$')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Model Predictions')\nplt.legend()\n</pre> # obtain latex expression of BMS theorist bms_model = theorist_bms.latex()  # plot model predictions against ground-truth import matplotlib.pyplot as plt plt.plot(condition_pool, ground_truth(condition_pool), label='Ground Truth') plt.plot(initial_conditions, initial_observations, 'o', label='Data Used for Model Identification') plt.plot(condition_pool, predicted_observations_lr, label='Linear Regression') plt.plot(condition_pool, predicted_observations_bms, label='Bayesian Machine Scientist: $' + bms_model + '$') plt.xlabel('x') plt.ylabel('y') plt.title('Model Predictions') plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x1f2b0a78c40&gt;</pre> <p>Note: There are various other types of theorists you can combine with AutoRA as long as they are implemented as <code>sklearn</code> estimators. This includes autora modules, any scikit learn estimators, as well as third-party packages, such as PySR for symbolic regression.</p> In\u00a0[\u00a0]: Copied! <pre>allowed_values = np.linspace(0, 2 * np.pi, 100)\nvariables.independent_variables[0].allowed_values = allowed_values\n</pre> allowed_values = np.linspace(0, 2 * np.pi, 100) variables.independent_variables[0].allowed_values = allowed_values <p>Now we can pass the grid pooler the list of independent variables from the <code>variables</code> object.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.grid import grid_pool\n\nnew_conditions = grid_pool(variables=variables)\n</pre> from autora.experimentalist.grid import grid_pool  new_conditions = grid_pool(variables=variables) <p>The resulting condition pool contains all experiment conditions from the grid:</p> In\u00a0[\u00a0]: Copied! <pre># return first 10 conditions\nfor idx, condition in enumerate(new_conditions.values):\n    print(condition)\n    if idx &gt; 9:\n        break\n</pre> # return first 10 conditions for idx, condition in enumerate(new_conditions.values):     print(condition)     if idx &gt; 9:         break <pre>[0.]\n[0.06346652]\n[0.12693304]\n[0.19039955]\n[0.25386607]\n[0.31733259]\n[0.38079911]\n[0.44426563]\n[0.50773215]\n[0.57119866]\n[0.63466518]\n</pre> <p>Alternatively, we may use the random pooler to randomly draw experimental conditions from the domains of each independent variable. The random pooler requires as input a list of discrete values from which to sample from. In this case, we can pass it <code>variables</code> for the independent variable. We can also specify the input argument <code>num_samples</code> to obtain 10 random samples.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\n\n# generate random pool of 10 conditions\nnum_samples = 10\nnew_conditions = random_pool(variables=variables,\n                             num_samples=num_samples)\n\n# print conditions\nfor idx, condition in enumerate(new_conditions.values):\n    print(condition)\n</pre> from autora.experimentalist.random import random_pool  # generate random pool of 10 conditions num_samples = 10 new_conditions = random_pool(variables=variables,                              num_samples=num_samples)  # print conditions for idx, condition in enumerate(new_conditions.values):     print(condition) <pre>[1.90399555]\n[4.75998887]\n[4.31572324]\n[0.57119866]\n[5.58505361]\n[4.82345539]\n[2.53866073]\n[2.0943951]\n[4.06185717]\n[3.87145761]\n</pre> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.novelty import novelty_sample\n\nnew_conditions_novelty = novelty_sample(conditions = condition_pool,\n                                 reference_conditions = initial_conditions,\n                                 num_samples = 2)\n\nprint(new_conditions_novelty)\n</pre> from autora.experimentalist.novelty import novelty_sample  new_conditions_novelty = novelty_sample(conditions = condition_pool,                                  reference_conditions = initial_conditions,                                  num_samples = 2)  print(new_conditions_novelty) <pre>           0\n36  2.284795\n37  2.348261\n</pre> <p>Another example for an experiment sampler is the falsification sampler. The falsification sampler identifies experiment conditions under which the loss of a candidate model (returned by the theorist) is predicted to be the highest. This loss is approximated with a neural network, which is trained to predict the loss of the candidate model, given some initial experimental conditions, respective initial observations, and the variables.</p> <p>The following code segment calls on the falsification sampler to return novel conditions based on the candidate model of the linear regression theorist introduced above. As with the novelty sampler, we seek to select 2 conditions.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.falsification import falsification_sample\n\nnew_conditions_falsification = falsification_sample(\n        conditions=condition_pool,\n        model=theorist_lr,\n        reference_conditions=initial_conditions,\n        reference_observations=initial_observations,\n        metadata=variables,\n        num_samples=2\n    )\n\nprint(new_conditions_falsification)\n</pre> from autora.experimentalist.falsification import falsification_sample  new_conditions_falsification = falsification_sample(         conditions=condition_pool,         model=theorist_lr,         reference_conditions=initial_conditions,         reference_observations=initial_observations,         metadata=variables,         num_samples=2     )  print(new_conditions_falsification) <pre>[[6.28318531]\n [6.21971879]]\n</pre> <p>We can plot the selected conditions for both samples relative to the selected samples. Since we don't have observations for those conditions, we plot them as vertical lines.</p> In\u00a0[\u00a0]: Copied! <pre>type(new_conditions_novelty)\n</pre> type(new_conditions_novelty) Out[\u00a0]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[\u00a0]: Copied! <pre># plot model predictions against ground-truth\ny_min = np.min(initial_observations)\ny_max  = np.max(initial_observations)\n\n# plot conditions obtained by novelty sampler\nfor idx, condition in enumerate(new_conditions_novelty):\n    if idx == 0:\n        plt.plot([condition, condition], [y_min, y_max], '--r', label='novelty conditions')\n    else: # we want to omit the label for all other conditions\n        plt.plot([condition, condition], [y_min, y_max], '--r')\n\n# plot conditions obtained by falsification sampler\nfor idx, condition in enumerate(new_conditions_falsification):\n    if idx == 0:\n        plt.plot([condition[0], condition[0]], [y_min, y_max], '--g', label='falsification conditions')\n    else: # we want to omit the label for all other conditions\n        plt.plot([condition[0], condition[0]], [y_min, y_max], '--g')\n\nplt.plot(condition_pool, ground_truth(condition_pool), '-', label='Ground Truth')\nplt.plot(initial_conditions, initial_observations, 'o', label='Initial Data')\nplt.plot(condition_pool, predicted_observations_lr, '-k', label='Prediction from Linear Regression')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sampled Experimental Conditions')\nplt.legend()\n</pre> # plot model predictions against ground-truth y_min = np.min(initial_observations) y_max  = np.max(initial_observations)  # plot conditions obtained by novelty sampler for idx, condition in enumerate(new_conditions_novelty):     if idx == 0:         plt.plot([condition, condition], [y_min, y_max], '--r', label='novelty conditions')     else: # we want to omit the label for all other conditions         plt.plot([condition, condition], [y_min, y_max], '--r')  # plot conditions obtained by falsification sampler for idx, condition in enumerate(new_conditions_falsification):     if idx == 0:         plt.plot([condition[0], condition[0]], [y_min, y_max], '--g', label='falsification conditions')     else: # we want to omit the label for all other conditions         plt.plot([condition[0], condition[0]], [y_min, y_max], '--g')  plt.plot(condition_pool, ground_truth(condition_pool), '-', label='Ground Truth') plt.plot(initial_conditions, initial_observations, 'o', label='Initial Data') plt.plot(condition_pool, predicted_observations_lr, '-k', label='Prediction from Linear Regression') plt.xlabel('x') plt.ylabel('y') plt.title('Sampled Experimental Conditions') plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x1f2daefbe80&gt;</pre>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#basic-tutorial-i-components","title":"Basic Tutorial I: Components\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20I%20Components/#introduction","title":"Introduction\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20I%20Components/#installation","title":"Installation\u00b6","text":"<p>The AutoRA ecosystem is a comprehensive collection of packages that together establish a framework for closed-loop empirical research. At the core of this framework is the <code>autora</code> package, which serves as the parent package and is essential for end users to install.  It provides functionalities for automating workflows in empirical research and includes vetted modules with minimal dependencies.</p> <p>However, the flexibility of autora extends further with the inclusion of optional modules as additional dependencies. Users have the freedom to selectively install these modules based on their specific needs and preferences.</p> <p>Optional dependencies enable users to customize their autora environment without worrying about conflicts with other packages within the broader autora ecosystem. To install an optional module, simply use the command <code>pip install autora[dependency-name]</code>, where <code>dependency-name</code> corresponds to the name of the desired module (see example below).</p> <p>To begin, we will install all the relevant optional dependencies. Our main focus will be on two experimentalists: <code>experimentalist-falsification</code> and <code>experimentalist-novelty</code>, along with a Bayesian Machine Scientist (BMS) implemented in the <code>theorist-bms</code> package. It's important to note that installing a module will automatically include the main <code>autora</code> package, as well as any required dependencies for workflow management and running synthetic experiments.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#automated-empirical-research-components","title":"Automated Empirical Research Components\u00b6","text":"<p>The goal of this section is to set up all <code>autora</code> components to enable a closed-loop discovery workflow with synthetic data. This involves specifying (1) the experiment runner, (2) a theorist for model discovery, (3) an experimentalist for identifying novel experiment conditions.</p> <ul> <li>Experiment Runner: A component that takes in conditions and collects corresponding observations.</li> <li>Theorist: A component that takes in the full collection of conditions and observations and outputs models that link the two.</li> <li>Experimentalist: A component that outputs new conditions, which are intended to yield novel observations.</li> </ul> <p>Each of these components automates a process of the scientific method that is generally conducted manually. The experiment runner parallels a research assistant that collects data from participants. The theorist takes the place of a computational scientist that applies modelling techniques to discover how to best describe the data. The experimentalist acts as a research design expert to determine the next iteration of experimentation. Each of these steps in the scientific method can be arduous and time consuming to conduct manually, and so <code>autora</code> allows for the automation of these steps and thus quickens the scientific method by leveraging data-driven techniques.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#toy-example-of-the-components","title":"Toy Example of the Components\u00b6","text":"<p>Before jumping into each component in detail, we will present a toy example to provide you with an overview on how these components work together within a closed-loop. After some setup, you will see steps 1-3, which uses the three components - namely, the <code>experimentalist</code> to propose new conditions, the <code>experiment runner</code> to retrieve new observations from those conditions, and the <code>theorist</code> to model the new data. We then finish this example by plotting our data and findings.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#experiment-runners","title":"Experiment Runners\u00b6","text":"<p><code>autora</code> provides support for experiment runners, which serve as interfaces for conducting both real-world and synthetic experiments. An experiment runner typically accepts experiment conditions as input (e.g., a 2-dimensional numpy array with columns representing different independent variables) and produces collected observations as output (e.g., a 2-dimensional numpy array with columns representing different dependent variables). These experiment runners can be combined with other <code>autora</code> components to facilitate closed-loop scientific discovery.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#types","title":"Types\u00b6","text":"<p><code>autora</code> offers two types of experiment runners: real-world experiments and synthetic experiments.</p> <p>For real-world experiments, experiment runners can include interfaces for various scenarios such as web-based experiments for behavioral data collection (e.g., using Firebase and Prolific) or experiments involving electrical circuits (e.g., using Tinkerforge). These runners often require external components such as databases to store collected observations or servers to host the experiments. You may refer to the respective tutorials for these interfaces on how to set up all required components.</p> <p>Synthetic experiments are conducted on synthetic experiment runners, which are functions that take experimental conditions as input and generate simulated observations as output. These experiments serve multiple purposes, including testing autora components before applying them to real-world experiments, benchmarking methods for automated scientific discovery, or conducting computational metascientific experiments.</p> <p>In this introductory tutorial, we primarily focus on simple synthetic experiments. For more complex synthetic experiments implementing various scientific models, you can utilize the autora-synthetic module.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#usage","title":"Usage\u00b6","text":"<p>To create a synthetic experiment runner, we begin with defining a ground truth from which to generate data. Here, we consider a simple sine function:</p> <p>$y = f(x) = \\sin(x)$</p> <p>In this case, $x$ corresponds to an independent variable (the variable we can manipulate in an experiment), $y$ corresponds to a dependent variable (the variable we can observe after conducting the experiment), and $f(x)$ is the ground-truth function (or \"mechanism\") that we seek to uncover via a combination of experimentation and model discovery.</p> <p>However, we assume that observations are obtained with a measurement error when running the experiment.</p> <p>$\\hat{y} = \\hat{f}(x) = f(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,0.1)$</p> <p>where $\\epsilon$ is the measurement error sampled from a normal distribution with $0$ mean and a standard deviation of $0.1$.</p> <p>The following code block defines the ground truth $f(x)$ and the experiment runner $\\hat{f}(x)$ as <code>lambda</code> functions.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#theorists","title":"Theorists\u00b6","text":"<p>The <code>autora</code> framework includes and interfaces with different methods for scientific model discovery. These methods are referred to as theorists and are implemented as sklearn estimators. For general information about theorists, see the respective AutoRA Documentation.</p> <p>Theorists take as input a set of conditions and observations. Conditions and observations can typically be passed as two-dimensional numpy arrays (with columns corresponding to variables and rows corresponding to different instances of those variables). Theorists then identify and fit a model which may be used to predict observations based on experiment conditions.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#types","title":"Types\u00b6","text":"<p>There are different types of theorists within the AutoRA framework, each with its own approach to scientific model discovery.</p> <p>Some theorists focus on fitting the parameters of a pre-specified model to the given data (see the scikit learn documentation for a selection of basic regressors). The model architecture in such cases is typically fixed, while the parameters are adjusted to optimize the model's performance. Linear regression is an example of a parameter-fitting theorist.</p> <p>Other theorists are concerned with identifying both the architecture of a model and its parameters. The model architectures can take various forms, such as equations, causal models, or process models. Implemented as scikit-learn estimators, these theorists aim to discover a model architecture that accurately describes the data. They often operate within a user-defined search space, which specifies the allowable operations or components that can be included in the model. This approach provides more flexibility in exploring different model architectures.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#usage","title":"Usage\u00b6","text":"<p>In this tutorial, we delve into two types of theorists: (1) a linear regression theorist, which focuses on fitting a linear model, and (2) a Bayesian Machine Scientist (Guimer\u00e0 et al., 2020, in Science Advances), which specializes in identifying and fitting a non-linear equation.</p> <p>Theorists are commonly instantiated as regressors within the <code>sklearn</code> library:</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#experimentalists","title":"Experimentalists\u00b6","text":"<p>The primary goal of an experimentalist is to design experiments that yield scientific merit. The <code>autora</code> framework offers various strategies for identifying informative new data points (e.g., by searching for experiment conditions that existing scientific models fail to explain, or by looking for novel conditions altogether).</p> <p>Experimentalists are implemented as functions that return a set of experiment conditions (e.g., in the form of a 2-dimensional numpy array in which columns correspond to independent variables), which can be subjected to an experiment. To determine these conditions, experimentalists may use information about candidate models obtained from a theorist, experimental conditions that have already been probed, or respective dependent measures. For more detailed information about experimentalists, please refer to the corresponding AutoRA Documentation.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#types","title":"Types\u00b6","text":"<p>There are generally two types of experimentalist functions: poolers and samplers.</p> <p>Poolers generate a novel set of experimental conditions \"from scratch\", e.g., by sampling from a grid. They usually require metadata describing independent variables of the experiment (e.g., their range or the set of allowed values).</p> <p>Samplers operate on an existing pool of experimental conditions. They typically require experimental conditions to be represented as a 2-dimensional numpy array in which columns correspond to independent variables and rows to different conditions. They then select experiment conditions from this pool.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#usage-poolers","title":"Usage: Poolers\u00b6","text":"<p>Experimentalist poolers are implemented as functions and can be called directly. For instance, the following grid pooler generates a grid based on the <code>allowed_values</code> of all independent variables in the <code>metadata</code> object that we defined above. We can simply add a list of allowed values to each independent variable. In this case, we only have one variable.</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#usage-samplers","title":"Usage: Samplers\u00b6","text":"<p>An experiment sampler typically requires an existing pool of conditions as input along with additional arguments. For instance, the novelty sampler requires, aside from a pool of conditions, a list of prior conditions. The user may also specify the number of samples <code>num_samples</code> to select from the pool.</p> <p>The novelty sampler will then select novel experiment conditions from the pool which are most dissimilar to some reference conditions, such as the <code>initial_conditions</code> obtained above:</p>"},{"location":"tutorials/basic/Tutorial%20I%20Components/#next-notebook","title":"Next Notebook\u00b6","text":"<p>After defining all the components required for the empirical research process, we can create an automated workflow using basic loop constructs. The next notebook, AutoRA Basic Tutorial II: Loop Constructs, illustrates the use of these loop constructs.</p>"},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/","title":"2 - Loop Constructs","text":"<p>AutoRA (Automated Research Assistant) is an open-source framework designed to automate various stages of empirical research, including model discovery, experimental design, and data collection.</p> <p>This notebook is the second of four notebooks within the basic tutorials of <code>autora</code>. We suggest that you go through these notebooks in order as each builds upon the last. However, each notebook is self-contained and so there is no need to run the content of the last notebook for your current notebook.</p> <p>These notebooks provide a comprehensive introduction to the capabilities of <code>autora</code>. It demonstrates the fundamental components of <code>autora</code>, and how they can be combined to facilitate automated (closed-loop) empirical research through synthetic experiments.</p> <p>How to use this notebook You can progress through the notebook section by section or directly navigate to specific sections. If you choose the latter, it is recommended to execute all cells in the notebook initially, allowing you to easily rerun the cells in each section later without issues.</p> In\u00a0[\u00a0]: Copied! <pre>#### Installation ####\n!pip install -q \"autora[experimentalist-falsification]\"\n!pip install -q \"autora[experimentalist-model-disagreement]\"\n!pip install -q \"autora[theorist-bms]\"\n\n#### Import modules ####\nimport numpy as np\nimport torch\nfrom autora.variable import Variable, ValueType, VariableCollection\nfrom autora.experimentalist.random import random_pool\nfrom autora.experimentalist.falsification import falsification_sample\nfrom autora.experimentalist.model_disagreement import model_disagreement_sample\nfrom autora.theorist.bms import BMSRegressor\nfrom sklearn import linear_model\n\n#### Set seeds ####\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n#### Define ground truth and experiment runner ####\nground_truth = lambda x: np.sin(x)\nrun_experiment = lambda x: ground_truth(x) + np.random.normal(0, 0.1, size=x.shape)\n\n#### Define condition pool ####\ncondition_pool = np.linspace(0, 2 * np.pi, 100)\n\n#### Define variables ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=condition_pool)\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\n#### Define theorists ####\ntheorist_lr = linear_model.LinearRegression()\ntheorist_bms = BMSRegressor(epochs=100)\n</pre> #### Installation #### !pip install -q \"autora[experimentalist-falsification]\" !pip install -q \"autora[experimentalist-model-disagreement]\" !pip install -q \"autora[theorist-bms]\"  #### Import modules #### import numpy as np import torch from autora.variable import Variable, ValueType, VariableCollection from autora.experimentalist.random import random_pool from autora.experimentalist.falsification import falsification_sample from autora.experimentalist.model_disagreement import model_disagreement_sample from autora.theorist.bms import BMSRegressor from sklearn import linear_model  #### Set seeds #### np.random.seed(42) torch.manual_seed(42)  #### Define ground truth and experiment runner #### ground_truth = lambda x: np.sin(x) run_experiment = lambda x: ground_truth(x) + np.random.normal(0, 0.1, size=x.shape)  #### Define condition pool #### condition_pool = np.linspace(0, 2 * np.pi, 100)  #### Define variables #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=condition_pool) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  #### Define theorists #### theorist_lr = linear_model.LinearRegression() theorist_bms = BMSRegressor(epochs=100) <pre>WARNING: typer 0.12.3 does not provide the extra 'all'\nWARNING: typer 0.12.3 does not provide the extra 'all'\nWARNING: typer 0.12.3 does not provide the extra 'all'\n</pre> In\u00a0[\u00a0]: Copied! <pre>num_cycles = 5 # number of empirical research cycles\nmeasurements_per_cycle = 3 # number of data points to collect for each cycle\n\n# generate an initial set of experimental conditions\nconditions = random_pool(variables=variables,\n                         num_samples=measurements_per_cycle)\n\n# convert iterator into 2-dimensional numpy array\nconditions = np.array(list(conditions.values)).reshape(-1, 1)\n\n# collect initial set of observations\nobservations = run_experiment(conditions)\n\nfor cycle in range(num_cycles):\n\n  # use BMS theorist to fit the model to the data\n  theorist_bms.fit(conditions, observations)\n\n  # obtain new conditions\n  new_conditions = falsification_sample(\n          conditions=condition_pool,\n          model=theorist_bms,\n          reference_conditions=conditions,\n          reference_observations=observations,\n          metadata=variables,\n          num_samples=measurements_per_cycle,\n      )\n\n  # obtain new observations\n  print(new_conditions)\n  new_observations = run_experiment(new_conditions)\n\n  # combine old and new conditions and observations\n  conditions = np.concatenate((conditions, new_conditions))\n  observations = np.concatenate((observations, new_observations))\n\n  # evaluate model of the theorist based on its ability to predict each observation from the ground truth, evaluated across the entire space of experimental conditions\n  loss = np.mean(np.square(theorist_bms.predict(condition_pool.reshape(-1,1)) - ground_truth(condition_pool)))\n  print(\"Loss in cycle {}: {}\".format(cycle, loss))\n  print(\"Discovered Model: \" +  theorist_bms.repr())\n</pre> num_cycles = 5 # number of empirical research cycles measurements_per_cycle = 3 # number of data points to collect for each cycle  # generate an initial set of experimental conditions conditions = random_pool(variables=variables,                          num_samples=measurements_per_cycle)  # convert iterator into 2-dimensional numpy array conditions = np.array(list(conditions.values)).reshape(-1, 1)  # collect initial set of observations observations = run_experiment(conditions)  for cycle in range(num_cycles):    # use BMS theorist to fit the model to the data   theorist_bms.fit(conditions, observations)    # obtain new conditions   new_conditions = falsification_sample(           conditions=condition_pool,           model=theorist_bms,           reference_conditions=conditions,           reference_observations=observations,           metadata=variables,           num_samples=measurements_per_cycle,       )    # obtain new observations   print(new_conditions)   new_observations = run_experiment(new_conditions)    # combine old and new conditions and observations   conditions = np.concatenate((conditions, new_conditions))   observations = np.concatenate((observations, new_observations))    # evaluate model of the theorist based on its ability to predict each observation from the ground truth, evaluated across the entire space of experimental conditions   loss = np.mean(np.square(theorist_bms.predict(condition_pool.reshape(-1,1)) - ground_truth(condition_pool)))   print(\"Loss in cycle {}: {}\".format(cycle, loss))   print(\"Discovered Model: \" +  theorist_bms.repr())  <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.43it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>[[0.        ]\n [0.06346652]\n [0.12693304]]\nLoss in cycle 0: 0.99\nDiscovered Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 22.62it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>[[0.        ]\n [0.44426563]\n [0.38079911]]\nLoss in cycle 1: 0.99\nDiscovered Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 20.91it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>[[0.        ]\n [0.57119866]\n [0.63466518]]\nLoss in cycle 2: 0.99\nDiscovered Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 20.60it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>[[0.        ]\n [6.28318531]\n [6.21971879]]\nLoss in cycle 3: 0.99\nDiscovered Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 22.85it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>[[6.28318531]\n [6.21971879]\n [6.15625227]]\nLoss in cycle 4: 0.99\nDiscovered Model: sin(X0)\n</pre> In\u00a0[\u00a0]: Copied! <pre>num_cycles = 5 # number of empirical research cycles\nmeasurements_per_cycle = 3 # number of data points to collect for each cycle\n\n# generate an initial set of experimental conditions\nconditions = random_pool(variables=variables,\n                         num_samples=measurements_per_cycle)\n\n# convert iterator into 2-dimensional numpy array\nconditions = np.array(list(conditions.values)).reshape(-1, 1)\n\n# collect initial set of observations\nobservations = run_experiment(conditions)\n\nfor cycle in range(num_cycles):\n\n  # use BMS theorist to fit the model to the data\n  theorist_bms.fit(conditions, observations)\n  theorist_lr.fit(conditions, observations)\n\n  # obtain new conditions\n  new_conditions = model_disagreement_sample(\n          condition_pool,\n          models = [theorist_bms, theorist_lr],\n          num_samples = measurements_per_cycle\n      )\n\n  # obtain new observations\n  print(new_conditions)\n  new_observations = run_experiment(new_conditions)\n\n  # combine old and new conditions and observations\n  conditions = np.concatenate((conditions, new_conditions))\n  observations = np.concatenate((observations, new_observations))\n\n  # evaluate model of the theorist based on its ability to predict each observation from the ground truth, evaluated across the entire space of experimental conditions\n  loss = np.mean(np.square(theorist_bms.predict(condition_pool.reshape(-1,1)) - ground_truth(condition_pool)))\n  print(\"Loss in cycle {}: {}\".format(cycle, loss))\n  print(\"Discovered BMS Model: \" +  theorist_bms.repr())\n</pre> num_cycles = 5 # number of empirical research cycles measurements_per_cycle = 3 # number of data points to collect for each cycle  # generate an initial set of experimental conditions conditions = random_pool(variables=variables,                          num_samples=measurements_per_cycle)  # convert iterator into 2-dimensional numpy array conditions = np.array(list(conditions.values)).reshape(-1, 1)  # collect initial set of observations observations = run_experiment(conditions)  for cycle in range(num_cycles):    # use BMS theorist to fit the model to the data   theorist_bms.fit(conditions, observations)   theorist_lr.fit(conditions, observations)    # obtain new conditions   new_conditions = model_disagreement_sample(           condition_pool,           models = [theorist_bms, theorist_lr],           num_samples = measurements_per_cycle       )    # obtain new observations   print(new_conditions)   new_observations = run_experiment(new_conditions)    # combine old and new conditions and observations   conditions = np.concatenate((conditions, new_conditions))   observations = np.concatenate((observations, new_observations))    # evaluate model of the theorist based on its ability to predict each observation from the ground truth, evaluated across the entire space of experimental conditions   loss = np.mean(np.square(theorist_bms.predict(condition_pool.reshape(-1,1)) - ground_truth(condition_pool)))   print(\"Loss in cycle {}: {}\".format(cycle, loss))   print(\"Discovered BMS Model: \" +  theorist_bms.repr())  <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.66it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>          0\n0  0.000000\n1  0.063467\n2  0.126933\nLoss in cycle 0: 0.5027103676355225\nDiscovered BMS Model: -0.09\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.51it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>           0\n26  1.650129\n25  1.586663\n27  1.713596\nLoss in cycle 1: 0.99\nDiscovered BMS Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.92it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>           0\n73  4.633056\n74  4.696522\n72  4.569589\nLoss in cycle 2: 0.99\nDiscovered BMS Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.94it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>           0\n28  1.777063\n27  1.713596\n29  1.840529\nLoss in cycle 3: 0.99\nDiscovered BMS Model: sin(X0)\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 24.37it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>           0\n71  4.506123\n72  4.569589\n70  4.442656\nLoss in cycle 4: 0.99\nDiscovered BMS Model: sin(X0)\n</pre>"},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#basic-tutorial-ii-loop-constructs","title":"Basic Tutorial II: Loop Constructs\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#introduction","title":"Introduction\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#tutorial-setup","title":"Tutorial Setup\u00b6","text":"<p>This tutorial is self-contained so that you do not need to run the previous notebook to begin. However, the four notebooks are continuous so that what we define in a previous notebook should still exist within this notebook. As such, we will here re-run relevant code from past tutorials. We will not again walk you through these, but if you need a reminder what they are then go see the descriptions in previous notebooks.</p>"},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#loop-constructs","title":"Loop Constructs\u00b6","text":"<p>After defining all the components required for the empirical research process, we can create an automated workflow using basic loop constructs in Python.</p> <p>The following code block demonstrates how to build such a workflow using the components introduced in the preceding notebook, such as</p> <ul> <li><code>variables</code> (object specifying variables of the experiment) </li> <li><code>run_experiment</code> (function for collecting data) </li> <li><code>theorist_bms</code> (scikit learn estimator for discovering equations using the Bayesian Machine Scientist) </li> <li><code>random_pool</code> (function for generating a random pool of experimental conditions) </li> <li><code>falsification_sample</code> (function for identifying novel experiment conditions using the falsification sampler) </li> </ul> <p>We begin with implementing the following workflow:</p> <ol> <li>Generate 3 seed experimental conditions using <code>random_pool</code></li> <li>Generate 3 seed observations using <code>run_experiment</code></li> <li>Loop through the following steps 5 times<ul> <li>Identify a model relating conditions to observations using <code>theorist_bms</code></li> <li>Identify 3 new experimental conditions using <code>falsification_sample</code></li> <li>Collect 3 new observations using <code>run_experiment</code></li> <li>Add new conditions and observations to the dataset</li> </ul> </li> </ol> <p>We will here begin using the naming convention <code>cycle</code> to refer to an entire AutoRA loop where the loop encounters all AutoRA components - experiment runner, theorist, experimentalist. Within the scientific method, a cycle would then be running a single iteration of the experiment. This requires the collection of data, the modelling of that data, and the conceptualization of the next iteration of this experiment. For example, if our research concerns how much information a person acquires from a photo (dependent variable) dependent on how bright the photo is (independent variable), we may first collect data with conditions of (let's say) 10%, 50%, and 90% brightness, then model our collected data to determine the relationship between brightness and photo perception, and finally determine which other brightness conditions may help us understand the true relationship. Probing other conditions - such as a brightness of 25% and of 75% would then be the next iteration of the experiment and thus, for us, the next cycle. The following code block will iterate through five of these cycles.</p>"},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#example-1-falsification-sampler","title":"Example 1: Falsification Sampler\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#example-2-model-disagreement-sampler","title":"Example 2: Model Disagreement Sampler\u00b6","text":"<p>We can easily replace components in the workflow above.</p> <p>In the following code block, we add a linear regression theorist, to fit a linear model to the data. In addition, we replace <code>falsification_sample</code> with  <code>model_disagreement_sample</code> to sample experimental conditions that differentiate most between the linear model and the model discovered by the BMS theorist.</p>"},{"location":"tutorials/basic/Tutorial%20II%20Loop%20Constructs/#next-notebook","title":"Next Notebook\u00b6","text":"<p>While the basic loop construct is flexible, there are more convenient ways to specify a research cycle in <code>autora</code>. The next notebook, AutoRA Basic Tutorial III: Functional Workflow, illustrates the use of these constructs.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/","title":"3 - Functional Workflow","text":"<p>AutoRA (Automated Research Assistant) is an open-source framework designed to automate various stages of empirical research, including model discovery, experimental design, and data collection.</p> <p>This notebook is the third of four notebooks within the basic tutorials of <code>autora</code>. We suggest that you go through these notebooks in order as each builds upon the last. However, each notebook is self-contained and so there is no need to run the content of the last notebook for your current notebook.</p> <p>These notebooks provide a comprehensive introduction to the capabilities of <code>autora</code>. It demonstrates the fundamental components of <code>autora</code>, and how they can be combined to facilitate automated (closed-loop) empirical research through synthetic experiments.</p> <p>How to use this notebook You can progress through the notebook section by section or directly navigate to specific sections. If you choose the latter, it is recommended to execute all cells in the notebook initially, allowing you to easily rerun the cells in each section later without issues.</p> In\u00a0[\u00a0]: Copied! <pre>#### Installation ####\n!pip install -q \"autora[theorist-bms]\"\n!pip install -q \"autora[experiment-runner-synthetic-abstract-equation]\"\n\n#### Import modules ####\nfrom typing import Optional\nimport numpy as np\nimport pandas as pd\nimport sympy as sp\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom autora.state import StandardState\n\n#### Set seeds ####\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n#### Define plot function ####\ndef plot_from_state(s: StandardState, expr: str):    \n    \n    \"\"\"\n    Plots the data, the ground truth model, and the current predicted model\n    \"\"\"\n    \n    #Determine labels and variables\n    model_label = f\"Model: {s.models[-1]}\" if hasattr(s.models[-1],'repr') else \"Model\"\n    experiment_data = s.experiment_data.sort_values(by=[\"x\"])\n    ground_x = np.linspace(s.variables.independent_variables[0].value_range[0],s.variables.independent_variables[0].value_range[1],100)\n    \n    #Determine predicted ground truth\n    equation = sp.simplify(expr)\n    ground_predicted_y = [equation.evalf(subs={'x':x}) for x in ground_x]\n    model_predicted_y = s.models[-1].predict(ground_x.reshape(-1, 1))\n\n    #Plot the data and models\n    f = plt.figure(figsize=(4,3))\n    plt.plot(experiment_data[\"x\"], experiment_data[\"y\"], 'o', label = None)\n    plt.plot(ground_x, model_predicted_y, alpha=.8, label=model_label)\n    plt.plot(ground_x, ground_predicted_y, alpha=.8,  label=f'Ground Truth: {expr}')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n</pre> #### Installation #### !pip install -q \"autora[theorist-bms]\" !pip install -q \"autora[experiment-runner-synthetic-abstract-equation]\"  #### Import modules #### from typing import Optional import numpy as np import pandas as pd import sympy as sp import torch import matplotlib.pyplot as plt  from autora.state import StandardState  #### Set seeds #### np.random.seed(42) torch.manual_seed(42)  #### Define plot function #### def plot_from_state(s: StandardState, expr: str):              \"\"\"     Plots the data, the ground truth model, and the current predicted model     \"\"\"          #Determine labels and variables     model_label = f\"Model: {s.models[-1]}\" if hasattr(s.models[-1],'repr') else \"Model\"     experiment_data = s.experiment_data.sort_values(by=[\"x\"])     ground_x = np.linspace(s.variables.independent_variables[0].value_range[0],s.variables.independent_variables[0].value_range[1],100)          #Determine predicted ground truth     equation = sp.simplify(expr)     ground_predicted_y = [equation.evalf(subs={'x':x}) for x in ground_x]     model_predicted_y = s.models[-1].predict(ground_x.reshape(-1, 1))      #Plot the data and models     f = plt.figure(figsize=(4,3))     plt.plot(experiment_data[\"x\"], experiment_data[\"y\"], 'o', label = None)     plt.plot(ground_x, model_predicted_y, alpha=.8, label=model_label)     plt.plot(ground_x, ground_predicted_y, alpha=.8,  label=f'Ground Truth: {expr}')     plt.xlabel('x')     plt.ylabel('y')     plt.legend()     plt.show() <pre>WARNING: typer 0.12.3 does not provide the extra 'all'\nWARNING: typer 0.12.3 does not provide the extra 'all'\n</pre> In\u00a0[\u00a0]: Copied! <pre>from autora.variable import Variable, ValueType, VariableCollection\nfrom autora.experimentalist.random import random_pool\nfrom autora.state import StandardState\n\n#### Define variable data ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\n#### Define seed condition data ####\nconditions = random_pool(variables, num_samples=10, random_state=0)\n\n#### Initialize State ####\ns = StandardState(\n    variables = variables,\n    conditions = conditions,\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"])\n)\n</pre> from autora.variable import Variable, ValueType, VariableCollection from autora.experimentalist.random import random_pool from autora.state import StandardState  #### Define variable data #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  #### Define seed condition data #### conditions = random_pool(variables, num_samples=10, random_state=0)  #### Initialize State #### s = StandardState(     variables = variables,     conditions = conditions,     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]) ) In\u00a0[\u00a0]: Copied! <pre>print(s)\n</pre> print(s) <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])\n</pre> <p>We can view all of the content we provided the state more directly if we choose.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"\\033[1mThe variables we provided:\\033[0m\")\nprint(s.variables)\n\nprint(\"\\n\\033[1mThe conditions we provided:\\033[0m\")\nprint(s.conditions)\n\nprint(\"\\n\\033[1mThe dataframe we provided:\\033[0m\")\nprint(s.experiment_data)\n</pre> print(\"\\033[1mThe variables we provided:\\033[0m\") print(s.variables)  print(\"\\n\\033[1mThe conditions we provided:\\033[0m\") print(s.conditions)  print(\"\\n\\033[1mThe dataframe we provided:\\033[0m\") print(s.experiment_data) <pre>The variables we provided:\nVariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[])\n\nThe conditions we provided:\n          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877\n\nThe dataframe we provided:\nEmpty DataFrame\nColumns: [x, y]\nIndex: []\n</pre> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\nfrom autora.state import on_state\n\nexperimentalist = on_state(random_pool, output=[\"conditions\"])\n</pre> from autora.experimentalist.random import random_pool from autora.state import on_state  experimentalist = on_state(random_pool, output=[\"conditions\"]) In\u00a0[\u00a0]: Copied! <pre>import sympy as sp\nfrom autora.experiment_runner.synthetic.abstract.equation import equation_experiment\nfrom autora.state import on_state\n\n#### Define variable data ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\n#### Equation Experiment Method ####\nsin_experiment = equation_experiment(sp.simplify('sin(x)'), variables.independent_variables, variables.dependent_variables[0])\nsin_runner = sin_experiment.run\n\nexperiment_runner = on_state(sin_runner, output=[\"experiment_data\"])\n</pre> import sympy as sp from autora.experiment_runner.synthetic.abstract.equation import equation_experiment from autora.state import on_state  #### Define variable data #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  #### Equation Experiment Method #### sin_experiment = equation_experiment(sp.simplify('sin(x)'), variables.independent_variables, variables.dependent_variables[0]) sin_runner = sin_experiment.run  experiment_runner = on_state(sin_runner, output=[\"experiment_data\"]) In\u00a0[\u00a0]: Copied! <pre>from autora.theorist.bms import BMSRegressor\nfrom autora.state import estimator_on_state\n\ntheorist = estimator_on_state(BMSRegressor(epochs=100))\n</pre> from autora.theorist.bms import BMSRegressor from autora.state import estimator_on_state  theorist = estimator_on_state(BMSRegressor(epochs=100)) In\u00a0[\u00a0]: Copied! <pre>print('\\033[1mPrevious Conditions:\\033[0m')\nprint(s.conditions)\n\ns = experimentalist(s, num_samples=10, random_state=42)\n\nprint('\\n\\033[1mUpdated Conditions:\\033[0m')\nprint(s.conditions)\n</pre> print('\\033[1mPrevious Conditions:\\033[0m') print(s.conditions)  s = experimentalist(s, num_samples=10, random_state=42)  print('\\n\\033[1mUpdated Conditions:\\033[0m') print(s.conditions) <pre>Previous Conditions:\n          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877\n\nUpdated Conditions:\n          x\n0  0.433323\n1  4.983216\n2  4.116570\n3  2.816600\n4  2.599939\n5  5.416539\n6  0.433323\n7  4.333231\n8  1.299969\n9  0.433323\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\\033[1mPrevious Data:\\033[0m\")\nprint(s.experiment_data)\n\ns = experiment_runner(s, added_noise=1.0, random_state=42)\n\nprint(\"\\n\\033[1mUpdated Data:\\033[0m\")\nprint(s.experiment_data)\n</pre> print(\"\\033[1mPrevious Data:\\033[0m\") print(s.experiment_data)  s = experiment_runner(s, added_noise=1.0, random_state=42)  print(\"\\n\\033[1mUpdated Data:\\033[0m\") print(s.experiment_data) <pre>Previous Data:\nEmpty DataFrame\nColumns: [x, y]\nIndex: []\n\nUpdated Data:\n          x         y\n0  0.433323  0.724606\n1  4.983216 -2.003534\n2  4.116570 -0.077238\n3  2.816600  1.259866\n4  2.599939 -1.435481\n5  5.416539 -2.064342\n6  0.433323  0.547730\n7  4.333231 -1.245219\n8  1.299969  0.946749\n9  0.433323 -0.433155\n</pre> In\u00a0[\u00a0]: Copied! <pre>s = theorist(s)\n\nprint(\"\\n\\033[1mUpdated Model:\\033[0m\")\nprint(s.models[-1])\n\nplot_from_state(s,'sin(x)')\n</pre> s = theorist(s)  print(\"\\n\\033[1mUpdated Model:\\033[0m\") print(s.models[-1])  plot_from_state(s,'sin(x)') <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 23.00it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nUpdated Model:\n-0.38\n</pre> In\u00a0[\u00a0]: Copied! <pre>s = experimentalist(s, num_samples=10, random_state=42)\ns = experiment_runner(s, added_noise=1.0, random_state=42)\ns = theorist(s)\n\nprint(s)\nplot_from_state(s,'sin(x)')\n</pre> s = experimentalist(s, num_samples=10, random_state=42) s = experiment_runner(s, added_noise=1.0, random_state=42) s = theorist(s)  print(s) plot_from_state(s,'sin(x)') <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 20.52it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>StandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  0.433323\n1  4.983216\n2  4.116570\n3  2.816600\n4  2.599939\n5  5.416539\n6  0.433323\n7  4.333231\n8  1.299969\n9  0.433323, experiment_data=           x         y\n0   0.433323  0.724606\n1   4.983216 -2.003534\n2   4.116570 -0.077238\n3   2.816600  1.259866\n4   2.599939 -1.435481\n5   5.416539 -2.064342\n6   0.433323  0.547730\n7   4.333231 -1.245219\n8   1.299969  0.946749\n9   0.433323 -0.433155\n10  0.433323  0.724606\n11  4.983216 -2.003534\n12  4.116570 -0.077238\n13  2.816600  1.259866\n14  2.599939 -1.435481\n15  5.416539 -2.064342\n16  0.433323  0.547730\n17  4.333231 -1.245219\n18  1.299969  0.946749\n19  0.433323 -0.433155, models=[-0.38, -0.38])\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=42)\n\ns = StandardState(\n    variables = variables,\n    conditions = conditions,\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"])\n)\n\n### Then we cycle through the pipeline we built three more times ###\nnum_cycles = 3 # number of empirical research cycles\nfor cycle in range(num_cycles):\n    #Run pipeline\n    s = experimentalist(s, num_samples=10, random_state=42+cycle)\n    s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)\n    s = theorist(s)\n    \n    #Report metrics\n    print(f\"\\n\\033[1mRunning Cycle {cycle+1}:\\033[0m\")\n    print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")\n    plot_from_state(s,'sin(x)')\n</pre> #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=42)  s = StandardState(     variables = variables,     conditions = conditions,     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]) )  ### Then we cycle through the pipeline we built three more times ### num_cycles = 3 # number of empirical research cycles for cycle in range(num_cycles):     #Run pipeline     s = experimentalist(s, num_samples=10, random_state=42+cycle)     s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)     s = theorist(s)          #Report metrics     print(f\"\\n\\033[1mRunning Cycle {cycle+1}:\\033[0m\")     print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")     plot_from_state(s,'sin(x)') <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 22.28it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 1:\nCycle 1 model: -0.38\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05&lt;00:00, 18.47it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 2:\nCycle 2 model: -0.27\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.55it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 3:\nCycle 3 model: sin(x)\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=42)\n\ns = StandardState(\n    variables = variables,\n    conditions = conditions,\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"])\n)\n\n### Then we cycle through the pipeline we built until we reach our stopping criterion ###\ncycle = 0\nwhile len(s.experiment_data) &lt; 40: #Run until we have at least 40 datapoints\n    #Run pipeline\n    s = experimentalist(s, num_samples=10, random_state=42+cycle)\n    s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)\n    s = theorist(s)\n    \n    #Report metrics\n    print(f\"\\n\\033[1mRunning Cycle {cycle+1}, number of datapoints: {len(s.experiment_data)}\\033[0m\")\n    print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")\n    plot_from_state(s,'sin(x)')\n    \n    #Increase count\n    cycle += 1\n\nprint(f\"\\n\\033[1mNumber of datapoints: {len(s.experiment_data)}\\033[0m\")\nprint(f\"\\033[1mDetermined Model: {s.models[-1]}\\033[0m\")\n</pre> #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=42)  s = StandardState(     variables = variables,     conditions = conditions,     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]) )  ### Then we cycle through the pipeline we built until we reach our stopping criterion ### cycle = 0 while len(s.experiment_data) &lt; 40: #Run until we have at least 40 datapoints     #Run pipeline     s = experimentalist(s, num_samples=10, random_state=42+cycle)     s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)     s = theorist(s)          #Report metrics     print(f\"\\n\\033[1mRunning Cycle {cycle+1}, number of datapoints: {len(s.experiment_data)}\\033[0m\")     print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")     plot_from_state(s,'sin(x)')          #Increase count     cycle += 1  print(f\"\\n\\033[1mNumber of datapoints: {len(s.experiment_data)}\\033[0m\") print(f\"\\033[1mDetermined Model: {s.models[-1]}\\033[0m\") <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 23.04it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 1, number of datapoints: 10\nCycle 1 model: -0.38\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05&lt;00:00, 19.61it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 2, number of datapoints: 20\nCycle 2 model: -0.27\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 22.35it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 3, number of datapoints: 30\nCycle 3 model: -0.07\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.30it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 4, number of datapoints: 40\nCycle 4 model: sin(x)\n</pre> <pre>\nNumber of datapoints: 40\nDetermined Model: sin(x)\n</pre> <p>For example, we can choose a different experimentalist depending on the number of datapoints we have collected.</p> In\u00a0[\u00a0]: Copied! <pre>#### We will first define a new experimentalist\ndef uniform_sample(variables: VariableCollection, conditions: pd.DataFrame, num_samples: int = 1, random_state: Optional[int] = None):\n\n    \"\"\"\n    An experimentalist that selects the least represented datapoints\n    \"\"\"\n\n    #Set random state\n    rng = np.random.default_rng(random_state)\n    \n    #Retrieve the possible values\n    allowed_values = variables.independent_variables[0].allowed_values\n    \n    #Determine the representation of each value\n    conditions_count = np.array([conditions[\"x\"].isin([value]).sum(axis=0) for value in allowed_values])\n    \n    #Sort to determine the least represented values\n    conditions_sort = conditions_count.argsort()\n    \n    conditions_count = conditions_count[conditions_sort]\n    values_count = allowed_values[conditions_sort]\n    \n    #Sample from values with the smallest frequency\n    x = values_count[conditions_count&lt;=conditions_count[num_samples-1]]\n    x = rng.choice(x,num_samples)\n    \n    return pd.DataFrame({\"x\": x})\n</pre> #### We will first define a new experimentalist def uniform_sample(variables: VariableCollection, conditions: pd.DataFrame, num_samples: int = 1, random_state: Optional[int] = None):      \"\"\"     An experimentalist that selects the least represented datapoints     \"\"\"      #Set random state     rng = np.random.default_rng(random_state)          #Retrieve the possible values     allowed_values = variables.independent_variables[0].allowed_values          #Determine the representation of each value     conditions_count = np.array([conditions[\"x\"].isin([value]).sum(axis=0) for value in allowed_values])          #Sort to determine the least represented values     conditions_sort = conditions_count.argsort()          conditions_count = conditions_count[conditions_sort]     values_count = allowed_values[conditions_sort]          #Sample from values with the smallest frequency     x = values_count[conditions_count&lt;=conditions_count[num_samples-1]]     x = rng.choice(x,num_samples)          return pd.DataFrame({\"x\": x}) In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\n\n#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=42)\n\ns = StandardState(\n    variables = variables,\n    conditions = conditions,\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"])\n)\n\n#### Initiate both experimentalists ####\nuniform_experimentalist = on_state(uniform_sample, output=[\"conditions\"])\nrandom_experimentalist = on_state(random_pool, output=['conditions'])\n\n### Then we cycle through the pipeline we built until we reach our stopping criteria ###\ncycle = 0\nwhile len(s.experiment_data) &lt; 40:\n    \n    #Run pipeline\n    if len(s.experiment_data) &lt; 20: #Conditional experimentalist: random for first half of cycles\n        print('\\n#==================================================#')\n        print('\\033[1mUsing random pooler experimentalist...\\033[0m')\n        s = random_experimentalist(s, num_samples=10, random_state=42+cycle)\n    else: #Conditional experimentalist: uniform for last half of cycles\n        print('\\n#==================================================#')\n        print('\\033[1mUsing uniform sampler experimentalist...\\033[0m')\n        s = uniform_experimentalist(s, num_samples=10, random_state=42+cycle)\n        \n    s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)\n    s = theorist(s)\n    \n    #Report metrics\n    print(f\"\\n\\033[1mRunning Cycle {cycle+1}:\\033[0m\")\n    print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")\n    plot_from_state(s,'sin(x)')\n    \n    #Increase count\n    cycle += 1\n</pre> from autora.experimentalist.random import random_pool  #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=42)  s = StandardState(     variables = variables,     conditions = conditions,     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]) )  #### Initiate both experimentalists #### uniform_experimentalist = on_state(uniform_sample, output=[\"conditions\"]) random_experimentalist = on_state(random_pool, output=['conditions'])  ### Then we cycle through the pipeline we built until we reach our stopping criteria ### cycle = 0 while len(s.experiment_data) &lt; 40:          #Run pipeline     if len(s.experiment_data) &lt; 20: #Conditional experimentalist: random for first half of cycles         print('\\n#==================================================#')         print('\\033[1mUsing random pooler experimentalist...\\033[0m')         s = random_experimentalist(s, num_samples=10, random_state=42+cycle)     else: #Conditional experimentalist: uniform for last half of cycles         print('\\n#==================================================#')         print('\\033[1mUsing uniform sampler experimentalist...\\033[0m')         s = uniform_experimentalist(s, num_samples=10, random_state=42+cycle)              s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)     s = theorist(s)          #Report metrics     print(f\"\\n\\033[1mRunning Cycle {cycle+1}:\\033[0m\")     print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")     plot_from_state(s,'sin(x)')          #Increase count     cycle += 1 <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\n#==================================================#\nUsing random pooler experimentalist...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 24.25it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 1:\nCycle 1 model: -0.38\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\n#==================================================#\nUsing random pooler experimentalist...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 24.66it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 2:\nCycle 2 model: -0.27\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\n#==================================================#\nUsing uniform sampler experimentalist...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 24.70it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 3:\nCycle 3 model: sin(x)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\n#==================================================#\nUsing uniform sampler experimentalist...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 23.21it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>\nRunning Cycle 4:\nCycle 4 model: sin(x)\n</pre> <p>In addition, we can dynamically change parameters across cycles.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import random_pool\n\n#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=42)\n\ns = StandardState(\n    variables = variables,\n    conditions = conditions,\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"])\n)\n\n#### Initiate both experimentalists ####\nrandom_experimentalist = on_state(random_pool, output=['conditions'])\n\n### Then we cycle through the pipeline we built until we reach our stopping criteria ###\nfor cycle, num_samples in enumerate([5, 10, 20, 50, 100]):\n    \n    #Report metrics\n    print(f\"\\n\\033[1mRunning Cycle {cycle+1} with {num_samples} new samples:\\033[0m\")\n\n    #Run pipeline\n    s = random_experimentalist(s, num_samples=num_samples, random_state=42+cycle)\n    s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)\n    s = theorist(s)\n    \n    #Report metrics\n    print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")\n    plot_from_state(s,'sin(x)')\n</pre> from autora.experimentalist.random import random_pool  #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=42)  s = StandardState(     variables = variables,     conditions = conditions,     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]) )  #### Initiate both experimentalists #### random_experimentalist = on_state(random_pool, output=['conditions'])  ### Then we cycle through the pipeline we built until we reach our stopping criteria ### for cycle, num_samples in enumerate([5, 10, 20, 50, 100]):          #Report metrics     print(f\"\\n\\033[1mRunning Cycle {cycle+1} with {num_samples} new samples:\\033[0m\")      #Run pipeline     s = random_experimentalist(s, num_samples=num_samples, random_state=42+cycle)     s = experiment_runner(s, added_noise=1.0, random_state=42+cycle)     s = theorist(s)          #Report metrics     print(f\"\\033[1mCycle {cycle+1} model: {s.models[-1]}\\033[0m\")     plot_from_state(s,'sin(x)') <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\nRunning Cycle 1 with 5 new samples:\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03&lt;00:00, 25.28it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>Cycle 1 model: -0.31\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\nRunning Cycle 2 with 10 new samples:\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03&lt;00:00, 26.00it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>Cycle 2 model: -0.21\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\nRunning Cycle 3 with 20 new samples:\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 24.62it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>Cycle 3 model: sin(x)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\nRunning Cycle 4 with 50 new samples:\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.80it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>Cycle 4 model: (x * -0.13)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>\nRunning Cycle 5 with 100 new samples:\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 21.16it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>Cycle 5 model: sin(x)\n</pre>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#basic-tutorial-iii-functional-workflow","title":"Basic Tutorial III: Functional Workflow\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#introduction","title":"Introduction\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#tutorial-setup","title":"Tutorial Setup\u00b6","text":"<p>We will here import some standard python packages, set seeds for replicability, and define a plotting function.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#states","title":"States\u00b6","text":"<p>Using the functions and objects in <code>autora.state</code>, we can build flexible pipelines and cycles which operate on state objects. State objects represent data from an experiment, like the conditions, observed experiment data and models. The State also includes rules on how to update those data if new data are provided using the \"Delta mechanism\". This state can be acted upon by experimentalists, experiment runners, and theorists.</p> <p>In tutorial I, we had experimentalists define new conditions, experiment runners collect new observations, and theorists model the data. To do this, we used the output of one as the input of the other, such as:</p> <p><code>conditions = experimentalist(...)</code> $\\rightarrow$  <code>observations = experiment_runner(conditions,...)</code> $\\rightarrow$  <code>model = theorist(conditions, observations)</code> </p> <p>This chaining is embedded within the <code>State</code> functionality. To act on a state, we must wrap each of our experimentalist(s), experiment_runner(s), and theorist(s) so that they:</p> <ul> <li>operate on the <code>State</code>, and</li> <li>return a modified object of the same type <code>State</code>.</li> </ul>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#defining-the-state","title":"Defining The State\u00b6","text":"<p>We use the <code>StandardState</code> object. Let's begin by populating the state with variable information (<code>variables</code>), seed condition data (<code>conditions</code>), and a dataframe (<code>pd.DataFrame(columns=[\"x\",\"y\"])</code>) that will hold our conditions (<code>x</code>) and observations (<code>y</code>).</p> <p>Note: Some <code>autora</code> components have a <code>random_state</code> parameter that sets the seed for random number generators. Using this parameter ensures reproducibility of your code, but is optional.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#viewing-the-state","title":"Viewing the State\u00b6","text":"<p>Now, let's view the contents of the state we just initialized.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#autora-components-and-the-state","title":"AutoRA Components and the State\u00b6","text":"<p>Now that we have initialized the state, we need to start preparing components of <code>autora</code> to work with the state - namely, experimentalists, experiment runners, and theorists.</p> <p>These components are defined in the same way as past tutorials. All we need to do so that these can function within the state is to wrap them in specialized state functions. Note that as the theorists are using the <code>scikit-learn</code> interface, this will need to be wrapped differently than the experiment runners and experimentalists. The wrappers are:</p> <ul> <li><code>on_state()</code> for experiment runners and experimentalists</li> <li><code>estimator_on_state()</code> for theorists (specifically, scikit-learn estimators)</li> </ul> <p>The first argument for each wrapper should be your corresponding function (i.e., the experiment runner, the experimentalist, and the theorist). The <code>on_state</code> wrapper takes a second argument, <code>output</code>, to determine where in the state the component is acting on. For the experimentalist this will be <code>output=[\"conditions\"]</code>, and for the experiment runner this will be <code>output=[\"experiment_data\"]</code>.</p> <p>Once the components are wrapped, their functionality changes to act on the state, meaning that they now expect a state as the first input and will return a modified version of that state.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#wrapping-components-to-work-with-state","title":"Wrapping Components to Work with State\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#experimentalist-defined-and-wrapped-with-state","title":"Experimentalist Defined and Wrapped with State\u00b6","text":"<p>We will use autora's <code>random_pool</code> pooler for our experimentalist. We import this and then wrap it so that it functions with the state.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#experiment-runner-defined-and-wrapped-with-state","title":"Experiment Runner Defined and Wrapped with State\u00b6","text":"<p>We define a sine experiment runner and then wrap it so that it functions with the state.</p> <p>To create our experiment runner, we will use an <code>AutoRA</code> function called <code>equation_experiment()</code>. This function takes in an equation wrapped as a <code>sympy</code> object using <code>sp.simplify()</code> and then allows us to solve for any input (<code>x</code>) given. Further, we constrain the values that this function can output by passing it the <code>variable</code> information.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#theorist-defined-and-wrapped-with-state","title":"Theorist Defined and Wrapped with State\u00b6","text":"<p>We will use autora's <code>BMSRegressor</code> theorist. We import this and then wrap it so that if functions with the state.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#running-each-component-with-the-state","title":"Running Each Component with the State\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#run-the-experimentalist","title":"Run the Experimentalist\u00b6","text":"<p>Let's run the experimentalist with the state and see how the state changes.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#run-the-experiment-runner","title":"Run the Experiment Runner\u00b6","text":"<p>Let's run the experiment runner and see how the state changes.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#run-the-theorist","title":"Run the Theorist\u00b6","text":"<p>Let's run the theorist and see how the state changes. Note that theorists should return a single model directly or multiple models within a list to work with the state.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#component-chaining","title":"Component Chaining\u00b6","text":"<p>As such, we have our <code>autora</code> components wrapped to work with the state. Remember, this means that they take the state as an input and returns the updated state as an output. As the components all act on the state, they can easily be chained.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#the-cycle","title":"The Cycle\u00b6","text":"<p>Moreover, we can use these chained components within a loop to run multiple cycles.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#cycle-using-number-of-cycles","title":"Cycle using Number of Cycles\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#cycle-using-stopping-criteria","title":"Cycle using Stopping Criteria\u00b6","text":"<p>Alternatively, we can run the chain until we reach a stopping criterion. For example, here we will loop until we get 40 datapoints.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#the-conditional-cycle","title":"The Conditional Cycle\u00b6","text":"<p>Because <code>autora</code> components (theorist, experiment runner, experimentalist) act on the state, building a pipeline can have a lot of flexibility. Above, we demonstrated using a single set of components in different loops, but the components can also change respective to your criteria. In other words, you can use <code>if-else</code> statements to control which component is acting on the state.</p>"},{"location":"tutorials/basic/Tutorial%20III%20Functional%20Workflow/#next-notebook","title":"Next Notebook\u00b6","text":"<p>This concludes the tutorial on <code>autora</code> functionality. However, <code>autora</code> is a flexible framework in which users can integrate their own theorists, experimentalists, and experiment_runners in an automated empirical research workflow. The next notebook, AutoRA Basic Tutorial IV: Customization, illustrates how to add your own custom theorists and experimentalists to use with <code>autora</code>.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/","title":"4 - Customization","text":"<p>AutoRA (Automated Research Assistant) is an open-source framework designed to automate various stages of empirical research, including model discovery, experimental design, and data collection.</p> <p>This notebook is the fourth of four notebooks within the basic tutorials of <code>autora</code>. We suggest that you go through these notebooks in order as each builds upon the last. However, each notebook is self-contained and so there is no need to run the content of the last notebook for your current notebook.</p> <p>These notebooks provide a comprehensive introduction to the capabilities of <code>autora</code>. It demonstrates the fundamental components of <code>autora</code>, and how they can be combined to facilitate automated (closed-loop) empirical research through synthetic experiments.</p> <p>How to use this notebook You can progress through the notebook section by section or directly navigate to specific sections. If you choose the latter, it is recommended to execute all cells in the notebook initially, allowing you to easily rerun the cells in each section later without issues.</p> In\u00a0[\u00a0]: Copied! <pre>#### Installation ####\n!pip install -q \"autora[theorist-bms]\"\n!pip install -q \"autora[experiment-runner-synthetic-abstract-equation]\"\n\n#### Import modules ####\nfrom typing import Optional\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sympy as sp\nimport torch\n\nfrom autora.variable import Variable, ValueType, VariableCollection\nfrom autora.state import StandardState, on_state, estimator_on_state\nfrom autora.experimentalist.random import random_pool\nfrom autora.theorist.bms import BMSRegressor\nfrom autora.experiment_runner.synthetic.abstract.equation import equation_experiment\n\n#### Set seeds ####\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n#### Define plot function ####\ndef plot_from_state(s: StandardState, expr: str):    \n    \n    \"\"\"\n    Plots the data, the ground truth model, and the current predicted model\n    \"\"\"\n    \n    #Determine labels and variables\n    print(s.models[-1])\n    model_label = f\"Model: {s.models[-1]}\" if hasattr(s.models[-1],'repr') else \"Model\"\n    experiment_data = s.experiment_data.sort_values(by=[\"x\"])\n    ground_x = np.linspace(s.variables.independent_variables[0].value_range[0],s.variables.independent_variables[0].value_range[1],100)\n    \n    #Determine predicted ground truth\n    equation = sp.simplify(expr)\n    ground_predicted_y = [equation.evalf(subs={'x':x}) for x in ground_x]\n    model_predicted_y = s.models[-1].predict(ground_x.reshape(-1, 1))\n\n    #Plot the data and models\n    f = plt.figure(figsize=(4,3))\n    plt.plot(experiment_data[\"x\"], experiment_data[\"y\"], 'o', label = None)\n    plt.plot(ground_x, model_predicted_y, alpha=.8, label=model_label)\n    plt.plot(ground_x, ground_predicted_y, alpha=.8,  label=f'Ground Truth: {expr}')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n</pre> #### Installation #### !pip install -q \"autora[theorist-bms]\" !pip install -q \"autora[experiment-runner-synthetic-abstract-equation]\"  #### Import modules #### from typing import Optional import numpy as np import pandas as pd import matplotlib.pyplot as plt import sympy as sp import torch  from autora.variable import Variable, ValueType, VariableCollection from autora.state import StandardState, on_state, estimator_on_state from autora.experimentalist.random import random_pool from autora.theorist.bms import BMSRegressor from autora.experiment_runner.synthetic.abstract.equation import equation_experiment  #### Set seeds #### np.random.seed(42) torch.manual_seed(42)  #### Define plot function #### def plot_from_state(s: StandardState, expr: str):              \"\"\"     Plots the data, the ground truth model, and the current predicted model     \"\"\"          #Determine labels and variables     print(s.models[-1])     model_label = f\"Model: {s.models[-1]}\" if hasattr(s.models[-1],'repr') else \"Model\"     experiment_data = s.experiment_data.sort_values(by=[\"x\"])     ground_x = np.linspace(s.variables.independent_variables[0].value_range[0],s.variables.independent_variables[0].value_range[1],100)          #Determine predicted ground truth     equation = sp.simplify(expr)     ground_predicted_y = [equation.evalf(subs={'x':x}) for x in ground_x]     model_predicted_y = s.models[-1].predict(ground_x.reshape(-1, 1))      #Plot the data and models     f = plt.figure(figsize=(4,3))     plt.plot(experiment_data[\"x\"], experiment_data[\"y\"], 'o', label = None)     plt.plot(ground_x, model_predicted_y, alpha=.8, label=model_label)     plt.plot(ground_x, ground_predicted_y, alpha=.8,  label=f'Ground Truth: {expr}')     plt.xlabel('x')     plt.ylabel('y')     plt.legend()     plt.show() <pre>WARNING: typer 0.12.3 does not provide the extra 'all'\nWARNING: typer 0.12.3 does not provide the extra 'all'\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### Define metadata ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\n#### Define condition pool ####\nconditions = random_pool(variables, num_samples=10, random_state=0)\n\n#### Define state ####\ns = StandardState(\n    variables = variables,\n    conditions = conditions,\n    experiment_data = pd.DataFrame(columns=[\"x\",\"y\"])\n)\n\n#### Define experimentalist and wrap with state functionality ####\nexperimentalist = on_state(random_pool, output=[\"conditions\"])\n\n#### Define experiment runner and wrap with state functionality ####\nsin_experiment = equation_experiment(sp.simplify('sin(x)'), variables.independent_variables, variables.dependent_variables[0])\nsin_runner = sin_experiment.run\n\nexperiment_runner = on_state(sin_runner, output=[\"experiment_data\"])\n\n#### Define theorist and wrap with state functionality ####\ntheorist = estimator_on_state(BMSRegressor(epochs=100))\n</pre> #### Define metadata #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  #### Define condition pool #### conditions = random_pool(variables, num_samples=10, random_state=0)  #### Define state #### s = StandardState(     variables = variables,     conditions = conditions,     experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]) )  #### Define experimentalist and wrap with state functionality #### experimentalist = on_state(random_pool, output=[\"conditions\"])  #### Define experiment runner and wrap with state functionality #### sin_experiment = equation_experiment(sp.simplify('sin(x)'), variables.independent_variables, variables.dependent_variables[0]) sin_runner = sin_experiment.run  experiment_runner = on_state(sin_runner, output=[\"experiment_data\"])  #### Define theorist and wrap with state functionality #### theorist = estimator_on_state(BMSRegressor(epochs=100)) <p>We should quickly test to make sure everything works as expected.</p> In\u00a0[\u00a0]: Copied! <pre>print('\\033[1mPrevious State:\\033[0m')\nprint(s)\n\nfor cycle in range(2):\n    s = experimentalist(s, num_samples=10, random_state=42+cycle)\n    s = experiment_runner(s, added_noise=0.5, random_state=42+cycle)\n    s = theorist(s)\n    \n    plot_from_state(s, 'sin(x)')\n\nprint('\\n\\033[1mUpdated State:\\033[0m')\nprint(s)\n</pre> print('\\033[1mPrevious State:\\033[0m') print(s)  for cycle in range(2):     s = experimentalist(s, num_samples=10, random_state=42+cycle)     s = experiment_runner(s, added_noise=0.5, random_state=42+cycle)     s = theorist(s)          plot_from_state(s, 'sin(x)')  print('\\n\\033[1mUpdated State:\\033[0m') print(s) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>Previous State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 20.17it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>sin(x)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05&lt;00:00, 19.45it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>sin(x)\n</pre> <pre>\nUpdated State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  3.249923\n1  4.116570\n2  2.599939\n3  0.216662\n4  3.683247\n5  0.000000\n6  1.733292\n7  5.416539\n8  2.816600\n9  3.683247, experiment_data=           x         y\n0   0.433323  0.572248\n1   4.983216 -1.483542\n2   4.116570 -0.452463\n3   2.816600  0.789584\n4   2.599939 -0.459964\n5   5.416539 -1.413252\n6   0.433323  0.483809\n7   4.333231 -1.087098\n8   1.299969  0.955149\n9   0.433323 -0.006633\n10  3.249923  0.013996\n11  4.116570 -0.488600\n12  2.599939  0.222789\n13  0.216662 -0.239366\n14  3.683247 -1.511473\n15  0.000000  0.485811\n16  1.733292  0.995155\n17  5.416539 -0.659296\n18  2.816600 -0.072496\n19  3.683247  0.097695, models=[sin(x), sin(x)])\n</pre> In\u00a0[\u00a0]: Copied! <pre>#==================================================================#\n#                 Option 1 - Wrapping our Component                #\n#==================================================================#\n\ndef uniform_sample(variables: VariableCollection, conditions: pd.DataFrame, num_samples: int = 1, random_state: Optional [int] = None):\n\n    \"\"\"\n    An experimentalist that selects the least represented datapoints\n    \"\"\"\n    #Set rng seed\n    rng = np.random.default_rng(random_state)\n\n    #Retrieve the possible values\n    allowed_values = variables.independent_variables[0].allowed_values\n    \n    #Determine the representation of each value\n    conditions_count = np.array([conditions[\"x\"].isin([value]).sum(axis=0) for value in allowed_values])\n    \n    #Sort to determine the least represented values\n    conditions_sort = conditions_count.argsort()\n    \n    conditions_count = conditions_count[conditions_sort]\n    values_count = allowed_values[conditions_sort]\n    \n    #Sample from values with the smallest frequency\n    x = values_count[conditions_count&lt;=conditions_count[num_samples-1]]\n    x = rng.choice(x,num_samples)\n    \n    return pd.DataFrame({\"x\": x})\n\ncustom_experimentalist = on_state(uniform_sample, output=[\"conditions\"])\n\n#==================================================================#\n#                   Option 2 - Using a Decorator                   #\n#==================================================================#\n\n@on_state(output=[\"conditions\"])\ndef custom_experimentalist(variables: VariableCollection, conditions: pd.DataFrame, num_samples: int = 1, random_state: Optional [int] = None):\n\n    \"\"\"\n    An experimentalist that selects the least represented datapoints\n    \"\"\"\n    #Set rng seed\n    rng = np.random.default_rng(random_state)\n\n    #Retrieve the possible values\n    allowed_values = variables.independent_variables[0].allowed_values\n    \n    #Determine the representation of each value\n    conditions_count = np.array([conditions[\"x\"].isin([value]).sum(axis=0) for value in allowed_values])\n    \n    #Sort to determine the least represented values\n    conditions_sort = conditions_count.argsort()\n    \n    conditions_count = conditions_count[conditions_sort]\n    values_count = allowed_values[conditions_sort]\n    \n    #Sample from values with the smallest frequency\n    x = values_count[conditions_count&lt;=conditions_count[num_samples-1]]\n    x = rng.choice(x,num_samples)\n    \n    return pd.DataFrame({\"x\": x})\n</pre> #==================================================================# #                 Option 1 - Wrapping our Component                # #==================================================================#  def uniform_sample(variables: VariableCollection, conditions: pd.DataFrame, num_samples: int = 1, random_state: Optional [int] = None):      \"\"\"     An experimentalist that selects the least represented datapoints     \"\"\"     #Set rng seed     rng = np.random.default_rng(random_state)      #Retrieve the possible values     allowed_values = variables.independent_variables[0].allowed_values          #Determine the representation of each value     conditions_count = np.array([conditions[\"x\"].isin([value]).sum(axis=0) for value in allowed_values])          #Sort to determine the least represented values     conditions_sort = conditions_count.argsort()          conditions_count = conditions_count[conditions_sort]     values_count = allowed_values[conditions_sort]          #Sample from values with the smallest frequency     x = values_count[conditions_count&lt;=conditions_count[num_samples-1]]     x = rng.choice(x,num_samples)          return pd.DataFrame({\"x\": x})  custom_experimentalist = on_state(uniform_sample, output=[\"conditions\"])  #==================================================================# #                   Option 2 - Using a Decorator                   # #==================================================================#  @on_state(output=[\"conditions\"]) def custom_experimentalist(variables: VariableCollection, conditions: pd.DataFrame, num_samples: int = 1, random_state: Optional [int] = None):      \"\"\"     An experimentalist that selects the least represented datapoints     \"\"\"     #Set rng seed     rng = np.random.default_rng(random_state)      #Retrieve the possible values     allowed_values = variables.independent_variables[0].allowed_values          #Determine the representation of each value     conditions_count = np.array([conditions[\"x\"].isin([value]).sum(axis=0) for value in allowed_values])          #Sort to determine the least represented values     conditions_sort = conditions_count.argsort()          conditions_count = conditions_count[conditions_sort]     values_count = allowed_values[conditions_sort]          #Sample from values with the smallest frequency     x = values_count[conditions_count&lt;=conditions_count[num_samples-1]]     x = rng.choice(x,num_samples)          return pd.DataFrame({\"x\": x}) <p>Now, we will re-run our initial workflow while incorporating our custom experimentalist.</p> In\u00a0[\u00a0]: Copied! <pre>#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=0)\n\ns = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))\n\n#Report previous state\nprint('\\033[1mPrevious State:\\033[0m')\nprint(s)\n\n#Cycle\nfor cycle in range(5):\n    s = custom_experimentalist(s, num_samples = 10, random_state=42+cycle) #Our custom experimentalist\n    s = experiment_runner(s, added_noise=0.5, random_state=42+cycle)\n    s = theorist(s)\n    \n    plot_from_state(s,'sin(x)')\n\n#Report updated state\nprint('\\n\\033[1mUpdated State:\\033[0m')\nprint(s)\n</pre> #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=0)  s = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))  #Report previous state print('\\033[1mPrevious State:\\033[0m') print(s)  #Cycle for cycle in range(5):     s = custom_experimentalist(s, num_samples = 10, random_state=42+cycle) #Our custom experimentalist     s = experiment_runner(s, added_noise=0.5, random_state=42+cycle)     s = theorist(s)          plot_from_state(s,'sin(x)')  #Report updated state print('\\n\\033[1mUpdated State:\\033[0m') print(s) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>Previous State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 23.27it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>-0.21\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05&lt;00:00, 19.52it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>cos((-1.49 + x))\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 20.38it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>sin(x)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05&lt;00:00, 18.74it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>sin(x)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00, 20.01it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>sin(x)\n</pre> <pre>\nUpdated State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  4.983216\n1  5.849862\n2  3.466585\n3  4.116570\n4  1.733292\n5  3.249923\n6  3.249923\n7  5.199877\n8  3.683247\n9  4.333231, experiment_data=           x         y\n0   5.849862 -0.267531\n1   1.516631  0.478541\n2   2.383277  1.062925\n3   3.683247 -0.045271\n4   3.683247 -1.491071\n5   2.599939 -0.135536\n6   5.849862 -0.355969\n7   2.383277  0.529578\n8   4.766554 -1.006934\n9   5.849862 -0.846411\n10  2.816600  0.441416\n11  1.949954  1.268066\n12  3.466585 -0.612066\n13  5.633201 -1.059511\n14  3.033262 -0.887800\n15  0.000000  0.485811\n16  4.333231 -0.920648\n17  0.649985  0.708040\n18  6.066524 -0.606768\n19  2.166616  1.440938\n20  1.299969  1.686531\n21  3.683247 -0.464401\n22  5.849862 -0.256513\n23  4.983216 -0.395319\n24  1.299969  1.375672\n25  2.383277  0.977178\n26  3.899908 -0.877285\n27  4.549893 -1.496263\n28  5.416539 -0.574078\n29  4.766554 -1.254513\n30  1.949954  0.700044\n31  0.216662 -0.096459\n32  0.866646  0.829807\n33  0.649985  1.027126\n34  0.649985  0.530925\n35  6.283185  0.131242\n36  2.166616  1.093523\n37  1.516631  1.343437\n38  0.649985  0.159225\n39  3.033262  0.342529\n40  4.983216 -1.215008\n41  5.849862  0.189056\n42  3.466585 -0.454861\n43  4.116570 -0.462597\n44  1.733292  0.402174\n45  3.249923 -0.822852\n46  3.249923 -0.119755\n47  5.199877 -1.107408\n48  3.683247 -0.471516\n49  4.333231 -0.666329, models=[sin(x), sin(x), sin(x), sin(x), sin(x)])\n</pre> In\u00a0[\u00a0]: Copied! <pre>#==================================================================#\n#                 Option 1 - Wrapping our Component                #\n#==================================================================#\n\ndef quadratic_experiment(conditions: pd.DataFrame, added_noise: int = 0.01, random_state: Optional[int] = None):\n    \n    #Set rng seed\n    rng = np.random.default_rng(random_state)\n    \n    #Extract conditions\n    x = conditions[\"x\"]\n    \n    #Compute data\n    y = (x + x**2) + rng.normal(0, added_noise, size=x.shape)\n    \n    #Assign to dataframe\n    observations = conditions.assign(y = y)\n    \n    return observations\n\ncustom_experiment_runner = on_state(quadratic_experiment, output=[\"experiment_data\"])\n\n#==================================================================#\n#                   Option 2 - Using a Decorator                   #\n#==================================================================#\n\n@on_state(output=[\"experiment_data\"])\ndef quadratic_experiment(conditions: pd.DataFrame, added_noise: int = 0.01, random_state: Optional[int] = None):\n    \n    #Set rng seed\n    rng = np.random.default_rng(random_state)\n    \n    #Extract conditions\n    x = conditions[\"x\"]\n    \n    #Compute data\n    y = (x + x**2) + rng.normal(0, added_noise, size=x.shape)\n    \n    #Assign to dataframe\n    observations = conditions.assign(y = y)\n    \n    return observations\n</pre> #==================================================================# #                 Option 1 - Wrapping our Component                # #==================================================================#  def quadratic_experiment(conditions: pd.DataFrame, added_noise: int = 0.01, random_state: Optional[int] = None):          #Set rng seed     rng = np.random.default_rng(random_state)          #Extract conditions     x = conditions[\"x\"]          #Compute data     y = (x + x**2) + rng.normal(0, added_noise, size=x.shape)          #Assign to dataframe     observations = conditions.assign(y = y)          return observations  custom_experiment_runner = on_state(quadratic_experiment, output=[\"experiment_data\"])  #==================================================================# #                   Option 2 - Using a Decorator                   # #==================================================================#  @on_state(output=[\"experiment_data\"]) def quadratic_experiment(conditions: pd.DataFrame, added_noise: int = 0.01, random_state: Optional[int] = None):          #Set rng seed     rng = np.random.default_rng(random_state)          #Extract conditions     x = conditions[\"x\"]          #Compute data     y = (x + x**2) + rng.normal(0, added_noise, size=x.shape)          #Assign to dataframe     observations = conditions.assign(y = y)          return observations <p>Now, we will re-run our initial workflow while incorporating our custom experiment runner.</p> In\u00a0[\u00a0]: Copied! <pre>#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=0)\n\ns = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))\n\n#Report previous state\nprint('\\033[1mPrevious State:\\033[0m')\nprint(s)\n\n#Cycle\nfor cycle in range(5):\n    s = experimentalist(s, num_samples = 10, random_state=42+cycle)\n    s = custom_experiment_runner(s, added_noise=0.5, random_state=42+cycle)\n    s = theorist(s)\n    \n    plot_from_state(s, 'x + x**2')\n\n#Report updated state\nprint('\\n\\033[1mUpdated State:\\033[0m')\nprint(s)\n</pre> #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=0)  s = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))  #Report previous state print('\\033[1mPrevious State:\\033[0m') print(s)  #Cycle for cycle in range(5):     s = experimentalist(s, num_samples = 10, random_state=42+cycle)     s = custom_experiment_runner(s, added_noise=0.5, random_state=42+cycle)     s = theorist(s)          plot_from_state(s, 'x + x**2')  #Report updated state print('\\n\\033[1mUpdated State:\\033[0m') print(s) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n</pre> <pre>Previous State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:06&lt;00:00, 15.44it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>(x + ((x * x) * 0.99))\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:06&lt;00:00, 14.67it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>((x + 0.45) ** 2)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:06&lt;00:00, 15.63it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>((x * (x + x)) ** 0.87)\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05&lt;00:00, 17.21it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>(x / (1.0 / (x + 1.0)))\n</pre> <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:06&lt;00:00, 15.98it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <pre>((x + 1.0) * x)\n</pre> <pre>\nUpdated State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  3.249923\n1  5.849862\n2  1.299969\n3  0.433323\n4  3.466585\n5  1.733292\n6  1.516631\n7  3.899908\n8  0.866646\n9  6.066524, experiment_data=           x          y\n0   0.433323   0.773451\n1   4.983216  29.295665\n2   4.116570  21.437941\n3   2.816600  11.220120\n4   2.599939   8.384103\n5   5.416539  34.104345\n6   0.433323   0.685012\n7   4.333231  22.952003\n8   1.299969   2.981489\n9   0.433323   0.194570\n10  3.249923  13.934041\n11  4.116570  21.401805\n12  2.599939   9.066856\n13  0.216662  -0.190733\n14  3.683247  16.253633\n15  0.000000   0.485811\n16  1.733292   4.745924\n17  5.416539  34.858300\n18  2.816600  10.358040\n19  3.683247  17.862801\n20  4.333231  23.833106\n21  0.649985   1.123617\n22  5.199877  32.401980\n23  1.516631   4.385032\n24  4.116570  21.474838\n25  2.599939   9.649098\n26  0.433323   0.431507\n27  6.283185  45.252166\n28  3.466585  15.671881\n29  0.866646   1.361742\n30  5.849862  39.841817\n31  3.683247  16.938122\n32  4.549893  25.319062\n33  3.249923  14.233877\n34  3.249923  13.737676\n35  4.766554  27.617837\n36  4.549893  25.517251\n37  5.199877  32.583507\n38  3.466585  15.037847\n39  3.249923  14.046336\n40  3.249923  13.560468\n41  5.849862  40.679695\n42  1.299969   2.854330\n43  0.433323   0.986184\n44  3.466585  14.899144\n45  1.733292   4.022862\n46  1.516631   3.805164\n47  3.899908  18.885295\n48  0.866646   1.661760\n49  6.066524  43.131882, models=[((x + 1.0) * x), ((x + 1.0) * x), ((x + 1.0) * x), ((x + 1.0) * x), ((x + 1.0) * x)])\n</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom sklearn.base import BaseEstimator\n\nclass PolynomialRegressor(BaseEstimator):\n\n    def __init__(self, degree: int = 3):\n        self.degree = degree\n\n    def fit(self, conditions: pd.DataFrame, observations: pd.DataFrame):\n        c = np.array(conditions)\n        o = np.array(observations)\n\n        # polyfit expects a 1D array\n        if c.ndim &gt; 1:\n            c = c.flatten()\n\n        if o.ndim &gt; 1:\n            o = o.flatten()\n\n        # fit polynomial\n        self.coeff = np.polyfit(c, o, self.degree)\n        self.polynomial = np.poly1d(self.coeff)\n        return self\n\n    def predict(self, conditions: pd.DataFrame):\n        c = np.array(conditions)\n        return self.polynomial(c)\n    \ncustom_theorist = estimator_on_state(PolynomialRegressor())\n</pre> import numpy as np from sklearn.base import BaseEstimator  class PolynomialRegressor(BaseEstimator):      def __init__(self, degree: int = 3):         self.degree = degree      def fit(self, conditions: pd.DataFrame, observations: pd.DataFrame):         c = np.array(conditions)         o = np.array(observations)          # polyfit expects a 1D array         if c.ndim &gt; 1:             c = c.flatten()          if o.ndim &gt; 1:             o = o.flatten()          # fit polynomial         self.coeff = np.polyfit(c, o, self.degree)         self.polynomial = np.poly1d(self.coeff)         return self      def predict(self, conditions: pd.DataFrame):         c = np.array(conditions)         return self.polynomial(c)      custom_theorist = estimator_on_state(PolynomialRegressor()) <p>Now, we will re-run our initial workflow while incorporating our custom theorist.</p> In\u00a0[\u00a0]: Copied! <pre>#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=0)\n\ns = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))\n\n#Report previous state\nprint('\\033[1mPrevious State:\\033[0m')\nprint(s)\n\n#Cycle\nfor cycle in range(5):\n    s = experimentalist(s, num_samples=10, random_state=42+cycle)\n    s = experiment_runner(s, added_noise=0.5, random_state=42+cycle)\n    s = custom_theorist(s)\n    \n    print(s.models[-1])\n    plot_from_state(s, 'sin(x)')\n\n#Report updated state\nprint('\\n\\033[1mUpdated State:\\033[0m')\nprint(s)\n</pre> #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=0)  s = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))  #Report previous state print('\\033[1mPrevious State:\\033[0m') print(s)  #Cycle for cycle in range(5):     s = experimentalist(s, num_samples=10, random_state=42+cycle)     s = experiment_runner(s, added_noise=0.5, random_state=42+cycle)     s = custom_theorist(s)          print(s.models[-1])     plot_from_state(s, 'sin(x)')  #Report updated state print('\\n\\033[1mUpdated State:\\033[0m') print(s) <pre>Previous State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])\nPolynomialRegressor()\nPolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\nPolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\nPolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\nPolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\nPolynomialRegressor()\n</pre> <pre>\nUpdated State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  3.249923\n1  5.849862\n2  1.299969\n3  0.433323\n4  3.466585\n5  1.733292\n6  1.516631\n7  3.899908\n8  0.866646\n9  6.066524, experiment_data=           x         y\n0   0.433323  0.572248\n1   4.983216 -1.483542\n2   4.116570 -0.452463\n3   2.816600  0.789584\n4   2.599939 -0.459964\n5   5.416539 -1.413252\n6   0.433323  0.483809\n7   4.333231 -1.087098\n8   1.299969  0.955149\n9   0.433323 -0.006633\n10  3.249923  0.013996\n11  4.116570 -0.488600\n12  2.599939  0.222789\n13  0.216662 -0.239366\n14  3.683247 -1.511473\n15  0.000000  0.485811\n16  1.733292  0.995155\n17  5.416539 -0.659296\n18  2.816600 -0.072496\n19  3.683247  0.097695\n20  4.333231 -0.205995\n21  0.649985  0.656327\n22  5.199877 -0.720136\n23  1.516631  1.566765\n24  4.116570 -0.415567\n25  2.599939  0.805032\n26  0.433323  0.230304\n27  6.283185 -0.509437\n28  3.466585 -0.131217\n29  0.866646  0.506182\n30  5.849862 -0.648822\n31  3.683247 -0.826983\n32  4.549893 -0.919182\n33  3.249923  0.313832\n34  3.249923 -0.182368\n35  4.766554 -0.867292\n36  4.549893 -0.720993\n37  5.199877 -0.538608\n38  3.466585 -0.765251\n39  3.249923  0.126291\n40  3.249923 -0.359577\n41  5.849862  0.189056\n42  1.299969  0.827990\n43  0.433323  0.784981\n44  3.466585 -0.903954\n45  1.733292  0.272093\n46  1.516631  0.986897\n47  3.899908 -0.911596\n48  0.866646  0.806200\n49  6.066524  0.047677, models=[PolynomialRegressor(), PolynomialRegressor(), PolynomialRegressor(), PolynomialRegressor(), PolynomialRegressor()])\n</pre> In\u00a0[\u00a0]: Copied! <pre>#### First, let's reinitialize the state object to get a clean state ####\niv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30))\ndv = Variable(name=\"y\", type=ValueType.REAL)\nvariables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])\n\nconditions = random_pool(variables, num_samples=10, random_state=0)\n\ns = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))\n\n#Report previous state\nprint('\\033[1mPrevious State:\\033[0m')\nprint(s)\n\n#Cycle\nfor cycle in range(5):\n    s = custom_experimentalist(s, num_samples=10, random_state=42+cycle)\n    s = custom_experiment_runner(s, added_noise=0.5, random_state=42+cycle)\n    s = custom_theorist(s)\n    \n    plot_from_state(s, 'x + x**2')\n\n#Report updated state\nprint('\\n\\033[1mUpdated State:\\033[0m')\nprint(s)\n</pre> #### First, let's reinitialize the state object to get a clean state #### iv = Variable(name=\"x\", value_range=(0, 2 * np.pi), allowed_values=np.linspace(0, 2 * np.pi, 30)) dv = Variable(name=\"y\", type=ValueType.REAL) variables = VariableCollection(independent_variables=[iv],dependent_variables=[dv])  conditions = random_pool(variables, num_samples=10, random_state=0)  s = StandardState(variables = variables, conditions = conditions, experiment_data = pd.DataFrame(columns=[\"x\",\"y\"]))  #Report previous state print('\\033[1mPrevious State:\\033[0m') print(s)  #Cycle for cycle in range(5):     s = custom_experimentalist(s, num_samples=10, random_state=42+cycle)     s = custom_experiment_runner(s, added_noise=0.5, random_state=42+cycle)     s = custom_theorist(s)          plot_from_state(s, 'x + x**2')  #Report updated state print('\\n\\033[1mUpdated State:\\033[0m') print(s) <pre>Previous State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  5.416539\n1  4.116570\n2  3.249923\n3  1.733292\n4  1.949954\n5  0.216662\n6  0.433323\n7  0.000000\n8  1.083308\n9  5.199877, experiment_data=Empty DataFrame\nColumns: [x, y]\nIndex: [], models=[])\nPolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\n</pre> <pre>PolynomialRegressor()\n</pre> <pre>\nUpdated State:\nStandardState(variables=VariableCollection(independent_variables=[Variable(name='x', value_range=(0, 6.283185307179586), allowed_values=array([0.        , 0.21666156, 0.43332312, 0.64998469, 0.86664625,\n       1.08330781, 1.29996937, 1.51663094, 1.7332925 , 1.94995406,\n       2.16661562, 2.38327719, 2.59993875, 2.81660031, 3.03326187,\n       3.24992343, 3.466585  , 3.68324656, 3.89990812, 4.11656968,\n       4.33323125, 4.54989281, 4.76655437, 4.98321593, 5.1998775 ,\n       5.41653906, 5.63320062, 5.84986218, 6.06652374, 6.28318531]), units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], dependent_variables=[Variable(name='y', value_range=None, allowed_values=None, units='', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='', rescale=1, is_covariate=False)], covariates=[]), conditions=          x\n0  4.983216\n1  5.849862\n2  3.466585\n3  4.116570\n4  1.733292\n5  3.249923\n6  3.249923\n7  5.199877\n8  3.683247\n9  4.333231, experiment_data=           x          y\n0   5.849862  40.223108\n1   1.516631   3.296808\n2   2.383277   8.438513\n3   3.683247  17.719834\n4   3.683247  16.274034\n5   2.599939   8.708530\n6   5.849862  40.134670\n7   2.383277   7.905166\n8   4.766554  27.478194\n9   5.849862  39.644228\n10  2.816600  10.871952\n11  1.949954   6.091364\n12  3.466585  15.191032\n13  5.633201  36.911813\n14  3.033262  11.238020\n15  0.000000   0.485811\n16  4.333231  23.118453\n17  0.649985   1.175330\n18  6.066524  42.477437\n19  2.166616   7.474088\n20  1.299969   3.712871\n21  3.683247  17.300704\n22  5.849862  40.234126\n23  4.983216  30.383888\n24  1.299969   3.402012\n25  2.383277   8.352766\n26  3.899908  18.919606\n27  4.549893  24.741981\n28  5.416539  34.943519\n29  4.766554  27.230615\n30  1.949954   5.523342\n31  0.216662  -0.047826\n32  0.866646   1.685367\n33  0.649985   1.494416\n34  0.649985   0.998215\n35  6.283185  45.892845\n36  2.166616   7.126673\n37  1.516631   4.161704\n38  0.649985   0.626515\n39  3.033262  12.468350\n40  4.983216  29.564199\n41  5.849862  40.679695\n42  3.466585  15.348237\n43  4.116570  21.427807\n44  1.733292   4.152943\n45  3.249923  13.097192\n46  3.249923  13.800289\n47  5.199877  32.014707\n48  3.683247  17.293589\n49  4.333231  23.372772, models=[PolynomialRegressor(), PolynomialRegressor(), PolynomialRegressor(), PolynomialRegressor(), PolynomialRegressor()])\n</pre> <p>Let's run the controller with the new theorist for 3 research cycles, defined by the number of models generated.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#tutorial-iv-customization","title":"Tutorial IV Customization\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#introduction","title":"Introduction\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#tutorial-setup","title":"Tutorial Setup\u00b6","text":"<p>We will here import some standard python packages, set seeds for replicability, and define a plotting function.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#customizing-automated-empirical-research-components","title":"Customizing Automated Empirical Research Components\u00b6","text":"<p><code>autora</code> is a flexible framework in which users can integrate their own experimentalists, experiment runners, and theorists in an automated empirical research workflow. This section illustrates the integration of custom <code>autora</code> components. For more information on how to contribute your own modules to the <code>autora</code> ecosystem, please refer to the Contributor Documentation.</p> <p>To illustrate the use of custom experimentalists, experiment runners, and theorists, we consider a simple workflow:</p> <ol> <li>Generate 10 seed experimental conditions using <code>random_pool</code></li> <li>Iterate through the following steps<ul> <li>Identify 3 new experimental conditions using an <code>experimentalist</code></li> <li>Collect observations using the <code>experiment_runner</code></li> <li>Identify a model relating conditions to observations using a <code>theorist</code></li> </ul> </li> </ol> <p>Once this workflow is setup, we will replace each component with a custom function.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#custom-experimentalists","title":"Custom Experimentalists\u00b6","text":"<p>Experimentalists must be implemented as functions. For instance, an experimentalist sampler function expects a pool of experimental conditions and returns a modified set of experimental conditions.</p> <p>Requirements for working with the state:</p> <ul> <li>The function has a <code>variables</code> argument that accepts the <code>VariableCollection</code> type</li> <li>The function has a <code>conditions</code> argument that accepts a <code>pandas.DataFrame</code></li> <li>The function returns a <code>pandas.DataFrame</code></li> </ul> <p>The custom <code>uniform_sampler</code> below will select conditions that are the least represented in the data.</p> <p>Note that when building custom experimentalists, we can either wrap the function with <code>on_state(output=['conditions'])</code> as we did in tutorial III, or else we can use the <code>@on_state(output=['conditions'])</code> decorator.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#custom-experiment-runner","title":"Custom Experiment Runner\u00b6","text":"<p>Experiment runners must be implemented as functions.</p> <p>Requirements for working with the state:</p> <ul> <li>The function has a <code>conditions</code> argument that accepts a <code>pandas.DataFrame</code></li> <li>The function returns a <code>pandas.DataFrame</code></li> </ul> <p>The custom <code>quadratic_experiment</code> below will apply a quadratic transform (<code>x + x**2</code>) to the conditions.</p> <p>Note that when building custom experiment runners, we can either wrap the function with <code>on_state(output=['experiment_data'])</code> as we did in tutorial III, or else we can use the <code>@on_state(output=['experiment_data'])</code> decorator.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#custom-theorists","title":"Custom Theorists\u00b6","text":"<p>Theorists must be implemented as classes that inherit from  <code>sklearn.base.BaseEstimator</code>. The class must implement the following methods:</p> <ul> <li><code>fit(self, conditions, observations)</code></li> <li><code>predict(self, conditions)</code></li> </ul> <p>Requirements for working with the state:</p> <ul> <li>The fit module function has a <code>conditions</code> argument that accepts a <code>pandas.DataFrame</code></li> <li>The fit module function has an <code>observations</code> argument that accepts a <code>pandas.DataFrame</code></li> <li>the fit function returns <code>self</code> (i.e., the model itself)</li> </ul> <p>The custom <code>PolynomialRegressor</code> below fits a polynomial of a specified degree.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#altogether-now","title":"Altogether Now\u00b6","text":"<p>We have now created custom experimentalists, experiment runners, and theorists. Let's add them all to the same workflow to see our first fully customized <code>autora</code> workflow.</p>"},{"location":"tutorials/basic/Tutorial%20IV%20Customization/#help","title":"Help\u00b6","text":"<p>We hope that this tutorial helped demonstrate the fundamental components of <code>autora</code>, and how they can be combined to facilitate automated (closed-loop) empirical research through synthetic experiments. We encourage you to explore other tutorials and check out the documentation.</p> <p>If you encounter any issues, bugs, or questions, please reach out to us through the AutoRA Forum. Feel free to report any bugs by creating an issue in the AutoRA repository.</p> <p>You may also post questions directly into the User Q&amp;A Section.</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/","title":"1a - Theorists","text":"<p>Install relevant subpackages from AutoRA</p> In\u00a0[\u00a0]: Copied! <pre># fix default prior and release new package version\n!pip install -q \"autora[theorist-bms]\"\n</pre> # fix default prior and release new package version !pip install -q \"autora[theorist-bms]\" <p>Import relevant modules from AutoRA</p> In\u00a0[\u00a0]: Copied! <pre>from autora.theorist.bms import BMSRegressor\nfrom autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\nimport numpy as np\nfrom sklearn.base import BaseEstimator\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n</pre> from autora.theorist.bms import BMSRegressor from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law import numpy as np from sklearn.base import BaseEstimator import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay In\u00a0[\u00a0]: Copied! <pre>def present_results(model, x_test, y_test, arg='default', model_name=None, variable_names=None, select_indices=None, figsize=None, *args):\n  compare_results(models=[model], x_test=x_test, y_test=y_test, arg=arg, model_names=[model_name], variable_names=variable_names, select_indices=select_indices, figsize=figsize, *args)\n  \ndef compare_results(models, x_test, y_test, arg='default', model_names=None, variable_names=None, observation_name=None, select_indices=None, figsize=None, *args):\n  if model_names is None or model_names == [None]:\n    names = ['Model '+str(i+1) for i in range(len(models))]\n  else:\n    names = model_names\n  if len(x_test.shape) == 1:\n    x_test = x_test.reshape(1, -1)\n  num_var = x_test.shape[1]\n  if variable_names is None:\n    var_names = ['Variable '+str(i+1) for i in range(num_var)]\n  else:\n    var_names = variable_names\n  if observation_name is None:\n    obs_label = 'Observations'\n  else:\n    obs_label = observation_name\n  match arg:\n    case 'default':\n      for i, model in enumerate(models):\n        print(model)\n        synthetic_runner.plotter(model)\n    case '2d':\n      if figsize is None:\n        size = (8,3)\n      else:\n        assert len(figsize) == 2 and isinstance(figsize, tuple), 'incorrect format for figure shape\\nshould be tuple of form (i,j)'\n        size = figsize\n      for i, model in enumerate(models):\n        fig = plt.figure(figsize=size)\n        axes = []\n        y_predict = model.predict(x_test)\n        for j in range(num_var):\n          axes.append(fig.add_subplot(1, num_var, j+1))\n          axes[j].set_xlabel(var_names[j])\n          axes[j].set_ylabel(obs_label)\n          axes[j].set_title(names[i]+' fit on '+var_names[j])\n          axes[j].scatter(x_test[:,j], y_test, label='Ground Truth', alpha=0.5)\n          axes[j].scatter(x_test[:,j], y_predict, label='Predicted', alpha=0.5)\n          axes[j].legend()\n          for arg in args:\n            assert isinstance(arg, str), 'arguments must be in the form of a string'\n            try:\n              exec('axes[j].'+arg)\n            except:\n              raise RuntimeError(f'argument \"{arg}\" could not be executed')\n\n      fig.tight_layout()\n      plt.show()\n    case '3d':\n      if figsize is None:\n        size = (15,5)\n      else:\n        assert len(figsize) == 2 and isinstance(figsize, tuple), 'incorrect format for figure shape\\nshould be tuple of form (i,j)'\n        size = figsize\n      axes = []\n      fig = plt.figure(figsize=size)\n      if select_indices is None:\n        idx = (0,1)\n      else:\n        len(select_indices) == 2 and isinstance(select_indices, tuple), 'incorrect format for select_indices\\nshould be tuple of form (i,j)'\n        idx = select_indices\n      for i, model in enumerate(models):\n        y_predict = model.predict(x_test)\n        ax = fig.add_subplot(1, 3, i+1, projection='3d')\n        ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        axes.append(ax)\n        axes[i].set_xlabel(var_names[idx[0]])\n        axes[i].set_ylabel(var_names[idx[1]])\n        axes[i].set_zlabel(obs_label)\n        axes[i].scatter(x_test[:, idx[0]], x_test[:, idx[1]], y_test, s=1, label='Ground Truth')\n        axes[i].scatter(x_test[:, idx[0]], x_test[:, idx[1]], y_predict, s=1, label='Predicted')\n        axes[i].set_title(names[i])\n        axes[i].legend()\n        axes[i].set_facecolor('white')\n        for arg in args:\n            assert isinstance(arg, str), 'arguments must be in the form of a string'\n            try:\n              exec('axes[j].'+arg)\n            except:\n              raise RuntimeError(f'argument \"{arg}\" could not be executed')\n      fig.tight_layout()\n      plt.show()\n    case 'choice':\n      for model in models:\n        y_pred = np.where(model.predict(x_test) &gt; 0.5, 1, 0)\n        cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n        cmd = ConfusionMatrixDisplay(cm)\n        cmd.plot()\n        plt.show()\n</pre> def present_results(model, x_test, y_test, arg='default', model_name=None, variable_names=None, select_indices=None, figsize=None, *args):   compare_results(models=[model], x_test=x_test, y_test=y_test, arg=arg, model_names=[model_name], variable_names=variable_names, select_indices=select_indices, figsize=figsize, *args)    def compare_results(models, x_test, y_test, arg='default', model_names=None, variable_names=None, observation_name=None, select_indices=None, figsize=None, *args):   if model_names is None or model_names == [None]:     names = ['Model '+str(i+1) for i in range(len(models))]   else:     names = model_names   if len(x_test.shape) == 1:     x_test = x_test.reshape(1, -1)   num_var = x_test.shape[1]   if variable_names is None:     var_names = ['Variable '+str(i+1) for i in range(num_var)]   else:     var_names = variable_names   if observation_name is None:     obs_label = 'Observations'   else:     obs_label = observation_name   match arg:     case 'default':       for i, model in enumerate(models):         print(model)         synthetic_runner.plotter(model)     case '2d':       if figsize is None:         size = (8,3)       else:         assert len(figsize) == 2 and isinstance(figsize, tuple), 'incorrect format for figure shape\\nshould be tuple of form (i,j)'         size = figsize       for i, model in enumerate(models):         fig = plt.figure(figsize=size)         axes = []         y_predict = model.predict(x_test)         for j in range(num_var):           axes.append(fig.add_subplot(1, num_var, j+1))           axes[j].set_xlabel(var_names[j])           axes[j].set_ylabel(obs_label)           axes[j].set_title(names[i]+' fit on '+var_names[j])           axes[j].scatter(x_test[:,j], y_test, label='Ground Truth', alpha=0.5)           axes[j].scatter(x_test[:,j], y_predict, label='Predicted', alpha=0.5)           axes[j].legend()           for arg in args:             assert isinstance(arg, str), 'arguments must be in the form of a string'             try:               exec('axes[j].'+arg)             except:               raise RuntimeError(f'argument \"{arg}\" could not be executed')        fig.tight_layout()       plt.show()     case '3d':       if figsize is None:         size = (15,5)       else:         assert len(figsize) == 2 and isinstance(figsize, tuple), 'incorrect format for figure shape\\nshould be tuple of form (i,j)'         size = figsize       axes = []       fig = plt.figure(figsize=size)       if select_indices is None:         idx = (0,1)       else:         len(select_indices) == 2 and isinstance(select_indices, tuple), 'incorrect format for select_indices\\nshould be tuple of form (i,j)'         idx = select_indices       for i, model in enumerate(models):         y_predict = model.predict(x_test)         ax = fig.add_subplot(1, 3, i+1, projection='3d')         ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))         ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))         ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))         axes.append(ax)         axes[i].set_xlabel(var_names[idx[0]])         axes[i].set_ylabel(var_names[idx[1]])         axes[i].set_zlabel(obs_label)         axes[i].scatter(x_test[:, idx[0]], x_test[:, idx[1]], y_test, s=1, label='Ground Truth')         axes[i].scatter(x_test[:, idx[0]], x_test[:, idx[1]], y_predict, s=1, label='Predicted')         axes[i].set_title(names[i])         axes[i].legend()         axes[i].set_facecolor('white')         for arg in args:             assert isinstance(arg, str), 'arguments must be in the form of a string'             try:               exec('axes[j].'+arg)             except:               raise RuntimeError(f'argument \"{arg}\" could not be executed')       fig.tight_layout()       plt.show()     case 'choice':       for model in models:         y_pred = np.where(model.predict(x_test) &gt; 0.5, 1, 0)         cm = confusion_matrix(y_true=y_test, y_pred=y_pred)         cmd = ConfusionMatrixDisplay(cm)         cmd.plot()         plt.show() In\u00a0[\u00a0]: Copied! <pre>constant = 3.0\n\n# synthetic experiment from autora inventory\nsynthetic_runner = weber_fechner_law(constant=constant)\n\n# experiment meta data:\nsynthetic_runner.variables\n</pre> constant = 3.0  # synthetic experiment from autora inventory synthetic_runner = weber_fechner_law(constant=constant)  # experiment meta data: synthetic_runner.variables <p>First, let's take a look at the variables in the synthetic experiment.</p> In\u00a0[\u00a0]: Copied! <pre># independent variables\nivs = [iv.name for iv in synthetic_runner.variables.independent_variables]\n\n# dependent variable\ndvs = [dv.name for dv in synthetic_runner.variables.dependent_variables]\n\nivs, dvs\n</pre> # independent variables ivs = [iv.name for iv in synthetic_runner.variables.independent_variables]  # dependent variable dvs = [dv.name for dv in synthetic_runner.variables.dependent_variables]  ivs, dvs <p>We can obtain the experimental data by running the synthetic experiment. The <code>conditions</code> contain values for the independent variables. Once we have the conditions, we can run the experiment to obtain the <code>experiment_data</code> containing both the conditions and the observations from the synthetic experiment.</p> In\u00a0[\u00a0]: Copied! <pre># experimental data\nconditions = synthetic_runner.domain()\nexperiment_data = synthetic_runner.run(conditions, added_noise=0.01)\n\n# observations\nobservations = experiment_data[dvs]\n\nexperiment_data\n</pre> # experimental data conditions = synthetic_runner.domain() experiment_data = synthetic_runner.run(conditions, added_noise=0.01)  # observations observations = experiment_data[dvs]  experiment_data <p>Next, we split the data into training and testing datasets.</p> In\u00a0[\u00a0]: Copied! <pre># split into train and test datasets\nconditions_train, conditions_test, observations_train, observations_test = train_test_split(conditions, observations)\n</pre> # split into train and test datasets conditions_train, conditions_test, observations_train, observations_test = train_test_split(conditions, observations) <p>The polynomial regressor has a high interpretability, but it is limited in expressivity. It can only fit polynomial functions.</p> <p>We first initialize the polynomial regressor. The polynomial regressor fits a polynomial function to the data. Below, we set the degree of the polynomial to 3.</p> <p>Note that the PolynomialRegressor class is a simple implementation of a polynomial regressor using sklearn's PolynomialFeatures and LinearRegression classes.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n\nclass PolynomialRegressor:\n    \"\"\"\n    This theorist fits a polynomial function to the data.\n    \"\"\"\n\n    def __init__(self, degree: int = 3):\n      self.poly = PolynomialFeatures(degree=degree, include_bias=False)\n      self.model = LinearRegression()\n\n    def fit(self, x, y):\n      features = self.poly.fit_transform(x, y)\n      self.model.fit(features, y)\n      return self\n\n    def predict(self, x):\n      features = self.poly.fit_transform(x)\n      return self.model.predict(features)\n</pre> from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression   class PolynomialRegressor:     \"\"\"     This theorist fits a polynomial function to the data.     \"\"\"      def __init__(self, degree: int = 3):       self.poly = PolynomialFeatures(degree=degree, include_bias=False)       self.model = LinearRegression()      def fit(self, x, y):       features = self.poly.fit_transform(x, y)       self.model.fit(features, y)       return self      def predict(self, x):       features = self.poly.fit_transform(x)       return self.model.predict(features) <p>Let's initialize the model</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(0)\npoly_model = PolynomialRegressor(degree=3)\n</pre> np.random.seed(0) poly_model = PolynomialRegressor(degree=3) <p>And fit it to the training data, consisting of the experimental conditions and corresponding observations of our synthetic experiment.</p> In\u00a0[\u00a0]: Copied! <pre>poly_model.fit(conditions_train, observations_train)\n</pre> poly_model.fit(conditions_train, observations_train) <p>Finally, we can plot the results of the polynomial regressor to evaluate the model's performance.</p> In\u00a0[\u00a0]: Copied! <pre>present_results(model=poly_model, x_test=conditions_test, y_test=observations_test)\n</pre> present_results(model=poly_model, x_test=conditions_test, y_test=observations_test) In\u00a0[\u00a0]: Copied! <pre>present_results(model=poly_model, x_test=conditions_test, y_test=observations_test, arg='2d')\n</pre> present_results(model=poly_model, x_test=conditions_test, y_test=observations_test, arg='2d') In\u00a0[\u00a0]: Copied! <pre>present_results(model=poly_model, x_test=conditions_test, y_test=observations_test, arg='3d')\n</pre> present_results(model=poly_model, x_test=conditions_test, y_test=observations_test, arg='3d') <p>In summary, we can see that the polynomial regressor is a simple model. It has high interpretability which allows us to quantify the law underlying the data in terms of a polynomial. However, it has some trouble fitting the data generated from the logarithmic psychophysics law due to its limited expressivity.</p> <p>Neural networks are known for their high expressivity, allowing to fit any function. However, they are often considered black-box models due to their complex structure, limiting interpretability for the user.</p> <p>For this section, we are using torch: an open-source machine learning library. It provides a flexible and dynamic computational graph, allowing for complex neural network architectures.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q torch\n</pre> !pip install -q torch <p>We can now initialize a simple multi-layer perceptron (MLP) regressor using the <code>MLPRegressor</code> class from the <code>sklearn.neural_network</code> module. We will train it for a maximum of 500 iterations.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.neural_network import MLPRegressor\nimport torch\nnn_model = MLPRegressor(random_state=1, max_iter=500)\n</pre> from sklearn.neural_network import MLPRegressor import torch nn_model = MLPRegressor(random_state=1, max_iter=500) <p>Similar to the polynomial regressor above, we can fit the neural network regressor to the training data.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(0)\ntorch.manual_seed(0)\nnn_model.fit(conditions_train, observations_train)\n</pre> np.random.seed(0) torch.manual_seed(0) nn_model.fit(conditions_train, observations_train) <p>And plot the results of the neural network regressor.</p> In\u00a0[\u00a0]: Copied! <pre>synthetic_runner.plotter(nn_model)\n</pre> synthetic_runner.plotter(nn_model) <p>We may observe that the neural network regressor does a better job of fitting the data but it is less interpretable than the polynomial regressor. The neural network is a more complex model that can fit a wider range of functions, but it is also a black-box model that does not provide an interpretable equation, limiting its utility for scientific discovery.</p> <p>We initialize the BMS regressor to run 1500 epochs.</p> In\u00a0[\u00a0]: Copied! <pre>bms_model = BMSRegressor(epochs=1500)\n</pre> bms_model = BMSRegressor(epochs=1500) <p>We can also fit the model to the training data...</p> In\u00a0[\u00a0]: Copied! <pre>bms_model.fit(conditions_train, observations_train, seed=0)\n</pre> bms_model.fit(conditions_train, observations_train, seed=0) <p>...and plot the results.</p> In\u00a0[\u00a0]: Copied! <pre>synthetic_runner.plotter(bms_model)\n</pre> synthetic_runner.plotter(bms_model) <p>In addition, we can print the discovered equation.</p> In\u00a0[\u00a0]: Copied! <pre>print(bms_model)\n</pre> print(bms_model) <p>Note that BMS is not only producing good fits but is also capable of recovering the underlying equation of the synthetic data, the Weber-Fechner law.</p> <p>In summary, the BMS regressor is a powerful tool for fitting a wide range of functions while providing an interpretable equation. It strikes a balance between expressivity and interpretability, making it a valuable tool for scientific discovery. In our case, it is capable of re-discovering the Weber-Fechner law from the synthetic data.</p> In\u00a0[\u00a0]: Copied! <pre>models = [poly_model, nn_model, bms_model]\nnames =['poly_model', 'nn_model', 'bms_model']\ncompare_results(models=models, x_test=conditions_test, y_test=observations_test, model_names=names, arg='2d')\n</pre> models = [poly_model, nn_model, bms_model] names =['poly_model', 'nn_model', 'bms_model'] compare_results(models=models, x_test=conditions_test, y_test=observations_test, model_names=names, arg='2d') <p>We can also compare the models in 3D space.</p> In\u00a0[\u00a0]: Copied! <pre>compare_results(models=models, x_test=conditions_test, y_test=observations_test, model_names=names, arg='3d')\n</pre> compare_results(models=models, x_test=conditions_test, y_test=observations_test, model_names=names, arg='3d')"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#basic-tutorial-ib-theorists","title":"Basic Tutorial Ib: Theorists\u00b6","text":"<p>Theorists are classes designed to automate the construction of interpretable models from data. AutoRA theorists are implemented as sklearn regressors and can be used with the <code>fit</code> and <code>predict</code> methods.</p> <p>In order to use a theorist, you must first install the corresponding theorist package. Some theorists are installed by default when you install <code>autora</code>. Once a theorist is installed, you can instantiate it and use it as you would any other sklearn regressor. That is, you can call the <code>fit</code> function of the theorist by passing in experimental conditions and corresponding observations, and then call the <code>predict</code> function to generate predicted observations for novel experimental conditions using the discovered model.</p> <p>The following tutorial demonstrates how to use the <code>BMSRegressor</code> (Guimer\u00e0 et al., 2020, in Sci. Adv.)\u2013a theorist that can discover an interpretable equation relating the independent variables of an experiment (experiment conditions) to predicted dependent variables (observations).</p> <p>We will compare the performance of the <code>BMSRegressor</code> with two other methods: a polynomial regressor and a neural network regressor. The polynomial regressor is a simple model that can only fit polynomial functions, while the neural network regressor is a more complex model that can fit a wider range of functions. The <code>BMSRegressor</code> is a hybrid model that can fit a wide range of functions while also providing an interpretable, potentially non-linear equation.</p> <p>Note: this tutorial requires Python 3.10 to run successfully.</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#installing-and-importing-relevant-packages","title":"Installing and Importing Relevant Packages\u00b6","text":""},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#functions-for-plotting-results","title":"Functions for Plotting Results\u00b6","text":"<p>Before we begin, we also define some functions to plot the results of our models. Simply execute the following code block to define the plotting functions.</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#our-study-object-the-weber-fechner-law","title":"Our Study Object: The Weber-Fechner Law\u00b6","text":"<p>We will evaluate our models to recover Weber-Fechner law. The Weber-Fechner law quantifies the minimum change in a stimulus required to be noticeable. Similar to Steven's power law, the greater the intensity of a stimulus, the larger the change needed to be perceivable. This relationship is hypothesized to be proportional to the logarithm of the ratio between the two stimuli:</p> <p>$y = c \\log\\left(\\dfrac{x_1}{x_2}\\right)$</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#generating-data-from-a-synthetic-psychophysics-experiment","title":"Generating Data From a Synthetic Psychophysics Experiment\u00b6","text":"<p>Here, we leverage a synthetic experiment to generate data from this equation. It is parameterized by the constant $c$. The independent variables are $x_1$ and $x_2$, corresponding to the intensity of a stimulus and the baseline stimulus intensity, respectively. The dependent variable is $y$ perceived stimulus intensity.</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#fitting-models-to-the-data","title":"Fitting Models to the Data\u00b6","text":"<p>In this section, we will fit the data with three techniques:</p> <ul> <li>Polynomial Regressor</li> <li>Neural Network Regressor</li> <li>Bayesian Machine Scientist</li> </ul> <p>The last technique is an equation discovery algorithm implemented in the AutoRA framework.</p> <p>We will repeat the following steps for each method:</p> <ol> <li>Initialize Model</li> <li>Fit Model to the Data</li> <li>Plot the Results</li> </ol>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#polynomial-regressor","title":"Polynomial Regressor\u00b6","text":"<p>Expressivity: Low</p> <p>Interpretability: High</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#neural-network","title":"Neural Network\u00b6","text":"<p>Expressivity: High</p> <p>Interpretability: Low</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#bayesian-machine-scientist","title":"Bayesian Machine Scientist\u00b6","text":"<p>Expressivity: Medium</p> <p>Interpretability: High</p> <p>The Bayesian Machine Scientist (BMS) is one of the theorists that comes with the autora package. It is an equation discovery method that can fit a wide range of functions while providing an interpretable equation. It uses MCMC-Sampling to explore the space of possible equations and find the best-fitting equation for the data while minimizing the number of parameters in the equation.</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#comparing-the-models","title":"Comparing the Models\u00b6","text":"<p>Finally, we can compare all three fitted models in terms of their fit to the hold-out data using the <code>compare_results</code> function. This function plots the ground truth and predicted values for each model, allowing us to visually compare their performance.</p>"},{"location":"tutorials/basic/Tutorial%20Ia%20Theorists/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, we compared the performance of three different theorists: a polynomial regressor, a neural network regressor, and the Bayesian Machine Scientist (BMS) regressor. The polynomial regressor is a simple model that can only fit polynomial functions, while the neural network regressor is a more complex model that can fit a wider range of functions. The BMS regressor is a hybrid model that can fit a wide range of functions while also providing an interpretable, potentially non-linear equation.</p> <p>AutoRA provides interfaces for using theorists as sklearn regressors. This allows you to easily fit and evaluate theorists using the <code>fit</code> and <code>predict</code> methods. Note that such theorists may not be limited to fitting functions but may also discover complex computational models or algorithms describing the data.</p>"},{"location":"tutorials/basic/Tutorial%20Ib%20Experimentalists/","title":"1b - Experimentalists","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[experimentalist-falsification]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[experimentalist-falsification]\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom autora.variable import DV, IV, ValueType, VariableCollection\nfrom autora.experimentalist.falsification import falsification_sample, falsification_score_sample\n</pre> import numpy as np from sklearn.linear_model import LinearRegression from autora.variable import DV, IV, ValueType, VariableCollection from autora.experimentalist.falsification import falsification_sample, falsification_score_sample <p>In order to reproduce our results, we also import torch and set the seed.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\ntorch.manual_seed(180)\nnp.random.seed(180)\n</pre> import torch torch.manual_seed(180) np.random.seed(180) In\u00a0[\u00a0]: Copied! <pre>X = np.linspace(0, 2 * np.pi, 100)\nY = np.sin(X)\n</pre> X = np.linspace(0, 2 * np.pi, 100) Y = np.sin(X) <p>Next, we need to define metadata object, so the falsification sampler knows what data it is supposed to generate. We can do this by defining the independent variable $x$, which underlies experimental conditions $X$, and the dependent variable $y$, which underlies the observations $Y$. We specify that $x$ is a continuous variable with a range of $[0, 2\\pi]$, and $y$ is a real-valued variable.</p> In\u00a0[\u00a0]: Copied! <pre># Specify independent variable\niv = IV(\n    name=\"x\",\n    value_range=(0, 2 * np.pi),\n)\n\n# specify dependent variable\ndv = DV(\n    name=\"y\",\n    type=ValueType.REAL,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n</pre> # Specify independent variable iv = IV(     name=\"x\",     value_range=(0, 2 * np.pi), )  # specify dependent variable dv = DV(     name=\"y\",     type=ValueType.REAL, )  # Variable collection with ivs and dvs metadata = VariableCollection(     independent_variables=[iv],     dependent_variables=[dv], ) <p>Next, we can specify the model that we would like to fit to the data. In this case, we will use a linear model.</p> In\u00a0[\u00a0]: Copied! <pre>model = LinearRegression()\nmodel.fit(X.reshape(-1, 1), Y)\n</pre> model = LinearRegression() model.fit(X.reshape(-1, 1), Y) Out[\u00a0]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted Parameters fit_intercept\u00a0 True copy_X\u00a0 True tol\u00a0 1e-06 n_jobs\u00a0 None positive\u00a0 False <p>Finally, we can generate novel experimental conditions $X'$ from the falsification sampler. We will select 5 novel experimental conditions from a candidate set of 14 experiment conditions.</p> In\u00a0[\u00a0]: Copied! <pre>X_prime = np.linspace(0, 6.5, 14)\n\nnew_conditions = falsification_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=5,\n        plot=True,\n    )\n</pre> X_prime = np.linspace(0, 6.5, 14)  new_conditions = falsification_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=5,         plot=True,     ) <p>Before we examine the novel conditions, let's have a look at the three plots generated by the falsification sampler, going from last to first.</p> <ul> <li><p>Model Prediction vs. Data. The model trained on the data is shown in red, and the model prediction is shown in blue. The model prediction is a straight line, which is a poor fit to the data. This is expected, since the data is generated from a sine function, which is not linear.  </p> </li> <li><p>Loss of the Falsification Network. The plot shows the learning curve for the falsification network that is trained to predict the error of the (linear) model as a function of experimental conditions. The error (loss) of this network decreases as a function of the number of training epochs.  </p> </li> <li><p>Prediction of Falsification Experimentalist. The plot shows the predicted loss of the model as a function of the experimental condition. The model is predicted to perform the worst at the extremes of the domain, which is expected since the model is a poor fit to the data. The red dots show the true loss of the model at the corresponding experimental condition. The predicted loss is a good approximation of the true loss.</p> </li> </ul> <p>The falsification sampler will identify novel experimental conditions that maximize the predicted loss (shown as a blue line in the plot \"Prediction of Falsification Experimentalist\").</p> <p>Before examining the selected new conditions, we need to convert them to a numpy array.</p> In\u00a0[\u00a0]: Copied! <pre>new_conditions = new_conditions.to_numpy()\nprint(new_conditions)\n</pre> new_conditions = new_conditions.to_numpy() print(new_conditions) <pre>[[0. ]\n [6.5]\n [6. ]\n [2. ]\n [4. ]]\n</pre> <p>Note that the new conditions are all at the limits of the domain $\\{0, 2\\pi\\}$, as well as around the peaks of the sinusoid, which is expected since the model is a poor fit to the data at those points. We can also plot the new conditions on top of the data.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Old Data\")\nax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"g\", label=\"New Experimental Conditions\")\nax.plot(X, model.predict(X.reshape(-1, 1)), c=\"b\", label=\"Model Prediction\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Old Data\") ax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"g\", label=\"New Experimental Conditions\") ax.plot(X, model.predict(X.reshape(-1, 1)), c=\"b\", label=\"Model Prediction\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x732bfc69fd10&gt;</pre> <p>We can also obtain \"falsification\" scores for the sampled experiment conditions using ``falsification_score_sample''. The scores are z-scored with respect to all conditions from the candidate set. In the following example, we sample 5 conditions and return their falsification scores.</p> In\u00a0[\u00a0]: Copied! <pre>scores_df = falsification_score_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=5,\n    )\n\nnew_conditions = np.asarray(X_prime)[scores_df.index]\nscores = scores_df['score'].to_numpy()\n\nprint(new_conditions)\nprint(scores)\n</pre> scores_df = falsification_score_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=5,     )  new_conditions = np.asarray(X_prime)[scores_df.index] scores = scores_df['score'].to_numpy()  print(new_conditions) print(scores)  <pre>[0.  0.5 1.  1.5 2. ]\n[2.634674   1.864585   0.15935703 0.14204112 0.10674001]\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.datasets import make_blobs\nX, Y = make_blobs(n_samples=100, n_features=1, centers=2, random_state=0)\n</pre> from sklearn.datasets import make_blobs X, Y = make_blobs(n_samples=100, n_features=1, centers=2, random_state=0) <p>Next, we need to define metadata object, so the falsification sampler knows what data it is supposed to generate. We can do this by defining the independent variable $x$ underlying the experimental conditions $X$ and the dependent variable $y$ underlying the observations $Y$ as \"VariableCollection\" objects. We specify that $X$ is a continuous variable with a range of $[-1, 6]$, and $Y$ is a categorical variable.</p> In\u00a0[\u00a0]: Copied! <pre># Specify independent variable\niv = IV(\n    name=\"X\",\n    value_range=(-1, 6),\n)\n\n# specify dependent variable\ndv = DV(\n    name=\"Y\",\n    type=ValueType.CLASS,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n</pre> # Specify independent variable iv = IV(     name=\"X\",     value_range=(-1, 6), )  # specify dependent variable dv = DV(     name=\"Y\",     type=ValueType.CLASS, )  # Variable collection with ivs and dvs metadata = VariableCollection(     independent_variables=[iv],     dependent_variables=[dv], ) <p>Next, we can specify the model that we would like to fit to the data. In this case, we will use a Gaussian mixture model with 2 components.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.mixture import GaussianMixture\nmodel = GaussianMixture(n_components=2, random_state=2)\nmodel.fit(X, Y)\n\n# plot model fit against data\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Data\")\nax.scatter(X, model.predict(X), c=\"b\", label=\"Model Prediction\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> from sklearn.mixture import GaussianMixture model = GaussianMixture(n_components=2, random_state=2) model.fit(X, Y)  # plot model fit against data import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Data\") ax.scatter(X, model.predict(X), c=\"b\", label=\"Model Prediction\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x732bfc5941a0&gt;</pre> <p>In this case, the model appears to predict most of the data points quite well but fails to predict data points around $x=3$. Let's see if the falsification sampler can identify this region of the domain. We will select samples from a candidate set of 71 experiment conditions.</p> In\u00a0[\u00a0]: Copied! <pre>X_prime = np.linspace(-1, 6, 71)\n</pre> X_prime = np.linspace(-1, 6, 71) <p>and call the falsification sampler.</p> In\u00a0[\u00a0]: Copied! <pre>new_conditions = falsification_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=10,\n        plot=True,\n    )\n</pre> new_conditions = falsification_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=10,         plot=True,     ) <p>As shown in the \"Prediction of Falsification Network\" plot, the model is predicted to perform the worst around $x=3$. Let's have a look at the selected new conditions.</p> In\u00a0[\u00a0]: Copied! <pre>new_conditions = new_conditions.to_numpy()\nprint(new_conditions)\n</pre> new_conditions = new_conditions.to_numpy() print(new_conditions) <pre>[[2.9]\n [3.2]\n [3.3]\n [2.5]\n [2.6]\n [2.8]\n [2.4]\n [2.7]\n [2.3]\n [2.2]]\n</pre> <p>Indeed, the new conditions mostly located around $x=3$, reflecting a poor fit of the model for those conditions. Finally, we can plot the new conditions on top of the data.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Data\")\nax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"b\", label=\"New Experimental Conditions\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Data\") ax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"b\", label=\"New Experimental Conditions\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x732bfc449190&gt;</pre> <p>Note that in this example the new experimental conditions are concentrated around x \u2248 3, near the model\u2019s decision boundary. This is expected, since the falsification strategy targets regions where the current model is most likely to misclassify or show high prediction error. We can plot these new conditions alongside the original data and the model\u2019s predictions.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Old Data\")\nax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"g\", label=\"New Experimental Conditions\")\nax.plot(X, model.predict(X.reshape(-1, 1)), c=\"b\", label=\"Model Prediction\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Old Data\") ax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"g\", label=\"New Experimental Conditions\") ax.plot(X, model.predict(X.reshape(-1, 1)), c=\"b\", label=\"Model Prediction\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x732bfc388da0&gt;</pre> <p>We can also obtain \"falsification\" scores for the sampled experiment conditions using ``falsification_score_sample''. The scores are z-scored with respect to all conditions from the candidate set. In the following example, we sample 5 conditions and return their falsification scores.</p> In\u00a0[\u00a0]: Copied! <pre>scores_df = falsification_score_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=5,\n)\n\nscored_conditions = np.asarray(X_prime)[scores_df.index]\nscores = scores_df[\"score\"].to_numpy()\n\nprint(scored_conditions)\nprint(scores)\n</pre> scores_df = falsification_score_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=5, )  scored_conditions = np.asarray(X_prime)[scores_df.index] scores = scores_df[\"score\"].to_numpy()  print(scored_conditions) print(scores)  <pre>[-1.  -0.9 -0.8 -0.7 -0.6]\n[1.6834128 1.6422206 1.5483896 1.4479377 1.2526897]\n</pre>"},{"location":"tutorials/basic/Tutorial%20Ib%20Experimentalists/#basic-tutorial-ib-experimentalists","title":"Basic Tutorial Ib: Experimentalists\u00b6","text":"<p>Experimentalists are functions designed to return novel experimental conditions that yield scientific merit.</p> <p></p> <p>Experimentalists may use information about candidate models $M$ obtained from a theorist, experiment conditions that have already been probed $\\vec{x}' \\in X'$, or respective dependent measures $\\vec{y}' \\in Y'$.</p> <p>The following tutorial demonstrates how to use a falsification experimentalist referred to as falsification sampler. The falsification sampler identifies experiment conditions under which the loss $\\hat{\\mathcal{L}}(M,X,Y,\\vec{x})$ of the best candidate model is predicted to be the highest. This loss is approximated with a multi-layer perceptron, which is trained to predict the loss of a candidate model, $M$, given experiment conditions $X$  and dependent measures $Y$ that have already been probed.</p> <p>We begin with importing the relevant packages.</p>"},{"location":"tutorials/basic/Tutorial%20Ib%20Experimentalists/#example-1-sampling-from-a-sine-function","title":"Example 1: Sampling From A Sine Function\u00b6","text":"<p>In this example, we will consider a dataset resembling the sine function. We will then fit a linear model to the data and use the falsification sampler to identify experiment conditions under which the model is predicted to perform the worst.</p> <p>First, we define the experiment conditions $X$ and the observations $Y$. We consider a domain of $X \\in [0, 2\\pi]$, and sample 100 data points from this domain.</p>"},{"location":"tutorials/basic/Tutorial%20Ib%20Experimentalists/#example-2-sampling-from-a-gaussian-mixture-model","title":"Example 2: Sampling From A Gaussian Mixture Model\u00b6","text":"<p>In this example, we will consider a dataset sampled from a Gaussian mixture model. We will fit a Gaussian mixture model to the data and use the falsification sampler to identify experiment conditions under which the model is predicted to perform the worst.</p> <p>First, we define the experimental conditions $X$ and the observations $Y$, and sample 100 data points. The dependent variable is a categorical variable with 2 categories.</p>"},{"location":"user-cookiecutter/docs/","title":"Closed Loop Online Experiment","text":"<p>To establish an online closed-loop for AutoRA, there are two key components that need to be configured:</p> <ol> <li> <p>AutoRA Workflow</p> <ul> <li>This workflow can be executed locally, on a server, or using <code>Cylc</code>. It must have the ability to communicate with a website, allowing for the writing of new conditions and reading of observation data.</li> <li>The AutoRA workflow can be customized by adding or removing AutoRA functions, such as AutoRA experimentalists or AutoRA theorists. It relies on an AutoRA Prolific Firebase runner to collect data from an online experiment hosted via Firebase and recruit participants via prolific.</li> </ul> </li> <li> <p>Website To Conduct Experiment:</p> <ul> <li>The website serves as a platform for conducting experiments and needs to be compatible with the AutoRA workflow.</li> <li>In this setup, we use <code>Firebase</code> to host on website.</li> </ul> </li> </ol> <p>To simplify the setup process, we provide a <code>cookiecutter</code> template that generates a project folder containing the following two directories:</p> <ol> <li> <p>Researcher Hub:</p> <ul> <li>This directory includes a basic example of an AutoRA workflow.</li> </ul> </li> <li> <p>Testing Zone:</p> <ul> <li>This directory provides a basic example of a website served with Firebase, ensuring compatibility with the AutoRA workflow.</li> </ul> </li> </ol> <p>The following steps outline how to set up the project:</p>"},{"location":"user-cookiecutter/docs/#set-up-the-project-on-the-firebase-website","title":"Set Up The Project On The Firebase Website","text":"<p>To serve a website via Firebase and use the Firestore Database, it is necessary to set up a Firebase project. Follow the steps below to get started:</p>"},{"location":"user-cookiecutter/docs/#google-account","title":"Google Account","text":"<p>You'll need a Google account to use Firebase.</p>"},{"location":"user-cookiecutter/docs/#firebase-project","title":"Firebase Project","text":"<p>While logged in into your Google account, head over to the Firebase website. Then, create a new project:</p> <ul> <li>Click on <code>Get started</code>.</li> <li>Click on the plus sign with <code>add project</code>.</li> <li>Name your project and click on <code>continue</code>.</li> <li>For now, we don't use Google Analytics (you can leave it enabled if you want to use it in the future).</li> <li>Click <code>Create project</code>.</li> </ul>"},{"location":"user-cookiecutter/docs/#adding-a-webapp-to-your-project","title":"Adding A Webapp To Your Project","text":"<p>Now, we add a webapp to the project. Navigate to the project and follow these steps:</p> <ul> <li>Click on <code>&lt;\\&gt;</code>.</li> <li>Name the app (can be the same as your project) and check the box <code>Also set up Firebase Hosting</code>. Click on <code>Register app</code>.</li> <li>We will use <code>npm</code>. We will use the configuration details later, but for now, click on <code>Next</code>.</li> <li>We will install firebase tools later, for now, click on <code>Next</code>.</li> <li>We will login and deploy our website later, for now, click on <code>Continue to console</code>.</li> </ul>"},{"location":"user-cookiecutter/docs/#adding-firestore-to-your-project","title":"Adding Firestore To Your Project","text":"<p>For the online closed loop system, we will use a Firestore Database to communicate between the AutoRA workflow and the website conducting the experiment. We will upload experiment conditions to the database and store experiment data in the database. To build a Firestore Database, follow these steps:</p> <ul> <li>In the left-hand menu of your project console, click on <code>Build</code> and select <code>Firestore Database</code>.</li> <li>Click on <code>Create database</code>.</li> <li>Leave <code>Start in production mode</code> selected and click on <code>Next</code>.</li> <li>Select a Firestore location and click on <code>Enable</code>.</li> <li>To check if the database is set up correctly, click on the gear symbol next to the <code>Project overview</code> in the left-hand menu and select <code>Project settings</code>.</li> <li>Under <code>Default GCP resource location</code>, you should see the Firestore location that you selected.</li> <li>If you don't see the location, select one now (click on the <code>pencil-symbol</code> and then on <code>Done</code> in the pop-up window).</li> </ul>"},{"location":"user-cookiecutter/docs/#set-up-the-project-on-your-system","title":"Set Up The Project On Your System","text":"<p>After setting up the project on Firebase, we will setup the project on our system. Here, we will use <code>cookiecutter</code> to setup an example.</p>"},{"location":"user-cookiecutter/docs/#prerequisite","title":"Prerequisite","text":"<p>To set up an online AutoRA closed-loop, you need both <code>Python</code> and <code>Node</code>.</p> <p>You should also consider using an IDE. We recommend: </p> <ul> <li>PyCharm. This is a <code>Python</code>-specific integrated development environment that comes with useful tools    for changing the structure of <code>Python</code> code, running tests, etc. </li> <li>Visual Studio Code. This is a powerful general text editor with plugins to support <code>Python</code> development.</li> </ul>"},{"location":"user-cookiecutter/docs/#install-python-and-node","title":"Install <code>Python</code> and <code>Node</code>","text":"<ul> <li>You can install python using the instructions at python.org</li> <li>You can find information about how to install on the official Node website</li> </ul>"},{"location":"user-cookiecutter/docs/#create-a-virtual-environment","title":"Create A Virtual Environment","text":"<p>Success</p> <p>We recommend setting up your virtual environment using a manager like <code>venv</code>, which creates isolated <code>Python</code>  environments. Other environment managers, such as  virtualenv, pipenv, virtualenvwrapper,  hatch,  poetry, </p>"},{"location":"user-cookiecutter/docs/#prolific","title":"Prolific","text":"<p>To recruit participants via Prolific, you will need a Prolific account and an api token. Both can be obtained on the Prolific website. </p>"},{"location":"user-cookiecutter/docs/#run-cookiecutter","title":"Run Cookiecutter","text":"<p>After we have installed <code>Python</code> and <code>Node</code> and set up a virtual environment, we use the <code>cookiecutter</code>. First, install it:</p> <pre><code>pip install cookiecutter\n</code></pre> <p>Then, run <code>cookiecutter</code> and select the <code>basic</code> option. </p> <p>Hint</p> <p>If you select advanced, there are more features, but the instructions here focus on the basic template. If you select advanced, make sure to also select <code>autora[experiment-runner-firebase-prolific]</code> and <code>set up as Firebase experiment</code>.</p> <pre><code>cookiecutter https://github.com/AutoResearch/autora-user-cookiecutter\n</code></pre> <p>This command will result in two directories, <code>researcher_hub</code> and <code>testing_zone</code>, which are described next.</p>"},{"location":"user-cookiecutter/docs/#researcher-hub-autora-workflow","title":"Researcher Hub: AutoRA Workflow","text":"<p>The <code>researcher_hub</code> contains a basic template for an AutoRA workflow. </p> <p>To install the necessary dependencies, move to the directory to the  and install the requirements.</p> <p>Move to the <code>researcher_hub</code> directory: <pre><code>cd researcher_hub\n</code></pre></p> <p>Install the requirements: <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>You can find documentation for all parts of the AutoRA workflow in the User Guide</p>"},{"location":"user-cookiecutter/docs/#testing-zone-firebase-website","title":"Testing Zone: Firebase Website","text":"<p>The <code>testing_zone</code> contains a basic template for a website that is compatible with the AutoRA Experimentation Manager for Firebase and the AutoRA Recruitment Manager for Prolific. You can find a complete example on how to use Prolific and Firebase here: AutoRA Firebase Prolific Runner</p> <p>You can find documentation on how to connect the website to an AutoRA workflow, as well as how to build and deploy it in the documentation for Firebase Integration</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/","title":"Mathematical Model Discovery","text":"<p>In this experiment, we are interested in the congruency effect: The orientation of the triangles and the movement direction can align (congruent trials) or not (incongruent trials). Typically, in such experiments, congruent trials are easier and faster than incongruent trials. Here, we are interested in how the coherence of each feature (orientation and movement) acts on the congruency effect. The coherence of the orientation is the number of triangles facing in the coherent direction as opposed to randomly facing. The coherence of the movement is the number of triangles moving in the random direction as opposed to random.</p> <p>We can express the experiment in terms of dependent and independent variables:</p> <ul> <li>The dependent variable is the congruency effect. We calculate this effect as the difference in reaction times (on correct trials) between congruent and incongruent trial.</li> <li>The independent variables are movement coherence and orientation coherence</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom autora.variable import Variable, VariableCollection\n\n# ** Dependent Variable ** #\n# Here, we assume the (normalized) congruency effect as a number between 0 and 1.\ncongruency_effect = Variable(name='congruency_effect', value_range=(0, 1))\n\n# ** Independent Variables ** #\n# Here, coherence is the ratio between coherent and random movement (or orientation) in steps of .01\ncoherence_movement = Variable(name='coherence_movement', value_range=(0, 1), allowed_values=np.linspace(.01, 1, 100))\ncoherence_orientation = Variable(name='coherence_orientation', value_range=(0, 1), allowed_values=np.linspace(.01, 1, 100))\n\n# ** Variable Collection ** #\nvariables = VariableCollection(dependent_variables=[congruency_effect], independent_variables=[coherence_movement, coherence_orientation])\n</pre> import numpy as np from autora.variable import Variable, VariableCollection  # ** Dependent Variable ** # # Here, we assume the (normalized) congruency effect as a number between 0 and 1. congruency_effect = Variable(name='congruency_effect', value_range=(0, 1))  # ** Independent Variables ** # # Here, coherence is the ratio between coherent and random movement (or orientation) in steps of .01 coherence_movement = Variable(name='coherence_movement', value_range=(0, 1), allowed_values=np.linspace(.01, 1, 100)) coherence_orientation = Variable(name='coherence_orientation', value_range=(0, 1), allowed_values=np.linspace(.01, 1, 100))  # ** Variable Collection ** # variables = VariableCollection(dependent_variables=[congruency_effect], independent_variables=[coherence_movement, coherence_orientation]) In\u00a0[\u00a0]: Copied! <pre>from autora.state import StandardState\nstate = StandardState(variables=variables)\n</pre> from autora.state import StandardState state = StandardState(variables=variables) In\u00a0[\u00a0]: Copied! <pre># ** Grid Pooler ** #\n# We create a grid of conditions (each combination of the coherence_movement and coherence_orientation\nfrom autora.state import on_state\nfrom autora.experimentalist.grid import grid_pool\n\n# To wrap the grid_pool function for the use on the state, we pass the function and the field into the on_state method:\ngrid_pool_on_state = on_state(grid_pool, output=[\"conditions\"])\n\n# Run the function on the state:\nstate = grid_pool_on_state(state)\nprint(state.conditions)\n</pre> # ** Grid Pooler ** # # We create a grid of conditions (each combination of the coherence_movement and coherence_orientation from autora.state import on_state from autora.experimentalist.grid import grid_pool  # To wrap the grid_pool function for the use on the state, we pass the function and the field into the on_state method: grid_pool_on_state = on_state(grid_pool, output=[\"conditions\"])  # Run the function on the state: state = grid_pool_on_state(state) print(state.conditions) In\u00a0[\u00a0]: Copied! <pre># ** Random Sampler ** #\n# Since we don't want our experiment to run on the full grid, we can now randomly sample from the conditions using a random sampler:\nfrom autora.experimentalist.random import random_sample\n\n# Define the function on the state:\nrandom_sample_on_state = on_state(random_sample, output=[\"conditions\"])\n\n# Run the function on the state:\nstate = random_sample_on_state(state, num_samples=3)\nstate.conditions\n</pre> # ** Random Sampler ** # # Since we don't want our experiment to run on the full grid, we can now randomly sample from the conditions using a random sampler: from autora.experimentalist.random import random_sample  # Define the function on the state: random_sample_on_state = on_state(random_sample, output=[\"conditions\"])  # Run the function on the state: state = random_sample_on_state(state, num_samples=3) state.conditions <p>Note: This workflow (creating a grid and then pooling) is not very efficient for various reasons: (1) Creating a grid might not be feasible if there are more independent variables or if their resolution is higher (2) The <code>conditions</code>-field is overwritten by the random sampler. Therefore, each time the user wants to have different conditions, the grid has to be created again.</p> <p>AutoRA provides a method to directly create random conditions, here: <code>autora.experimentalst.random import pool</code>. Also, we could create a custom state with the field <code>pool</code> and only create the grid pool once. The example above was chosen to demonstrate the ability to chain experimentalists.</p> In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install autora-synthetic-abstract-equation\n</pre> %%capture !pip install autora-synthetic-abstract-equation <p>We use sympy to create an equation experiment and wrap it to run on state:</p> In\u00a0[\u00a0]: Copied! <pre># ** Equation Runner (arbitrary) ** #\nfrom autora.experiment_runner.synthetic.abstract.equation import equation_experiment\nfrom sympy import symbols\n\n# Declare our variables\ncoherence_movement, coherence_orientation = symbols(\"coherence_movement coherence_orientation\")\n\n# Declare the expression (this is arbitrary here)\nexpr = (coherence_movement**2)/3 + (coherence_orientation**2)/3 + coherence_movement * coherence_orientation/3\n\n# We declare the experiment\nequation_experiment_arbitrary = equation_experiment(expr, variables.independent_variables, variables.dependent_variables[0])\n\n# Wrap this to run it on state (Note: We wrap the run function not the full experiment which includes more functionality for example to plot data)\nrun_eq_arbitrary_on_state = on_state(equation_experiment_arbitrary.run, output=['experiment_data'])\n</pre> # ** Equation Runner (arbitrary) ** # from autora.experiment_runner.synthetic.abstract.equation import equation_experiment from sympy import symbols  # Declare our variables coherence_movement, coherence_orientation = symbols(\"coherence_movement coherence_orientation\")  # Declare the expression (this is arbitrary here) expr = (coherence_movement**2)/3 + (coherence_orientation**2)/3 + coherence_movement * coherence_orientation/3  # We declare the experiment equation_experiment_arbitrary = equation_experiment(expr, variables.independent_variables, variables.dependent_variables[0])  # Wrap this to run it on state (Note: We wrap the run function not the full experiment which includes more functionality for example to plot data) run_eq_arbitrary_on_state = on_state(equation_experiment_arbitrary.run, output=['experiment_data']) <p>After defining an equation experiment on stat, we can use it just as we used the experimentalists:</p> In\u00a0[\u00a0]: Copied! <pre>state = run_eq_arbitrary_on_state(state)\nstate.experiment_data\n</pre> state = run_eq_arbitrary_on_state(state) state.experiment_data In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install pysr\n</pre> %%capture !pip install pysr <p>We create a theorist from a regressor by defining it and wrapping it with the on_state function so it is useable on the state:</p> In\u00a0[\u00a0]: Copied! <pre>from pysr import PySRRegressor\n\n\n# The PySRRegressor doesn't use the standard sklearn output format for the predict function, here we adjust this:\nclass AdjustedPySRRegressor(PySRRegressor):\n    def predict(self, X, index=None):\n        y = super().predict(X, index)\n        if len(y.shape) &lt; 2:\n            return np.array([[el] for el in y])\n        return y\n\n\n# We define a set of mathematical operations for the symbolic regression algorithm\nbinary_operators = [\"+\", \"-\", \"*\", \"/\", \"^\"]\nunary_operators = [\"sin\", \"cos\", \"tan\", \"exp\", \"log\", \"sqrt\", \"abs\"]\n# Theorists\npysr_regressor = AdjustedPySRRegressor(niterations=100,\n                                       binary_operators=[\"+\", \"-\", \"*\", \"/\", \"^\"],\n                                       unary_operators=[\"cos\", \"sin\", \"tan\", \"exp\", \"log\", \"sqrt\"],\n                                       batching=True,\n                                       multithreading=True,\n                                       temp_equation_file=False)\n\n# Here, we show how to use the on_state wrapper as decorator. Note, if state fields should be used as input arguments to the wrapped\n# function, then the argument names have to align with the field names (here: experiment_data and variables). The same is true for the output\n# Delta. Here, `models` is a field of the StandardState\nfrom autora.state import Delta\n\n\n@on_state()\ndef pysr_theorist_on_state(experiment_data, variables: VariableCollection):\n    ivs = [v.name for v in variables.independent_variables]\n    dvs = [v.name for v in variables.dependent_variables]\n    X, y = experiment_data[ivs], experiment_data[dvs]\n    new_model = pysr_regressor.fit(X, y)\n    return Delta(models=[new_model])\n</pre> from pysr import PySRRegressor   # The PySRRegressor doesn't use the standard sklearn output format for the predict function, here we adjust this: class AdjustedPySRRegressor(PySRRegressor):     def predict(self, X, index=None):         y = super().predict(X, index)         if len(y.shape) &lt; 2:             return np.array([[el] for el in y])         return y   # We define a set of mathematical operations for the symbolic regression algorithm binary_operators = [\"+\", \"-\", \"*\", \"/\", \"^\"] unary_operators = [\"sin\", \"cos\", \"tan\", \"exp\", \"log\", \"sqrt\", \"abs\"] # Theorists pysr_regressor = AdjustedPySRRegressor(niterations=100,                                        binary_operators=[\"+\", \"-\", \"*\", \"/\", \"^\"],                                        unary_operators=[\"cos\", \"sin\", \"tan\", \"exp\", \"log\", \"sqrt\"],                                        batching=True,                                        multithreading=True,                                        temp_equation_file=False)  # Here, we show how to use the on_state wrapper as decorator. Note, if state fields should be used as input arguments to the wrapped # function, then the argument names have to align with the field names (here: experiment_data and variables). The same is true for the output # Delta. Here, `models` is a field of the StandardState from autora.state import Delta   @on_state() def pysr_theorist_on_state(experiment_data, variables: VariableCollection):     ivs = [v.name for v in variables.independent_variables]     dvs = [v.name for v in variables.dependent_variables]     X, y = experiment_data[ivs], experiment_data[dvs]     new_model = pysr_regressor.fit(X, y)     return Delta(models=[new_model]) <p>Now, we can run this on the state:</p> In\u00a0[\u00a0]: Copied! <pre>state = pysr_theorist_on_state(state)\n</pre> state = pysr_theorist_on_state(state) <p>We can access the current result of the equation search by accessing the last model of the state</p> In\u00a0[\u00a0]: Copied! <pre>state.models[-1].sympy()\n</pre> state.models[-1].sympy() In\u00a0[\u00a0]: Copied! <pre>for _ in range(3):\n    state = grid_pool_on_state(state)\n    state = random_sample_on_state(state, num_samples=3)\n    state = run_eq_arbitrary_on_state(state)\n    state = pysr_theorist_on_state(state)\n    print(state.models[-1].sympy())\n</pre> for _ in range(3):     state = grid_pool_on_state(state)     state = random_sample_on_state(state, num_samples=3)     state = run_eq_arbitrary_on_state(state)     state = pysr_theorist_on_state(state)     print(state.models[-1].sympy()) In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install \"autora[experimentalist-falsification]\"\n</pre> %%capture !pip install \"autora[experimentalist-falsification]\" In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.falsification import falsification_sample\n\n# Wrap to use on state:\n@on_state()\ndef falsification_sample_on_state(conditions, models, experiment_data, variables, num_samples):\n    ivs = [v.name for v in variables.independent_variables]\n    dvs = [v.name for v in variables.dependent_variables]\n    X, y = experiment_data[ivs], experiment_data[dvs]\n    return Delta(conditions=falsification_sample(conditions, models[-1], X, y, variables, num_samples=num_samples))\n</pre> from autora.experimentalist.falsification import falsification_sample  # Wrap to use on state: @on_state() def falsification_sample_on_state(conditions, models, experiment_data, variables, num_samples):     ivs = [v.name for v in variables.independent_variables]     dvs = [v.name for v in variables.dependent_variables]     X, y = experiment_data[ivs], experiment_data[dvs]     return Delta(conditions=falsification_sample(conditions, models[-1], X, y, variables, num_samples=num_samples)) In\u00a0[\u00a0]: Copied! <pre># create a grid\nstate = grid_pool_on_state(state)\n# sample 100 samples as candidates for the falsification sampler\nstate =  random_sample_on_state(state, num_samples=100)\n# sample 3 samples via the falsification sampler\nstate = falsification_sample_on_state(state, num_samples=3)\nstate.conditions\n</pre> # create a grid state = grid_pool_on_state(state) # sample 100 samples as candidates for the falsification sampler state =  random_sample_on_state(state, num_samples=100) # sample 3 samples via the falsification sampler state = falsification_sample_on_state(state, num_samples=3) state.conditions <p>With this, we can create a closed loop with falsification sampler. Here, we alternate between random sampling and falsification sampling. Note, the falsification sampler will fail on the first cycle since there is no model or experiment_data to falsify.</p> In\u00a0[\u00a0]: Copied! <pre># reset the state:\nstate = StandardState(variables)\n\nfor cycle in range(4):\n    state = grid_pool_on_state(state)\n    if not cycle % 2:  # cycle is odd\n        state = random_sample_on_state(state, num_samples=3)\n    else:  # cycle is even\n        state = random_sample_on_state(state, num_samples=100)\n        state = falsification_sample_on_state(state, num_samples=3)\n    state = run_eq_arbitrary_on_state(state)\n    state = pysr_theorist_on_state(state)\n    print(state.models[-1].sympy())\n</pre> # reset the state: state = StandardState(variables)  for cycle in range(4):     state = grid_pool_on_state(state)     if not cycle % 2:  # cycle is odd         state = random_sample_on_state(state, num_samples=3)     else:  # cycle is even         state = random_sample_on_state(state, num_samples=100)         state = falsification_sample_on_state(state, num_samples=3)     state = run_eq_arbitrary_on_state(state)     state = pysr_theorist_on_state(state)     print(state.models[-1].sympy()) In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install sweetpea\n</pre> %%capture !pip install sweetpea <p>Define a function to generate trial sequences in sweetpea</p> In\u00a0[\u00a0]: Copied! <pre>from sweetpea import (\n    Factor, DerivedLevel, WithinTrial,\n    Transition, MinimumTrials, CrossBlock,\n    synthesize_trials, CMSGen, experiments_to_dicts\n)\n\n\ndef condition_to_trial_sequence(coherence_movement,\n                                coherence_orientation,\n                                number_blocks=2,\n                                minimum_trials=48):\n    # ** Regular Factors ** #\n    # Define factors with static levels (here, 0 and 180 degree for left and right)\n    direction_movement = Factor('direction_movement', [0, 180])\n    direction_orientation = Factor('direction_orientation', [0, 180])\n\n    # ** Derived Factors ** #\n    # Derived factors are derived from other factors either within (WithinTrials) or across (Transition)\n    # First, we define boolean functions for each level of a derived factor:\n\n    # * Within: Congruency * #\n    # Here, ``congruency` has two levels: `congruent` and `incongruent`\n    # A trial is `congruent` if the direction of the movement is equal to the direction of the orientation\n    def is_congruent(dir_mov, dir_or):\n        return dir_mov == dir_or\n\n    # A trial is `incongruent` if it is not congruent\n    def is_incongruent(dir_mov, dir_or):\n        return not is_congruent(dir_mov, dir_or)\n\n    # The level `congruent` is \"calculated\" by the function `is_congruent`\n    # by passing in the factors `direction_movement` and `direction_orientation`\n    congruent = DerivedLevel('congruent',\n                             WithinTrial(is_congruent,\n                                         [direction_movement, direction_orientation])\n                             )\n    # The level `incongruent` is \"calculated\" by the function `is_incongruent`\n    # by passing in the factors `direction_movement` and `direction_orientation`\n    incongruent = DerivedLevel('incongruent',\n                               WithinTrial(is_incongruent,\n                                           [direction_movement, direction_orientation])\n                               )\n    # The factor congruency is defined by its name and the two levels it can have\n    congruency = Factor('congruency', [congruent, incongruent])\n\n    # * Transition: Congruency Transition * #\n    # We creat the factor `congruency_transition` with four levels (all possible transitions).\n    # First, we define the boolean functions\n    # Here, we index the trials with 0 being the current and -1 being the previous trial:\n    def is_transition_cc(cong):\n        return cong[-1] == 'congruent' and cong[0] == 'congruent'\n\n    def is_transition_ci(cong):\n        return cong[-1] == 'congruent' and cong[0] == 'incongruent'\n\n    def is_transition_ic(cong):\n        return cong[-1] == 'incongruent' and cong[0] == 'congruent'\n\n    def is_transition_ii(cong):\n        return cong[-1] == 'incongruent' and cong[0] == 'incongruent'\n\n    # Name and define the levels with their function and the argument to pass in:\n    transition_cc = DerivedLevel('cc', Transition(is_transition_cc, [congruency]))\n    transition_ci = DerivedLevel('ci', Transition(is_transition_ci, [congruency]))\n    transition_ic = DerivedLevel('ic', Transition(is_transition_ic, [congruency]))\n    transition_ii = DerivedLevel('ii', Transition(is_transition_ii, [congruency]))\n\n    # Name and define the factor with its levels\n    congruency_transition = Factor('congruency_transition',\n                                   [transition_cc, transition_ci, transition_ic, transition_ii])\n\n    # Design: All the factors that define a trial\n    design = [direction_movement, direction_orientation, congruency, congruency_transition]\n    # Crossing: Which factors to cross\n    # In this case,  the crossing of direction_movement and congruency_transition ensures\n    # the crossing of the other factors.\n    crossing = [direction_movement, congruency_transition]\n    # Constraints: SweetPea allows to contraint the sequence (for example how many congruent\n    # should be allowed in a trial). In this case, we only constraint the minimum number of trials\n    constraints = [MinimumTrials(minimum_trials)]\n\n    # Create a block\n    block = CrossBlock(design, crossing, constraints)\n\n    # Synthesize trial sequence using the CMSGen sampler (for more information about the different\n    # samplers, see the SweetPea documentation)\n    experiments = synthesize_trials(block, number_blocks, CMSGen)\n\n    # Format the experiment for the convenient use in jsPsych\n    sequence_list = experiments_to_dicts(block, experiments)\n\n    # For each trial, add the coherence\n    for sequence in sequence_list:\n        for trial in sequence:\n            trial['coherence_movement'] = coherence_movement\n            trial['coherence_orientation'] = coherence_orientation\n    return sequence_list\n</pre> from sweetpea import (     Factor, DerivedLevel, WithinTrial,     Transition, MinimumTrials, CrossBlock,     synthesize_trials, CMSGen, experiments_to_dicts )   def condition_to_trial_sequence(coherence_movement,                                 coherence_orientation,                                 number_blocks=2,                                 minimum_trials=48):     # ** Regular Factors ** #     # Define factors with static levels (here, 0 and 180 degree for left and right)     direction_movement = Factor('direction_movement', [0, 180])     direction_orientation = Factor('direction_orientation', [0, 180])      # ** Derived Factors ** #     # Derived factors are derived from other factors either within (WithinTrials) or across (Transition)     # First, we define boolean functions for each level of a derived factor:      # * Within: Congruency * #     # Here, ``congruency` has two levels: `congruent` and `incongruent`     # A trial is `congruent` if the direction of the movement is equal to the direction of the orientation     def is_congruent(dir_mov, dir_or):         return dir_mov == dir_or      # A trial is `incongruent` if it is not congruent     def is_incongruent(dir_mov, dir_or):         return not is_congruent(dir_mov, dir_or)      # The level `congruent` is \"calculated\" by the function `is_congruent`     # by passing in the factors `direction_movement` and `direction_orientation`     congruent = DerivedLevel('congruent',                              WithinTrial(is_congruent,                                          [direction_movement, direction_orientation])                              )     # The level `incongruent` is \"calculated\" by the function `is_incongruent`     # by passing in the factors `direction_movement` and `direction_orientation`     incongruent = DerivedLevel('incongruent',                                WithinTrial(is_incongruent,                                            [direction_movement, direction_orientation])                                )     # The factor congruency is defined by its name and the two levels it can have     congruency = Factor('congruency', [congruent, incongruent])      # * Transition: Congruency Transition * #     # We creat the factor `congruency_transition` with four levels (all possible transitions).     # First, we define the boolean functions     # Here, we index the trials with 0 being the current and -1 being the previous trial:     def is_transition_cc(cong):         return cong[-1] == 'congruent' and cong[0] == 'congruent'      def is_transition_ci(cong):         return cong[-1] == 'congruent' and cong[0] == 'incongruent'      def is_transition_ic(cong):         return cong[-1] == 'incongruent' and cong[0] == 'congruent'      def is_transition_ii(cong):         return cong[-1] == 'incongruent' and cong[0] == 'incongruent'      # Name and define the levels with their function and the argument to pass in:     transition_cc = DerivedLevel('cc', Transition(is_transition_cc, [congruency]))     transition_ci = DerivedLevel('ci', Transition(is_transition_ci, [congruency]))     transition_ic = DerivedLevel('ic', Transition(is_transition_ic, [congruency]))     transition_ii = DerivedLevel('ii', Transition(is_transition_ii, [congruency]))      # Name and define the factor with its levels     congruency_transition = Factor('congruency_transition',                                    [transition_cc, transition_ci, transition_ic, transition_ii])      # Design: All the factors that define a trial     design = [direction_movement, direction_orientation, congruency, congruency_transition]     # Crossing: Which factors to cross     # In this case,  the crossing of direction_movement and congruency_transition ensures     # the crossing of the other factors.     crossing = [direction_movement, congruency_transition]     # Constraints: SweetPea allows to contraint the sequence (for example how many congruent     # should be allowed in a trial). In this case, we only constraint the minimum number of trials     constraints = [MinimumTrials(minimum_trials)]      # Create a block     block = CrossBlock(design, crossing, constraints)      # Synthesize trial sequence using the CMSGen sampler (for more information about the different     # samplers, see the SweetPea documentation)     experiments = synthesize_trials(block, number_blocks, CMSGen)      # Format the experiment for the convenient use in jsPsych     sequence_list = experiments_to_dicts(block, experiments)      # For each trial, add the coherence     for sequence in sequence_list:         for trial in sequence:             trial['coherence_movement'] = coherence_movement             trial['coherence_orientation'] = coherence_orientation     return sequence_list <p>This function returns a list of trial sequences</p> In\u00a0[\u00a0]: Copied! <pre>condition_to_trial_sequence(.5, .3)\n</pre> condition_to_trial_sequence(.5, .3) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n\ndef trial_list_to_experiment_data(trial_sequence):\n    \"\"\"\n    Parse a trial sequence (from jsPsych) into dependent and independent variables.\n    Independent: coherence_movement, coherence_orientation.\n    Dependent: congruency_effect (rt_incongruent - rt_congruent) / max_response_time.\n    \"\"\"\n    # Initialize a dictionary to collect trial data\n    res_dict = {\n        'coherence_movement': [],\n        'coherence_orientation': [],\n        'congruency': [],\n        'rt': []\n    }\n\n    # Loop through each trial in the sequence\n    for trial in trial_sequence:\n        # Filter out non-relevant trials (not 'rok') and incorrect responses\n        if trial['trial_type'] != 'rok' or 'correct' not in trial or not trial['correct']:\n            continue\n\n        # Determine congruency\n        congruency = trial['coherent_movement_direction'] == trial['coherent_orientation']\n\n        # Normalize coherence values\n        coherence_movement = trial['coherence_movement'] / 100.0\n        coherence_orientation = trial['coherence_orientation'] / 100.0\n\n        # Extract reaction time\n        rt = trial['rt']\n\n        # Append values to the dictionary\n        res_dict['congruency'].append(int(congruency))\n        res_dict['coherence_movement'].append(coherence_movement)\n        res_dict['coherence_orientation'].append(coherence_orientation)\n        res_dict['rt'].append(rt)\n\n    # Convert the dictionary to a DataFrame\n    dataframe_raw = pd.DataFrame(res_dict)\n\n    # Group by coherence_movement and coherence_orientation\n    grouped = dataframe_raw.groupby(['coherence_movement', 'coherence_orientation'])\n\n    # Calculate mean reaction times for congruent and incongruent trials\n    mean_rt = grouped.apply(lambda x: pd.Series({\n        'mean_rt_congruent': x[x['congruency'] == 1]['rt'].mean(),\n        'mean_rt_incongruent': x[x['congruency'] == 0]['rt'].mean()\n    }, index=['mean_rt_congruent', 'mean_rt_incongruent']), include_groups=False).reset_index()\n\n    # Calculate the congruency effect\n    max_response_time = 2000  # Assuming max response time is 2000ms\n    mean_rt['congruency_effect'] = (mean_rt['mean_rt_incongruent'] - mean_rt['mean_rt_congruent']) / max_response_time\n\n    # Return the relevant columns\n    return mean_rt[['coherence_movement', 'coherence_orientation', 'congruency_effect']]\n</pre> import pandas as pd   def trial_list_to_experiment_data(trial_sequence):     \"\"\"     Parse a trial sequence (from jsPsych) into dependent and independent variables.     Independent: coherence_movement, coherence_orientation.     Dependent: congruency_effect (rt_incongruent - rt_congruent) / max_response_time.     \"\"\"     # Initialize a dictionary to collect trial data     res_dict = {         'coherence_movement': [],         'coherence_orientation': [],         'congruency': [],         'rt': []     }      # Loop through each trial in the sequence     for trial in trial_sequence:         # Filter out non-relevant trials (not 'rok') and incorrect responses         if trial['trial_type'] != 'rok' or 'correct' not in trial or not trial['correct']:             continue          # Determine congruency         congruency = trial['coherent_movement_direction'] == trial['coherent_orientation']          # Normalize coherence values         coherence_movement = trial['coherence_movement'] / 100.0         coherence_orientation = trial['coherence_orientation'] / 100.0          # Extract reaction time         rt = trial['rt']          # Append values to the dictionary         res_dict['congruency'].append(int(congruency))         res_dict['coherence_movement'].append(coherence_movement)         res_dict['coherence_orientation'].append(coherence_orientation)         res_dict['rt'].append(rt)      # Convert the dictionary to a DataFrame     dataframe_raw = pd.DataFrame(res_dict)      # Group by coherence_movement and coherence_orientation     grouped = dataframe_raw.groupby(['coherence_movement', 'coherence_orientation'])      # Calculate mean reaction times for congruent and incongruent trials     mean_rt = grouped.apply(lambda x: pd.Series({         'mean_rt_congruent': x[x['congruency'] == 1]['rt'].mean(),         'mean_rt_incongruent': x[x['congruency'] == 0]['rt'].mean()     }, index=['mean_rt_congruent', 'mean_rt_incongruent']), include_groups=False).reset_index()      # Calculate the congruency effect     max_response_time = 2000  # Assuming max response time is 2000ms     mean_rt['congruency_effect'] = (mean_rt['mean_rt_incongruent'] - mean_rt['mean_rt_congruent']) / max_response_time      # Return the relevant columns     return mean_rt[['coherence_movement', 'coherence_orientation', 'congruency_effect']] <p>For example:</p> In\u00a0[\u00a0]: Copied! <pre>jsPsych_trial_data = [\n    {'rt': 1500, 'correct': 1, 'trial_type': 'rok',\n     'coherent_movement_direction': 0, 'coherent_orientation': 180,\n     'coherence_movement': 20, 'coherence_orientation': 80},\n    {'rt': 500, 'correct': 0, 'trial_type': 'rok',\n     'coherent_movement_direction': 180, 'coherent_orientation': 0,\n     'coherence_movement': 20, 'coherence_orientation': 80},\n    {'rt': 900, 'correct': 1, 'trial_type': 'rok',\n     'coherent_movement_direction': 180, 'coherent_orientation': 180,\n     'coherence_movement': 20, 'coherence_orientation': 80},\n    {'rt': 700, 'correct': 1, 'trial_type': 'rok',\n     'coherent_movement_direction': 0, 'coherent_orientation': 0,\n     'coherence_movement': 20, 'coherence_orientation': 80}\n]\ntrial_list_to_experiment_data(jsPsych_trial_data)\n</pre> jsPsych_trial_data = [     {'rt': 1500, 'correct': 1, 'trial_type': 'rok',      'coherent_movement_direction': 0, 'coherent_orientation': 180,      'coherence_movement': 20, 'coherence_orientation': 80},     {'rt': 500, 'correct': 0, 'trial_type': 'rok',      'coherent_movement_direction': 180, 'coherent_orientation': 0,      'coherence_movement': 20, 'coherence_orientation': 80},     {'rt': 900, 'correct': 1, 'trial_type': 'rok',      'coherent_movement_direction': 180, 'coherent_orientation': 180,      'coherence_movement': 20, 'coherence_orientation': 80},     {'rt': 700, 'correct': 1, 'trial_type': 'rok',      'coherent_movement_direction': 0, 'coherent_orientation': 0,      'coherence_movement': 20, 'coherence_orientation': 80} ] trial_list_to_experiment_data(jsPsych_trial_data) In\u00a0[\u00a0]: Copied! <pre>firebase_credentials = {\n    \"type\": \"type\",\n    \"project_id\": \"project_id\",\n    \"private_key_id\": \"private_key_id\",\n    \"private_key\": \"private_key\",\n    \"client_email\": \"client_email\",\n    \"client_id\": \"client_id\",\n    \"auth_uri\": \"auth_uri\",\n    \"token_uri\": \"token_uri\",\n    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n    \"client_x509_cert_url\": \"client_x509_cert_url\"\n}\n</pre> firebase_credentials = {     \"type\": \"type\",     \"project_id\": \"project_id\",     \"private_key_id\": \"private_key_id\",     \"private_key\": \"private_key\",     \"client_email\": \"client_email\",     \"client_id\": \"client_id\",     \"auth_uri\": \"auth_uri\",     \"token_uri\": \"token_uri\",     \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",     \"client_x509_cert_url\": \"client_x509_cert_url\" } <p>Install the runner</p> In\u00a0[\u00a0]: Copied! <pre>%%capture\n!pip install \"autora[experiment-runner-firebase-prolific]\"\n</pre> %%capture !pip install \"autora[experiment-runner-firebase-prolific]\" <p>Initialize the runner and wrap it on state:</p> In\u00a0[\u00a0]: Copied! <pre>import json\n\nfrom autora.experiment_runner.firebase_prolific import firebase_runner\n\nexperiment_runner_firebase = firebase_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=100,\n    sleep_time=5)\n\n\n@on_state()\ndef run_firebase_on_state(conditions, num_blocks, num_trials):\n    trial_sequences = []\n    # Loop through the rows of the condition DataFrame and create a trial sequence for each\n    for _, row in conditions.iterrows():\n        trial_sequences.append(condition_to_trial_sequence(\n            row['coherence_movement'],\n            row['coherence_orientation'],\n            num_blocks,\n            num_trials))\n\n    # Run the experiment online and wait for the data of num_samples participants\n    # (head over to the website given in the firebase console and try the experiment)\n    data_raw = experiment_runner_firebase(trial_sequences)\n    # initialize experiment data as DataFrame\n    experiment_data = pd.DataFrame()\n    # data raw is a list of trial sequences from jsPsych including all trials, also fixation cross, instructions, ... for each participant\n    for item in data_raw:\n        # process the data\n        _lst = json.loads(item)['trials']\n        _df = trial_list_to_experiment_data(_lst)\n        # append the data to the experiment data\n        experiment_data = pd.concat([experiment_data, _df], axis=0)\n    return Delta(experiment_data=experiment_data)\n</pre> import json  from autora.experiment_runner.firebase_prolific import firebase_runner  experiment_runner_firebase = firebase_runner(     firebase_credentials=firebase_credentials,     time_out=100,     sleep_time=5)   @on_state() def run_firebase_on_state(conditions, num_blocks, num_trials):     trial_sequences = []     # Loop through the rows of the condition DataFrame and create a trial sequence for each     for _, row in conditions.iterrows():         trial_sequences.append(condition_to_trial_sequence(             row['coherence_movement'],             row['coherence_orientation'],             num_blocks,             num_trials))      # Run the experiment online and wait for the data of num_samples participants     # (head over to the website given in the firebase console and try the experiment)     data_raw = experiment_runner_firebase(trial_sequences)     # initialize experiment data as DataFrame     experiment_data = pd.DataFrame()     # data raw is a list of trial sequences from jsPsych including all trials, also fixation cross, instructions, ... for each participant     for item in data_raw:         # process the data         _lst = json.loads(item)['trials']         _df = trial_list_to_experiment_data(_lst)         # append the data to the experiment data         experiment_data = pd.concat([experiment_data, _df], axis=0)     return Delta(experiment_data=experiment_data) <p>Now, we can replace the synthetic runner from earlier with the online firebase runner:</p> In\u00a0[\u00a0]: Copied! <pre>state = StandardState(variables=variables)\nfor cycle in range(4):\n    state = grid_pool_on_state(state)\n    if not cycle % 2:  # cycle is odd\n        state = random_sample_on_state(state, num_samples=3)\n    else:  # cycle is even\n        state = random_sample_on_state(state, num_samples=100)\n        state = falsification_sample_on_state(state, num_samples=3)\n    # state = run_eq_arbitrary_on_state(state)\n    state = run_firebase_on_state(state, num_blocks=2, num_trials=48)\n    state = pysr_theorist_on_state(state)\n    print(state.models[-1].sympy())\n</pre> state = StandardState(variables=variables) for cycle in range(4):     state = grid_pool_on_state(state)     if not cycle % 2:  # cycle is odd         state = random_sample_on_state(state, num_samples=3)     else:  # cycle is even         state = random_sample_on_state(state, num_samples=100)         state = falsification_sample_on_state(state, num_samples=3)     # state = run_eq_arbitrary_on_state(state)     state = run_firebase_on_state(state, num_blocks=2, num_trials=48)     state = pysr_theorist_on_state(state)     print(state.models[-1].sympy()) In\u00a0[\u00a0]: Copied! <pre>from autora.experiment_runner.firebase_prolific import firebase_prolific_runner\n\nYOUR_PROLIFIC_TOKEN = 'your_token'\n\nexperiment_runner_prolific = firebase_prolific_runner(\n    firebase_credentials=firebase_credentials,\n    sleep_time=10,\n    study_name='Triangle Chaos',\n    study_description='Reaction time experiment in under 1 minute.',\n    study_url='your_url',\n    study_completion_time= 1,\n    prolific_token=YOUR_PROLIFIC_TOKEN\n)\n\n# The run function is the same with a different runner:\n@on_state()\ndef run_prolific_on_state(conditions, num_blocks, num_trials):\n    trial_sequences = []\n    for _, row in conditions.iterrows():\n        trial_sequences.append(condition_to_trial_sequence(\n            row['coherence_movement'],\n            row['coherence_orientation'],\n            num_blocks,\n            num_trials))\n    data_raw = experiment_runner_prolific(trial_sequences)\n    experiment_data = pd.DataFrame()\n    for item in data_raw:\n        _lst = json.loads(item)['trials']\n        _df = trial_list_to_experiment_data(_lst)\n        experiment_data = pd.concat([experiment_data, _df], axis=0)\n    return Delta(experiment_data=experiment_data)\n</pre> from autora.experiment_runner.firebase_prolific import firebase_prolific_runner  YOUR_PROLIFIC_TOKEN = 'your_token'  experiment_runner_prolific = firebase_prolific_runner(     firebase_credentials=firebase_credentials,     sleep_time=10,     study_name='Triangle Chaos',     study_description='Reaction time experiment in under 1 minute.',     study_url='your_url',     study_completion_time= 1,     prolific_token=YOUR_PROLIFIC_TOKEN )  # The run function is the same with a different runner: @on_state() def run_prolific_on_state(conditions, num_blocks, num_trials):     trial_sequences = []     for _, row in conditions.iterrows():         trial_sequences.append(condition_to_trial_sequence(             row['coherence_movement'],             row['coherence_orientation'],             num_blocks,             num_trials))     data_raw = experiment_runner_prolific(trial_sequences)     experiment_data = pd.DataFrame()     for item in data_raw:         _lst = json.loads(item)['trials']         _df = trial_list_to_experiment_data(_lst)         experiment_data = pd.concat([experiment_data, _df], axis=0)     return Delta(experiment_data=experiment_data) <p>Now, we can replace the firebase runner from earlier with the online firebase runner with prolific recruitment. Here, we also store the state to save the data on disk:</p> In\u00a0[\u00a0]: Copied! <pre>from autora.serializer import dump_state\nfrom pathlib import Path\n\nstate = StandardState(variables=variables)\nfor cycle in range(4):\n    state = grid_pool_on_state(state)\n    if not cycle % 2:  # cycle is odd\n        state = random_sample_on_state(state, num_samples=3)\n    else:  # cycle is even\n        state = random_sample_on_state(state, num_samples=100)\n        state = falsification_sample_on_state(state, num_samples=3)\n    # state = run_firebase_on_state(state, num_blocks=2, num_trials=48)\n    state = pysr_theorist_on_state(state)\n    dump_state(state, Path('./prolific_state.pkl'))\n    print(state.models[-1].sympy())\n</pre> from autora.serializer import dump_state from pathlib import Path  state = StandardState(variables=variables) for cycle in range(4):     state = grid_pool_on_state(state)     if not cycle % 2:  # cycle is odd         state = random_sample_on_state(state, num_samples=3)     else:  # cycle is even         state = random_sample_on_state(state, num_samples=100)         state = falsification_sample_on_state(state, num_samples=3)     # state = run_firebase_on_state(state, num_blocks=2, num_trials=48)     state = pysr_theorist_on_state(state)     dump_state(state, Path('./prolific_state.pkl'))     print(state.models[-1].sympy())"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#mathematical-model-discovery-for-a-psychophysics-experiment","title":"Mathematical Model Discovery For a Psychophysics Experiment\u00b6","text":"<p>In this tutorial, we will explore how to use AutoRA for automatic mathematical model discovery. We'll start by using a synthetic runner, then transition to a runner that conducts the experiment on a website. This allows us to test the experiment both locally and online. Next, we'll demonstrate how to use a participant recruiter to gather participants from Prolific.</p> <p>To follow along with this tutorial, you should first set up an AutoRA project via Cookiecutter. To do so, follow the tutorial here: Closed Loop Online Experiment. Choose the advanced setting and the example \"Mathematical Model Discovery.\" This will allow you to easily deploy a website. The generated project also contains an AutoRA script generated, which includes the finished Prolific runner version we will create step-by-step in this tutorial.</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#experiment-paradigm","title":"Experiment Paradigm\u00b6","text":"<p>As experiment paradigm, we are going to use the Random Object Kinematogram. In our version of the experiment, the participant sees oriented triangles moving. The participant has to react to the orientation as quickly and as accurately as possible.</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#sweetpea-counterbalanced-trial-sequences","title":"SweetPea: Counterbalanced Trial Sequences\u00b6","text":"<p>From the sampled conditions (movement coherence and orientation coherence), we will create a trial sequence. A trial sequence is a list of trial features that contain more information than just the coherences. For example, the direction of the movement and orientation. Here we use SweetPea to create such trial sequences. Using SweetPea has several advantages, such as allowing us to counterbalance factors like direction and transitions. For example, we aim for congruent and incongruent conditions to appear equally often, and also to balance their transitions (congruent to congruent, congruent to incongruent, incongruent to congruent, and incongruent to incongruent). This counterbalancing ensures that our trial sequences are well-structured and that each condition and transition type is adequately represented.</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#autora-core-defining-the-variables","title":"AutoRA-Core: Defining the Variables\u00b6","text":"<p>To define an AutoRA experiment, we first need to define the variables and a variable collection. In AutoRA, we can name the variables and define value ranges. These value ranges can be used in other AutoRA-components. For example, the allowed value range is used in grid poolers to determine a grid of possible experiment conditions.</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#autora-core-state","title":"AutoRA-Core: State\u00b6","text":"<p>An AutoRA workflow is state-based. This means each component runs on the same state object. The state object can have multiple fields that get altered via the AutoRA component. For example, an experimentalist typically populates the <code>conditions</code>-field, an experiment-runner typically populated the <code>experiment-data</code>-field, and a theorist typically populates the <code>models</code>-field. The user can define custom state fields, but here we use the <code>StandardState</code> that consists of a <code>variable</code>, a <code>conditions</code>, an <code>experiment-data</code> and a <code>models</code>-field:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#autora-component-random-experimentalist","title":"AutoRA-Component: Random Experimentalist\u00b6","text":"<p>Here, we use a random experimentalist as first AutoRA component. This experimentalist only needs the variable definition and creates experimental conditions. Later in this tutorial, we will see more sophisticated experimentalists, that use active learning methods to determine experimental conditions based on model predictions.</p> <p>Here, we also are introduced to the <code>on_state</code> and <code>Delta</code> mechanic. To use any function on the state, we wrap it with the <code>on_state</code> functionality.</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#autora-component-experiment-runner-synthetic","title":"AutoRA-Component: Experiment Runner (Synthetic)\u00b6","text":"<p>Before running an experiment online, it is often a good idea to simulate data. This might be beneficial not only for debugging but also to sanity check predictions or computational models. Here, we use an equation runner to run the experiment on predefined equations.</p> <p>The equation-runner is not part of the core AutoRA package and needs to be installed separately:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#autora-component-theorist-pysr-regressor","title":"AutoRA-Component: Theorist (PySR-Regressor)\u00b6","text":"<p>To analyse the data, we use symbolic regression, a method that searches a mathematical equation that best fits the data. Specifically, we use a PySR.</p> <p>First, we install pysr:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#closed-loop-random-sampling-synthetic-runner-pysr","title":"Closed Loop: Random Sampling + Synthetic Runner + PySR\u00b6","text":"<p>After creating the functions, we can now run them in a closed loop:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#active-learning-falsification-experimentalist","title":"Active Learning - Falsification Experimentalist\u00b6","text":"<p>The full potential of a closed loop is realized when active learning strategies are employed to select the conditions to probe, as opposed to random sampling. In this context, we are using a falsification sampler, which identifies novel experimental conditions where the loss of the best candidate model is predicted to be the highest. This sampler is part of the AutoRA ecosystem:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#online-experiment-firebase-experiment-runner","title":"Online Experiment - Firebase Experiment Runner\u00b6","text":"<p>After testing the experimental strategy and the theorist on synthetic data, we can use experiment runner that collects data from a website. Here, we assume that you have set up a website on firebase (for example by following the tutorial here: Closed Loop Online Experiment and chose the example \"Mathematical Model Discovery\".</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#generating-a-trial-sequence","title":"Generating a Trial Sequence\u00b6","text":"<p>Instead of conditions (coherence of movement and orientation), we generate trial sequences to send to Firebase. The format of the conditions depends on the experiment website. Here, we create a trial sequence and use it as timeline variable for a jsPsych experiment (see, jsPsych - timelineVariables) To generate a (counterbalanced) trial sequence, we use SweetPea. For more information on SweetPea and tutorials, see SweetPea - Tutorials.</p> <p>The following code creates sequences with counterbalanced directions, counterbalanced congruency, and counterbalanced congruency transitions.</p> <p>Installation:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#postprocessing-data","title":"Postprocessing Data\u00b6","text":"<p>Here, we will get the data in json format from jsPsych as list of trials with trial data and response data. We need to process this data to <code>autora experiment_data</code>. This is a pandas DataFrame with columns for the independent and dependent variables.</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#runner","title":"Runner\u00b6","text":"<p>With these two functions in place, we can create a firebase runner. Here, we also need Firebase credentials (found in the project settings of your firebase project.</p> <p>Credentials:</p>"},{"location":"user-cookiecutter/docs/examples/Mathematical%20Model%20Discovery/#online-experiment-prolific-recruitment-experiment-runner","title":"Online Experiment - Prolific Recruitment Experiment Runner\u00b6","text":"<p>After testing the online experiment with the firebase runner, we can add prolific recruitment. For this, we need to make sure that the experiment is deployed online, and we have an url for it. We also will need an api token from Prolific and enough money on our account to run the experiment.</p> <p>Instead of using the firebase runner, we use the firebase_prolific_runner which expects additional argument:</p> <p>firebase_credentials: a dict with firebase service account credentials sleep_time: the time between checks to the firebase database and updates of the prolific experiment study_name: a name for the study showing up in prolific study_description: a description for the study showing up in prolific study_url: the url to your experiment study_completion_time: the average completion time for a participant to complete the study prolific_token: api token from prolific</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/","title":"Firebase Experimentation Manager","text":"<p>Firebase Experimentation Manager provides functionality to manage communication of conditions and observations between AutoRA and an experiment on Firebase.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/SweetPea/","title":"SweetPea","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install sweetpea\n!pip install \"autora[experiment-runner-experimentation-manager-firebase]\"\n</pre> !pip install sweetpea !pip install \"autora[experiment-runner-experimentation-manager-firebase]\" <p>First we are going to create a counterbalanced stroop trial sequence (additional tutorials on how to use SweetPea can be found here: Tutorials)</p> In\u00a0[\u00a0]: Copied! <pre># IMPORTS:\nfrom sweetpea import (\n    Factor, DerivedLevel, WithinTrial,\n    CrossBlock, synthesize_trials,\n    CMSGen, MinimumTrials\n)\n\n# DEFINE COLOR AND WORD FACTORS\ncolor      = Factor(\"color\",  [\"red\", \"blue\", \"green\", \"brown\"])\nword       = Factor(\"word\", [\"red\", \"blue\", \"green\", \"brown\"])\n\n# DEFINE CONGRUENCY FACTOR\n\ndef congruent(color, word):\n    return color == word\n\ndef incongruent(color, word):\n    return not congruent(color, word)\n\n\nconLevel = DerivedLevel(\"con\", WithinTrial(congruent,   [color, word]))\nincLevel = DerivedLevel(\"inc\", WithinTrial(incongruent,   [color, word]))\n\ncongruency = Factor(\"congruency\", [\n    conLevel,\n    incLevel\n])\n\n# DEFINE EXPERIMENT\ndesign       = [color, word, congruency]\ncrossing     = [congruency]\nblock        = CrossBlock(design, crossing, [MinimumTrials(48)])\n\n# SYNTHESIZE 5 TRIAL SEQUENCES\n\nexperiments  = synthesize_trials(block, 5, CMSGen)\n</pre> # IMPORTS: from sweetpea import (     Factor, DerivedLevel, WithinTrial,     CrossBlock, synthesize_trials,     CMSGen, MinimumTrials )  # DEFINE COLOR AND WORD FACTORS color      = Factor(\"color\",  [\"red\", \"blue\", \"green\", \"brown\"]) word       = Factor(\"word\", [\"red\", \"blue\", \"green\", \"brown\"])  # DEFINE CONGRUENCY FACTOR  def congruent(color, word):     return color == word  def incongruent(color, word):     return not congruent(color, word)   conLevel = DerivedLevel(\"con\", WithinTrial(congruent,   [color, word])) incLevel = DerivedLevel(\"inc\", WithinTrial(incongruent,   [color, word]))  congruency = Factor(\"congruency\", [     conLevel,     incLevel ])  # DEFINE EXPERIMENT design       = [color, word, congruency] crossing     = [congruency] block        = CrossBlock(design, crossing, [MinimumTrials(48)])  # SYNTHESIZE 5 TRIAL SEQUENCES  experiments  = synthesize_trials(block, 5, CMSGen) <p>Let's see, what the experiment object is:</p> In\u00a0[\u00a0]: Copied! <pre>print('All sequences:')\nprint(experiments)\nprint('One sequence:')\nprint(experiments[0])\n</pre> print('All sequences:') print(experiments) print('One sequence:') print(experiments[0]) <p>For the firebase experiments, it is more convenient to have lists of trials instead of feature list:</p> In\u00a0[\u00a0]: Copied! <pre>firebase_formatted = []\nfor el in experiments:\n    trials = []\n    for color, word, congruency in zip(el[\"color\"], el[\"word\"], el[\"congruency\"]):\n        trial = {\"color\": color, \"word\": word, \"congruency\": congruency}\n        trials.append(trial)\n    firebase_formatted.append(trials)\nprint(firebase_formatted[0])\n</pre> firebase_formatted = [] for el in experiments:     trials = []     for color, word, congruency in zip(el[\"color\"], el[\"word\"], el[\"congruency\"]):         trial = {\"color\": color, \"word\": word, \"congruency\": congruency}         trials.append(trial)     firebase_formatted.append(trials) print(firebase_formatted[0]) In\u00a0[\u00a0]: Copied! <pre>from autora.experiment_runner.experimentation_manager.firebase import send_conditions\nhelp(send_conditions)\n</pre> from autora.experiment_runner.experimentation_manager.firebase import send_conditions help(send_conditions) <p>collection_name: is a name that we choose (here we choose autora) conditions: the conditions (in this case the trial sequences) firebase_credentials: a dict with the credentials for firebase (found here: Firebase under project-settings -&gt; service accounts -&gt; generate key</p> In\u00a0[\u00a0]: Copied! <pre>collection_name = 'autora'\nconditions = firebase_formatted\nfirebase_credentials = {\n  \"type\": \"\",\n  \"project_id\": \"\",\n  \"private_key_id\": \"\",\n  \"private_key\": \"\",\n  \"client_email\": \"\",\n  \"client_id\": \"\",\n  \"auth_uri\": \"\",\n  \"token_uri\": \"\",\n  \"auth_provider_x509_cert_url\": \"\",\n  \"client_x509_cert_url\": \"\"\n}\n</pre> collection_name = 'autora' conditions = firebase_formatted firebase_credentials = {   \"type\": \"\",   \"project_id\": \"\",   \"private_key_id\": \"\",   \"private_key\": \"\",   \"client_email\": \"\",   \"client_id\": \"\",   \"auth_uri\": \"\",   \"token_uri\": \"\",   \"auth_provider_x509_cert_url\": \"\",   \"client_x509_cert_url\": \"\" } In\u00a0[\u00a0]: Copied! <pre>send_conditions(collection_name=collection_name, conditions=conditions, firebase_credentials=firebase_credentials)\n</pre> send_conditions(collection_name=collection_name, conditions=conditions, firebase_credentials=firebase_credentials) <p>Check out your Firebase in the browser. The conditions were added.</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experiment_runner.experimentation_manager.firebase import check_firebase_status\n\nstatus = check_firebase_status(collection_name=collection_name, firebase_credentials=firebase_credentials, time_out=100)\nprint(status)\n</pre> from autora.experiment_runner.experimentation_manager.firebase import check_firebase_status  status = check_firebase_status(collection_name=collection_name, firebase_credentials=firebase_credentials, time_out=100) print(status) <p>This gives you a string (unavailable, available or finished):</p> <ul> <li>If all conditions are in use but not finished the function returns <code>unavailable</code> (the time_out governs how long a participant is allowed to take part in the experiment till the condition gets freed for the next user)</li> <li>If there are available spots (meaning there are conditions not started yet or there are conditions that have been timed out), the function returns <code>available</code></li> <li>If all conditions are used and the data for these conditions has been collected, the function returns <code>finished</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>from autora.experiment_runner.experimentation_manager.firebase import get_observations\n\nobservations = get_observations(collection_name=collection_name, firebase_credentials=firebase_credentials)\nprint(observations)\n</pre> from autora.experiment_runner.experimentation_manager.firebase import get_observations  observations = get_observations(collection_name=collection_name, firebase_credentials=firebase_credentials) print(observations)"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/SweetPea/#sweetpea-with-firebase","title":"SweetPea With Firebase\u00b6","text":"<p>Here we use autora to upload counterbalanced trial sequences to an experiment hosted with Firebase. You can find a tutorial on how to set up a firebase experiment, that is configured to work with AutoRA here: Firebase Tutorial</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/SweetPea/#create-the-trial-sequences","title":"Create The Trial Sequences\u00b6","text":""},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/SweetPea/#upload-to-firebase","title":"Upload To Firebase\u00b6","text":"<p>Let's import the send_condition from the experimentation_manager and see what arguments are expected:</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/SweetPea/#check-firebase-status","title":"Check Firebase Status\u00b6","text":"<p>To check the status of the experiment you can use the function <code>check_status</code> (this is helpful to build a closed loop, where a <code>autora-recruitment-manager</code> starts and pauses the recruitment based on the status of the experiment, while the <code>autora-experimentation-manager</code> retrieves the observations and sends new conditions from the <code>autora-experimentalist</code> when data-collection is finished.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/SweetPea/#get-observations","title":"Get Observations\u00b6","text":"<p>To download the observations from the Firestore databse, we can use the <code>get_observations</code> function:</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-experimentation-manager/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Firebase Experimentation Manager is a part of the <code>autora</code> package:</p> <pre><code>pip install -U \"aurora[experiment-runner-experimentation-manager-firebase]\"\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experiment_runner.experimentation_manager.firebase import send_conditions\"\n</code></pre></p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/","title":"Firebase Prolific Experiment Runner","text":"<p>Firebase Prolific Runner provides runners to run experiments with Firebase and Prolific</p> <p>WARNING: The firebase prolific runner creates an experiment on prolific and runs recruits participants automatically. This is an early alpha version and should be used with extreme caution.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\n</pre>     import numpy as np import pandas as pd  In\u00a0[\u00a0]: Copied! <pre>from autora.state import StandardState, on_state, Delta\nfrom autora.variable import VariableCollection, Variable\n\n# *** Set up variables *** #\n# independent variable is coherence in percent (0 - 100)\n# dependent variable is accuracy (0 - 1)\nvariables = VariableCollection(\n    independent_variables=[Variable(name=\"coherence\", allowed_values=np.linspace(0, 1, 101))],\n    dependent_variables=[Variable(name=\"accuracy\", value_range=(0, 1))])\n\n# *** State *** #\n# With the variables, we can set up a state. The state object represents the state of our\n# closed loop experiment.\n\nstate = StandardState(\n    variables=variables,\n)\n</pre> from autora.state import StandardState, on_state, Delta from autora.variable import VariableCollection, Variable  # *** Set up variables *** # # independent variable is coherence in percent (0 - 100) # dependent variable is accuracy (0 - 1) variables = VariableCollection(     independent_variables=[Variable(name=\"coherence\", allowed_values=np.linspace(0, 1, 101))],     dependent_variables=[Variable(name=\"accuracy\", value_range=(0, 1))])  # *** State *** # # With the variables, we can set up a state. The state object represents the state of our # closed loop experiment.  state = StandardState(     variables=variables, ) In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\n# ** Theorist ** #\ntheorist = LinearRegression()\n\n# To use the theorist on the state object, we wrap it with the on_state functionality and return a\n# Delta object.\n# Note: The if the input arguments of the theorist_on_state function are state-fields like\n# experiment_data, variables, ... , then using this function on a state object will automatically\n# use those state fields.\n# The output of these functions is always a Delta object. The keyword argument in this case, tells\n# the state object witch field to update.\n\n\n@on_state()\ndef theorist_on_state(experiment_data, variables):\n    ivs = [iv.name for iv in variables.independent_variables]\n    dvs = [dv.name for dv in variables.dependent_variables]\n    x = experiment_data[ivs]\n    y = experiment_data[dvs]\n    return Delta(models=[theorist.fit(x, y)])\n</pre> from sklearn.linear_model import LinearRegression # ** Theorist ** # theorist = LinearRegression()  # To use the theorist on the state object, we wrap it with the on_state functionality and return a # Delta object. # Note: The if the input arguments of the theorist_on_state function are state-fields like # experiment_data, variables, ... , then using this function on a state object will automatically # use those state fields. # The output of these functions is always a Delta object. The keyword argument in this case, tells # the state object witch field to update.   @on_state() def theorist_on_state(experiment_data, variables):     ivs = [iv.name for iv in variables.independent_variables]     dvs = [dv.name for dv in variables.dependent_variables]     x = experiment_data[ivs]     y = experiment_data[dvs]     return Delta(models=[theorist.fit(x, y)]) In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.random import pool\n# ** Experimentalist ** #\n@on_state()\ndef experimentalist_on_state(variables, num_samples):\n    return Delta(conditions=pool(variables, num_samples))\n</pre> from autora.experimentalist.random import pool # ** Experimentalist ** # @on_state() def experimentalist_on_state(variables, num_samples):     return Delta(conditions=pool(variables, num_samples)) In\u00a0[\u00a0]: Copied! <pre># We will need json to parse the date from the runner\nimport json\nfrom autora.experiment_runner.firebase_prolific import firebase_prolific_runner\n\n# We will run our experiment on firebase and need credentials. You will find them here:\n# (https://console.firebase.google.com/)\n#   -&gt; project -&gt; project settings -&gt; service accounts -&gt; generate new private key\n\nfirebase_credentials = {\n    \"type\": \"type\",\n    \"project_id\": \"project_id\",\n    \"private_key_id\": \"private_key_id\",\n    \"private_key\": \"private_key\",\n    \"client_email\": \"client_email\",\n    \"client_id\": \"client_id\",\n    \"auth_uri\": \"auth_uri\",\n    \"token_uri\": \"token_uri\",\n    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n    \"client_x509_cert_url\": \"client_x509_cert_url\"\n}\n\n# Sleep time (seconds): The time between checks to the firebase database and updates of the prolific experiment\nsleep_time = 30\n\n# Study name: This will be the name that will appear on prolific, participants that have participated in a study with the same name will be\n# excluded automatically\nstudy_name = 'my autora experiment'\n\n# Study description: This will appear as study description on prolific\nstudy_description= 'RDK experiment'\n\n# Study Url: The url of your study (you can find this in the Firebase Console)\nstudy_url = 'www.my-autora-experiment.com'\n\n# Study completion time (minutes): The estimated time a participant will take to finish your study. We use the compensation suggested by Prolific to calculate how much a participant will earn based on the completion time.\nstudy_completion_time = 5\n\n# Prolific Token: You can generate a token on your Prolific account\nprolific_token = 'my prolific token'\n\n# Completion code: The code a participant gets to prove they participated. If you are using the standard set up (with cookiecutter), please make sure this is the same code that you have providede in the .env file of the testing zone.\ncompletion_code = 'my completion code'\n\n# Exclude Studies\n\n# simple experiment runner that runs the experiment on firebase\nexperiment_runner = firebase_prolific_runner(\n    firebase_credentials=firebase_credentials,\n    time_out=100,\n    sleep_time=5)\n\nexperiment_runner = firebase_prolific_runner(\n            firebase_credentials=firebase_credentials,\n            sleep_time=sleep_time,\n            study_name=study_name,\n            study_description=study_description,\n            study_url=study_url,\n            study_completion_time=study_completion_time,\n            prolific_token=prolific_token,\n            completion_code=completion_code,\n        )\n\n\n# We need to wrap the runner to use it on the state. Here, we send the raw conditions.\n@on_state()\ndef runner_on_state(conditions):\n    data = experiment_runner(conditions)\n    # We parse the return value of the runner. The return value depends on the specific\n    # implementation of your online experiment (see testing_zone/src/design/main.js).\n    # In this example, the experiment runner returns a list of strings, that contain json formatted\n    # dictionaries.\n    # Example:\n    # data = ['{'coherence':.3, accuracy':.8}', ...]\n    result = []\n    for item in data:\n        result.append(json.loads(item))\n    return Delta(experiment_data=pd.DataFrame(result))\n</pre> # We will need json to parse the date from the runner import json from autora.experiment_runner.firebase_prolific import firebase_prolific_runner  # We will run our experiment on firebase and need credentials. You will find them here: # (https://console.firebase.google.com/) #   -&gt; project -&gt; project settings -&gt; service accounts -&gt; generate new private key  firebase_credentials = {     \"type\": \"type\",     \"project_id\": \"project_id\",     \"private_key_id\": \"private_key_id\",     \"private_key\": \"private_key\",     \"client_email\": \"client_email\",     \"client_id\": \"client_id\",     \"auth_uri\": \"auth_uri\",     \"token_uri\": \"token_uri\",     \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",     \"client_x509_cert_url\": \"client_x509_cert_url\" }  # Sleep time (seconds): The time between checks to the firebase database and updates of the prolific experiment sleep_time = 30  # Study name: This will be the name that will appear on prolific, participants that have participated in a study with the same name will be # excluded automatically study_name = 'my autora experiment'  # Study description: This will appear as study description on prolific study_description= 'RDK experiment'  # Study Url: The url of your study (you can find this in the Firebase Console) study_url = 'www.my-autora-experiment.com'  # Study completion time (minutes): The estimated time a participant will take to finish your study. We use the compensation suggested by Prolific to calculate how much a participant will earn based on the completion time. study_completion_time = 5  # Prolific Token: You can generate a token on your Prolific account prolific_token = 'my prolific token'  # Completion code: The code a participant gets to prove they participated. If you are using the standard set up (with cookiecutter), please make sure this is the same code that you have providede in the .env file of the testing zone. completion_code = 'my completion code'  # Exclude Studies  # simple experiment runner that runs the experiment on firebase experiment_runner = firebase_prolific_runner(     firebase_credentials=firebase_credentials,     time_out=100,     sleep_time=5)  experiment_runner = firebase_prolific_runner(             firebase_credentials=firebase_credentials,             sleep_time=sleep_time,             study_name=study_name,             study_description=study_description,             study_url=study_url,             study_completion_time=study_completion_time,             prolific_token=prolific_token,             completion_code=completion_code,         )   # We need to wrap the runner to use it on the state. Here, we send the raw conditions. @on_state() def runner_on_state(conditions):     data = experiment_runner(conditions)     # We parse the return value of the runner. The return value depends on the specific     # implementation of your online experiment (see testing_zone/src/design/main.js).     # In this example, the experiment runner returns a list of strings, that contain json formatted     # dictionaries.     # Example:     # data = ['{'coherence':.3, accuracy':.8}', ...]     result = []     for item in data:         result.append(json.loads(item))     return Delta(experiment_data=pd.DataFrame(result)) In\u00a0[\u00a0]: Copied! <pre># Now, we can run our components\nfor _ in range(3):\n    state = experimentalist_on_state(state, num_samples=2)  # Collect 2 conditions per iteration\n    state = runner_on_state(state)  # This will collect data from two participants\n    state = theorist_on_state(state)\n</pre> # Now, we can run our components for _ in range(3):     state = experimentalist_on_state(state, num_samples=2)  # Collect 2 conditions per iteration     state = runner_on_state(state)  # This will collect data from two participants     state = theorist_on_state(state)"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#recruitment-via-prolific","title":"Recruitment Via Prolific\u00b6","text":"<p>In this tutorial, you will learn how to set up a random dot motion (RDK) experiment, sample experimental conditions, collect data via the recruitment platform Prolific and use linear regression to analyse the data. We will use the following components:</p> <ul> <li>theorist: linear regression</li> <li>experiment-runner: serving the experiment via Firebase and recruiting participants via Prolific</li> <li>experimentalist: random sampling</li> </ul>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#prerequisites","title":"Prerequisites\u00b6","text":"<p>In this example, we assume that you have set up a RDK experiment on Firebase. For example, following the cookiecutter tutorial Here, we focus on how to integrate recruitment via Prolific.</p> <p>!!! Warning: The <code>firebase-prolific-runner</code> will automatically set up a study on Prolific and recruit participants. It is highly recommended to test the experiment before recruiting participants, to have approval from an ethics committee, and to adhere to the ethics guidelines. For example include a consent page.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#workflow","title":"Workflow\u00b6","text":""},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#variables-and-state","title":"Variables And State\u00b6","text":"<p>We set up an Autora experiment by defining variables and a state. Here, we will use the coherence of the random dot motion kinematogram as the dependent variable. This is the ratio of dots moving in a coherent direction as opposed to moving randomly.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#theorist","title":"Theorist\u00b6","text":"<p>We use linear regression as a theorist and wrapt the regressor in on_state functionality to use it on the state</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#experimentalist","title":"Experimentalist\u00b6","text":"<p>Here, we use a random pool and use the wrapper to create an on state function Note: The argument num_samples is not a state field. Instead, we will pass it in when calling the function</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#experiment-runner","title":"Experiment Runner\u00b6","text":"<p>Here, we will serve the experiment and store data via Firebase and recruit participants via Prolific. We assume that you already set up an experiment on Firebase. It is recommended to use the <code>autora-firebase-runner</code> and manually test the setup before using the <code>autora-firebase-prolific-runner</code>.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/Basic%20Usage/#closed-loop","title":"Closed Loop\u00b6","text":"<p>After setting up all components, we can use the runner just as other runners (see https://autoresearch.github.io/autora/)</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/firebase-prolific-runner/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Firebase Prolific Experiment Runner is a part of the <code>autora</code> package:</p> <pre><code>pip install -U \"autora[experiment-runner-firebase-prolific]\"\n</code></pre> <p>WARNING: Both runners work with a specific set up of the firebase database. For starters, follow this guide to set up an experiment using firebase here: https://github.com/AutoResearch/cra-template-autora-firebase</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/prolific-recruitment-manager/","title":"Prolific Recruitment Manager","text":"<p>Prolific Recruitment Manager provides functionality to recruit participants via Prolific to set up an experiment runner for AutoRA.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/prolific-recruitment-manager/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install autora-experiment-runner-recruitment-manager-prolific\n</pre> !pip install autora-experiment-runner-recruitment-manager-prolific <p>Import the relevant functions (we want to set up an experiment and have a way to pause/unpause it)</p> In\u00a0[\u00a0]: Copied! <pre>from autora.experiment_runner.recruitment_manager.prolific import setup_study, start_study, pause_study\n</pre> from autora.experiment_runner.recruitment_manager.prolific import setup_study, start_study, pause_study <p>Let's see, what arguments setup_study expects:</p> In\u00a0[\u00a0]: Copied! <pre>help(setup_study)\n</pre> help(setup_study) <p>Most arguments have a default option and in this example we'll leave them as they are, but we have to set 5 arguments: (1) name: This is the name the users on prolific will see (2) description: This is the description the users on prolific will see. (3) external_study_url: This is the link to the website were you host the experiment (for a detailed explanation on how to use Firebase to setup an online experiment visit: https://autoresearch.github.io/autora/ You can add url-variables here. There are special ones that will be set by prolific if specified: www.example-experiment.de?PROLIFIC_PID={{%PROLIFIC_PID%}}&amp;STUDY_ID={{%STUDY_ID%}}&amp;SESSION_ID={{%SESSION_ID%}} will set the Prolific_pid, study_id and session_id, and you can retrieve these variables in your website (4) estimated_completion_time: The time a participant will take on average to finish the experiment in minutes (this also sets a maximum allowed time and the reward if not specified differently) (5) prolific_token: The api token to your prolific account (you can get here: https://app.prolific.co/ (after you logged in, under settings)</p> In\u00a0[\u00a0]: Copied! <pre># Set up the parameters\nname = 'Awesome AutoRA Study'\ndescription = 'Descriptive description for the example study'\nexternal_study_url = 'https://autoresearch.github.io/autora/'\nestimated_completion_time = 5\nprolific_token = \"your-token-goes-in-here\"\n</pre> # Set up the parameters name = 'Awesome AutoRA Study' description = 'Descriptive description for the example study' external_study_url = 'https://autoresearch.github.io/autora/' estimated_completion_time = 5 prolific_token = \"your-token-goes-in-here\" In\u00a0[\u00a0]: Copied! <pre>study = setup_study(name = name,\n                    description = description,\n                    external_study_url= external_study_url,\n                    estimated_completion_time =  estimated_completion_time,\n                    prolific_token = prolific_token)\n</pre> study = setup_study(name = name,                     description = description,                     external_study_url= external_study_url,                     estimated_completion_time =  estimated_completion_time,                     prolific_token = prolific_token) <p>Checkout your prolific account. A new (paused) study with the provided name, description, url and completion time should have appeared.</p> <p>The setup_study function returns a dictionary with the keys id and maximum allowed time. The id can be used as a handle to pause and unpause the study or to perform other actions. The maximum allowed time can be useful to set up timeouts to recruit new participants.</p> <p>We can now start the study |Warning This will start the prolific recruitment!|</p> In\u00a0[\u00a0]: Copied! <pre>start_study(study['id'], prolific_token)\n</pre> start_study(study['id'], prolific_token) <p>And immediately pause the study again</p> In\u00a0[\u00a0]: Copied! <pre>pause_study(study['id'], prolific_token)\n</pre> pause_study(study['id'], prolific_token)"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/prolific-recruitment-manager/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here, we learn the basic functionality of the prolific <code>recruitment manager</code>. A tool to automatically set up, run and pause a study on prolific. In an autora cycle this is typically used in conjunction with an <code>experimentation manager</code>, that manages the sending of conditions, retrieving of observations and setup of a study on a hosting service. The combination of a <code>recruitment manager</code> and an <code>experimentation manager</code> is called <code>experiment-runner</code> in autora terms.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/prolific-recruitment-manager/Basic%20Usage/#example","title":"Example\u00b6","text":"<p>A prolific <code>recruitment manager</code> runs and over-watches the recruitment of participant on prolific and sends them to a website. The website is managed by a <code>experimentation manager</code> that sends condition to a database, that then get read by the website. The <code>experimentation-manager</code> also reads observation from the database and tells the <code>recruitment manager</code> when to stop or interupts if something unexpected happens.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/prolific-recruitment-manager/Basic%20Usage/#in-this-tutorial","title":"In This Tutorial\u00b6","text":"<p>Here we show how to set up a study, pause and unpause it.</p>"},{"location":"user-guide/experiment-runners/behavioral-web-experiments/prolific-recruitment-manager/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Prolific Recruitment Manager is a part of the <code>autora</code> package:</p> <pre><code>pip install \"aurora[experiment-runner-recruitment-manager-prolific]\"\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experiment_runner.recruitment_manager.prolific import setup_study\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/bandit-random/","title":"bandit-random","text":"<p>This package provides functions to randomly sample a list of</p> <ul> <li>probability sequences</li> <li>reward sequences</li> </ul>"},{"location":"user-guide/experimentalists/bandit-random/#probability-sequence","title":"Probability sequence","text":"<p>A probability sequence is a sequence of vectors with elements in the range between 0 and 1:</p> <p>Example for a probability function that can be used in a 3-arm bandit task:</p> <pre><code>[[0, 1., .3], [.6, .2, .8], ...]\n</code></pre>"},{"location":"user-guide/experimentalists/bandit-random/#reward-sequence","title":"Reward sequence","text":"<p>A reward sequences uses the probabilities to generate a sequence with elements of either 0 or 1:</p> <p>Example for a probability function that can be used in a 3-arm bandit task:</p> <pre><code>[[0, 1, 0], [1, 0, 1], ...]\n</code></pre> <p>The probability sequence can be created by specifying an initial probability for each element and a drift:</p> <p>For example:</p> <pre><code>initial_proba = [0, .5, 1.]\ndrift = [.1, 0., -.1]\n...\nsequence = [[0, .5, 1.], [.1, .5, .9], [.2, .5, .8], [.3, .5, .7]...]\n</code></pre> <p>Instead of fixed values for the initial probability and the drift, we can also use ranges. In that case the values are randomly sampled from the range.</p> <pre><code>initial_proa = [[0, .3], [.4, .7], [.8, 1.]]\ndrift = [[0, .1], [.1, .2], [.2, .3]]\n</code></pre>"},{"location":"user-guide/experimentalists/bandit-random/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nfrom autora.experimentalist.bandit_random import bandit_random_pool_proba, \\\n    bandit_random_pool_from_proba, bandit_random_pool\n</pre> import pandas as pd  from autora.experimentalist.bandit_random import bandit_random_pool_proba, \\     bandit_random_pool_from_proba, bandit_random_pool <p>This package provides functions to randomly sample a list of</p> <ul> <li>probability sequences</li> <li>reward sequences</li> </ul> In\u00a0[28]: Copied! <pre>default_probability_sequences = bandit_random_pool_proba(num_probabilities=2, sequence_length=4)\ndefault_probability_sequences\n</pre> default_probability_sequences = bandit_random_pool_proba(num_probabilities=2, sequence_length=4) default_probability_sequences Out[28]: <pre>[[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]</pre> <p>We also can set initial values:</p> In\u00a0[29]: Copied! <pre>constant_probability_sequence = bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[.1, .9])\nconstant_probability_sequence\n</pre> constant_probability_sequence = bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[.1, .9]) constant_probability_sequence Out[29]: <pre>[[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]]</pre> <p>We can do the same for drift rates:</p> In\u00a0[30]: Copied! <pre>changing_probability_sequence =  bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[0.1, .9], drift_rates=[.1, -.1])\nchanging_probability_sequence\n</pre> changing_probability_sequence =  bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[0.1, .9], drift_rates=[.1, -.1]) changing_probability_sequence Out[30]: <pre>[[[0.1, 0.9],\n  [0.2, 0.8],\n  [0.30000000000000004, 0.7000000000000001],\n  [0.4, 0.6000000000000001]]]</pre> <p>Instead of having a fixed initial value and drift rate, we can also sample them from a range:</p> In\u00a0[31]: Copied! <pre>random_probability_sequence = bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[[0.,.1], [.8, 1.]], drift_rates=[[0,.1],[-.1,0]])\nrandom_probability_sequence\n</pre> random_probability_sequence = bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[[0.,.1], [.8, 1.]], drift_rates=[[0,.1],[-.1,0]]) random_probability_sequence Out[31]: <pre>[[[0.015097359670462374, 0.975340226214809],\n  [0.08820028316160722, 0.954897827401469],\n  [0.16130320665275205, 0.934455428588129],\n  [0.23440613014389688, 0.914013029774789]]]</pre> <p>We pass in the number of sequence to generate as <code>num_samples</code></p> In\u00a0[32]: Copied! <pre>random_probability_sequence = bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[[0.,.1], [.8, 1.]], drift_rates=[[0,.1],[-.1,0]], num_samples=4)\nrandom_probability_sequence\n</pre> random_probability_sequence = bandit_random_pool_proba(num_probabilities=2, sequence_length=4, initial_probabilities=[[0.,.1], [.8, 1.]], drift_rates=[[0,.1],[-.1,0]], num_samples=4) random_probability_sequence Out[32]: <pre>[[[0.05277517161024149, 0.9157516813620797],\n  [0.06601093773147637, 0.8512254219581823],\n  [0.07924670385271125, 0.786699162554285],\n  [0.09248246997394613, 0.7221729031503876]],\n [[0.03308990634055732, 0.8608567922155729],\n  [0.05423027527564794, 0.8348824396142384],\n  [0.07537064421073857, 0.8089080870129038],\n  [0.09651101314582919, 0.7829337344115693]],\n [[0.05228116419768012, 0.9571430988304549],\n  [0.10872837330001228, 0.922489870191641],\n  [0.16517558240234442, 0.887836641552827],\n  [0.22162279150467656, 0.8531834129140131]],\n [[0.017985053533171515, 0.9696895439983294],\n  [0.07759069582130446, 0.9603867806583171],\n  [0.13719633810943738, 0.9510840173183047],\n  [0.1968019803975703, 0.9417812539782924]]]</pre> <p>We can use the created probability sequences to create reward sequences:</p> In\u00a0[33]: Copied! <pre>reward_sequences = bandit_random_pool_from_proba(random_probability_sequence)\nreward_sequences\n</pre> reward_sequences = bandit_random_pool_from_proba(random_probability_sequence) reward_sequences Out[33]: <pre>[[[0, 1], [0, 1], [0, 1], [0, 1]],\n [[0, 1], [0, 1], [0, 1], [0, 0]],\n [[0, 1], [0, 1], [0, 1], [0, 0]],\n [[0, 1], [0, 0], [1, 1], [0, 1]]]</pre> <p>Or, we can use <code>bandit_random_pool</code> with the same arguments as in the <code>bandit_random_pool_proba</code> to generate reward sequences directly:</p> In\u00a0[34]: Copied! <pre>reward_sequences = bandit_random_pool(num_rewards=2, sequence_length=4, initial_probabilities=[[0.,.1], [.8, 1.]], drift_rates=[[0,.1],[-.1,0]], num_samples=4)\nreward_sequences\n</pre> reward_sequences = bandit_random_pool(num_rewards=2, sequence_length=4, initial_probabilities=[[0.,.1], [.8, 1.]], drift_rates=[[0,.1],[-.1,0]], num_samples=4) reward_sequences Out[34]: <pre>[[[0, 0], [0, 1], [0, 1], [1, 1]],\n [[0, 1], [0, 1], [0, 1], [1, 1]],\n [[0, 1], [0, 1], [0, 1], [1, 1]],\n [[0, 1], [0, 0], [0, 1], [0, 1]]]</pre> In\u00a0[41]: Copied! <pre># First, we define the variables:\nfrom autora.variable import VariableCollection, Variable\n\nvariables = VariableCollection(\n    independent_variables=[Variable(name=\"reward-trajectory\")],\n    dependent_variables=[Variable(name=\"choice-trajectory\")]\n)\n\n# With these variables, we initialize a StandardState\nfrom autora.state import StandardState\n\nstate = StandardState()\n\n# Here, we want to create a random reward-sequences directly as on state function\nfrom autora.state import Delta, on_state\n\n\n@on_state()\ndef pool_on_state(num_rewards=2, sequence_length=10, num_samples=1, initial_probabilities=None,\n                  drift_rates=None):\n    sequence_as_list = bandit_random_pool(\n        num_rewards=num_rewards, sequence_length=sequence_length, num_samples=num_samples,\n        initial_probabilities=initial_probabilities, drift_rates=drift_rates)\n    # the condition of the state expect a pandas DataFrame,\n    sequence_as_df = pd.DataFrame({\"reward-trajectory\": sequence_as_list})\n    return Delta(conditions=sequence_as_df)\n\n\n# now we can use the pool_on_state on the state to create conditions:\nstate = pool_on_state(state)\nstate.conditions\n</pre> # First, we define the variables: from autora.variable import VariableCollection, Variable  variables = VariableCollection(     independent_variables=[Variable(name=\"reward-trajectory\")],     dependent_variables=[Variable(name=\"choice-trajectory\")] )  # With these variables, we initialize a StandardState from autora.state import StandardState  state = StandardState()  # Here, we want to create a random reward-sequences directly as on state function from autora.state import Delta, on_state   @on_state() def pool_on_state(num_rewards=2, sequence_length=10, num_samples=1, initial_probabilities=None,                   drift_rates=None):     sequence_as_list = bandit_random_pool(         num_rewards=num_rewards, sequence_length=sequence_length, num_samples=num_samples,         initial_probabilities=initial_probabilities, drift_rates=drift_rates)     # the condition of the state expect a pandas DataFrame,     sequence_as_df = pd.DataFrame({\"reward-trajectory\": sequence_as_list})     return Delta(conditions=sequence_as_df)   # now we can use the pool_on_state on the state to create conditions: state = pool_on_state(state) state.conditions Out[41]: reward-trajectory 0 [[0, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 1... <p>We can pass in keyword arguments into the on_state function as well. Here, we create 3 sequences with initial values for the first arm between 0 and .3 and for the second arm between .7 and 1. And drift rates are sampled between 0 and .1, or -.1 and 0, respectively:</p> In\u00a0[42]: Copied! <pre>state = pool_on_state(state, num_samples=3, initial_probabilities=[[0, .3], [.7, 1.]], drift_rates=[[0, .1], [-.1,0]])\nstate.conditions\n</pre> state = pool_on_state(state, num_samples=3, initial_probabilities=[[0, .3], [.7, 1.]], drift_rates=[[0, .1], [-.1,0]]) state.conditions Out[42]: reward-trajectory 0 [[1, 1], [1, 1], [0, 0], [1, 0], [1, 0], [0, 0... 1 [[1, 0], [0, 1], [1, 1], [1, 1], [0, 0], [0, 0... 2 [[0, 1], [0, 0], [0, 0], [1, 0], [0, 1], [0, 1... In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/bandit-random/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here, we show how to randomly sample a sequence of rewards which can be used in a bandit task. A bandit task provides the participant with multiple options (number of arms). Each arm has a reward probability. Here, we show how to create a sequence of reward probabilities and rewards for a 2-arm bandit task.</p>"},{"location":"user-guide/experimentalists/bandit-random/Basic%20Usage/#pool_proba","title":"Pool_proba\u00b6","text":"<p>First, we can use default values, to create a sequence, where the reward probability is .5 for each arm. We need to pass in the number of arms and the length of the sequence that we want to generate:</p>"},{"location":"user-guide/experimentalists/bandit-random/Basic%20Usage/#use-in-state","title":"Use in State\u00b6","text":"<p>!!!Warning If you want to use this in the AutoRa <code>StandardState</code> you need to convert the return value into a <code>pd.DataFrame</code>:</p>"},{"location":"user-guide/experimentalists/bandit-random/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*probability-sequence-random is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-bandit-random\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.bandit_random import Example\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/falsification/","title":"Falsification Experimentalist","text":"<p>The falsification sampler identifies novel experimental conditions \\(X'\\) under  which the loss \\(\\hat{\\mathcal{L}}(M,X,Y,X')\\) of the best  candidate model is predicted to be the highest. This loss is  approximated with a multi-layer perceptron, which is trained to  predict the loss of a candidate model, \\(M\\), given experiment  conditions \\(X\\)  and dependent measures \\(Y\\) that have already been probed:</p> \\[ \\underset{X'}{argmax}~\\hat{\\mathcal{L}}(M,X,Y,X'). \\]"},{"location":"user-guide/experimentalists/falsification/#example","title":"Example","text":"<p>To illustrate the falsification strategy, consider a dataset representing the sine function:</p> \\[ f(X) = \\sin(X). \\] <p>The dataset consists of 100 data points ranging from \\(X=0\\) to \\(X=2\\pi\\).</p> <p>In addition, let's consider a linear regression as a model (\\(M\\)) of the data. </p> <p>The following figure illustrates the prediction of the fitted linear regression (shown in blue) for the pre-collected sine dataset (conditions \\(X\\) and observations \\(Y\\); shown in red):</p> <p></p> <p>One can observe that the linear regression is a poor fit for the sine data, in particular for regions around the  extrema of the sine function, as well as the lower and upper bounds of the domain.</p> <p>The figure below shows the mean-squared error (MSE) of the linear regression  as a function of the input \\(X\\) (red dots):</p> <p></p> <p>The falsification sampler attempts to predict the MSE of the linear regression using a neural network (shown in blue).</p> <p>Once the falsiifcaiton sampler has been trained, it can be used to sample novel experimental conditions \\(X'\\)  that are predicted to maximize the predicted MSE, such as at the boundaries of the domain,  as well as around the extrema of the sine function. For instance, consider the following pool of candidate conditions:</p> \\(X\\) 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 <p>An example output of the falsification sampler is:</p> <pre><code>[[0. ]\n [6.5]\n [6. ]\n [2. ]]\n</code></pre> <p>The selected conditons are predicted to yield the highest error from for the linear regression model. </p> <p>You may also use the falsification pooler to obtain novel experiment conditions from the range of values associated  with each independent variable. To prevent the falsification pooler from sampling at the limits of the domain (\\(0\\) and \\(2/pi\\)), it can be provided with optional parameter <code>limit_repulsion</code> that bias samples for new experimental conditions away from the boundaries of \\(X\\), as shown in the second example below.</p>"},{"location":"user-guide/experimentalists/falsification/#example-code-for-falsification-sampler","title":"Example Code for Falsification Sampler","text":"<pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom autora.variable import DV, IV, ValueType, VariableCollection\nfrom autora.experimentalist.falsification import falsification_sample\nfrom autora.experimentalist.falsification import falsification_score_sample\n\n# Specify X and Y\nX = np.linspace(0, 2 * np.pi, 100)\nY = np.sin(X)\nX_prime = np.linspace(0, 6.5, 14)\n\n# We need to provide the pooler with some metadata specifying the independent and dependent variables\n# Specify independent variable\niv = IV(\n    name=\"x\",\n    value_range=(0, 2 * np.pi),\n)\n\n# specify dependent variable\ndv = DV(\n    name=\"y\",\n    type=ValueType.REAL,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n\n# Fit a linear regression to the data\nmodel = LinearRegression()\nmodel.fit(X.reshape(-1, 1), Y)\n\n# Sample four novel conditions\nX_selected = falsification_sample(\n    conditions=X_prime,\n    model=model,\n    reference_conditions=X,\n    reference_observations=Y,\n    metadata=metadata,\n    num_samples=4,\n)\n\n# convert Iterable to numpy array\nX_selected = np.array(list(X_selected))\n\n# We may also obtain samples along with their z-scored novelty scores\nX_selected = falsification_score_sample(\n    conditions=X_prime,\n    model=model,\n    reference_conditions=X,\n    reference_observations=Y,\n    metadata=metadata,\n    num_samples=4)\n\nprint(X_selected)\n</code></pre> <p>Output: <pre><code>    0     score\n0  6.5  2.676909\n1  0.0  1.812108\n2  4.5  0.138694\n3  2.0  0.137721\n</code></pre></p>"},{"location":"user-guide/experimentalists/falsification/#example-code-for-falsification-pooler","title":"Example Code for Falsification Pooler","text":"<pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom autora.variable import DV, IV, ValueType, VariableCollection\nfrom autora.experimentalist.falsification import falsification_pool\n\n# Specify X and Y\nX = np.linspace(0, 2 * np.pi, 100)\nY = np.sin(X)\n\n# We need to provide the pooler with some metadata specifying the independent and dependent variables\n# Specify independent variable\niv = IV(\n    name=\"x\",\n    value_range=(0, 2 * np.pi),\n)\n\n# specify dependent variable\ndv = DV(\n    name=\"y\",\n    type=ValueType.REAL,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n\n# Fit a linear regression to the data\nmodel = LinearRegression()\nmodel.fit(X.reshape(-1, 1), Y)\n\n# Sample four novel conditions\nX_sampled = falsification_pool(\n    model=model,\n    reference_conditions=X,\n    reference_observations=Y,\n    metadata=metadata,\n    num_samples=4,\n    limit_repulsion=0.01,\n)\n\n# convert Iterable to numpy array\nX_sampled = np.array(list(X_sampled))\n\nprint(X_sampled)\n</code></pre> <p>Output: <pre><code>[[6.28318531]\n [2.16611028]\n [2.16512322]\n [2.17908978]]\n</code></pre></p>"},{"location":"user-guide/experimentalists/falsification/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[experimentalist-falsification]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[experimentalist-falsification]\" In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom autora.variable import DV, IV, ValueType, VariableCollection\nfrom autora.experimentalist.falsification import falsification_sample, falsification_score_sample, falsification_pool\n</pre> import numpy as np from sklearn.linear_model import LinearRegression from autora.variable import DV, IV, ValueType, VariableCollection from autora.experimentalist.falsification import falsification_sample, falsification_score_sample, falsification_pool <p>In order to reproduce our results, we also import torch and set the seed.</p> In\u00a0[3]: Copied! <pre>import torch\ntorch.manual_seed(180)\nnp.random.seed(180)\n</pre> import torch torch.manual_seed(180) np.random.seed(180) In\u00a0[4]: Copied! <pre>X = np.linspace(0, 2 * np.pi, 100)\nY = np.sin(X)\n</pre> X = np.linspace(0, 2 * np.pi, 100) Y = np.sin(X) <p>Next, we need to define metadata object, so the falsification sampler knows what data it is supposed to generate. We can do this by defining the independent variable $x$, which underlies experimental conditions $X$, and the dependent variable $y$, which underlies the observations $Y$. We specify that $x$ is a continuous variable with a range of $[0, 2\\pi]$, and $y$ is a real-valued variable.</p> In\u00a0[5]: Copied! <pre># Specify independent variable\niv = IV(\n    name=\"x\",\n    value_range=(0, 2 * np.pi),\n)\n\n# specify dependent variable\ndv = DV(\n    name=\"y\",\n    type=ValueType.REAL,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n</pre> # Specify independent variable iv = IV(     name=\"x\",     value_range=(0, 2 * np.pi), )  # specify dependent variable dv = DV(     name=\"y\",     type=ValueType.REAL, )  # Variable collection with ivs and dvs metadata = VariableCollection(     independent_variables=[iv],     dependent_variables=[dv], ) <p>Next, we can specify the model that we would like to fit to the data. In this case, we will use a linear model.</p> In\u00a0[6]: Copied! <pre>model = LinearRegression()\nmodel.fit(X.reshape(-1, 1), Y)\n</pre> model = LinearRegression() model.fit(X.reshape(-1, 1), Y) Out[6]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <p>Finally, we can generate novel experimental conditions $X'$ from the falsification sampler. We will select 5 novel experimental conditions from a candidate set of 14 experiment conditions.</p> In\u00a0[19]: Copied! <pre>X_prime = np.linspace(0, 6.5, 14)\n\nnew_conditions = falsification_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=5,\n        plot=True,\n    )\n</pre> X_prime = np.linspace(0, 6.5, 14)  new_conditions = falsification_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=5,         plot=True,     ) <p>Before we examine the novel conditions, let's have a look at the three plots generated by the falsification sampler, going from last to first.</p> <ul> <li><p>Model Prediction vs. Data. The model trained on the data is shown in red, and the model prediction is shown in blue. The model prediction is a straight line, which is a poor fit to the data. This is expected, since the data is generated from a sine function, which is not linear.  </p> </li> <li><p>Loss of the Falsification Network. The plot shows the learning curve for the falsification network that is trained to predict the error of the (linear) model as a function of experimental conditions. The error (loss) of this network decreases as a function of the number of training epochs.  </p> </li> <li><p>Prediction of Falsification Experimentalist. The plot shows the predicted loss of the model as a function of the experimental condition. The model is predicted to perform the worst at the extremes of the domain, which is expected since the model is a poor fit to the data. The red dots show the true loss of the model at the corresponding experimental condition. The predicted loss is a good approximation of the true loss.</p> </li> </ul> <p>The falsification sampler will identify novel experimental conditions that maximize the predicted loss (shown as a blue line in the plot \"Prediction of Falsification Experimentalist\").</p> In\u00a0[22]: Copied! <pre>new_conditions\n</pre> new_conditions Out[22]: 0 0 0.0 1 2.0 2 6.5 3 6.0 4 1.5 <p>Note that the new conditions are all at the limits of the domain $\\{0, 2\\pi\\}$, as well as around the peaks of the sinusoid, which is expected since the model is a poor fit to the data at those points. We can also plot the new conditions on top of the data.</p> In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Old Data\")\nax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"g\", label=\"New Experimental Conditions\")\nax.plot(X, model.predict(X.reshape(-1, 1)), c=\"b\", label=\"Model Prediction\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Old Data\") ax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"g\", label=\"New Experimental Conditions\") ax.plot(X, model.predict(X.reshape(-1, 1)), c=\"b\", label=\"Model Prediction\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[23]: <pre>&lt;matplotlib.legend.Legend at 0x2b3919690&gt;</pre> <p>We can also obtain \"falsification\" scores for the sampled experiment conditions using ``falsification_score_sample''. The scores are z-scored with respect to all conditions from the candidate set. In the following example, we sample 5 conditions and return their falsification scores.</p> In\u00a0[24]: Copied! <pre>new_conditions = falsification_score_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=5,\n    )\n\nnew_conditions\n</pre> new_conditions = falsification_score_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=5,     )  new_conditions Out[24]: 0 score 0 6.5 2.560483 1 0.0 1.933319 2 4.5 0.164119 3 2.0 0.147904 4 6.0 0.140968 <p>Finally, in addition to identifying samples from a candidate set of conditions, we can also generate conditions based on the value ranges of the independent variables as described in the <code>metadata</code> object. In the following example, we will generate 5 new conditions from the value range of the independent variable $x$.</p> In\u00a0[25]: Copied! <pre>new_conditions = falsification_pool(\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=5,\n        plot=False,\n    )\n\nnew_conditions\n</pre> new_conditions = falsification_pool(         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=5,         plot=False,     )  new_conditions Out[25]: 0 0 6.283185 1 1.897793 2 6.283185 3 0.000000 4 6.283185 In\u00a0[26]: Copied! <pre>from sklearn.datasets import make_blobs\nX, Y = make_blobs(n_samples=100, n_features=1, centers=2, random_state=0)\n</pre> from sklearn.datasets import make_blobs X, Y = make_blobs(n_samples=100, n_features=1, centers=2, random_state=0) <p>Next, we need to define metadata object, so the falsification sampler knows what data it is supposed to generate. We can do this by defining the independent variable $x$ underlying the experimental conditions $X$ and the dependent variable $y$ underlying the observations $Y$ as \"VariableCollection\" objects. We specify that $X$ is a continuous variable with a range of $[-1, 6]$, and $Y$ is a categorical variable.</p> In\u00a0[27]: Copied! <pre># Specify independent variable\niv = IV(\n    name=\"X\",\n    value_range=(-1, 6),\n)\n\n# specify dependent variable\ndv = DV(\n    name=\"Y\",\n    type=ValueType.CLASS,\n)\n\n# Variable collection with ivs and dvs\nmetadata = VariableCollection(\n    independent_variables=[iv],\n    dependent_variables=[dv],\n)\n</pre> # Specify independent variable iv = IV(     name=\"X\",     value_range=(-1, 6), )  # specify dependent variable dv = DV(     name=\"Y\",     type=ValueType.CLASS, )  # Variable collection with ivs and dvs metadata = VariableCollection(     independent_variables=[iv],     dependent_variables=[dv], ) <p>Next, we can specify the model that we would like to fit to the data. In this case, we will use a Gaussian mixture model with 2 components.</p> In\u00a0[28]: Copied! <pre>from sklearn.mixture import GaussianMixture\nmodel = GaussianMixture(n_components=2, random_state=2)\nmodel.fit(X, Y)\n\n# plot model fit against data\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Data\")\nax.scatter(X, model.predict(X), c=\"b\", label=\"Model Prediction\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> from sklearn.mixture import GaussianMixture model = GaussianMixture(n_components=2, random_state=2) model.fit(X, Y)  # plot model fit against data import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Data\") ax.scatter(X, model.predict(X), c=\"b\", label=\"Model Prediction\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[28]: <pre>&lt;matplotlib.legend.Legend at 0x2b3814ed0&gt;</pre> <p>In this case, the model appears to predict most of the data points quite well but fails to predict data points around $x=3$. Let's see if the falsification sampler can identify this region of the domain. We will select samples from a candidate set of 71 experiment conditions.</p> In\u00a0[29]: Copied! <pre>X_prime = np.linspace(-1, 6, 71)\n</pre> X_prime = np.linspace(-1, 6, 71) <p>and call the falsification sampler.</p> In\u00a0[30]: Copied! <pre>new_conditions = falsification_sample(\n        conditions=X_prime,\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=10,\n        plot=True,\n    )\n</pre> new_conditions = falsification_sample(         conditions=X_prime,         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=10,         plot=True,     ) <p>Alternatively, we could have generated 10 new conditions from the value range of the independent variable $x$ using the falsification pooler.</p> In\u00a0[40]: Copied! <pre>new_conditions = falsification_pool(\n        model=model,\n        reference_conditions=X,\n        reference_observations=Y,\n        metadata=metadata,\n        num_samples=10,\n        plot=False,\n    )\n</pre> new_conditions = falsification_pool(         model=model,         reference_conditions=X,         reference_observations=Y,         metadata=metadata,         num_samples=10,         plot=False,     ) <p>As shown in the \"Prediction of Falsification Network\" plot, the model is predicted to perform the worst around $x=3$. Let's have a look at the selected new conditions.</p> In\u00a0[41]: Copied! <pre>new_conditions\n</pre> new_conditions Out[41]: 0 0 3.216909 1 3.216909 2 2.525215 3 3.216909 4 3.216909 5 3.216909 6 3.216909 7 3.216909 8 2.895246 9 3.216909 <p>Indeed, the new conditions mostly located around $x=3$, reflecting a poor fit of the model for those conditions. Finally, we can plot the new conditions on top of the data.</p> In\u00a0[42]: Copied! <pre>import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(X, Y, c=\"r\", label=\"Data\")\nax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"b\", label=\"New Experimental Conditions\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n</pre> import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(X, Y, c=\"r\", label=\"Data\") ax.scatter(new_conditions, np.zeros_like(new_conditions), c=\"b\", label=\"New Experimental Conditions\") ax.set_xlabel(\"X\") ax.set_ylabel(\"Y\") ax.legend() Out[42]: <pre>&lt;matplotlib.legend.Legend at 0x2bb3a52d0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/falsification/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The falsification experimentalist identifies experiment conditions under which the loss $\\hat{\\mathcal{L}}(M,X,Y,\\vec{x})$ of the best candidate model is predicted to be the highest. This loss is approximated with a multi-layer perceptron, which is trained to predict the loss of a candidate model, $M$, given experiment conditions $X$  and dependent measures $Y$ that have already been probed.</p> <p>We begin with importing the relevant packages.</p>"},{"location":"user-guide/experimentalists/falsification/Basic%20Usage/#example-1-sampling-from-a-sine-function","title":"Example 1: Sampling From A Sine Function\u00b6","text":"<p>In this example, we will consider a dataset resembling the sine function. We will then fit a linear model to the data and use the falsification sampler to identify experiment conditions under which the model is predicted to perform the worst.</p> <p>First, we define the experiment conditions $X$ and the observations $Y$. We consider a domain of $X \\in [0, 2\\pi]$, and sample 100 data points from this domain.</p>"},{"location":"user-guide/experimentalists/falsification/Basic%20Usage/#example-2-sampling-from-a-gaussian-mixture-model","title":"Example 2: Sampling From A Gaussian Mixture Model\u00b6","text":"<p>In this example, we will consider a dataset sampled from a Gaussian mixture model. We will fit a Gaussian mixture model to the data and use the falsification sampler to identify experiment conditions under which the model is predicted to perform the worst.</p> <p>First, we define the experimental conditions $X$ and the observations $Y$, and sample 100 data points. The dependent variable is a categorical variable with 2 categories.</p>"},{"location":"user-guide/experimentalists/falsification/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Falsification Experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-falsification\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.falsification import falsification_sample\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/inequality/","title":"Inequality Experimentalist","text":"<p>The inequality experimentalist is a method used to compare experimental conditions and select new conditions based on a pairwise distance metric. Here's how it works:</p> <p>Given: - Existing experimental conditions represented by \\(\\vec{x}\\) in the set \\(X\\). - Candidate experimental conditions represented by \\(\\vec{x}'\\) in the set \\(X'\\). - A pairwise distance metric \\(d(\\vec{x}, \\vec{x}')\\) that calculates the distance between \\(\\vec{x}\\) and \\(\\vec{x}'\\). - A threshold value (default = 0) that determines the maximum allowable distance for two conditions to be considered equal. - A number \\(n\\) of conditions to sample.</p> <p>The inequality experimentalist operates as follows:</p> <ol> <li>For each candidate condition \\(\\vec{x}'\\) in \\(X'\\) calculate an \\(inequality\\) \\(score\\):</li> <li>Calculate the distances \\(d(\\vec{x}, \\vec{x}')\\) between \\(\\vec{x}\\) and \\(\\vec{x}'\\) using the pairwise distance metric for all \\(\\vec{x}\\) in \\(X\\).</li> <li>If \\(d(\\vec{x}, \\vec{x}')\\) is greater than the threshold:<ul> <li>Consider \\(\\vec{x}'\\) as different from the existing condition \\(\\vec{x}\\).</li> <li>add 1 to the \\(inequality\\) \\(score\\) for \\(\\vec{x'}\\)</li> </ul> </li> <li>If \\(d(\\vec{x}, \\vec{x}')\\) is less than the threshold:<ul> <li>Consider \\(\\vec{x}'\\) as equal to the existing condition \\(\\vec{x}\\).</li> <li>Do not add 1 to the score for \\(\\vec{x'}\\)</li> </ul> </li> </ol> <p>The \\(n\\) \\(\\vec{x'}\\) with the highest \\(inequality\\) \\(scores\\) are chosen as new conditions.</p>"},{"location":"user-guide/experimentalists/inequality/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[experimentalist-inequality]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[experimentalist-inequality]\" In\u00a0[\u00a0]: Copied! <pre>from autora.experimentalist.inequality import summed_inequality_sample\nimport numpy as np\n</pre> from autora.experimentalist.inequality import summed_inequality_sample import numpy as np <p>Next, we define the existing experimental conditons $X$.</p> In\u00a0[\u00a0]: Copied! <pre>X = np.array([1, 2, 3])\n</pre> X = np.array([1, 2, 3]) <p>We define the candidate experimental conditons $X'$ from which we seek to sample.</p> In\u00a0[\u00a0]: Copied! <pre>X_prime = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n</pre> X_prime = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) <p>Next, we need to specify how many samples we would like to collect. In this case, we pick $n=2$.</p> In\u00a0[\u00a0]: Copied! <pre>n = 2\n</pre> n = 2 <p>Finally, we can call the inequality experimentalist. Note that $X'$ is the first argument to the experimentalist, followed by the \"reference\" conditions $X$, and the number of samples.</p> In\u00a0[\u00a0]: Copied! <pre>X_sampled = summed_inequality_sample(conditions=X_prime, reference_conditions=X, num_samples=n)\nprint(X_sampled)\n</pre> X_sampled = summed_inequality_sample(conditions=X_prime, reference_conditions=X, num_samples=n) print(X_sampled) <p>The inequality experimentalist also works for experiments with multiple independent variables. In the following example, we define $X$ as a single experimental condition composed of three independent factors. We choose from a pool $X'$ composed of four experimental conditions.</p> In\u00a0[\u00a0]: Copied! <pre>X = np.array([[1, 2, 3]])\nX_prime = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n</pre> X = np.array([[1, 2, 3]]) X_prime = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) <p>Next, we sample a single experimental condition from the pool $X'$ which yields the greatest summed Euclidean distance to the existing condition in $X$.</p> In\u00a0[\u00a0]: Copied! <pre>X_sampled = summed_inequality_sample(conditions=X_prime, reference_conditions=X, num_samples=n)\nprint(X_sampled)\n</pre> X_sampled = summed_inequality_sample(conditions=X_prime, reference_conditions=X, num_samples=n) print(X_sampled) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/inequality/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The inequality experimentalist selects $n$ experimental conditions from a pool of candidate experimental conditions $X'$. The choice is informed based on the similarity of the candidate conditions $X'$ with respect to previously examined experiment conditions $X$. We begin with importing the relevant packages.</p>"},{"location":"user-guide/experimentalists/inequality/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Inequality experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[experimentalist-inequality]\n</code></pre> <p>Check your installation by running: <pre><code>python -c from autora.experimentalist.inequality import summed_inequality_sample\n</code></pre></p>"},{"location":"user-guide/experimentalists/leverage/","title":"The Leverage Experimentalist","text":"<p>This experimentalist uses the statistical concept of leverage by refitting the provided models iteratively with the leave-one-out method. </p> <p>WARNING:  This experimentalist needs to fit each model you provide it n times, where n corresponds to the number of datapoints you have.  As such, the computational time and power needed to run this experimentalist increases exponentially with increasing number of models and datapoints.</p> <p>In each iteration, it computes the degree to which the currently removed datapoint has influence on the model.  If the model remains stable, the datapoint is deemed to have little influence on the model, and as such will have a low likelyhood of being selected for further investigation. In contrast, if the model changes, the datapoint is influential on the model, and has a higher likelihood of being selected for further investigation.</p> <p>Specifically, you provide the experimentalist with a model that has been trained on all of the data. On each iteration, the experimentalist fits a new model with all data aside from one datapoint.  Both models (\\(m\\)) then predict Y scores (\\(Y'\\)) from the original X variable and compute a mean squared error (MSE) for each X score (\\(i\\)):</p> \\[ MSE_{m,i} = \\sum(Y'_{m,i} - Y_{i})^{2}  \\] <p>The experimentalist then computes a ratio of the MSE scores between the experimentalist model and the original model that you provided:</p> <p>$$ {MSE_{Ratio}}{m,i} = {MSE{experimentalist}}{m,i}/{MSE{original}}_{m} $$ As such, values above one indicates that the original model fit the data better than the experimentalist model when removing that datapoint (\\(i\\)). In contrast, values below one indicates that the experimentalist model fit the data better than the original model when removing that datapoint (\\(i\\)). And a value of one indicates that both models fit the data equally. If you provide multiple models, it will then average across these models to result in an aggregate MSE score for each X score. In the future, it might be a good idea to incorporate multiple models in a more sophisticated way.</p> <p>Finally, the experimentalist then uses these aggregated ratios to select the next set of datapoints to explore in one of three ways, declared with the 'fit' parameter.     -'increase' will choose samples focused on X scores where the fits got better (i.e., the smallest MSE ratios)     -'decrease' will choose samples focused on X scores where the fits got worse (i.e., the largest MSE ratios)     -'both' will do both of the above, or in other words focus on X scores with the most extreme scores.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install autora\n# !pip install \"autora[theorist-darts]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install autora # !pip install \"autora[theorist-darts]\" In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom autora.theorist.darts import DARTSRegressor; DARTSRegressor()\nfrom autora.experimentalist.leverage import leverage_sample\n</pre> import numpy as np import matplotlib.pyplot as plt from autora.theorist.darts import DARTSRegressor; DARTSRegressor() from autora.experimentalist.leverage import leverage_sample In\u00a0[4]: Copied! <pre>#Define X with some noise\nX = np.linspace(start=-3, stop=6, num=25).reshape(-1, 1)\nnoise = np.array([np.random.normal(0,.5) for r in range(len(X))]).reshape(-1,1)\nX = X + noise\nX.reshape(-1).sort(kind='mergesort')\n\n#Define ground truth model\ndef ground_truth(xs):\n    y = (xs ** 2.0)\n    y[xs &lt; 0] = 0\n    return y\n</pre> #Define X with some noise X = np.linspace(start=-3, stop=6, num=25).reshape(-1, 1) noise = np.array([np.random.normal(0,.5) for r in range(len(X))]).reshape(-1,1) X = X + noise X.reshape(-1).sort(kind='mergesort')  #Define ground truth model def ground_truth(xs):     y = (xs ** 2.0)     y[xs &lt; 0] = 0     return y In\u00a0[5]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.show() In\u00a0[6]: Copied! <pre>%%capture\n\n#Initiate theorists\ndarts_theorist = DARTSRegressor()\n\n#Fit theorists\ndarts_theorist.fit(X,ground_truth(X))\n</pre> %%capture  #Initiate theorists darts_theorist = DARTSRegressor()  #Fit theorists darts_theorist.fit(X,ground_truth(X)) In\u00a0[7]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.plot(X, darts_theorist.predict(X), alpha = .5, label = 'DARTS Theorist')\nplt.legend()\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.plot(X, darts_theorist.predict(X), alpha = .5, label = 'DARTS Theorist') plt.legend() plt.show() In\u00a0[8]: Copied! <pre>sampler_proposal = leverage_sample(X, ground_truth(X.ravel()), [darts_theorist], fit = 'both', num_samples = 20, sd=.2)\n\nprint('New datapoints:\\n' + str(sampler_proposal))\n</pre> sampler_proposal = leverage_sample(X, ground_truth(X.ravel()), [darts_theorist], fit = 'both', num_samples = 20, sd=.2)  print('New datapoints:\\n' + str(sampler_proposal)) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>New datapoints:\n           0\n0   6.464159\n1   6.132953\n2   4.315494\n3   4.850175\n4   3.941828\n5  -3.477950\n6   5.529659\n7  -1.218220\n8  -0.632884\n9  -0.369711\n10  1.548444\n11  1.985621\n12  0.943885\n13  1.218785\n14  1.440813\n15 -2.020381\n16  0.510689\n17 -2.342074\n18 -2.073968\n19  2.202574\n</pre> In\u00a0[9]: Copied! <pre>plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints')\nplt.plot(sampler_proposal, ground_truth(sampler_proposal), 'o', alpha = .5, label = 'Proposed Datapoints')\nplt.legend()\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints') plt.plot(sampler_proposal, ground_truth(sampler_proposal), 'o', alpha = .5, label = 'Proposed Datapoints') plt.legend() plt.show() In\u00a0[11]: Copied! <pre>#Create new independent variable\noriginal_X = X\nX = np.concatenate((X, np.array(sampler_proposal).reshape(-1,1)))\nX.reshape(-1).sort(kind='mergesort')\n\n#Refit models\noriginal_darts_theorist = DARTSRegressor()\ndarts_theorist = DARTSRegressor()\noriginal_darts_theorist.fit(original_X,ground_truth(original_X))\ndarts_theorist.fit(X,ground_truth(X))\n\n#Plot data and models\nplt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints')\nplt.plot(sampler_proposal, ground_truth(sampler_proposal), 'o', alpha = .5, label = 'Proposed Datapoints')\nplt.plot(original_X, original_darts_theorist.predict(original_X), alpha = .5, label = 'Original DARTS Theorist')\nplt.plot(X, darts_theorist.predict(X), alpha = .5, label = 'New DARTS Theorist')\nplt.legend()\nplt.show()\n</pre> #Create new independent variable original_X = X X = np.concatenate((X, np.array(sampler_proposal).reshape(-1,1))) X.reshape(-1).sort(kind='mergesort')  #Refit models original_darts_theorist = DARTSRegressor() darts_theorist = DARTSRegressor() original_darts_theorist.fit(original_X,ground_truth(original_X)) darts_theorist.fit(X,ground_truth(X))  #Plot data and models plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints') plt.plot(sampler_proposal, ground_truth(sampler_proposal), 'o', alpha = .5, label = 'Proposed Datapoints') plt.plot(original_X, original_darts_theorist.predict(original_X), alpha = .5, label = 'Original DARTS Theorist') plt.plot(X, darts_theorist.predict(X), alpha = .5, label = 'New DARTS Theorist') plt.legend() plt.show() <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#import-modules","title":"Import Modules\u00b6","text":"<p>We will need some basic packages, but will also need to import different AutoRA theorists. Here, we will import two theorists: Differentiable Architecture Search, and Logistic Regression.</p> <p>Note that we are currently re-structuring the AutoRA package so that all experimentalists and theorists are their own sub-package. As such, once this is complete for the theorists, these imports should be modified to conform to the new structure.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#define-meta-space","title":"Define Meta-Space\u00b6","text":"<p>We will here define X values of interest as well as a ground truth model to derive y values.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#plot-the-data","title":"Plot the Data\u00b6","text":"<p>Let's plot the data to see what we are working with.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#define-and-fit-theorists","title":"Define and Fit Theorists\u00b6","text":"<p>Next, we initialize each theorist and then train them on the data.</p> <p>Note that this can take quite some time, especially for the BSR Theorist.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#plot-theorists-on-data","title":"Plot Theorists on Data\u00b6","text":"<p>We can then plot each theorist to see how well it recovered the data.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#run-and-leverage-samples","title":"Run and Leverage Samples\u00b6","text":"<p>Now we will get a proposal from the experimentalist as to which datapoints to investigate next. We will retrieve 5 new datapoints in this example.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#plot-new-datapoints-with-old","title":"Plot New Datapoints With Old\u00b6","text":"<p>We can then plot our new datapoints with our previous ones to demonstrate our new dataset of investigation for then next cycle.</p>"},{"location":"user-guide/experimentalists/leverage/Basic%20Usage/#plot-data-with-new-models","title":"Plot Data with New Models\u00b6","text":"<p>We will now refit our models with the new, extended, dataset to see if it looks any better</p>"},{"location":"user-guide/experimentalists/leverage/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>leverage experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-leverage\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.leverage import leverage_sample\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/mixture/","title":"Mixture Experimentalist","text":"<p>The Mixture Experimentalist identifies novel experimental conditions under which a hybrid of different experimental sampling strategies is used.  This mixture can include any custom strategies such as falsification, novelty, crucial experimentation, uncertainty, elimination, aesthetic preferences, and arbitrary preferred/dispreferred regions of the space. The selection of conditions is based on a weighted sum of the scores obtained from these strategies.</p>"},{"location":"user-guide/experimentalists/mixture/basic-usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following lines when running on Google Colab\n# !pip install \"autora[experimentalits-mixture]\"\n# !pip install \"autora[experimentalist-falsification]\"\n# !pip install \"autora[experimentalist-novelty]\"\n</pre> # Uncomment the following lines when running on Google Colab # !pip install \"autora[experimentalits-mixture]\" # !pip install \"autora[experimentalist-falsification]\" # !pip install \"autora[experimentalist-novelty]\" In\u00a0[1]: Copied! <pre>\n</pre> <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-1-5feb8d753802&gt; in &lt;module&gt;\n----&gt; 1 from autora.experimentalist.sampler.mixture import mixture_sample\n\nModuleNotFoundError: No module named 'autora'</pre> In\u00a0[132]: Copied! <pre>from autora.experimentalist.mixture import mixture_sample\nfrom autora.experimentalist.falsification import falsification_sample, falsification_score_sample\nfrom autora.experimentalist.novelty import novelty_sampler, novelty_score_sample\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\n</pre> from autora.experimentalist.mixture import mixture_sample from autora.experimentalist.falsification import falsification_sample, falsification_score_sample from autora.experimentalist.novelty import novelty_sampler, novelty_score_sample import numpy as np import matplotlib.pyplot as plt  from sklearn.linear_model import LinearRegression  In\u00a0[138]: Copied! <pre># Define the domain\nx = np.linspace(0, 2*np.pi, 100)\n# Compute the values of the sine function at the points in the domain\ny = np.sin(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, \"-\")\n\n# Add labels and title\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.title('Ground truth')\n\n# Display the plot\nplt.show()\n</pre> # Define the domain x = np.linspace(0, 2*np.pi, 100) # Compute the values of the sine function at the points in the domain y = np.sin(x)  # Create the plot plt.figure(figsize=(8, 6)) plt.plot(x, y, \"-\")  # Add labels and title plt.xlabel('x') plt.ylabel('sin(x)') plt.title('Ground truth')  # Display the plot plt.show() <p>Now, we can try to use a mixture sampler to sample using the novelty strategy with different levels of randomness.</p> In\u00a0[139]: Copied! <pre>new_X = np.linspace(0, 2 * np.pi, 100)\n# old points\nX = np.linspace(0, 0.5, 50)\nY = np.sin(X)\n\nparams = {\n    \"novelty\": {\"reference_conditions\": X},\n}\n</pre> new_X = np.linspace(0, 2 * np.pi, 100) # old points X = np.linspace(0, 0.5, 50) Y = np.sin(X)  params = {     \"novelty\": {\"reference_conditions\": X}, } In\u00a0[140]: Copied! <pre>selected_conditions = mixture_sample(\n    conditions=new_X,\n    temperature=0.01,\n    samplers=[[novelty_score_sample, \"novelty\", [0.8, 0.2]]],\n    params=params,\n    num_samples=10\n)\n</pre> selected_conditions = mixture_sample(     conditions=new_X,     temperature=0.01,     samplers=[[novelty_score_sample, \"novelty\", [0.8, 0.2]]],     params=params,     num_samples=10 ) In\u00a0[141]: Copied! <pre>selected_conditions\n</pre> selected_conditions Out[141]: <pre>array([6.28318531, 6.21971879, 6.15625227, 6.09278575, 6.02931923,\n       5.96585272, 5.9023862 , 5.83891968, 5.77545316, 5.71198664])</pre> In\u00a0[143]: Copied! <pre># Define the domain\nx = np.linspace(0, 2*np.pi, 100)\n\n# Compute the values of the sine function at the points in the domain\ny = np.sin(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, \"-\")\nplt.plot(X, Y, \"bo\")\n\n# Add vertical lines at the sampled x-values\nfor val in selected_conditions:\n    plt.axvline(x=val, color='black')\n\n# Add labels and title\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.title('Ground Truth, Training Data, and Sampled Conditions')\n\n# Display the plot\nplt.grid(True)\nplt.show()\n</pre> # Define the domain x = np.linspace(0, 2*np.pi, 100)  # Compute the values of the sine function at the points in the domain y = np.sin(x)  # Create the plot plt.figure(figsize=(8, 6)) plt.plot(x, y, \"-\") plt.plot(X, Y, \"bo\")  # Add vertical lines at the sampled x-values for val in selected_conditions:     plt.axvline(x=val, color='black')  # Add labels and title plt.xlabel('x') plt.ylabel('sin(x)') plt.title('Ground Truth, Training Data, and Sampled Conditions')  # Display the plot plt.grid(True) plt.show()  In\u00a0[144]: Copied! <pre>selected_conditions = mixture_sample(\n    conditions=new_X,\n    temperature=2,\n    samplers=[[novelty_score_sample, \"novelty\", [0.8, 0.2]]],\n    params=params,\n    num_samples=10\n)\n</pre> selected_conditions = mixture_sample(     conditions=new_X,     temperature=2,     samplers=[[novelty_score_sample, \"novelty\", [0.8, 0.2]]],     params=params,     num_samples=10 ) In\u00a0[145]: Copied! <pre>selected_conditions\n</pre> selected_conditions Out[145]: <pre>array([2.66559377, 5.45812057, 5.33118753, 6.21971879, 0.6981317 ,\n       5.58505361, 6.28318531, 5.26772102, 3.99839065, 4.95038842])</pre> In\u00a0[146]: Copied! <pre># Define the domain\nx = np.linspace(0, 2*np.pi, 100)\n\n# Compute the values of the sine function at the points in the domain\ny = np.sin(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, \"-\")\nplt.plot(X, Y, \"bo\")\n\n# Add vertical lines at the sampled x-values\nfor val in selected_conditions:\n    plt.axvline(x=val, color='black')\n\n# Add labels and title\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.title('Ground Truth, Training Data, and Sampled Conditions')\n\n# Display the plot\nplt.grid(True)\nplt.show()\n</pre> # Define the domain x = np.linspace(0, 2*np.pi, 100)  # Compute the values of the sine function at the points in the domain y = np.sin(x)  # Create the plot plt.figure(figsize=(8, 6)) plt.plot(x, y, \"-\") plt.plot(X, Y, \"bo\")  # Add vertical lines at the sampled x-values for val in selected_conditions:     plt.axvline(x=val, color='black')  # Add labels and title plt.xlabel('x') plt.ylabel('sin(x)') plt.title('Ground Truth, Training Data, and Sampled Conditions')  # Display the plot plt.grid(True) plt.show()  In\u00a0[147]: Copied! <pre>model = LinearRegression()\nmodel.fit(X.reshape(-1, 1), Y)\n</pre> model = LinearRegression() model.fit(X.reshape(-1, 1), Y) Out[147]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> <p>Now, we have new conditions to choose sampling from!</p> In\u00a0[148]: Copied! <pre>new_X = np.linspace(0, 2 * np.pi, 100)\n\nparams = {\n    \"novelty\": {\"reference_conditions\": X},\n    \"falsification\": {\"reference_conditions\": X, \"reference_observations\": Y, \"model\": model}\n}\n</pre> new_X = np.linspace(0, 2 * np.pi, 100)  params = {     \"novelty\": {\"reference_conditions\": X},     \"falsification\": {\"reference_conditions\": X, \"reference_observations\": Y, \"model\": model} }  In\u00a0[149]: Copied! <pre>selected_conditions = mixture_sample(\n    conditions=new_X,\n    temperature=0.01,\n    samplers=[[novelty_score_sample, \"novelty\", [0.7, 0.1]], [falsification_score_sample, \"falsification\", [0.5, 0]]],\n    params=params,\n    num_samples=10\n)\n</pre> selected_conditions = mixture_sample(     conditions=new_X,     temperature=0.01,     samplers=[[novelty_score_sample, \"novelty\", [0.7, 0.1]], [falsification_score_sample, \"falsification\", [0.5, 0]]],     params=params,     num_samples=10 ) In\u00a0[150]: Copied! <pre>selected_conditions\n</pre> selected_conditions Out[150]: <pre>array([0.57119866, 0.63466518, 0.6981317 , 0.50773215, 0.76159822,\n       0.82506474, 6.28318531, 0.88853126, 6.21971879, 6.09278575])</pre> In\u00a0[151]: Copied! <pre>x = np.linspace(0, 2*np.pi, 100)\n\n# Compute the values of the sine function at the points in the domain\ny = np.sin(x)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, \"-\")\nplt.plot(X, Y, \"bo\")\n\n# Add vertical lines at the sampled x-values\nfor val in selected_conditions:\n    plt.axvline(x=val, color='black')\n\n# Add labels and title\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.title('Ground Truth, Training Data, and Sampled Conditions')\n\n# Display the plot\nplt.grid(True)\nplt.show()\n</pre> x = np.linspace(0, 2*np.pi, 100)  # Compute the values of the sine function at the points in the domain y = np.sin(x)  # Create the plot plt.figure(figsize=(8, 6)) plt.plot(x, y, \"-\") plt.plot(X, Y, \"bo\")  # Add vertical lines at the sampled x-values for val in selected_conditions:     plt.axvline(x=val, color='black')  # Add labels and title plt.xlabel('x') plt.ylabel('sin(x)') plt.title('Ground Truth, Training Data, and Sampled Conditions')  # Display the plot plt.grid(True) plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/mixture/basic-usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The goal of this notebook is to demonstrate the usage of the <code>mixture_sample</code> function, which combines different sampling strategies to select experimental conditions. The <code>mixture_sample</code> function takes a pool of experimental conditions, a temperature parameter, a list of samplers with their weights, a dictionary of parameters for the samplers, and an optional number of samples to return.</p> <p>We begin with importing the necessary packages.</p>"},{"location":"user-guide/experimentalists/mixture/basic-usage/#example-sampling-fom-a-sine-function","title":"Example: Sampling fom a Sine Function\u00b6","text":"<p>In this example, we will consider ground truth in a form of the sine function. We will then use mixture_sampler to choose experimental conditions according to the 1) novelty strategy with different degrees of randomness and 2) a mixture of novelty and falsification strategies.</p> <p>First, we define the experiment conditions $X$ and the observations $Y$. We consider a domain of $X \\in [0, 2\\pi]$.</p>"},{"location":"user-guide/experimentalists/mixture/basic-usage/#mixture-sampler-using-the-novelty-strategy","title":"Mixture sampler using the novelty strategy\u00b6","text":""},{"location":"user-guide/experimentalists/mixture/basic-usage/#low-temperature-points-are-sampled-deterministically","title":"Low temperature: points are sampled deterministically.\u00b6","text":""},{"location":"user-guide/experimentalists/mixture/basic-usage/#high-temperature-randomness-is-added-to-the-sampling-process","title":"High temperature: randomness is added to the sampling process.\u00b6","text":""},{"location":"user-guide/experimentalists/mixture/basic-usage/#mixture-experimentalist-combining-novelty-and-falsification","title":"Mixture experimentalist combining novelty and falsification\u00b6","text":""},{"location":"user-guide/experimentalists/mixture/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*mixture_experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-sampler-mixture\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.sampler.mixture import mixture_sample\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/model-disagreement/","title":"Model Disagreement Experimentalist","text":"<p>The model disagreement experimentalist identifies experimental conditions \\(\\vec{x}' \\in X'\\) with respect to a pairwise distance metric between theorist models, \\(P_{M_{i}}(\\hat{y}, \\vec{x}')\\):</p> \\[ \\underset{\\vec{x}'}{\\arg\\max}~(P_{M_{1}}(\\hat{y}, \\vec{x}') - P_{M_{2}}(\\hat{y}, \\vec{x}'))^2 \\]"},{"location":"user-guide/experimentalists/model-disagreement/#example-code","title":"Example Code","text":"<pre><code>from autora.experimentalist.model_disagreement import model_disagreement_sample\nfrom autora.theorist.bms import BMSRegressor; BMSRegressor()\nfrom autora.theorist.darts import DARTSRegressor; DARTSRegressor()\nimport numpy as np\n\n#Meta-Setup\nX = np.linspace(start=-3, stop=6, num=10).reshape(-1, 1)\ny = (X**2).reshape(-1, 1)\nn = 5\n\n#Theorists\nbms_theorist = BMSRegressor()\ndarts_theorist = DARTSRegressor()\nbms_theorist.fit(X,y)\ndarts_theorist.fit(X,y)\n\n#Experimentalist\nX_new = model_disagreement_sample(X, [bms_theorist, darts_theorist], n)\n</code></pre>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following lines when running on Google Colab\n# !pip install \"autora[theorist-bms]\"\n# !pip install \"autora[theorist-bsr]\"\n# !pip install \"autora[theorist-darts]\"\n# !pip install \"autora[experimentalist-model-disagreement]\"\n</pre>  # Uncomment the following lines when running on Google Colab # !pip install \"autora[theorist-bms]\" # !pip install \"autora[theorist-bsr]\" # !pip install \"autora[theorist-darts]\" # !pip install \"autora[experimentalist-model-disagreement]\" In\u00a0[23]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom autora.theorist.bms import BMSRegressor; BMSRegressor()\nfrom autora.theorist.bsr import BSRRegressor; BSRRegressor()\nfrom autora.theorist.darts import DARTSRegressor; DARTSRegressor()\nfrom autora.experimentalist.model_disagreement import model_disagreement_sample\n</pre> import numpy as np import matplotlib.pyplot as plt from autora.theorist.bms import BMSRegressor; BMSRegressor() from autora.theorist.bsr import BSRRegressor; BSRRegressor() from autora.theorist.darts import DARTSRegressor; DARTSRegressor() from autora.experimentalist.model_disagreement import model_disagreement_sample In\u00a0[24]: Copied! <pre>#Define meta-parameters\nX = np.linspace(start=-3, stop=6, num=10).reshape(-1, 1)\n\n#Define ground truth model\ndef ground_truth(xs):\n    y = (xs ** 2.0)\n    y[xs &lt; 0] = 0\n    return y\n</pre> #Define meta-parameters X = np.linspace(start=-3, stop=6, num=10).reshape(-1, 1)  #Define ground truth model def ground_truth(xs):     y = (xs ** 2.0)     y[xs &lt; 0] = 0     return y In\u00a0[25]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.show() In\u00a0[5]: Copied! <pre>%%capture\n\n#Initiate theorists\nbms_theorist = BMSRegressor()\nbsr_theorist = BSRRegressor()\ndarts_theorist = DARTSRegressor()\n\n#Fit theorists\nbms_theorist.fit(X,ground_truth(X))\nbsr_theorist.fit(X,ground_truth(X))\ndarts_theorist.fit(X,ground_truth(X))\n</pre> %%capture  #Initiate theorists bms_theorist = BMSRegressor() bsr_theorist = BSRRegressor() darts_theorist = DARTSRegressor()  #Fit theorists bms_theorist.fit(X,ground_truth(X)) bsr_theorist.fit(X,ground_truth(X)) darts_theorist.fit(X,ground_truth(X)) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\nINFO:autora.theorist.bms.regressor:BMS fitting finished\nINFO:autora.theorist.darts.regressor:Starting fit initialization\nINFO:autora.theorist.darts.regressor:Starting fit.\n</pre> In\u00a0[26]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.plot(X, bms_theorist.predict(X), alpha = .5, label = 'BMS Theorist')\nplt.plot(X, bsr_theorist.predict(X), alpha = .5, label = 'BSR Theorist')\nplt.plot(X, darts_theorist.predict(X), alpha = .5, label = 'DARTS Theorist')\nplt.legend()\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.plot(X, bms_theorist.predict(X), alpha = .5, label = 'BMS Theorist') plt.plot(X, bsr_theorist.predict(X), alpha = .5, label = 'BSR Theorist') plt.plot(X, darts_theorist.predict(X), alpha = .5, label = 'DARTS Theorist') plt.legend() plt.show() In\u00a0[27]: Copied! <pre>fig, ax = plt.subplots(1,3, figsize=[12,4])\n\ndef plot_disagreement(X, model_a, model_b, plot_index, labels):\n    ax[plot_index].plot(X, model_a.predict(X), alpha = .5, label = labels[0]) #Plot model a\n    ax[plot_index].plot(X, model_b.predict(X), alpha = .5, label = labels[1]) #Plot model b\n    for xi, _ in enumerate(X):\n        ax[plot_index].plot([X[xi],X[xi]], [model_a.predict(X)[xi], model_b.predict(X)[xi]], alpha = .7, c = 'grey', linestyle = '--') #Plot disagreement\n    ax[plot_index].legend()\n        \nplot_disagreement(X, bms_theorist, bsr_theorist, 0, ['BSM Theorist', 'BSR Theorist'])\nplot_disagreement(X, bms_theorist, darts_theorist, 1, ['BSM Theorist', 'DARTS Theorist'])\nplot_disagreement(X, bsr_theorist, darts_theorist, 2, ['BSR Theorist', 'BSM Theorist'])\nplt.show()\n</pre> fig, ax = plt.subplots(1,3, figsize=[12,4])  def plot_disagreement(X, model_a, model_b, plot_index, labels):     ax[plot_index].plot(X, model_a.predict(X), alpha = .5, label = labels[0]) #Plot model a     ax[plot_index].plot(X, model_b.predict(X), alpha = .5, label = labels[1]) #Plot model b     for xi, _ in enumerate(X):         ax[plot_index].plot([X[xi],X[xi]], [model_a.predict(X)[xi], model_b.predict(X)[xi]], alpha = .7, c = 'grey', linestyle = '--') #Plot disagreement     ax[plot_index].legend()          plot_disagreement(X, bms_theorist, bsr_theorist, 0, ['BSM Theorist', 'BSR Theorist']) plot_disagreement(X, bms_theorist, darts_theorist, 1, ['BSM Theorist', 'DARTS Theorist']) plot_disagreement(X, bsr_theorist, darts_theorist, 2, ['BSR Theorist', 'BSM Theorist']) plt.show() In\u00a0[29]: Copied! <pre>sampler_proposal = model_disagreement_sample(X, [bms_theorist, bsr_theorist, darts_theorist], 5)\n\nprint('New datapoints:\\n' + str(sampler_proposal))\n</pre> sampler_proposal = model_disagreement_sample(X, [bms_theorist, bsr_theorist, darts_theorist], 5)  print('New datapoints:\\n' + str(sampler_proposal)) <pre>New datapoints:\n[[6.]\n [5.]\n [4.]\n [3.]\n [2.]]\n</pre> In\u00a0[30]: Copied! <pre>labels = ['Proposed Datapoints',[None]*(len(sampler_proposal)-1)]\nlabels\n</pre> labels = ['Proposed Datapoints',[None]*(len(sampler_proposal)-1)] labels Out[30]: <pre>['Proposed Datapoints', [None, None, None, None]]</pre> In\u00a0[31]: Copied! <pre>#Set proposal labels\nlabels = [None]*len(sampler_proposal)\nlabels[0] = 'Proposed Datapoints'\n\n#Plot data and proposals\nplt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints')\n[plt.axvline(condition, color = '#ff7f0e', linestyle = '--', alpha = .5, label = labels[index]) for index,condition in enumerate(sampler_proposal)]\nplt.legend()\nplt.show()\n</pre> #Set proposal labels labels = [None]*len(sampler_proposal) labels[0] = 'Proposed Datapoints'  #Plot data and proposals plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints') [plt.axvline(condition, color = '#ff7f0e', linestyle = '--', alpha = .5, label = labels[index]) for index,condition in enumerate(sampler_proposal)] plt.legend() plt.show()"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#import-modules","title":"Import Modules\u00b6","text":"<p>We will need some basic packages, but will also need to import different AutoRA theorists. Here, we will import our three theorists: Bayesian Machine Scientist, Bayesian Symbolic Regression, and Differentiable Architecture Search.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#define-meta-space","title":"Define Meta-Space\u00b6","text":"<p>We will here define X values of interest as well as a ground truth model to derive y values.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#plot-the-data","title":"Plot The Data\u00b6","text":"<p>Let's plot the data to see what we are working with.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#define-and-fit-theorists","title":"Define And Fit Theorists\u00b6","text":"<p>Next, we initialize each theorist and then train them on the data.</p> <p>Note that this can take quite some time, especially for the BSR Theorist.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#plot-theorists-on-data","title":"Plot Theorists On Data\u00b6","text":"<p>We can then plot each theorist to see how well it recovered the data.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#plot-model-disagreements","title":"Plot Model Disagreements\u00b6","text":"<p>We can also plot the disagreement between each pair of theorists. The grey dashed lines represent the degree of disagreement for each datapoint. Computationally the Model Disagreement Experimentalist takes the square of these numbers to determine where disagreement is largest.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#run-and-report-model-disagreement-samples","title":"Run And Report Model Disagreement Samples\u00b6","text":"<p>Now we will get a proposal from the experimentalist as to which datapoints to investigate next. We will retrieve 5 new datapoints in this example.</p>"},{"location":"user-guide/experimentalists/model-disagreement/Basic%20Usage/#plot-new-datapoints-with-old","title":"Plot New Datapoints With Old\u00b6","text":"<p>We can then plot our new datapoints with our previous ones to demonstrate our new dataset of investigation for then next cycle.</p>"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/","title":"RnnSindy Synthetic","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install autora-theorist-rnn-sindy-rl\n# !pip install autora-experimentalist-bandit-random\n</pre> # !pip install autora-theorist-rnn-sindy-rl # !pip install autora-experimentalist-bandit-random <p>import packages</p> In\u00a0[\u00a0]: Copied! <pre># Python Core\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List\n\n# External Vendors\nimport pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator\nimport torch\n\n# General AutoRA\nfrom autora.variable import VariableCollection, Variable\nfrom autora.state import StandardState, on_state, Delta\n\n# Experimentalists\nfrom autora.experimentalist.bandit_random import bandit_random_pool\nfrom autora.experimentalist.model_disagreement import model_disagreement_sampler_custom_distance\n\n# Experiment Runner\nfrom autora.experiment_runner.synthetic.psychology.q_learning import q_learning\n\n# Theorist\nfrom autora.theorist.rnn_sindy_rl import RNNSindy\n</pre> # Python Core from dataclasses import dataclass, field from typing import Optional, List  # External Vendors import pandas as pd import numpy as np from sklearn.base import BaseEstimator import torch  # General AutoRA from autora.variable import VariableCollection, Variable from autora.state import StandardState, on_state, Delta  # Experimentalists from autora.experimentalist.bandit_random import bandit_random_pool from autora.experimentalist.model_disagreement import model_disagreement_sampler_custom_distance  # Experiment Runner from autora.experiment_runner.synthetic.psychology.q_learning import q_learning  # Theorist from autora.theorist.rnn_sindy_rl import RNNSindy <p>Setting constants</p> In\u00a0[\u00a0]: Copied! <pre>TRIALS_PER_PARTICIPANTS = 100\nSAMPLES_PER_CYCLE = 1\nPARTICIPANTS_PER_CYCLE = 40\nCYCLES = 4\nINITIAL_REWARD_PROBABILITY_RANGE = [.2, .8]\nSIGMA_RANGE = [.2, .2]\n\nEPOCHS = 10 # 100\n\nseed = 11\n</pre> TRIALS_PER_PARTICIPANTS = 100 SAMPLES_PER_CYCLE = 1 PARTICIPANTS_PER_CYCLE = 40 CYCLES = 4 INITIAL_REWARD_PROBABILITY_RANGE = [.2, .8] SIGMA_RANGE = [.2, .2]  EPOCHS = 10 # 100  seed = 11 <p>Setting seeds for reproducible results</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(seed)\ntorch.manual_seed(seed)\n</pre> np.random.seed(seed) torch.manual_seed(seed) In\u00a0[\u00a0]: Copied! <pre>variables = VariableCollection(\n    independent_variables=[Variable(name=\"reward-trajectory\")],\n    dependent_variables=[Variable(name=\"choice-trajectory\")]\n)\n</pre> variables = VariableCollection(     independent_variables=[Variable(name=\"reward-trajectory\")],     dependent_variables=[Variable(name=\"choice-trajectory\")] ) In\u00a0[\u00a0]: Copied! <pre>@dataclass(frozen=True)\nclass RnnState(StandardState):\n    models_additional:  List[BaseEstimator] = field(\n        default_factory=list,\n        metadata={\"delta\": \"extend\"},\n    )\n\n# initialize the state:\nstate = RnnState(variables=variables)\n</pre> @dataclass(frozen=True) class RnnState(StandardState):     models_additional:  List[BaseEstimator] = field(         default_factory=list,         metadata={\"delta\": \"extend\"},     )  # initialize the state: state = RnnState(variables=variables)  In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef pool_on_state(num_samples, n_trials=TRIALS_PER_PARTICIPANTS):\n    \"\"\"\n    This is creates `num_samples` randomized reward trajectories of length `n_trials`\n    \"\"\"\n    sigma = np.random.uniform(SIGMA_RANGE[0], SIGMA_RANGE[1])\n    trajectory_array = bandit_random_pool(\n        num_rewards=2,\n        sequence_length=n_trials,\n        initial_probabilities=[INITIAL_REWARD_PROBABILITY_RANGE, INITIAL_REWARD_PROBABILITY_RANGE],\n        sigmas=[sigma, sigma],\n        num_samples=num_samples\n    )\n    trajectory_df = pd.DataFrame({'reward-trajectory': trajectory_array})\n    return Delta(conditions=trajectory_df)\n</pre> @on_state() def pool_on_state(num_samples, n_trials=TRIALS_PER_PARTICIPANTS):     \"\"\"     This is creates `num_samples` randomized reward trajectories of length `n_trials`     \"\"\"     sigma = np.random.uniform(SIGMA_RANGE[0], SIGMA_RANGE[1])     trajectory_array = bandit_random_pool(         num_rewards=2,         sequence_length=n_trials,         initial_probabilities=[INITIAL_REWARD_PROBABILITY_RANGE, INITIAL_REWARD_PROBABILITY_RANGE],         sigmas=[sigma, sigma],         num_samples=num_samples     )     trajectory_df = pd.DataFrame({'reward-trajectory': trajectory_array})     return Delta(conditions=trajectory_df) In\u00a0[\u00a0]: Copied! <pre>state = pool_on_state(state, num_samples=3)\nstate.conditions\n</pre> state = pool_on_state(state, num_samples=3) state.conditions In\u00a0[\u00a0]: Copied! <pre>runner = q_learning()\n\n@on_state()\ndef runner_on_state(conditions):\n    choices, choice_probabilities = runner.run(conditions, return_choice_probabilities=True)\n    experiment_data = pd.DataFrame({\n        'reward-trajectory': conditions['reward-trajectory'].tolist(),\n        'choice-trajectory': choices,\n        'choice-probability-trajectory': choice_probabilities\n    })\n    return Delta(experiment_data=experiment_data)\n</pre> runner = q_learning()  @on_state() def runner_on_state(conditions):     choices, choice_probabilities = runner.run(conditions, return_choice_probabilities=True)     experiment_data = pd.DataFrame({         'reward-trajectory': conditions['reward-trajectory'].tolist(),         'choice-trajectory': choices,         'choice-probability-trajectory': choice_probabilities     })     return Delta(experiment_data=experiment_data) In\u00a0[\u00a0]: Copied! <pre>state = runner_on_state(state)\nstate.experiment_data\n</pre> state = runner_on_state(state) state.experiment_data In\u00a0[\u00a0]: Copied! <pre>theorist = RNNSindy(2, epochs=EPOCHS, polynomial_degree=2)\ntheorist_additional = RNNSindy(2, epochs=EPOCHS, polynomial_degree=1)\n\n@on_state()\ndef theorist_on_state(experiment_data):\n    x = experiment_data['reward-trajectory']\n    y = experiment_data['choice-trajectory']\n    return Delta(models=[theorist.fit(x, y)])\n\n\n@on_state()\ndef theorist_additional_on_state(experiment_data):\n    x = experiment_data['reward-trajectory']\n    y = experiment_data['choice-trajectory']\n    return Delta(models_additional=[theorist_additional.fit(x, y)])\n</pre> theorist = RNNSindy(2, epochs=EPOCHS, polynomial_degree=2) theorist_additional = RNNSindy(2, epochs=EPOCHS, polynomial_degree=1)  @on_state() def theorist_on_state(experiment_data):     x = experiment_data['reward-trajectory']     y = experiment_data['choice-trajectory']     return Delta(models=[theorist.fit(x, y)])   @on_state() def theorist_additional_on_state(experiment_data):     x = experiment_data['reward-trajectory']     y = experiment_data['choice-trajectory']     return Delta(models_additional=[theorist_additional.fit(x, y)]) In\u00a0[\u00a0]: Copied! <pre>state = theorist_additional_on_state(state)\nstate = theorist_on_state(state)\n\nprint(len(state.models_additional))\nprint(len(state.models))\n</pre> state = theorist_additional_on_state(state) state = theorist_on_state(state)  print(len(state.models_additional)) print(len(state.models))  In\u00a0[\u00a0]: Copied! <pre>state.models[-1].predict(state.conditions)\n</pre> state.models[-1].predict(state.conditions) <p>Here, we see the prediction for a model is a list of two-dimensional vectors: array([[0.5, 0.5], [0.68..., 0.31...], ...]). The standard model disagreement sampler only works on predictions that are single numbers. Therefore, we define our own distance functions, that works on two lists with the described format</p> In\u00a0[\u00a0]: Copied! <pre>def custom_distance(prob_array_a, prob_array_b):\n    return np.mean([(prob_array_a[0] - prob_array_b[0])**2 + (prob_array_a[1] - prob_array_b[1])**2])\n\n# test \npred_1 = state.models[-1].predict(state.conditions)[0]  # first prediction of model 1\npred_2 = state.models_additional[-1].predict(state.conditions)[0]  # first prediction of model 2\n\ncustom_distance(pred_1, pred_2)\n</pre> def custom_distance(prob_array_a, prob_array_b):     return np.mean([(prob_array_a[0] - prob_array_b[0])**2 + (prob_array_a[1] - prob_array_b[1])**2])  # test  pred_1 = state.models[-1].predict(state.conditions)[0]  # first prediction of model 1 pred_2 = state.models_additional[-1].predict(state.conditions)[0]  # first prediction of model 2  custom_distance(pred_1, pred_2) <p>We can now use the <code>custom_distance</code> function in our sampler:</p> In\u00a0[\u00a0]: Copied! <pre>@on_state()\ndef model_disagreement_on_state(\n        conditions, models, models_additional, num_samples):\n    conditions = model_disagreement_sampler_custom_distance(\n        conditions=conditions['reward-trajectory'],\n        models=[models[-1], models_additional[-1]],\n        distance_fct=custom_distance,\n        num_samples=num_samples,\n    )\n    return Delta(conditions=conditions)\n</pre> @on_state() def model_disagreement_on_state(         conditions, models, models_additional, num_samples):     conditions = model_disagreement_sampler_custom_distance(         conditions=conditions['reward-trajectory'],         models=[models[-1], models_additional[-1]],         distance_fct=custom_distance,         num_samples=num_samples,     )     return Delta(conditions=conditions) <p>Now, we can run a full loop with a rnn synthetic model</p> In\u00a0[\u00a0]: Copied! <pre>state = RnnState(variables=variables)\n</pre> state = RnnState(variables=variables) In\u00a0[\u00a0]: Copied! <pre>for c in range(1, CYCLES + 1):\n    \n    if len(state.models) &gt; 0:\n        state = pool_on_state(state, num_samples=20)\n        state = model_disagreement_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n    else:\n        state = pool_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n    \n    state = runner_on_state(state)\n    \n    state = theorist_on_state(state)\n    state = theorist_additional_on_state(state)\n</pre> for c in range(1, CYCLES + 1):          if len(state.models) &gt; 0:         state = pool_on_state(state, num_samples=20)         state = model_disagreement_on_state(state, num_samples=SAMPLES_PER_CYCLE)     else:         state = pool_on_state(state, num_samples=SAMPLES_PER_CYCLE)          state = runner_on_state(state)          state = theorist_on_state(state)     state = theorist_additional_on_state(state)  In\u00a0[\u00a0]: Copied! <pre>out = state.models[-1].predict(state.conditions['reward-trajectory'])\n</pre> out = state.models[-1].predict(state.conditions['reward-trajectory'])"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#rnnsindy-theorist-and-synthetic-runner","title":"RnnSindy Theorist and Synthetic Runner\u00b6","text":"<p>Install the packages</p>"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#set-up-variables","title":"Set up variables\u00b6","text":"<p>independent variable is \"reward-trajectory\": A 2 x n_trials Vector with entries between 0 and 1 dependent variable is \"choice-trajectory\": A 2 x n_trials Vector with boolean entries (one hot encoded)</p>"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#state","title":"State\u00b6","text":"<p>We use a non-standard state by extending the standard state with an additional model</p>"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#autora-components","title":"Autora Components\u00b6","text":""},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#experimentalists","title":"Experimentalists\u00b6","text":""},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#random-pool","title":"Random Pool\u00b6","text":"<p>Create a pooler on state that creates a pool of conditions</p>"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#experiment-runner","title":"Experiment Runner\u00b6","text":"<p>Here, we create a synthetic runner that uses a q-learning algorithm</p>"},{"location":"user-guide/experimentalists/model-disagreement/RnnSindy%20Synthetic/#theorists","title":"Theorists\u00b6","text":"<p>Here we create two RNNSindy theorists</p>"},{"location":"user-guide/experimentalists/model-disagreement/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Model Disagreement Experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-model-disagreement\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.model_disagreement import model_disagreement_sample\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/nearest-value/","title":"Nearest Value Experimentalist","text":"<p>A experimentalist which returns the nearest values between the input samples and the allowed values, without replacement.</p>"},{"location":"user-guide/experimentalists/nearest-value/#example-code","title":"Example Code","text":"<pre><code>from autora.experimentalist.nearest_value import nearest_values_sample\nimport numpy as np\n\n#Meta-Setup\nX_allowed = np.linspace(-3, 6, 10)\nX = np.random.choice(X_allowed,10)\nn = 5\n\n#Experimentalist\nX_new = nearest_values_sample(X, X_allowed, n)\n</code></pre>"},{"location":"user-guide/experimentalists/nearest-value/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[experimentalist-nearest-value]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[experimentalist-nearest-value]\" In\u00a0[3]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom autora.experimentalist.nearest_value import nearest_values_sample\n</pre> import numpy as np import matplotlib.pyplot as plt from autora.experimentalist.nearest_value import nearest_values_sample In\u00a0[4]: Copied! <pre>#Define meta-parameters\nX_allowed = np.linspace(-3, 6, 10)\nX = np.random.choice(X_allowed,10)\n\n#Define ground truth model\ndef ground_truth(xs):\n    y = (xs ** 2.0)\n    y[xs &lt; 0] = 0\n    return y\n</pre> #Define meta-parameters X_allowed = np.linspace(-3, 6, 10) X = np.random.choice(X_allowed,10)  #Define ground truth model def ground_truth(xs):     y = (xs ** 2.0)     y[xs &lt; 0] = 0     return y In\u00a0[5]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.show() In\u00a0[6]: Copied! <pre>sampler_proposal = nearest_values_sample(X_allowed, X, 5)\nprint(sampler_proposal)\n</pre> sampler_proposal = nearest_values_sample(X_allowed, X, 5) print(sampler_proposal) <pre>     0\n0  5.0\n1 -3.0\n2  1.0\n3  4.0\n4  6.0\n</pre> In\u00a0[7]: Copied! <pre>plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints')\nplt.plot(sampler_proposal, ground_truth(sampler_proposal), 'o', alpha = .5, label = 'New Datapoints')\nplt.legend()\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints') plt.plot(sampler_proposal, ground_truth(sampler_proposal), 'o', alpha = .5, label = 'New Datapoints') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/nearest-value/Basic%20Usage/#import-modules","title":"Import Modules\u00b6","text":""},{"location":"user-guide/experimentalists/nearest-value/Basic%20Usage/#define-meta-space","title":"Define Meta-Space\u00b6","text":"<p>We will here define X values of interest as well as a ground truth model to derive y values.</p>"},{"location":"user-guide/experimentalists/nearest-value/Basic%20Usage/#plot-the-data","title":"Plot The Data\u00b6","text":"<p>Let's plot the data to see what we are working with.</p>"},{"location":"user-guide/experimentalists/nearest-value/Basic%20Usage/#run-and-report-uncertainty-samples","title":"Run And Report Uncertainty Samples\u00b6","text":"<p>Now we will get a proposal from the experimentalist as to which datapoints to investigate next. We will retrieve 5 new datapoints in this example.</p>"},{"location":"user-guide/experimentalists/nearest-value/Basic%20Usage/#plot-new-datapoints-with-old","title":"Plot New Datapoints With Old\u00b6","text":"<p>We can then plot our new datapoints with our previous ones to demonstrate our new dataset of investigation for then next cycle.</p>"},{"location":"user-guide/experimentalists/nearest-value/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*Nearest-Value-Experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-nearest-value\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.nearest_value import nearest_values_sample\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/novelty/","title":"Novelty Experimentalist","text":"<p>The novelty experimentalist identifies experimental conditions \\(\\vec{x}' \\in X'\\) with respect to a pairwise distance metric applied to existing experimental conditions \\(\\vec{x} \\in X\\):</p> \\[ \\underset{\\vec{x}'}{\\arg\\max}~f(d(\\vec{x}, \\vec{x}')) \\] <p>where \\(f\\) is an integration function applied to all pairwise  distances.</p>"},{"location":"user-guide/experimentalists/novelty/#example","title":"Example","text":"<p>For instance, the integration function \\(f(x)=\\min(x)\\) and distance function \\(d(x, x')=|x-x'|\\) identifies condition \\(\\vec{x}'\\) with the greatest minimal Euclidean distance to all existing conditions in \\(\\vec{x} \\in X\\).</p> \\[ \\underset{\\vec{x}}{\\arg\\max}~\\min_i(\\sum_{j=1}^n(x_{i,j} - x_{i,j}')^2) \\] <p>To illustrate this sampling strategy, consider the following four experimental conditions that were already probed:</p> \\(x_{i,0}\\) \\(x_{i,1}\\) \\(x_{i,2}\\) 0 0 0 1 0 0 0 1 0 0 0 1 <p>Fruthermore, let's consider the following three candidate conditions \\(X'\\):</p> \\(x_{i,0}'\\) \\(x_{i,1}'\\) \\(x_{i,2}'\\) 1 1 1 2 2 2 3 3 3 <p>If the novelty experimentalist is tasked to identify two novel conditions, it will select the last two candidate conditions \\(x'_{1,j}\\) and \\(x'_{2,j}\\) because they have the greatest minimal distance to all existing conditions \\(x_{i,j}\\):</p>"},{"location":"user-guide/experimentalists/novelty/#example-code","title":"Example Code","text":"<pre><code>import numpy as np\nfrom autora.experimentalist.novelty import novelty_sample, novelty_score_sample\n\n# Specify X and X'\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\nX_prime = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n\n# Here, we choose to identify two novel conditions\nn = 2\nX_sampled = novelty_sample(conditions=X_prime, reference_conditions=X, num_samples=n)\n\n# We may also obtain samples along with their z-scored novelty scores  \n(X_sampled, scores) = novelty_score_sample(conditions=X_prime, reference_conditions=X, num_samples=n)\n</code></pre>"},{"location":"user-guide/experimentalists/novelty/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[5]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[experimentalist-novelty]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[experimentalist-novelty]\" In\u00a0[6]: Copied! <pre>from autora.experimentalist.novelty import novelty_sample, novelty_score_sample\nimport numpy as np\n</pre> from autora.experimentalist.novelty import novelty_sample, novelty_score_sample import numpy as np <p>Next, we define the existing experimental conditons $X$.</p> In\u00a0[7]: Copied! <pre>X = np.array([1, 2, 3])\n</pre> X = np.array([1, 2, 3]) <p>We define the candidate experimental conditons $X'$ from which we seek to sample.</p> In\u00a0[8]: Copied! <pre>X_prime = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n</pre> X_prime = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) <p>Next, we need to specify how many samples we would like to collect. In this case, we pick $n=2$.</p> In\u00a0[9]: Copied! <pre>n = 2\n</pre> n = 2 <p>Finally, we can call the novelty experimentalist. Note that $X'$ is the first argument to the experimentalist, followed by the \"reference\" conditions $X$, and the number of samples.</p> In\u00a0[10]: Copied! <pre>X_sampled = novelty_sample(conditions = X_prime, reference_conditions = X, num_samples = n, metric = \"euclidean\", integration = \"sum\")\nprint(X_sampled)\n</pre> X_sampled = novelty_sample(conditions = X_prime, reference_conditions = X, num_samples = n, metric = \"euclidean\", integration = \"sum\") print(X_sampled) <pre>    0\n9  10\n8   9\n</pre> <p>The novelty experimentalist also works for experiments with multiple indendent variables. In the following example, we define $X$ as a single experimental condition composed of three independent factors. We choose from a pool $X'$ composed of four experimental conditons.</p> In\u00a0[11]: Copied! <pre>X = np.array([[1, 1, 1]])\nX_prime = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n</pre> X = np.array([[1, 1, 1]]) X_prime = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) <p>Next, we sample a single experimental condition from the pool $X'$ which yields the greatest summed Euclidean distance to the existing condition in $X$.</p> In\u00a0[12]: Copied! <pre>X_sampled = novelty_sample(conditions = X_prime, reference_conditions = X, num_samples = 1, metric = \"euclidean\", integration = \"sum\")\nprint(X_sampled)\n</pre> X_sampled = novelty_sample(conditions = X_prime, reference_conditions = X, num_samples = 1, metric = \"euclidean\", integration = \"sum\") print(X_sampled) <pre>    0   1   2\n3  10  11  12\n</pre> <p>We can also obtain \"novelty\" scores for the sampled experiment conditions using ``novelty_score_sample''. The scores are z-scored with respect to all conditions from the pool. In the following example, we sample 2 conditions and return their novelty scores.</p> In\u00a0[13]: Copied! <pre>X_sampled = novelty_score_sample(conditions = X_prime, reference_conditions = X, num_samples = 2, metric = \"euclidean\", integration = \"sum\")\nprint(X_sampled)\n</pre> X_sampled = novelty_score_sample(conditions = X_prime, reference_conditions = X, num_samples = 2, metric = \"euclidean\", integration = \"sum\") print(X_sampled) <pre>    0   1   2     score\n3  10  11  12  1.354019\n2   7   8   9  0.439289\n</pre> <p>The novelty scores align with the sampled experiment conditions (in descending order of the novelty score).</p>"},{"location":"user-guide/experimentalists/novelty/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The novelty experimentalist selects $n$ novel experimental conditions from a pool of candidate experimental conditions $X'$. The choice is informed based on the similarity of the candidate conditions $X'$ with respect to previously examined experiment conditions $X$. We begin with importing the relevant packages.</p>"},{"location":"user-guide/experimentalists/novelty/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>Novelty Experimentalsit is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-novelty\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.novelty import novelty_sample\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/prediction-filter/","title":"AutoRA Prediction Filter Experimentalist","text":"<p>Filter conditions based on the model's prediction.</p>"},{"location":"user-guide/experimentalists/prediction-filter/#example","title":"Example","text":"<p>For instance, we can filter conditions where the predictions are None values: If the function is \\(f(x)=\\sqrt((x**2 - 4))\\), then the filter can filter out conditions in the interval \\((-2, 2)\\).</p>"},{"location":"user-guide/experimentalists/prediction-filter/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[1]: Copied! <pre>import math\n\nimport numpy as np\n\nfrom autora.experimentalist.prediction_filter import prediction_filter\nimport pandas as pd\n</pre> import math  import numpy as np  from autora.experimentalist.prediction_filter import prediction_filter import pandas as pd <p>Create a model:</p> In\u00a0[2]: Copied! <pre>class ModelWithNans:\n    def predict(self, x):\n        try:\n            return math.sqrt((x**2 - 4))\n        except:\n            return None\nmodel = ModelWithNans()\n</pre> class ModelWithNans:     def predict(self, x):         try:             return math.sqrt((x**2 - 4))         except:             return None model = ModelWithNans() <p>Let's create our pool:</p> In\u00a0[3]: Copied! <pre>pool = pd.DataFrame({'x': np.linspace(-5, 5, 11)})\npool\n</pre> pool = pd.DataFrame({'x': np.linspace(-5, 5, 11)}) pool Out[3]: x 0 -5.0 1 -4.0 2 -3.0 3 -2.0 4 -1.0 5 0.0 6 1.0 7 2.0 8 3.0 9 4.0 10 5.0 <p>Let's create our filter function. This is a function that returns True if a value should be included</p> In\u00a0[4]: Copied! <pre>def filter_function(x):\n    return x is not None\n</pre> def filter_function(x):     return x is not None <p>Now, let's use our filter on the conditions:</p> In\u00a0[5]: Copied! <pre>filtered_conditions = prediction_filter(pool, model, filter_function)\nfiltered_conditions\n</pre> filtered_conditions = prediction_filter(pool, model, filter_function) filtered_conditions <pre>\n---------------------------------------------------------------------------\nAxisError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 filtered_conditions = prediction_filter(pool, model, filter_function)\n      2 filtered_conditions\n\nFile ~/Documents/GitHub/AutoResearch/autora-experimentalist-prediction-filter/src/autora/experimentalist/prediction_filter/__init__.py:71, in filter(conditions, model, filter_function)\n     16 \"\"\"\n     17 Filter conditions based on the expected outcome io the mdeol\n     18 \n   (...)\n     68     0  1  1\n     69 \"\"\"\n     70 _pred = model.predict(conditions)\n---&gt; 71 _filter = np.apply_along_axis(filter_function, 1, _pred)\n     72 _filter = _filter.reshape(1, -1)\n     74 new_conditions = conditions[list(_filter[0])]\n\nFile ~/Documents/GitHub/AutoResearch/autora-experimentalist-prediction-filter/venv/lib/python3.11/site-packages/numpy/lib/shape_base.py:361, in apply_along_axis(func1d, axis, arr, *args, **kwargs)\n    359 arr = asanyarray(arr)\n    360 nd = arr.ndim\n--&gt; 361 axis = normalize_axis_index(axis, nd)\n    363 # arr, with the iteration axis at the end\n    364 in_dims = list(range(nd))\n\nAxisError: axis 1 is out of bounds for array of dimension 0</pre> In\u00a0[5]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/prediction-filter/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here, we demonstrate how to filter conditions where we suspect the model to be undefined</p>"},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/","title":"Usage On State","text":"In\u00a0[78]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install autora\n# !pip install autora[synthetic]\n# !pip install autora[experimentalist-pprediction-filter]\n</pre> # Uncomment the following line when running on Google Colab # !pip install autora # !pip install autora[synthetic] # !pip install autora[experimentalist-pprediction-filter] In\u00a0[96]: Copied! <pre>from autora.state import StandardState, on_state, Delta, estimator_on_state\nfrom autora.variable import Variable, VariableCollection\nfrom autora.experimentalist.random import random_pool\nfrom autora.experimentalist.prediction_filter import prediction_filter\nfrom autora.experiment_runner.synthetic.psychology.luce_choice_ratio import luce_choice_ratio\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n</pre> from autora.state import StandardState, on_state, Delta, estimator_on_state from autora.variable import Variable, VariableCollection from autora.experimentalist.random import random_pool from autora.experimentalist.prediction_filter import prediction_filter from autora.experiment_runner.synthetic.psychology.luce_choice_ratio import luce_choice_ratio from sklearn.linear_model import LinearRegression from sklearn.linear_model import LogisticRegression In\u00a0[80]: Copied! <pre>experiment = luce_choice_ratio()\n</pre> experiment = luce_choice_ratio() In\u00a0[81]: Copied! <pre>state = StandardState(experiment.variables)\nstate.variables\n</pre> state = StandardState(experiment.variables) state.variables Out[81]: <pre>VariableCollection(independent_variables=[IV(name='similarity_category_A1', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category A1', rescale=1, is_covariate=False), IV(name='similarity_category_A2', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category A2', rescale=1, is_covariate=False), IV(name='similarity_category_B1', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category B1', rescale=1, is_covariate=False), IV(name='similarity_category_B2', value_range=(0.1, 10), allowed_values=array([ 0.1       ,  1.51428571,  2.92857143,  4.34285714,  5.75714286,\n        7.17142857,  8.58571429, 10.        ]), units='similarity', type=&lt;ValueType.REAL: 'real'&gt;, variable_label='Similarity with Category B2', rescale=1, is_covariate=False)], dependent_variables=[DV(name='choose_A1', value_range=(0, 1), allowed_values=None, units='probability', type=&lt;ValueType.PROBABILITY: 'probability'&gt;, variable_label='Probability of Choosing A1', rescale=1, is_covariate=False)], covariates=[])</pre> In\u00a0[82]: Copied! <pre>@on_state\ndef pool_on_state(variables, num_samples, random_state=42):\n    return Delta(conditions=random_pool(\n        variables=variables, \n        num_samples=num_samples, \n        random_state=random_state)\n    )\nstate = pool_on_state(state, num_samples=100)\nstate.conditions\n</pre> @on_state def pool_on_state(variables, num_samples, random_state=42):     return Delta(conditions=random_pool(         variables=variables,          num_samples=num_samples,          random_state=random_state)     ) state = pool_on_state(state, num_samples=100) state.conditions Out[82]: similarity_category_A1 similarity_category_A2 similarity_category_B1 similarity_category_B2 0 0.100000 8.585714 2.928571 2.928571 1 8.585714 1.514286 10.000000 8.585714 2 7.171429 8.585714 4.342857 0.100000 3 4.342857 0.100000 7.171429 10.000000 4 4.342857 8.585714 4.342857 5.757143 ... ... ... ... ... 95 2.928571 0.100000 5.757143 8.585714 96 1.514286 2.928571 8.585714 0.100000 97 7.171429 0.100000 4.342857 1.514286 98 7.171429 2.928571 2.928571 2.928571 99 1.514286 10.000000 0.100000 2.928571 <p>100 rows \u00d7 4 columns</p> In\u00a0[83]: Copied! <pre>@on_state\ndef run_on_state(conditions, random_state=42):\n    return Delta(experiment_data=experiment.run(conditions, random_state=random_state))\nstate = run_on_state(state)\nstate.experiment_data\n</pre> @on_state def run_on_state(conditions, random_state=42):     return Delta(experiment_data=experiment.run(conditions, random_state=random_state)) state = run_on_state(state) state.experiment_data Out[83]: similarity_category_A1 similarity_category_A2 similarity_category_B1 similarity_category_B2 choose_A1 0 0.100000 8.585714 2.928571 2.928571 0.010227 1 8.585714 1.514286 10.000000 8.585714 0.581342 2 7.171429 8.585714 4.342857 0.100000 0.425710 3 4.342857 0.100000 7.171429 10.000000 0.498484 4 4.342857 8.585714 4.342857 5.757143 0.279448 ... ... ... ... ... ... 95 2.928571 0.100000 5.757143 8.585714 0.440030 96 1.514286 2.928571 8.585714 0.100000 0.226442 97 7.171429 0.100000 4.342857 1.514286 0.819505 98 7.171429 2.928571 2.928571 2.928571 0.620568 99 1.514286 10.000000 0.100000 2.928571 0.122477 <p>100 rows \u00d7 5 columns</p> <p>Theorist: Linear Regression</p> In\u00a0[101]: Copied! <pre>@on_state\ndef theorist_on_state(X, y):\n    return Delta(models=[LinearRegression().fit(X, y)])\nstate = theorist_on_state(state)\nstate.models[-1]\n</pre> @on_state def theorist_on_state(X, y):     return Delta(models=[LinearRegression().fit(X, y)]) state = theorist_on_state(state) state.models[-1] Out[101]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> In\u00a0[102]: Copied! <pre>conditions = random_pool(variables=experiment.variables, num_samples=4, random_state=42)\nstate.models[-1].predict(conditions)\n</pre> conditions = random_pool(variables=experiment.variables, num_samples=4, random_state=42) state.models[-1].predict(conditions) Out[102]: <pre>array([[0.11630747],\n       [0.4675618 ],\n       [0.63628483],\n       [0.21396789]])</pre> <p>Here, we want predictions that are either smaller then .2 or larger then .8</p> In\u00a0[103]: Copied! <pre>filter_function = lambda x : x &lt; .2 or x &gt; .8\n</pre> filter_function = lambda x : x &lt; .2 or x &gt; .8 In\u00a0[104]: Copied! <pre>@on_state\ndef filter_on_state(conditions, models):\n    filtered = prediction_filter(conditions, models[-1], filter_function)\n    return Delta(conditions=filtered)\nstate = pool_on_state(state, num_samples=10, random_state=42)\nstate = filter_on_state(state)\nstate.conditions\n</pre> @on_state def filter_on_state(conditions, models):     filtered = prediction_filter(conditions, models[-1], filter_function)     return Delta(conditions=filtered) state = pool_on_state(state, num_samples=10, random_state=42) state = filter_on_state(state) state.conditions Out[104]: similarity_category_A1 similarity_category_A2 similarity_category_B1 similarity_category_B2 0 0.100000 5.757143 5.757143 4.342857 1 4.342857 8.585714 10.000000 5.757143 2 0.100000 5.757143 4.342857 8.585714 3 1.514286 8.585714 5.757143 2.928571 4 0.100000 4.342857 4.342857 7.171429 <p>Let's look at the prediction to validate our filter</p> In\u00a0[105]: Copied! <pre>state.models[-1].predict(state.conditions)\n</pre> state.models[-1].predict(state.conditions) Out[105]: <pre>array([[0.07178846],\n       [0.1923867 ],\n       [0.04447933],\n       [0.07532331],\n       [0.10101559]])</pre> <p>Let's run the experiment on these conditions and look at</p> In\u00a0[106]: Copied! <pre>state = run_on_state(state)\nstate.y[-5:]\n</pre> state = run_on_state(state) state.y[-5:] Out[106]: choose_A1 130 0.012385 131 0.256693 132 0.012034 133 0.124357 134 0.010328 In\u00a0[106]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#usage-with-state-functionality","title":"Usage With State Functionality\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#installation","title":"Installation\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#imports","title":"Imports\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#setup-experiment","title":"Setup: Experiment\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#setup-state","title":"Setup: State\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#experimentalist-random-pool-on-state","title":"Experimentalist: Random Pool on State\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#experiment-runner-run-experiment","title":"Experiment Runner: Run Experiment\u00b6","text":""},{"location":"user-guide/experimentalists/prediction-filter/Usage%20On%20State/#filter-prediction-filter","title":"Filter: Prediction Filter\u00b6","text":"<p>Let's look at predictions first:</p>"},{"location":"user-guide/experimentalists/prediction-filter/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*prediction-filter is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora-[\"experimentalist-prediction-filter\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.prediction_filter import prediction_filter\"\n</code></pre></p>"},{"location":"user-guide/experimentalists/uncertainty/","title":"Uncertainty Experimentalist","text":"<p>The uncertainty experimentalist identifies experimental conditions \\(\\vec{x}' \\in X'\\) with respect model uncertainty. Within the uncertainty experimentalist, there are three methods to determine uncertainty:</p> <p>Hint</p> <p>Note that model your model must have a <code>predict_proba</code> function, providing a probability for each label. </p>"},{"location":"user-guide/experimentalists/uncertainty/#least-confident","title":"Least Confident","text":"\\[ x^* = \\text{argmax} \\left( 1-P(\\hat{y}|x) \\right), \\] <p>where \\(\\hat{y} = \\text{argmax} P(y_i|x)\\)</p>"},{"location":"user-guide/experimentalists/uncertainty/#margin","title":"Margin","text":"\\[ x^* = \\text{argmax} \\left( P(\\hat{y}_1|x) - P(\\hat{y}_2|x) \\right), \\] <p>where \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) are the first and second most probable class labels under the model, respectively.</p>"},{"location":"user-guide/experimentalists/uncertainty/#entropy","title":"Entropy","text":"\\[  x^* = \\text{argmax} \\left( - \\sum P(y_i|x)\\text{log} P(y_i|x) \\right) \\]"},{"location":"user-guide/experimentalists/uncertainty/#example-code","title":"Example Code","text":"<pre><code>from autora.experimentalist.uncertainty import uncertainty_sample\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n#Meta-Setup\nX = np.linspace(start=-3, stop=6, num=10).reshape(-1, 1)\ny = (X**2).reshape(-1)\nn = 5\n\n#Theorists\nlr_theorist = LogisticRegression()\nlr_theorist.fit(X,y)\n\n#Experimentalist\nX_new = uncertainty_sample(X, lr_theorist, n, measure =\"least_confident\")\n</code></pre>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[1]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[experimentalist-uncertainty]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[experimentalist-uncertainty]\" In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom autora.experimentalist.uncertainty import uncertainty_sample\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from autora.experimentalist.uncertainty import uncertainty_sample In\u00a0[3]: Copied! <pre>#Define meta-parameters\nX = np.linspace(start=-3, stop=6, num=10).reshape(-1, 1)\n\n#Define ground truth model\ndef ground_truth(xs):\n    y = (xs ** 2.0)\n    y[xs &lt; 0] = 0\n    return y\n</pre> #Define meta-parameters X = np.linspace(start=-3, stop=6, num=10).reshape(-1, 1)  #Define ground truth model def ground_truth(xs):     y = (xs ** 2.0)     y[xs &lt; 0] = 0     return y In\u00a0[4]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.show() In\u00a0[5]: Copied! <pre>%%capture\n\n#Initiate theorists\nlr_theorist = LogisticRegression()\n\n#Fit theorists\nlr_theorist.fit(X,ground_truth(X))\n</pre> %%capture  #Initiate theorists lr_theorist = LogisticRegression()  #Fit theorists lr_theorist.fit(X,ground_truth(X)) In\u00a0[6]: Copied! <pre>plt.plot(X, ground_truth(X), 'o')\nplt.plot(X, lr_theorist.predict(X), alpha = .5)\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o') plt.plot(X, lr_theorist.predict(X), alpha = .5) plt.show() In\u00a0[7]: Copied! <pre>sampler_proposal_lc = uncertainty_sample(X, lr_theorist, 5, measure =\"least_confident\")\nsampler_proposal_marg = uncertainty_sample(X, lr_theorist, 5, measure =\"margin\")\nsampler_proposal_ent = uncertainty_sample(X, lr_theorist, 5, measure =\"entropy\")\n\nprint('New datapoints with Least Confident metric:\\n' + str(sampler_proposal_lc) + '\\n')\nprint('New datapoints with Margin metric:\\n' + str(sampler_proposal_marg) + '\\n')\nprint('New datapoints with Entropy metric:\\n' + str(sampler_proposal_ent))\n</pre> sampler_proposal_lc = uncertainty_sample(X, lr_theorist, 5, measure =\"least_confident\") sampler_proposal_marg = uncertainty_sample(X, lr_theorist, 5, measure =\"margin\") sampler_proposal_ent = uncertainty_sample(X, lr_theorist, 5, measure =\"entropy\")  print('New datapoints with Least Confident metric:\\n' + str(sampler_proposal_lc) + '\\n') print('New datapoints with Margin metric:\\n' + str(sampler_proposal_marg) + '\\n') print('New datapoints with Entropy metric:\\n' + str(sampler_proposal_ent)) <pre>New datapoints with Least Confident metric:\n     0\n0  3.0\n1  2.0\n2  4.0\n3  5.0\n4  1.0\n\nNew datapoints with Margin metric:\n     0\n0  4.0\n1  2.0\n2  3.0\n3  5.0\n4  1.0\n\nNew datapoints with Entropy metric:\n     0\n0  3.0\n1  2.0\n2  4.0\n3  5.0\n4  1.0\n</pre> In\u00a0[8]: Copied! <pre>plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints')\nplt.plot(sampler_proposal_lc, ground_truth(sampler_proposal_lc), 'o', alpha = .5, label = 'Least Confident')\nplt.plot(sampler_proposal_marg, ground_truth(sampler_proposal_marg), 'o', alpha = .5, label = 'Margin')\nplt.plot(sampler_proposal_ent, ground_truth(sampler_proposal_ent), 'o', alpha = .5, label = 'Entropy')\nplt.legend()\nplt.show()\n</pre> plt.plot(X, ground_truth(X), 'o', alpha = .5, label = 'Original Datapoints') plt.plot(sampler_proposal_lc, ground_truth(sampler_proposal_lc), 'o', alpha = .5, label = 'Least Confident') plt.plot(sampler_proposal_marg, ground_truth(sampler_proposal_marg), 'o', alpha = .5, label = 'Margin') plt.plot(sampler_proposal_ent, ground_truth(sampler_proposal_ent), 'o', alpha = .5, label = 'Entropy') plt.legend() plt.show() In\u00a0[8]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#import-modules","title":"Import Modules\u00b6","text":""},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#define-meta-space","title":"Define Meta-Space\u00b6","text":"<p>We will here define X values of interest as well as a ground truth model to derive y values.</p>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#plot-the-data","title":"Plot The Data\u00b6","text":"<p>Let's plot the data to see what we are working with.</p>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#define-and-fit-theorist","title":"Define And Fit Theorist\u00b6","text":"<p>Next, we initialize the theorist and then train it on the data.</p>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#plot-theorists-on-data","title":"Plot Theorists On Data\u00b6","text":"<p>We can then plot the theorist to see how well it recovered the data.</p>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#run-and-report-uncertainty-samples","title":"Run And Report Uncertainty Samples\u00b6","text":"<p>Now we will get a proposal from the experimentalist as to which datapoints to investigate next. We will retrieve 5 new datapoints in this example.</p>"},{"location":"user-guide/experimentalists/uncertainty/Basic%20Usage/#plot-new-datapoints-with-old","title":"Plot New Datapoints With Old\u00b6","text":"<p>We can then plot our new datapoints with our previous ones to demonstrate our new dataset of investigation for then next cycle.</p>"},{"location":"user-guide/experimentalists/uncertainty/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>*Uncertainty-Experimentalist is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[\"experimentalist-uncertainty\"]\n</code></pre> <p>Check your installation by running: <pre><code>python -c \"from autora.experimentalist.uncertainty import uncertainty_sample\"\n</code></pre></p>"},{"location":"user-guide/theorists/bms/","title":"Bayesian Machine Scientist","text":""},{"location":"user-guide/theorists/bms/#introduction","title":"Introduction","text":"<p>Symbolic regression (SR) refers to a class of algorithms that search for interpretable symbolic expressions which capture relationships within data. More specifically, SR attempts to find compositions of simple functions that accurately map independent variables to dependent variables within a given dataset. SR was traditionally tackled through genetic programming, wherein evolutionary algorithms mutated and crossbred equations billions of  times in search of the best match. There are problems with genetic programming, however, which stem from its inherent search constraints as well as its reliance upon heuristics and domain knowledge to balance goodness of fit and model complexity. To address these problems, Guimer\u00e0 et. al (2020) proposed a Bayesian Machine Scientist (BMS), which combines i) a Bayesian approach that specifies informed priors over expressions and computes their respective posterior probabilities given the data at hand, and ii) a Markov chain Monte Carlo (MCMC) algorithm that samples from the posterior over expressions to more effectively explore the  space of possible symbolic expressions.</p> <p>AutoRA provides an adapted version of BMS for automating the discovery of interpretable models of human information  processing.</p>"},{"location":"user-guide/theorists/bms/#references","title":"References","text":"<p>R. Guimer\u00e0 et al., A Bayesian machine scientist to aid in the solution of challenging scientific problems. Sci. Adv. 6, eaav697 (2020).</p>"},{"location":"user-guide/theorists/bms/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[theorist-bms]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[theorist-bms]\" <p>After importing the necessary modules,</p> In\u00a0[\u00a0]: Copied! <pre>from autora.theorist.bms import BMSRegressor\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> from autora.theorist.bms import BMSRegressor import numpy as np import matplotlib.pyplot as plt <p>we begin by generating data with a ground-truth equation, $y = \\sin(x) + x^3$.</p> In\u00a0[\u00a0]: Copied! <pre>x = np.expand_dims(np.linspace(start=-1, stop=1, num=500), 1)\ny = np.power(x, 3) + np.sin(x)\n</pre> x = np.expand_dims(np.linspace(start=-1, stop=1, num=500), 1) y = np.power(x, 3) + np.sin(x) <p>Then we set up the BMS regressor with our chosen meta-parameters. In this case, we will specify the number of <code>epochs</code> as well as temperatures (<code>ts</code>). Note, BMS also allows users to specify unique priors over the operations considered in the search space (<code>prior_par</code>), but in this simple example we will stick with those priors implemented by the original authors, Guimer\u00e0 et al. (2020).</p> In\u00a0[\u00a0]: Copied! <pre>temperatures = [1.0] + [1.04**k for k in range(1, 20)]\n\nbms_estimator = BMSRegressor(\n    epochs=500,\n    ts=temperatures,\n)\n</pre> temperatures = [1.0] + [1.04**k for k in range(1, 20)]  bms_estimator = BMSRegressor(     epochs=500,     ts=temperatures, ) <p>With our regressor initialized, we can call the <code>fit</code> method to discover an equation for our data and then use the <code>predict</code> method to generate predictions using our discovered equation.</p> In\u00a0[\u00a0]: Copied! <pre>bms_estimator.fit(x,y)\ny_pred = bms_estimator.predict(x)\n</pre> bms_estimator.fit(x,y) y_pred = bms_estimator.predict(x) <pre>INFO:autora.theorist.bms.regressor:BMS fitting started\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [04:47&lt;00:00,  1.74it/s]\nINFO:autora.theorist.bms.regressor:BMS fitting finished\n</pre> <p>Finally, we can plot the results.</p> In\u00a0[\u00a0]: Copied! <pre># plot out the ground truth versus predicted responses\nplt.figure()\nplt.plot(x, y, \"o\")\nplt.plot(x, y_pred, \"-\")\nplt.show()bms_estimator.model_.latex()\n</pre> # plot out the ground truth versus predicted responses plt.figure() plt.plot(x, y, \"o\") plt.plot(x, y_pred, \"-\") plt.show()bms_estimator.model_.latex() <p>In this simple case, the algorithm finds an equation with a perfect fit.</p>"},{"location":"user-guide/theorists/bms/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The following simple example shows out-of-the-box functionality of BMS</p>"},{"location":"user-guide/theorists/bms/how-it-works/","title":"How It Works","text":"<p>The Bayesian Machine Scientist (BMS) uses Bayesian inference to search the space of possible equations. The following are the relevant quantities in this Bayesian approach:</p> <ul> <li>\\(P(x):\\) Probability of \\(x\\)</li> <li>\\(P(x|\\theta)\\): Conditional Probability of \\(x\\) given \\(\\theta\\)</li> <li>\\(P(x,\\theta)\\): Joint Probability of \\(x\\) and \\(\\theta\\)</li> </ul> <p>Mathematically, we know:</p> <p>\\(P(x,\\theta)=P(x)P(\\theta|x)=P(\\theta)P(x|\\theta)\\)</p> <p>Rearranging this expression, we get Bayes rule:</p> <p>\\(P(\\theta|x)=\\dfrac{P(x|\\theta)P(\\theta)}{P(x)}\\)</p> <p>Here, \\(P(\\theta)\\) is the prior probability, \\(P(x|\\theta)\\) is the probability of data given the prior (also known as the 'likelihood'), \\(P(x)\\) is the probability of the data marginalized over all possible values of \\(\\theta\\), and \\(P(\\theta|x)\\) is the posterior probability. </p> <p>In essence, prior knowledge \\(P(\\theta)\\) is combined with evidence \\(P(x|\\theta)\\) to arrive at better knowledge \\(P(\\theta|x)\\). </p> <p>BMS capitalizes on this process for updating knowledge:</p> <p>1) It formulates the problem of fitting an equation to data by first specifying priors over equations. In their paper, Guimer\u00e0 et al. use the empirical frequency of equations on Wikipedia to specify these priors.</p> <p>\\(P(f_i|D)=\\dfrac{1}{Z}\\int_{\\Theta_i}P(D|f_i,\\theta_i)P(\\theta_i|f_i)P(f_i)d\\theta_i\\)</p> <p>\\(Z=P(D)\\) is a constant, so we can ignore it since we are only interested in finding the best equation for the specific data at hand.</p> <p>2) It then scores different candidate equations using description length as a loss function. Formally, this description length is the number of natural units of information (nats) needed to jointly encode the data and the equation optimally.</p> <p>\\(\\mathscr{L}(f_i)\\equiv-\\log[P(D,f_i)]=-\\log[P(f_i|D)P(D)]=-\\log[\\int_{\\Theta_i}P(D|f_i,\\theta_i)P(\\theta_i|f_i)P(f_i)d\\theta_i]\\)</p> <p>3) Since the loss function is computationally intractable, it uses an approximation:</p> <p>\\(\\mathscr{L}(f_i)\\approx\\dfrac{B(f_i)}{2} - \\log[P(f_i)]\\)</p> <p>where \\(B(f_i)=k\\log[n] - 2\\log[P(D|\\theta^*,f_i)]\\)</p> <p>In this formulation, the goodness of fit \\(p(D|\\theta^*,f_i)\\) and likelihood \\(p(f_i)\\) of an equation are equally and logarithmically weighted to each other (e.g., improving the fit by a factor of 2 is offset by halving the likelihood).</p> <p>To better frame the problem, equations are modeled as acyclic graphs (i.e., trees).</p> <p>Bayesian inference via MCMC is then applied to navigate the search space efficiently. Note, there are many sampling strategies other than MCMC that could be used.</p> <p>The search space is very rugged, and local minima are difficult to escape, so BMS employs parallel tempering to overcome this.</p> <p></p> <p>One incremental unit of search in this approach involves two steps:</p> <p>I) Markov chain Monte Carlo Sampling:</p> <pre><code>a) One of three mutations - Root Removal/Addition, Elementary Tree Replacement, Node Replacement - are selected for the equation tree.\nb) Choosing the operator associated with the mutation relies on how likely the operator is to turn up (encoded in the priors).\nc) Choosing a specific variable or parameter value is random.\nd) Accepting or rejecting the mutation depends on Metropolis' rule.\n</code></pre> <p></p> <p>II) Parallel Tree Swap:</p> <pre><code>a) Two parallel trees held at different temperatures are selected.\nb) The temperatures of the two trees are swapped.\nc) If this decreases the loss of the now colder tree, the tree temperatures are permanently swapped.\nd) If not, the trees are reverted to preexisting temperatures.\n</code></pre> <p>After iterating over these two steps for \\(n\\) epochs, the tree held at the lowest temperature is returned as the best fitted model for the data provided.</p>"},{"location":"user-guide/theorists/bms/how-it-works/#references","title":"References","text":"<p>R. Guimer\u00e0 et al., A Bayesian machine scientist to aid in the solution of challenging scientific problems. Sci. Adv. 6, eaav697 (2020). Wit, Ernst; Edwin van den Heuvel; Jan-Willem Romeyn (2012).</p>"},{"location":"user-guide/theorists/bms/meta-parameters/","title":"Meta Parameters","text":"<p>Meta parameters are used to control the search space and the search algorithm. This section provides a basic overview of these parameters along with a description of their effects. </p> <ul> <li><code>epochs</code>: The number of epochs to run BMS. This corresponds to the total number of equation mutations - one mcmc step for each parallel-tempered equation and one tree swap between a pair of parallel-tempered equations.</li> <li><code>prior_par</code>: A dictionary of priors for each operation. The keys correspond to operations and the respective values correspond to prior probabilities of those operations. The model comes with a default.  </li> <li><code>ts</code>: A list of temperature values. The machine scientist creates an equation tree for each of these values. Higher temperature trees are harder to fit, and thus they help prevent overfitting of the model.</li> </ul>"},{"location":"user-guide/theorists/bms/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>BMS is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[theorist-bms]\n</code></pre> <p>Check your installation by running: <pre><code>python -c from autora.theorist.bms import BMSRegressor; BMSRegressor() \n</code></pre></p>"},{"location":"user-guide/theorists/bms/search-space/","title":"Search Space","text":"<p>BMS searches the space of operations according to certain parameters to find the best model. As such, the search space is defined by the set of operations that can be applied in each computation step of the model. These operations are also referred to as primitives. We can select from the following set of primitives:</p> <ul> <li>\\(\\textit{constant}\\): The output of the computation \\(x_j\\) is a constant parameter value \\(a\\) where \\(a\\) is a fitted float value.</li> <li>+: The output of the computation \\(x_j\\) is the sum over its two inputs \\(x_i, x_{ii}\\): \\(x_j = x_i + x_{ii}\\).</li> <li>-: The output of the computation \\(x_j\\) is the respective difference between its inputs \\(x_i, x_{ii}\\): \\(x_j = x_i - x_{ii}\\).</li> <li>*: The output of the computation \\(x_j\\) is the product over its two inputs \\(x_i, x_{ii}\\): \\(x_j = x_i * x_{ii}\\).</li> <li>/: The output of the computation \\(x_j\\) is the respective quotient between its inputs \\(x_i, x_{ii}\\): \\(x_j = x_i / x_{ii}\\).</li> <li>abs: The output of the computation \\(x_j\\) is the absolute value of its input \\(x_i\\): \\(x_j = |(x_i)|\\).</li> <li>relu: The output of the computation \\(x_j\\) is a rectified linear function applied to its input \\(x_i\\): \\(x_j = \\max(0, x_i)\\).</li> <li>exp: The output of the computation \\(x_j\\) is the natural exponential function applied to its input \\(x_i\\): \\(x_j = \\exp(x_i)\\).</li> <li>log: The output of the computation \\(x_j\\) is the natural logarithm function applied to its input \\(x_i\\): \\(x_j = \\log(x_i)\\).</li> <li>sig: The output of the computation \\(x_j\\) is a logistic function applied to its input \\(x_i\\): \\(x_j = \\frac{1}{1 + \\exp(-b * x_i)}\\).</li> <li>fac: The output of the computation \\(x_j\\) is the generalized factorial function applied to its input \\(x_i\\): \\(x_j = \\Gamma(1 + x_i)\\).</li> <li>sqrt: The output of the computation \\(x_j\\) is the square root function applied to its input \\(x_i\\): \\(x_j = \\sqrt(x_i)\\).</li> <li>pow2: The output of the computation \\(x_j\\) is the square function applied to its input \\(x_i\\): \\(x_j\\) = \\(x_i^2\\).</li> <li>pow3: The output of the computation \\(x_j\\) is the cube function applied to its input \\(x_i\\): \\(x_j\\) = \\(x_i^3\\).</li> <li>sin: The output of the computation \\(x_j\\) is the sine function applied to its input \\(x_i\\): \\(x_j = \\sin(x_i)\\).</li> <li>sinh: The output of the computation \\(x_j\\) is the hyperbolic sine function applied to its input \\(x_i\\): \\(x_j = \\sinh(x_i)\\).</li> <li>cos: The output of the computation \\(x_j\\) is the cosine function applied to its input \\(x_i\\): \\(x_j = \\cos(x_i)\\).</li> <li>cosh: The output of the computation \\(x_j\\) is the hyperbolic cosine function applied to its input \\(x_i\\): \\(x_j = \\cosh(x_i)\\).</li> <li>tan: The output of the computation \\(x_j\\) is the tangent function applied to its input \\(x_i\\): \\(x_j = \\tan(x_i)\\).</li> <li>tanh: The output of the computation \\(x_j\\) is the hyperbolic tangent function applied to its input \\(x_i\\): \\(x_j = \\tanh(x_i)\\).</li> <li>**: The output of the computation \\(x_j\\) is the first input raised to the power of the second input \\(x_i,x_{ii}\\): \\(x_j\\) = \\(x_i^{x_{ii}}\\).</li> </ul>"},{"location":"user-guide/theorists/bsr/","title":"Bayesian Symbolic Regression","text":""},{"location":"user-guide/theorists/bsr/#introduction","title":"Introduction","text":"<p>Symbolic regression (SR) refers to a class of algorithms that search for interpretable symbolic expressions which capture relationships within data. More specifically, SR attempts to find compositions of simple functions that  accurately map independent variables to dependent variables within a given dataset. Bayesian Symbolic Regression (BSR), proposed by Jin et. al (2019), is a specific SR method that uses a Bayesian  framework to search for concise and interpretable expressions.</p> <p>AutoRA provides an adapted version of BSR for automating the discovery of interpretable models of human information  processing.</p>"},{"location":"user-guide/theorists/bsr/#references","title":"References","text":"<p>Jin et al., Bayesian Symbolic Regression. (2020).</p>"},{"location":"user-guide/theorists/bsr/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[theorist-bsr]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[theorist-bsr]\" <p>After importing the necessary modules,</p> In\u00a0[\u00a0]: Copied! <pre># imports\nfrom autora.theorist.bsr import BSRRegressor\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> # imports from autora.theorist.bsr import BSRRegressor import numpy as np import matplotlib.pyplot as plt <p>we begin by generating data with a ground-truth equation, $y = \\sin(x) + x^3$.</p> In\u00a0[\u00a0]: Copied! <pre>x = np.expand_dims(np.linspace(start=-1, stop=1, num=500), 1)\ny = np.power(x, 3) + np.sin(x)\n</pre> x = np.expand_dims(np.linspace(start=-1, stop=1, num=500), 1) y = np.power(x, 3) + np.sin(x) <p>Then we set up the BSR regressor with our chosen meta-parameters. In this case, we will keep the defaults but choose a small number of iterations, <code>itr_num</code>, for ease and efficiency of illustration.</p> In\u00a0[\u00a0]: Copied! <pre># initialize regressor\nbsr = BSRRegressor(itr_num = 500)\n</pre> # initialize regressor bsr = BSRRegressor(itr_num = 500) <p>With our regressor initialized, we can call the <code>fit</code> method to discover an equation for our data and then use the <code>predict</code> method to generate predictions using our discovered equation.</p> In\u00a0[\u00a0]: Copied! <pre>bsr.fit(x, y)\ny_pred = bsr.predict(x)\n</pre> bsr.fit(x, y) y_pred = bsr.predict(x) <p>Finally, we can plot the results.</p> In\u00a0[\u00a0]: Copied! <pre># plot out the ground truth versus predicted responses\nplt.figure()\nplt.plot(x, y, \"o\")\nplt.plot(x, y_pred, \"-\")\nplt.show()\n</pre> # plot out the ground truth versus predicted responses plt.figure() plt.plot(x, y, \"o\") plt.plot(x, y_pred, \"-\") plt.show() <p>In this simple case, the algorithm provides results that are quite good, even with a small number of iterations.</p>"},{"location":"user-guide/theorists/bsr/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":"<p>The following simple example shows out-of-the-box functionality of BSR.</p>"},{"location":"user-guide/theorists/bsr/Basic%20Usage/#references","title":"References\u00b6","text":"<p>Jin, Ying, et al. \"Bayesian symbolic regression.\" arXiv preprint arXiv:1910.08892 (2019).</p>"},{"location":"user-guide/theorists/bsr/how-it-works/","title":"How It Works","text":"<p>Bayesian Symbolic Regression (BSR) has the following features:</p> <ol> <li> <p>It models equations as expression trees, with root and intermediate tree nodes representing operators (e.g. <code>*</code> for a binary node and <code>sin</code> for a unary node) and leaf nodes representing features in the data. BSR then defines the search space as the union of the following three parts:</p> <ul> <li>Tree structure (T): this represents the structure of the expression tree (e.g. how to recursively construct the tree and when to stop by using leaf nodes), and also specifies the assignment of operators to non-leaf nodes.</li> <li>Leaf nodes (M): this assigns features to leaf nodes that are already defined from part T.</li> <li>Operator parameters (\\(\\Theta\\)): this uses a vector \\(\\Theta\\) to collect additional parameters for certain operators which require them (e.g. a linear operator <code>ln</code> with intercept and slope params).</li> </ul> </li> <li> <p>It specifies priors for each of the three parts above. <code>AutoRA</code>'s implementation of BSR allows users to either specify custom priors for part <code>T</code> or choose among a pre-specified set.</p> </li> <li> <p>It defines <code>actions</code> that mutate one expression tree (<code>original</code>) into a new expression tree (<code>proposed</code>), and supports the calculation of transition probabilities based on the likelihoods of the <code>original</code> and <code>proposed</code> models.</p> </li> <li> <p>It designs and implements a Reversible-Jump Markov-Chain Monte-Carlo algorithm (RJ-MCMC), which iteratively accepts new samples (where each sample is a valid expression tree) based on the transition probabilities calculated above. In each iteration, <code>K</code> expression trees are obtained either from the <code>original</code> samples or the new <code>proposed</code> samples.</p> </li> <li> <p>With each iteration, the candidate prediction model is a linear mixture of the <code>K</code> trees, wherein the ground truth response is regressed on the results generated by the <code>K</code> expression trees to obtain the linear regression parameters \\(\\beta\\).</p> </li> </ol> <p><code>AutoRA</code>'s implementation of BSR is adapted from original authors' codebase, and includes comprehensive refactoring of data structures and MCMC computations. It also provides new priors that suit the cognitive and behavioral sciences.</p>"},{"location":"user-guide/theorists/bsr/meta-parameters/","title":"Meta Parameters","text":"<p>Meta parameters are used to control the search space and the model configuration. In BSR, they are mainly defined in the theorist constructor (see <code>regressor.py</code>). Below is a basic overview of these parameters. Note, there are additional algorithm-irrelevant configurations that can be customized in the constructor; please refer to code documentation for their details.</p> <ul> <li><code>tree_num</code>: the number of expression trees to use in the linear mixture (final prediction model); also denoted by <code>K</code> in BSR.</li> <li><code>iter_num</code>: the number of RJ-MCMC steps to execute (note: this can also be understood as the number of <code>K</code>-samples to take in the fitting process).</li> <li><code>val</code>: the number of validation steps to execute following each iteration.</li> <li><code>beta</code>: the hyperparameter that controls growth of a new expression tree. This needs to be &lt; 0, and in general, smaller values of <code>beta</code> correspond to deeper expression trees.</li> </ul>"},{"location":"user-guide/theorists/bsr/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> </ul> <p>BSR is a part of the <code>autora</code> package:</p> <pre><code>pip install -U autora[theorist-bsr]\n</code></pre> <p>Check your installation by running: <pre><code>python -c from autora.theorist.bsr import BSRRegressor; BSRRegressor() \n</code></pre></p>"},{"location":"user-guide/theorists/bsr/search-space/","title":"Search Space","text":"<p>The following are built-in operators which constitute the search space:</p> <ul> <li>+: The output of the computation \\(x_j\\) is the sum over its inputs \\(x_i, x_{ii}\\): \\(x_j = x_i + x_{ii}\\).</li> <li>-: The output of the computation \\(x_j\\) is the respective difference between its inputs \\(x_i, x_{ii}\\): \\(x_j = x_i - x_{ii}\\).</li> <li>*: The output of the computation \\(x_j\\) is the product over its two inputs \\(x_i, x_{ii}\\): \\(x_j = x_i * x_{ii}\\).</li> <li>exp: The output of the computation \\(x_j\\) is the natural exponential function applied to its input \\(x_i\\): \\(x_j = \\exp(x_i)\\).</li> <li>pow2: The output of the computation \\(x_j\\) is the square function applied to its input \\(x_i\\): \\(x_j\\) = \\(x_i^2\\).</li> <li>pow3: The output of the computation \\(x_j\\) is the cube function applied to its input \\(x_i\\): \\(x_j\\) = \\(x_i^3\\).</li> <li>sin: The output of the computation \\(x_j\\) is the sine function applied to its input \\(x_i\\): \\(x_j = \\sin(x_i)\\).</li> <li>cos: The output of the computation \\(x_j\\) is the cosine function applied to its input \\(x_i\\): \\(x_j = \\cos(x_i)\\).</li> <li>ln: The output of the computation \\(x_j\\) is the linear transformation applied to its input \\(x_i\\): \\(x_j = a * x_i + b\\), where \\(a\\) and \\(b\\) are slope and intercept parameters.</li> </ul> <p>In BSR, a new operator can be added in two steps. First, define an operator as a function, as demonstrated in <code>operations.py</code>. Second, add the name of the operator and its prior information to the dictionaries in <code>__get_prior()</code> within <code>prior.py</code>.</p>"},{"location":"user-guide/theorists/darts/","title":"Differentiable Architecture Search","text":""},{"location":"user-guide/theorists/darts/#introduction","title":"Introduction","text":"<p>Neural Architecture Search refers to a family of methods for automating the discovery of useful neural network architectures. There are a number of methods to guide this search, such as evolutionary algorithms, reinforcement learning, or Bayesian optimization (for a recent survey of NAS search strategies, see Elsken, Metzen, &amp; Hutter, 2019). However, most of these methods are computationally demanding due to the nature of the optimization problem: The search space of candidate computation graphs is high-dimensional and discrete. To address this problem, Liu et al. (2018) proposed differentiable architecture search (DARTS), which relaxes the search space to become continuous, making architecture search amenable to gradient descent. </p> <p>DARTS has been shown to yield useful network architectures for image classification and language modeling that are on par with architectures designed by human researchers. AutoRA provides an adaptation of DARTS for automate the discovery of interpretable quantitative models to explain human information processing (Musslick, 2021).</p>"},{"location":"user-guide/theorists/darts/#references","title":"References","text":"<p>Liu, H., Simonyan, K., &amp; Yang, Y. (2018). Darts: Differentiable architecture search. In International Conference on Learning Representations. arXiv: https://arxiv.org/abs/1806.09055</p> <p>Elsken, T., Metzen, J. H., Hutter, F., et al. (2019). Neural architecture search: A survey. JMLR, 20(55), 1\u201321</p> <p>Musslick, S. (2021). Recovering quantitative models of human information processing with differentiable architecture search. In Proceedings of the 43rd Annual Conference of the Cognitive Science Society (pp. 348\u2013354). Vienna, AT. arXiv: https://arxiv.org/abs/2103.13939</p>"},{"location":"user-guide/theorists/darts/Basic%20Usage/","title":"Basic Usage","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment the following line when running on Google Colab\n# !pip install \"autora[theorist-darts]\"\n</pre> # Uncomment the following line when running on Google Colab # !pip install \"autora[theorist-darts]\" <p>Let's generate a simple data set with two features $x_1, x_2 \\in [0, 1]$ and a target $y$. We will use the following generative model: $y = 2 x_1 - e^{(5 x_2)}$</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nx_1 = np.linspace(0, 1, num=10)\nx_2 = np.linspace(0, 1, num=10)\nX = np.array(np.meshgrid(x_1, x_2)).T.reshape(-1,2)\n\ny = 2 * X[:,0] + np.exp(5 * X[:,1])\n</pre> import numpy as np  x_1 = np.linspace(0, 1, num=10) x_2 = np.linspace(0, 1, num=10) X = np.array(np.meshgrid(x_1, x_2)).T.reshape(-1,2)  y = 2 * X[:,0] + np.exp(5 * X[:,1]) <p>Now let us define the search space, that is, the space of operations to consider when searching over the space of computation graphs.</p> In\u00a0[\u00a0]: Copied! <pre>primitives = [\n    \"none\",\n    \"add\",\n    \"subtract\",\n    'mult',\n    \"logistic\",\n    'exp',\n    'relu',\n]\n</pre> primitives = [     \"none\",     \"add\",     \"subtract\",     'mult',     \"logistic\",     'exp',     'relu', ] In\u00a0[\u00a0]: Copied! <pre>from autora.theorist.darts import DARTSRegressor\n\ndarts_estimator = DARTSRegressor(\n    num_graph_nodes=1,\n    arch_updates_per_epoch=1,\n    arch_learning_rate_max=0.001,\n    param_updates_per_epoch=500,\n    param_momentum=0.9,\n    max_epochs=300,\n    output_type=\"real\",\n    primitives=primitives,\n)\n</pre> from autora.theorist.darts import DARTSRegressor  darts_estimator = DARTSRegressor(     num_graph_nodes=1,     arch_updates_per_epoch=1,     arch_learning_rate_max=0.001,     param_updates_per_epoch=500,     param_momentum=0.9,     max_epochs=300,     output_type=\"real\",     primitives=primitives, ) <p>Now we have everything to run differentiable architecture search and visualize the model resulting from the highest architecture weights. Note that the current model corresponds to the model with the highest architecture weights.</p> In\u00a0[\u00a0]: Copied! <pre>darts_estimator.fit(X, y)\ndarts_estimator.visualize_model()\n</pre> darts_estimator.fit(X, y) darts_estimator.visualize_model() <p>We can refine the fit by running the <code>fit</code> method again, after changing the parameters. This allows us to keep the same architecture but refit the parameters in the final sampled model, for example:</p> In\u00a0[\u00a0]: Copied! <pre>darts_estimator.set_params(\n    max_epochs=0,  # no epochs of architecture fitting\n    param_updates_for_sampled_model=1000,  # 1000 steps of param optimiziation\n)\ndarts_estimator.fit(X, y)\ndarts_estimator.visualize_model()\n</pre> darts_estimator.set_params(     max_epochs=0,  # no epochs of architecture fitting     param_updates_for_sampled_model=1000,  # 1000 steps of param optimiziation ) darts_estimator.fit(X, y) darts_estimator.visualize_model() <p>We can also change how the model is sampled from the architecture weight distribution:</p> In\u00a0[\u00a0]: Copied! <pre>darts_estimator.set_params(\n    max_epochs=0,  # no epochs of architecture fitting\n    sampling_strategy=\"sample\",  # overriding default \"max\"\n    param_updates_for_sampled_model=800,\n)\ndarts_estimator.fit(X, y)\ndarts_estimator.visualize_model()\n</pre> darts_estimator.set_params(     max_epochs=0,  # no epochs of architecture fitting     sampling_strategy=\"sample\",  # overriding default \"max\"     param_updates_for_sampled_model=800, ) darts_estimator.fit(X, y) darts_estimator.visualize_model() <p>To recover the initial model, we need to return the sampling strategy to the default <code>\"max\"</code>:</p> In\u00a0[\u00a0]: Copied! <pre>darts_estimator.set_params(\n    max_epochs=0,  # no epochs of architecture fitting\n    sampling_strategy=\"max\",\n    param_updates_for_sampled_model=1000,\n)\ndarts_estimator.fit(X, y)\ndarts_estimator.visualize_model()\n</pre> darts_estimator.set_params(     max_epochs=0,  # no epochs of architecture fitting     sampling_strategy=\"max\",     param_updates_for_sampled_model=1000, ) darts_estimator.fit(X, y) darts_estimator.visualize_model() <p>As long as the architecture has not been refitted in the meantime, the architecture should be identical to the initial result, as the <code>sampling_strategy=\"max\"</code> is deterministic. The coefficients of the architecture functions may, however, be different, as they have different starting values compared to when they were initially set.</p>"},{"location":"user-guide/theorists/darts/Basic%20Usage/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"user-guide/theorists/darts/Basic%20Usage/#set-up-the-darts-regressor","title":"Set Up The DARTS Regressor\u00b6","text":"<p>We will use the DARTS Regressor to predict the outcomes. There are a number of parameters that determine how the architecture search is performed. The most important ones are listed below:</p> <ul> <li>num_graph_nodes: The number of latent variables used to represent the model.</li> <li>arch_updates_per_epoch: The number of architecture updates per training epoch. These updates affect the architecture weights $\\alpha$ indicating the relative contribution of each operation for a given computation step.</li> <li>arch_learning_rate_max: The initial learning rate of the architecture weight optimizer.</li> <li>param_updates_per_epoch: The number of parameter updates per epoch. Once the architecture updates are complete, the parameters associated with each operation are updated.</li> <li>param_momentum: The momentum of the parameter optimizer.</li> <li>max_epochs: The maximum number of epochs to run DARTS.</li> <li>output_type: The type of output to produce. In our case, we treat the outcome as a real variable, i.e., \"real\".</li> </ul> <p>Let's set up the DARTS regressor with some default parameters.</p>"},{"location":"user-guide/theorists/darts/Weber%20Fechner%20Example/","title":"Weber-Fechner Law","text":"<p>Example file which shows some simple curve fitting using DARTSRegressor and some other estimators.</p> In\u00a0[\u00a0]: Copied! <pre># Uncomment the following lines when running on Google Colab\n# !pip install \"autora[theorist-darts]\"\n</pre> # Uncomment the following lines when running on Google Colab # !pip install \"autora[theorist-darts]\" In\u00a0[\u00a0]: Copied! <pre>from functools import partial\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom autora.theorist.darts import DARTSRegressor\nfrom autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law\n</pre> from functools import partial  import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.model_selection import GridSearchCV from sklearn.pipeline import make_pipeline from sklearn.preprocessing import PolynomialFeatures  from autora.theorist.darts import DARTSRegressor from autora.experiment_runner.synthetic.psychophysics.weber_fechner_law import weber_fechner_law In\u00a0[\u00a0]: Copied! <pre># %% Define some helper functions\n\ndef show_results_complete(\n    data_: pd.DataFrame,\n    estimator=None,\n    show_results=True,\n    projection=\"2d\",\n    label=None,\n):\n    \"\"\"\n    Function to plot input data (x_, y_) and the predictions of an estimator for the same x_.\n    \"\"\"\n    if projection == \"2d\":\n        plt.figure()\n        data_.plot.scatter(\n            \"S1\", \"S2\", c=\"difference_detected\", cmap=\"viridis\", zorder=10\n        )\n    elif projection == \"3d\":\n        fig = plt.figure()\n        ax = fig.add_subplot(projection=\"3d\")\n        ax.scatter(data_[\"S1\"], data_[\"S2\"], data_[\"difference_detected\"])\n\n        if estimator is not None:\n            xs, ys = np.mgrid[0:5:0.2, 0:5:0.2]  # type: ignore\n\n            zs = estimator.predict(np.column_stack((xs.ravel(), ys.ravel())))\n\n            ax.plot_surface(xs, ys, zs.reshape(xs.shape), alpha=0.5)\n\n    if label is not None:\n        plt.title(label)\n\n    if show_results:\n        plt.show()\n\n    return\n</pre> # %% Define some helper functions  def show_results_complete(     data_: pd.DataFrame,     estimator=None,     show_results=True,     projection=\"2d\",     label=None, ):     \"\"\"     Function to plot input data (x_, y_) and the predictions of an estimator for the same x_.     \"\"\"     if projection == \"2d\":         plt.figure()         data_.plot.scatter(             \"S1\", \"S2\", c=\"difference_detected\", cmap=\"viridis\", zorder=10         )     elif projection == \"3d\":         fig = plt.figure()         ax = fig.add_subplot(projection=\"3d\")         ax.scatter(data_[\"S1\"], data_[\"S2\"], data_[\"difference_detected\"])          if estimator is not None:             xs, ys = np.mgrid[0:5:0.2, 0:5:0.2]  # type: ignore              zs = estimator.predict(np.column_stack((xs.ravel(), ys.ravel())))              ax.plot_surface(xs, ys, zs.reshape(xs.shape), alpha=0.5)      if label is not None:         plt.title(label)      if show_results:         plt.show()      return  In\u00a0[\u00a0]: Copied! <pre># %% Load the data\ns = weber_fechner_law(resolution=20)\n# Get independent and dependent variables\nivs = [iv.name for iv in s.variables.independent_variables]\ndvs = [dv.name for dv in s.variables.dependent_variables]\nX = s.domain()\nexperiment_data = s.run(X, random_state=42)\ny = experiment_data[dvs]\n</pre> # %% Load the data s = weber_fechner_law(resolution=20) # Get independent and dependent variables ivs = [iv.name for iv in s.variables.independent_variables] dvs = [dv.name for dv in s.variables.dependent_variables] X = s.domain() experiment_data = s.run(X, random_state=42) y = experiment_data[dvs] In\u00a0[\u00a0]: Copied! <pre>show_results = partial(show_results_complete, data_=experiment_data, projection=\"3d\")\nshow_results(label=\"input data\")\n</pre> show_results = partial(show_results_complete, data_=experiment_data, projection=\"3d\") show_results(label=\"input data\") In\u00a0[\u00a0]: Copied! <pre># %% Fit first using a super-simple linear regression\n\nfirst_order_linear_estimator = LinearRegression()\nfirst_order_linear_estimator.fit(X, y)\n\nshow_results(estimator=first_order_linear_estimator, label=\"1st order linear\")\n</pre> # %% Fit first using a super-simple linear regression  first_order_linear_estimator = LinearRegression() first_order_linear_estimator.fit(X, y)  show_results(estimator=first_order_linear_estimator, label=\"1st order linear\") In\u00a0[\u00a0]: Copied! <pre># %% Fit using a 0-3 order polynomial, getting the best fit for the data.\npolynomial_estimator = GridSearchCV(\n    make_pipeline(PolynomialFeatures(), LinearRegression(fit_intercept=False)),\n    param_grid=dict(polynomialfeatures__degree=range(4)),\n)\npolynomial_estimator.fit(X, y)\n\nshow_results(estimator=polynomial_estimator, label=\"[0th-3rd]-order linear\")\n</pre> # %% Fit using a 0-3 order polynomial, getting the best fit for the data. polynomial_estimator = GridSearchCV(     make_pipeline(PolynomialFeatures(), LinearRegression(fit_intercept=False)),     param_grid=dict(polynomialfeatures__degree=range(4)), ) polynomial_estimator.fit(X, y)  show_results(estimator=polynomial_estimator, label=\"[0th-3rd]-order linear\") In\u00a0[\u00a0]: Copied! <pre>darts_estimator_tuned = DARTSRegressor(\n    batch_size=64,\n    arch_updates_per_epoch=100,\n    param_updates_per_epoch=100,\n    max_epochs=1500,\n    num_graph_nodes=5,\n    primitives=[\n        \"none\",\n        \"linear\",\n        \"logistic\",\n        ]\n)\n\ndarts_estimator_tuned.fit(X, y)\n\nshow_results(estimator=darts_estimator_tuned, label=\"DARTSRegressor\")\ndarts_estimator_tuned.visualize_model()\n</pre> darts_estimator_tuned = DARTSRegressor(     batch_size=64,     arch_updates_per_epoch=100,     param_updates_per_epoch=100,     max_epochs=1500,     num_graph_nodes=5,     primitives=[         \"none\",         \"linear\",         \"logistic\",         ] )  darts_estimator_tuned.fit(X, y)  show_results(estimator=darts_estimator_tuned, label=\"DARTSRegressor\") darts_estimator_tuned.visualize_model() In\u00a0[\u00a0]: Copied! <pre>darts_estimator_tuned.set_params(\n    arch_updates_per_epoch=0,\n    param_updates_per_epoch=1000,\n    sampling_strategy=\"sample\",\n    max_epochs=1\n)\ndarts_estimator_tuned.fit(X, y)\nshow_results(estimator=darts_estimator_tuned, label=\"resampled DARTSRegressor\")\ndarts_estimator_tuned.visualize_model()\n</pre> darts_estimator_tuned.set_params(     arch_updates_per_epoch=0,     param_updates_per_epoch=1000,     sampling_strategy=\"sample\",     max_epochs=1 ) darts_estimator_tuned.fit(X, y) show_results(estimator=darts_estimator_tuned, label=\"resampled DARTSRegressor\") darts_estimator_tuned.visualize_model()  In\u00a0[\u00a0]: Copied! <pre>darts_estimator_tuned.set_params(\n    arch_updates_per_epoch=0,\n    param_updates_per_epoch=1000,\n    sampling_strategy=\"max\",\n    max_epochs=0\n)\ndarts_estimator_tuned.fit(X, y)\nshow_results(estimator=darts_estimator_tuned, label=\"resampled DARTSRegressor\")\ndarts_estimator_tuned.visualize_model()\n</pre> darts_estimator_tuned.set_params(     arch_updates_per_epoch=0,     param_updates_per_epoch=1000,     sampling_strategy=\"max\",     max_epochs=0 ) darts_estimator_tuned.fit(X, y) show_results(estimator=darts_estimator_tuned, label=\"resampled DARTSRegressor\") darts_estimator_tuned.visualize_model()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"user-guide/theorists/darts/Weber%20Fechner%20Example/#weber-fechner-example","title":"Weber Fechner Example\u00b6","text":""},{"location":"user-guide/theorists/darts/how-it-works/","title":"How It Works","text":"<p>Regular DARTS treats the architecture of a neural network as a directed acyclic computation graph (DAG), containing \\(N\\) nodes in sequential order.</p> <p></p> <p>Each node \\(x_i\\) corresponds to a latent representation of the input space. Each directed edge \\(e_{i, j}\\) is associated with some operation  \\(o_{i,j}\\) that transforms the representation of the preceding node \\(i\\), and feeds it to node \\(j\\). Each intermediate node is computed by integrating over its transformed predecessors:</p> \\[ x_j = \\sum_{i&lt;j} o_{i,j} \\left( x_{i} \\right). \\] <p>Every output node is computed by linearly combining all intermediate nodes projecting to it. The goal of DARTS is to identify all operations \\(o_{i,j}\\) of the DAG. Following Liu et al. (2019), we define {\\(\\mathscr{O} = \\{o^1_{i,j}, o^2_{i,j}, \\dots, o^M_{i,j}\\}\\)} to be the set of \\(M\\) candidate operations associated with edge \\(e_{i, j}\\) where every operation \\(o^m_{i,j}(x_i)\\) corresponds to some function applied to \\(x_{i}\\) (e.g. linear,  exponential or logistic). DARTS relaxes the problem of searching over candidate operations by formulating the transformation associated with an edge as a mixture of all possible operations in \\(\\mathscr{O}\\) (cf. Figure A-B):</p> \\[ \\bar{o}_{i,j}(x) = \\sum_{o \\in \\mathscr{O}} \\frac{\\textrm{exp}({\\alpha^o_{i,j}})}{\\sum_{o' \\in \\mathscr{O}} \\textrm{exp}({\\alpha^{o'}_{i,j}})} \\cdot o_{i,j}(x). \\] <p>where each operation is weighted by the softmax transformation of its architectural weight \\(\\alpha^o_{i,j}\\). Every edge \\(e_{i, j}\\) is assigned a weight vector \\(\\alpha_{i,j}\\) of dimension \\(M\\), containing the weights of all possible candidate operations for that edge. The set of all architecture weight vectors \\(\\alpha = \\{\\alpha_{i,j}\\}\\) determines the architecture of the model. Thus, searching the architecture amounts to identifying \\(\\alpha\\). The key contribution of DARTS is that searching \\(\\alpha\\) becomes amenable to gradient descent after relaxing the search space to become continuous. However, minimizing the loss function of the model \\(\\mathscr{L}(w,\\alpha)\\) requires finding both \\(\\alpha^*\\) and \\(w^*\\)---the parameters of the computation graph.\\footnote{This includes the parameters of each candidate operation \\(o^m_{i,j}\\).} Liu et al. (2019) propose to learn \\(\\alpha\\) and \\(w\\) simultaneously using bi-level optimization:</p> \\[ \\min_\\alpha \\mathscr{L}_{\\textrm{val}}\\left(w^*(\\alpha),\\alpha\\right) \\\\ \\textrm{s.t. } w^*(\\alpha) = \\underset{w}{\\operatorname{argmin}}   \\mathscr{L}_{\\textrm{train}}(w, \\alpha). \\] <p>That is, one can obtain \\(\\alpha^*\\) through gradient descent, by iterating through the following steps:</p> <ul> <li>Obtain the optimal set of weights \\(w^*\\) for the current architecture \\(\\alpha\\) by minimizing the training loss \\(\\mathscr{L}_{\\textrm{train}}(w, \\alpha)\\).</li> <li>Update the architecture \\(\\alpha\\) (cf. Figure C) by following the gradient of the validation loss \\(\\nabla  \\mathscr{L}_{\\textrm{val}}\\left(w^*,\\alpha\\right)\\).</li> </ul> <p>Once \\(\\alpha^*\\) is found, one can obtain the final architecture by replacing \\(\\bar{o}_{i,j}\\) with the operation that has the highest architectural weight, i.e. \\(o_{i,j}\\leftarrow \\textrm{argmax}_o \\alpha^{*o}_{i,j}\\) (Figure D).</p>"},{"location":"user-guide/theorists/darts/how-it-works/#references","title":"References","text":"<p>Liu, H., Simonyan, K., &amp; Yang, Y. (2018). Darts: Differentiable architecture search. In International Conference on Learning Representations. arXiv: https://arxiv.org/abs/1806.09055</p>"},{"location":"user-guide/theorists/darts/meta-parameters/","title":"Meta Parameters","text":"<p>Meta parameters are used to control the search space and the search algorithm. DARTS has quite a lot of those parameters. This section provides a basic overview of all parameters along with a description of their effects. </p>"},{"location":"user-guide/theorists/darts/meta-parameters/#general-darts-meta-parameters","title":"General DARTS Meta Parameters","text":"<ul> <li><code>num_graph_nodes</code>: The number of latent variables used to represent the model.</li> <li><code>max_epochs</code>: The maximum number of epochs to run DARTS. This corresponds to the total number of architecture updates. These updates affect the architecture weights \\(\\alpha\\) indicating the relative contribution of each operation for a given computation step.</li> </ul>"},{"location":"user-guide/theorists/darts/meta-parameters/#meta-parameters-for-the-architecture-updates","title":"Meta Parameters For The Architecture Updates","text":"<p>The following parameters affect the updating of the architecture weights \\(\\alpha\\):</p> <ul> <li><code>arch_learning_rate_max</code>: The initial (maximum) learning rate for updating the architecture updates. The higher the learning rate, the larger the steps taken to update the architecture weights. The learning rate decays with each epoch.</li> <li><code>arch_weight_decay</code>: The weight decay for the architecture weights. The higher the weight decay, the more the high architecture weights are pressured to be small.</li> <li><code>arch_weight_decay_df</code>: An additional weight decay that scales with the number of parameters (degrees of freedom) per operation. The higher this weight decay, the more DARTS will favor operations with few parameters.</li> </ul>"},{"location":"user-guide/theorists/darts/meta-parameters/#meta-parameters-for-the-parameter-updates","title":"Meta Parameters For The Parameter Updates","text":"<p>The following parameters affect the updating of the parameters associated with each operation:</p> <ul> <li><code>param_updates_per_epoch</code>: The number of steps taken by the parameter optimizer per epoch. Once the architecture updates are complete, the parameters associated with each operation are updated by a stochastic gradient descent over this number of steps.</li> <li><code>param_learning_rate_max</code>: The initial (maximum) learning rate for updating the parameters. The higher the learning rate, the larger the steps taken to update the parameters. Note that the learning rate is scheduled to converge over the total number of parameter updates to <code>learning_rate_min</code>.</li> <li><code>param_learning_rate_min</code>: The smallest possible learning rate for updating the parameters.</li> <li><code>param_momentum</code>: The momentum for the architecture updates. The higher the momentum, the more the steps taken to update the architecture weights will be influenced by previous steps.</li> <li><code>param_weight_decay</code>: The weight decay for the parameters. The higher the weight decay, the more the high parameters of each operation are pressured to be small.</li> </ul>"},{"location":"user-guide/theorists/darts/meta-parameters/#meta-parameters-for-the-classifier","title":"Meta Parameters For The Classifier","text":"<p>The final output of the DARTS model is computed by concatenating all edges in the computation graph into a single vector and then adding a linear classifier. The linear classifier can attach a coefficient to each edge (weighing the contribution of that edge to the final output), and it can add a constant bias term. The following parameters affect the behavior of the classifier:</p> <ul> <li><code>train_classifier_coefficients</code>: If set to <code>True</code>, the classifier coefficient of each edge will be trained (otherwise each coefficient is set to <code>1</code>, reflecting an equal contribution of each edge to the final output).</li> <li><code>train_classifier_bias</code>: If set to <code>True</code>, the bias term of the classifier will be trained (otherwise the bias term is set to <code>0</code>).</li> </ul>"},{"location":"user-guide/theorists/darts/quickstart/","title":"Quickstart Guide","text":"<p>You will need:</p> <ul> <li><code>python</code> 3.8 or greater: https://www.python.org/downloads/</li> <li><code>graphviz</code> (optional, required for computation graph visualizations):    https://graphviz.org/download/</li> </ul> <p>Install DARTS as part of the <code>autora</code> package:</p> <pre><code>pip install -U \"autora[theorist-darts]\"\n</code></pre> <p>Success</p> <p>It is recommended to use a <code>python</code> environment manager like <code>virtualenv</code>.</p> <p>Check your installation by running: <pre><code>python -c \"from autora.theorist.darts import DARTSRegressor; DARTSRegressor()\"\n</code></pre></p>"},{"location":"user-guide/theorists/darts/search-space/","title":"Search Space","text":"<p>DARTS uses a search space of operations to find the best model. The search space is defined by the set of operations that can be applied in each computation step of the model. These operations are also referred to as primitives. We can select from the following space of primitives:</p> <ul> <li>zero: The output of the computation \\(x_j\\) is not dependent on its input \\(x_i\\).</li> <li>add: The output of the computation \\(x_j\\) amounts to its input \\(x_i\\).</li> <li>subtract: The output of the computation \\(x_j\\) amounts to \\(-x_i\\).</li> <li>mult: The output of the computation \\(x_j\\) is its input \\(x_i\\) multiplied by some constant \\(a\\).</li> <li>linear: The output of the computation \\(x_j\\) is linearly dependent on its input \\(x_i\\): \\(x_j = a * x_i + b\\).</li> <li>relu: The output of the computation \\(x_j\\) is a rectified linear function of its input \\(x_i\\): \\(x_j = \\max(0, x_i)\\).</li> <li>exp: The output of the computation \\(x_j\\) is exponentially dependent on its input \\(x_i\\): \\(x_j = \\exp(x_i)\\).</li> <li>logistic: The output of the computation \\(x_j\\) is a logistic function of its input \\(x_i\\): \\(x_j = \\frac{1}{1 + \\exp(-b * x_i)}\\).</li> <li>sin: The output of the computation \\(x_j\\) is the sine function of its input \\(x_i\\): \\(x_j = \\sin(x_i)\\).</li> <li>cos: The output of the computation \\(x_j\\) is the cosine function of its input \\(x_i\\): \\(x_j = \\cos(x_i)\\).</li> <li>tanh: The output of the computation \\(x_j\\) is the hyperbolic tangent function of its input \\(x_i\\): \\(x_j = \\tanh(x_i)\\).</li> </ul> <p>Some of the primitives above may also be preceded by a linear transformation, allowing for more degrees of freedom in the search space:</p> <ul> <li>linear_relu: The output of the computation \\(x_j\\) is a rectified linear function of its linearly transformed input \\(x_i\\): \\(x_j = \\max(0, (a * x_i + b)\\).</li> <li>linear_exp: The output of the computation \\(x_j\\) is exponentially dependent on its linearly transformed input \\(x_i\\): \\(x_j = \\exp(a * x_i + b)\\).</li> <li>linear_logistic: The output of the computation \\(x_j\\) is a logistic function of its linearly transformed input \\(x_i\\): \\(x_j = \\frac{1}{1 + \\exp(-b * (a * x_i + b))}\\).</li> <li>linear_sin: The output of the computation \\(x_j\\) the sine function of its linearly transformed input \\(x_i\\): \\(x_j = a * \\sin(a * x_i + b)\\).</li> <li>linear_cos: The output of the computation \\(x_j\\) the cosine function of its linearly transformed input \\(x_i\\): \\(x_j = a * \\cos(a * x_i + b)\\).</li> <li>linear_tanh: The output of the computation \\(x_j\\) the hyperbolic tangent function of its linearly transformed input \\(x_i\\): \\(x_j = a * \\tanh(a * x_i + b)\\).</li> </ul> <p>Note that the following functions are available but currently not identifiable by DARTS (please use the following functions with caution):</p> <ul> <li>reciprocal: The output of the computation \\(x_j\\) is the multiplicative inverse of its input \\(x_i\\): \\(x_j = \\frac{1}{x_i}\\).</li> <li>ln: The output of the computation \\(x_j\\) is the natural logarithm of its input \\(x_i\\): \\(x_j = \\ln(x_i)\\).</li> <li>softplus: The output of the computation \\(x_j\\) is a softplus function of its input \\(x_i\\): \\(x_j = \\log(1 + \\exp(a * x_i)) / a\\).</li> <li>softminus: The output of the computation \\(x_j\\) is a softminus function of its input \\(x_i\\): \\(x_j = x_j - \\log(1 + \\exp(a * x_i)) / a\\).</li> </ul>"},{"location":"user-guide/theorists/darts/search-space/#example","title":"Example","text":"<p>The following example sets up a search space with the following primitives:</p> <ul> <li>zero operation</li> <li>addition</li> <li>multiplication</li> <li>sigmoid operation</li> </ul> <pre><code>primitives = [\n    \"zero\",\n    \"add\",\n    \"mult\",\n    \"linear_exp\",\n]\n</code></pre> <p>We can then pass these primitives directly to the DARTS regressor:</p> <pre><code>from autora.skl.darts import DARTSRegressor\n\ndarts_estimator = DARTSRegressor(\n    primitives=primitives\n)\n</code></pre>"}]}
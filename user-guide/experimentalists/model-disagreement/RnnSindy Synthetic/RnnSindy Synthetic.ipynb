{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# [RnnSindy](https://github.com/AutoResearch/autora-theorist-rnn-ddm) Theorist and Synthetic Runner\n",
    "\n",
    "Install the packages"
   ],
   "id": "a1548dc246c139cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !pip install autora-theorist-rnn-sindy-rl\n",
    "# !pip install autora-experimentalist-bandit-random"
   ],
   "id": "277b08da6658c812",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "import packages",
   "id": "fb90c6dd77fb5fc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Python Core\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "# External Vendors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import torch\n",
    "\n",
    "# General AutoRA\n",
    "from autora.variable import VariableCollection, Variable\n",
    "from autora.state import StandardState, on_state, Delta\n",
    "\n",
    "# Experimentalists\n",
    "from autora.experimentalist.bandit_random import bandit_random_pool\n",
    "from autora.experimentalist.model_disagreement import model_disagreement_sampler_custom_distance\n",
    "\n",
    "# Experiment Runner\n",
    "from autora.experiment_runner.synthetic.psychology.q_learning import q_learning\n",
    "\n",
    "# Theorist\n",
    "from autora.theorist.rnn_sindy_rl import RNNSindy"
   ],
   "id": "51e904e115df3070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting constants",
   "id": "27a05646b67aee90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRIALS_PER_PARTICIPANTS = 100\n",
    "SAMPLES_PER_CYCLE = 1\n",
    "PARTICIPANTS_PER_CYCLE = 40\n",
    "CYCLES = 4\n",
    "INITIAL_REWARD_PROBABILITY_RANGE = [.2, .8]\n",
    "SIGMA_RANGE = [.2, .2]\n",
    "\n",
    "EPOCHS = 10 # 100\n",
    "\n",
    "seed = 11"
   ],
   "id": "c4bcfa0bc950bd45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting seeds for reproducible results",
   "id": "6f4af08018b28348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "id": "e878e50aacf4a643",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set up variables\n",
    "\n",
    "independent variable is \"reward-trajectory\": A 2 x n_trials Vector with entries between 0 and 1\n",
    "dependent variable is \"choice-trajectory\": A 2 x n_trials Vector with boolean entries (one hot encoded)"
   ],
   "id": "f4dc01da21cd9715"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "variables = VariableCollection(\n",
    "    independent_variables=[Variable(name=\"reward-trajectory\")],\n",
    "    dependent_variables=[Variable(name=\"choice-trajectory\")]\n",
    ")"
   ],
   "id": "c95fce71f88c8145",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## State\n",
    "\n",
    "We use a non-standard state by extending the standard state with an additional model "
   ],
   "id": "e3d2636ccf6c8e7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class RnnState(StandardState):\n",
    "    models_additional:  List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "\n",
    "# initialize the state:\n",
    "state = RnnState(variables=variables)\n"
   ],
   "id": "c05fefb9ce5aafa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Autora Components\n",
    "### Experimentalists\n",
    "#### Random Pool\n",
    "\n",
    "Create a pooler on state that creates a pool of conditions"
   ],
   "id": "e6bbcc2c6679331f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@on_state()\n",
    "def pool_on_state(num_samples, n_trials=TRIALS_PER_PARTICIPANTS):\n",
    "    \"\"\"\n",
    "    This is creates `num_samples` randomized reward trajectories of length `n_trials`\n",
    "    \"\"\"\n",
    "    sigma = np.random.uniform(SIGMA_RANGE[0], SIGMA_RANGE[1])\n",
    "    trajectory_array = bandit_random_pool(\n",
    "        num_rewards=2,\n",
    "        sequence_length=n_trials,\n",
    "        initial_probabilities=[INITIAL_REWARD_PROBABILITY_RANGE, INITIAL_REWARD_PROBABILITY_RANGE],\n",
    "        sigmas=[sigma, sigma],\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    trajectory_df = pd.DataFrame({'reward-trajectory': trajectory_array})\n",
    "    return Delta(conditions=trajectory_df)"
   ],
   "id": "54d41adb350f78a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state = pool_on_state(state, num_samples=3)\n",
    "state.conditions"
   ],
   "id": "3b5c89b4aa10ebf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Experiment Runner\n",
    "\n",
    "Here, we create a synthetic runner that uses a q-learning algorithm"
   ],
   "id": "5faacf9a6d9915a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "runner = q_learning()\n",
    "\n",
    "@on_state()\n",
    "def runner_on_state(conditions):\n",
    "    choices, choice_probabilities = runner.run(conditions, return_choice_probabilities=True)\n",
    "    experiment_data = pd.DataFrame({\n",
    "        'reward-trajectory': conditions['reward-trajectory'].tolist(),\n",
    "        'choice-trajectory': choices,\n",
    "        'choice-probability-trajectory': choice_probabilities\n",
    "    })\n",
    "    return Delta(experiment_data=experiment_data)"
   ],
   "id": "de5f4ea0bbee0e67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state = runner_on_state(state)\n",
    "state.experiment_data"
   ],
   "id": "2af808ac4e6fb95a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Theorists\n",
    "Here we create two RNNSindy theorists\n"
   ],
   "id": "8ec89f6ab74f7e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "theorist = RNNSindy(2, epochs=EPOCHS, polynomial_degree=2)\n",
    "theorist_additional = RNNSindy(2, epochs=EPOCHS, polynomial_degree=1)\n",
    "\n",
    "@on_state()\n",
    "def theorist_on_state(experiment_data):\n",
    "    x = experiment_data['reward-trajectory']\n",
    "    y = experiment_data['choice-trajectory']\n",
    "    return Delta(models=[theorist.fit(x, y)])\n",
    "\n",
    "\n",
    "@on_state()\n",
    "def theorist_additional_on_state(experiment_data):\n",
    "    x = experiment_data['reward-trajectory']\n",
    "    y = experiment_data['choice-trajectory']\n",
    "    return Delta(models_additional=[theorist_additional.fit(x, y)])"
   ],
   "id": "2cd001307f51577",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state = theorist_additional_on_state(state)\n",
    "state = theorist_on_state(state)\n",
    "\n",
    "print(len(state.models_additional))\n",
    "print(len(state.models))\n"
   ],
   "id": "afc777e5b8639a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "state.models[-1].predict(state.conditions)",
   "id": "3b115ca30370432e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we see the prediction for a model is a list of two-dimensional vectors:\n",
    "array([[0.5, 0.5], [0.68..., 0.31...], ...]). \n",
    "The standard model disagreement sampler only works on predictions that are single numbers. Therefore, we define our own distance functions, that works on two lists with the described format "
   ],
   "id": "a38335dabf6f7cd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def custom_distance(prob_array_a, prob_array_b):\n",
    "    return np.mean([(prob_array_a[0] - prob_array_b[0])**2 + (prob_array_a[1] - prob_array_b[1])**2])\n",
    "\n",
    "# test \n",
    "pred_1 = state.models[-1].predict(state.conditions)[0]  # first prediction of model 1\n",
    "pred_2 = state.models_additional[-1].predict(state.conditions)[0]  # first prediction of model 2\n",
    "\n",
    "custom_distance(pred_1, pred_2)"
   ],
   "id": "5e2ca8f2ef366591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now use the `custom_distance` function in our sampler:",
   "id": "14511a44b57f3934"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@on_state()\n",
    "def model_disagreement_on_state(\n",
    "        conditions, models, models_additional, num_samples):\n",
    "    conditions = model_disagreement_sampler_custom_distance(\n",
    "        conditions=conditions['reward-trajectory'],\n",
    "        models=[models[-1], models_additional[-1]],\n",
    "        distance_fct=custom_distance,\n",
    "        num_samples=num_samples,\n",
    "    )\n",
    "    return Delta(conditions=conditions)"
   ],
   "id": "4a15928655b36eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can run a full loop with a rnn synthetic model",
   "id": "890b6056c1d40059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "state = RnnState(variables=variables)",
   "id": "6ab460285c1010c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for c in range(1, CYCLES + 1):\n",
    "    \n",
    "    if len(state.models) > 0:\n",
    "        state = pool_on_state(state, num_samples=20)\n",
    "        state = model_disagreement_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n",
    "    else:\n",
    "        state = pool_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n",
    "    \n",
    "    state = runner_on_state(state)\n",
    "    \n",
    "    state = theorist_on_state(state)\n",
    "    state = theorist_additional_on_state(state)\n"
   ],
   "id": "79603ed1c4341d43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out = state.models[-1].predict(state.conditions['reward-trajectory'])",
   "id": "5df6e22f4f214281",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
